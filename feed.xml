<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://baiyangzhang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://baiyangzhang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-19T06:55:44+00:00</updated><id>https://baiyangzhang.github.io/feed.xml</id><title type="html">Baiyang Zhang</title><subtitle>A place dedicated to sharing insights and reflections on mathematics, physics, and social sciences. </subtitle><entry><title type="html">Note on the kink mass correction in 3D Part II</title><link href="https://baiyangzhang.github.io/blog/2024/Note-on-Kink-Mass-Correction-in-3D-Part-II/" rel="alternate" type="text/html" title="Note on the kink mass correction in 3D Part II"/><published>2024-03-11T00:00:00+00:00</published><updated>2024-03-11T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Note-on-Kink-Mass-Correction-in-3D-Part-II</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Note-on-Kink-Mass-Correction-in-3D-Part-II/"><![CDATA[<h1 id="domain-wall-in-phi-fourth-model">Domain wall in phi-fourth model</h1> <p>Recall that we are working in $D = d+1$ dimensional space-time, the space-dimension is $d$. For now let’s consider $d=2$. In 2-dimensional space, a kink can extend to form a domain wall, for more info about domain walls see my other <a href="https://www.mathlimbo.net/blog/2024/Domain-Wall/">note</a>. The world line of such a domain wall would be world sheet. Now let’s try to calculate the quantum corrections to the tension of such a domain wall.</p> <p>The energy is given by the space integral of the Hamiltonian density,</p> \[H = \int d^{d}x \, \mathcal{H},\quad \mathcal{H} = \frac{1}{\lambda}\left[ \frac{\tilde{\pi}^{2}}{2}+\frac{(\nabla \tilde{\phi})^{2}}{2} + V(\tilde{\phi}) \right]\] <p>where</p> \[\tilde{\phi}:= \sqrt{ \lambda } \phi,\quad \tilde{\pi} := \sqrt{ \lambda } \pi .\] <p>Note that we have written $\phi$ as $\tilde{\phi}$ so that the coupling becomes an overall pre-factor. Keep in mind that, in this formalism, the energy is no longer simply $H=\int \, V$ but $H = \int \, V / \lambda$.</p> <p>Let the spatial coordinates be $x$ and $y$. Consider the potential (expanded in the negative vacuum)</p> \[V(\tilde{\phi}) = \frac{\tilde{\phi}^{2}}{4} (\tilde{\phi}-\sqrt{ 2 }m)^{2}.\] <p>Now consider a flat domain wall in $x=0$ plane. The classical kink solution in the $x$-direction is</p> \[\widetilde{f}(x,y) = \frac{m}{\sqrt{ 2 }} \left( 1+\tanh \frac{mx}{2} \right)\] <p>where again, $\widetilde{f} = \sqrt{ \lambda }f$ by definition.</p> <p>The energy now reads</p> \[H = \int dy \int dx \, \frac{1}{\lambda} \left( \frac{1}{2} (\partial_ {x} \widetilde{f})^{2}+ V(\tilde{f}) \right) =: \int dy \, \rho_ {0}(y),\] <p>where $\rho_ {0}$ is now interpreted as the tension of the domain wall. The calculation is exactly the same as for the kink energy, but to refresh the memory I decided to do it again.</p> <p>Introduce a new variable $t := \frac{mx}{2}$, we have</p> \[\widetilde{f}(t) = \frac{m}{\sqrt{ 2 }} (1+\tanh t),\quad dx = \frac{2}{m} dt,\quad \partial_ {x} = \frac{m}{2} \partial_ {t}.\] <p>The potential contribution to the energy is</p> \[\begin{align*} \int dx \, \frac{1}{\lambda} V(\tilde{f}) &amp;= \frac{m^{3}}{8\lambda}\int dt \, (\tanh ^{2}(t)-1) ^{2}\\ &amp;= \frac{m^{3}}{8\lambda} \int dt \, \frac{1}{\cosh^{4}t} \\ &amp;= \frac{m^{3}}{6\lambda}, \tag{1} \end{align*}\] <p>where we have used</p> \[\int dt \, \frac{1}{\cosh^{4}t} = \frac{4}{3}.\] <p>Similarly, the contribution from the space-derivative term reads</p> \[\begin{align*} \frac{1}{\lambda} \int dx \, \frac{1}{2} (\partial_ {x}\widetilde{f})^{2} &amp;= \frac{m^{3}}{8\lambda} \int dt \, (\partial_ {t}\tanh t)^{2} \\ &amp;= \frac{m^{3}}{8\lambda} \int dt \, \frac{1}{\cosh^{4}t} \\ &amp;= \frac{m^{3}}{6\lambda}. \end{align*} \tag{2}\] <p>Putting Eq. (1) and Eq. (2) together, we have</p> \[\rho_ {0} = \frac{m^{3}}{3\lambda}.\] <hr/> <p>The dimensional analysis gives us</p> \[[\lambda] = 4-D = 3-d,\] <p>hence in $2+1$ dimension the coupling has dimension of mass.</p> <h1 id="normal-modes">Normal Modes</h1> <p>Since we assumed the domain wall to be flatly lying in the $y$-plane, the normal modes in 2-d space can be <em>factorized</em> into $x$ and $y$ components,</p> \[{\mathfrak g} _ {k_ {x}k_ {y}} (x,y) = {\mathfrak g}_ {k_ {x}}(x) \times e^{ -i y k_ {y}}.\] <p>The normal modes are the solution of the equation of motion in the kink background, or Poschl-Teller potential. These modes include a continuum</p> \[{\mathfrak g} _ {k}(x) = \frac{e^{-ikx}}{\omega_ {k} \sqrt{m^2+4k^2}}\left[2k^2-m^2+(3/2)m^2\text{sech}^2(m x/2)-3im k\tanh(m x/2)\right]\] <p>with eigenvalue $\omega_ {k} = \sqrt{ k^{2}+m^{2} }$, a zero mode</p> \[{\mathfrak g}_ {B} = -\sqrt{ \frac{3m}{8} } \text{sech}^{2}\left( \frac{mx}{2} \right)\] <p>with eigenvalue $0$, and a shape mode</p> \[{\mathfrak g} _ {S} = \frac{\sqrt{ 3m }}{2} \tanh \frac{mx}{2} \text{sech} \frac{mx}{2},\quad \omega_ {S} = \frac{\sqrt{ 3 }}{2}m\] <p>with eigenvalue less then the rest mass of excited particle.</p> <p>Momentum in the $y$-direction also contributes to the total energy, putting them together with the zero modes, shape modes and continuum in the $x$ direction we have</p> \[\boxed { \omega_ {k_ {B}k_ {y}} = \left\lvert k_ {y} \right\rvert ,\quad \omega_ {k_ {S}k_ {y}} = \sqrt{ \frac{3m^{2}}{4}+k_ {y}^{2} },\quad \omega_ {k_ {x} k_ {y}} = \sqrt{ m^{2}+k_ {x}^{2}+k_ {y}^{2} }. }\] <p>Note that</p> <ul> <li>there is a zero mode corresponding to $k_ {B},k_ {y}=0$,</li> <li>the mass gap disappears, due to the mass-gap-less of $y$-momentum.</li> </ul> <p>The formalism we developed in generic dimension $d$ surely also applies to $d=2$. Let $\vec{p}=(p_ {x},p_ {y})$ and $\vec{k}=(k_ {x},k_ {y})$. The Fourier transform is</p> \[\tilde{\mathfrak{g} }_ {k_ {x},k_ {y}} (\vec{p})= \int d^{2}x \, \mathfrak{g} _ {k}(\vec{x}) e^{ -i\vec{k}\cdot \vec{x} } = (2\pi)\delta(k_ {y}+p_ {y}) \times \tilde{\mathfrak{g}} _ k (p_ {x}). \tag{3}\] <p>Again we see the factorization in $x$ and $y$, the $\delta$-function in $y$ direction is due to the plane wave expansion.</p> <p>Note that the infinite volume of a flat $\mathbb{R}$ can be written as Dirac-$\delta$ function $\delta(0)$. To see that, recall the Fourier transform of a function is written as</p> \[\widetilde{f}(\vec{k}) = \int d^{d}x \, e^{ -i\vec{k}\cdot \vec{x} } f(\vec{x})\] <p>which means if we set $f(\vec{x})=1$ then</p> \[\tilde{f}(\vec{k}) = \int d^{d}x \, e^{ -i \vec{k}\cdot\vec{x} } = (2\pi)^{d} \delta^{d}(k),\] <p>If we further set $k=0$ then the integral becomes</p> \[\int d^{d}x \, 1\, = \text{Vol}^{d} = (2\pi)^{d}\delta^{d}(0).\] <p>In the case of 1-dimension, say coordinated by $y$, the total length would be $2\pi \delta(0)$.</p> <p>This identity comes in handy when we take the factorized expression Eq. (3) into the one-loop correction</p> \[\begin{align*} Q_ {1} &amp;= -\frac{1}{4} \sum\!\!\!\!\!\!\!\!\int \frac{\;d^{2}k}{(2\pi)^{2}} \, \int \frac{d^{2}p}{(2\pi)^{2}} \, \left\lvert \tilde{ \mathfrak{g} }_ {k}(\vec{p}) \right\rvert^{2} \frac{(\omega_ {k}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= -\frac{1}{4} \sum\!\!\!\!\!\!\!\!\int \frac{\;d^{2}k}{(2\pi)^{2}} \, \int \frac{d^{2}p}{(2\pi)^{2}} \, \left\lvert (2\pi)\delta(k_ {y}+p_ {y})\tilde{ \mathfrak{g} }_ {k}(p_ {x}) \right\rvert^{2} \frac{(\omega_ {k}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= -\frac{1}{4} \sum\!\!\!\!\!\!\!\!\int \; \frac{dk_ {x}}{2\pi} \int \frac{dk_ {y}}{2\pi} \int \frac{dp_ {x}}{2\pi} (2\pi)\delta(0)\left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \frac{(\omega_ {k_ {x} p_ {y}}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= - \frac{L_ {\text{DM}}}{4}\sum\!\!\!\!\!\!\!\!\int \;\frac{dk_ {x}}{2\pi} \int \frac{dk_ {y}}{2\pi} \int \frac{dp_ {x}}{2\pi} \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \frac{(\omega_ {k_ {x} p_ {y}}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= L_ {\text{DM}} \times (\text{tension correction}). \end{align*} \tag{4}\] <p>In the above equation we have set the dimensionality $d$ to $2$. $\omega_ {k}$ is the short-handed form for $\omega_ {k_ {x}k_ {y}}$, the same for $\omega_ {p}$. Note that <strong>starting from the third line</strong>, we have $\omega_ {k}=\omega_ {k_ {x} p_ {y}}$ due to the Dirac-$\delta$ function. The integral regarding $y$-coordinate gives as a term proportional to $(2\pi)\delta(0)$, which is the total length of the $y$-direction. $L_ {\text{DM}}$ is the length of the domain wall, $L_ {\text{DM}}=\int dy$. It is understood that in the final result $p_ {y}=-k_ {y}$. It agrees with our naive expectation that the correction is proportional to the total length of the domain wall.</p> <p>Eq. (4) can also be written as</p> \[\begin{align*} Q_ {1} &amp;= -\frac{1}{4} \sum\!\!\!\!\!\!\!\!\int \; \frac{dk_ {x}}{2\pi} \int \frac{dp_ {x}}{2\pi} \int \frac{dp_ {y}}{2\pi} (2\pi)\delta(0)\left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \frac{(\omega_ {k}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= - \frac{L_ {\text{DM}}}{4}\sum\!\!\!\!\!\!\!\!\int \;\frac{dk_ {x}}{2\pi} \int \frac{dp^{2}}{(2\pi)^{2}} \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \frac{(\omega_ {k}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= \int dy \, \left( - \frac{1}{4} \right)\sum\!\!\!\!\!\!\!\!\int \;\frac{dk_ {x}}{2\pi} \int \frac{dp^{2}}{(2\pi)^{2}} \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \frac{(\omega_ {k}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= : \int dy \, \rho_ {1}(y). \end{align*} \tag{5}\] <p>Here again $\omega_ {k}$ is understood to be $\omega_ {k_ {x} k_ {y}}$, and we have defined the one-loop correction $\rho_ {1}$ to the tension. Note that <em>one-loop correction comes not from the interaction, as in QFT models in the vacuum sector, but rather comes from the non-trivial soliton background.</em></p> <h1 id="fourier-transform">Fourier Transform</h1> <p>We still need to evaluate the one-loop correction</p> \[\boxed { \rho_ {1} = \left( - \frac{1}{4} \right)\sum\!\!\!\!\!\!\!\!\int \;\frac{dk_ {x}}{2\pi} \int \frac{dp^{2}}{(2\pi)^{2}} \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \frac{(\omega_ {k}-\omega_ {p})^{2}}{\omega_ {p}}. }\] <p>To do this, first we need to Fourier-Transform various normal modes.</p> <p>To Fourier transform it we use <code class="language-plaintext highlighter-rouge">Mathematica</code>. In Mathematica, the convention used for the Fourier transform is represented by the function <code class="language-plaintext highlighter-rouge">FourierTransform[expr, t, ω]</code>, where <code class="language-plaintext highlighter-rouge">expr</code> is the expression to be transformed, <code class="language-plaintext highlighter-rouge">t</code> is the time variable, and <code class="language-plaintext highlighter-rouge">ω</code> is the frequency variable. This gives the symbolic Fourier transform of the expression. For multidimensional Fourier transforms, you can use <code class="language-plaintext highlighter-rouge">FourierTransform[expr, {t1, t2, …}, {ω1, ω2, …}]</code>, but here we won’t need it.</p> <p>The basis function used by Mathematica’s <code class="language-plaintext highlighter-rouge">FourierTransform</code> is dependent on the <code class="language-plaintext highlighter-rouge">FourierParameters</code> setting. The <code class="language-plaintext highlighter-rouge">FourierParameters</code> option is specified as {a, b}, where a and b are parameters that influence the Fourier transform’s definition. Specifically, they appear in the exponential factor and the normalization constant of the Fourier transform equations. The default setting for <code class="language-plaintext highlighter-rouge">FourierParameters</code> in Mathematica is <code class="language-plaintext highlighter-rouge">{0, 1}</code>, which corresponds to using the basis function $e^{i t \omega}$ for the Fourier transform,</p> \[\mathcal{F}\left\lbrace f(t) \right\rbrace (\omega) = \frac{1}{\sqrt{ 2\pi }} \int_{-\infty}^{\infty} dt \, f(t) e^{ i \omega t }\] <p>where $\mathcal{F}\left\lbrace f \right\rbrace$ is the Fourier transform of function $f(t)$. In general, under <code class="language-plaintext highlighter-rouge">FourierParameters-&gt;{a,b}</code> the Fourier Transform is set to be</p> \[\mathcal{F}\left\lbrace f(t) \right\rbrace (\omega){\Large\mid}_ {\left\lbrace a,b \right\rbrace } = \sqrt{\frac{\left\lvert b \right\rvert }{( 2\pi)^{1-a} }} \int_{-\infty}^{\infty} d t \, f(t) e^{i b \omega t }\] <p>In order to change it to our desired basis, we need to set $a=1,b=-1$, which is the so-called physics convention. As a test let’s try to transform a plane wave $\exp(-ipx)$, with our convention mathematica should return $2\pi \delta(p+k)$. The code reads</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">FourierTransform</span><span class="p">[</span><span class="nb">Exp</span><span class="p">[</span><span class="o">-</span><span class="nb">I</span><span class="w"> </span><span class="nv">p</span><span class="w"> </span><span class="nv">x</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">k</span><span class="o">,</span><span class="w"> </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">
</span></code></pre></div></div> <p>which indeed returns <code class="language-plaintext highlighter-rouge">2 Pi DiracDelta[k + p]</code>.</p> <hr/> <p><strong>The shape mode</strong></p> <p>The shape mode as a function of $x$ reads</p> \[{\mathfrak g} _ {S}(x) = \frac{\sqrt{ 3m }}{2} \tanh \frac{mx}{2} \text{sech} \frac{mx}{2},\] <p>whose Fourier transform can be calculated with mathematica code</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">fkgS</span><span class="p">[</span><span class="nv">x</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">Sqrt</span><span class="p">[</span><span class="m">3</span><span class="w"> </span><span class="nv">m</span><span class="p">]</span><span class="o">/</span><span class="m">2</span><span class="w"> </span><span class="nb">Tanh</span><span class="p">[(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="nb">Sech</span><span class="p">[</span><span class="w"> </span><span class="p">(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">]</span><span class="o">;</span><span class="w">
</span><span class="nb">FourierTransform</span><span class="p">[</span><span class="nv">fkgS</span><span class="p">[</span><span class="nv">x</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">p</span><span class="o">,</span><span class="w"> </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">
</span></code></pre></div></div> <p>The result reads</p> \[\boxed { \tilde{ \mathfrak{g} }_ {S}(p) = -\frac{2i\sqrt{ 3 }\pi p}{m^{3/2}} \mathrm{sech}\left( \frac{p\pi}{m} \right). }\] <hr/> <p><strong>The zero mode</strong></p> <p>From the following Mathematica code</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">fkgB</span><span class="p">[</span><span class="nv">x</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">-</span><span class="nb">Sqrt</span><span class="p">[((</span><span class="m">3</span><span class="w"> </span><span class="nv">m</span><span class="p">)</span><span class="o">/</span><span class="m">8</span><span class="p">)]</span><span class="w"> </span><span class="nb">Sech</span><span class="p">[(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">]</span><span class="o">^</span><span class="m">2</span><span class="o">;</span><span class="w">
</span><span class="nb">FourierTransform</span><span class="p">[</span><span class="nv">fkgB</span><span class="p">[</span><span class="nv">x</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">k</span><span class="o">,</span><span class="w"> </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">
</span></code></pre></div></div> <p>we have</p> \[\boxed { \tilde{ \mathfrak{g} }_ {B}(p) = -\frac{\sqrt{ 6 }\pi p}{m^{3/2}}\text{csch}\left(\frac{\pi p}{m} \right) }\] <hr/> <p><strong>The continuum</strong></p> <p>The Fourier transform is given by the following Mathematica code,</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">fkgk</span><span class="p">[</span><span class="nv">x</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="nb">E</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="nb">I</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nv">\[Omega]k</span><span class="w"> </span><span class="nb">Sqrt</span><span class="p">[</span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="o">+</span><span class="m">4</span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="p">])(</span><span class="m">2</span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="o">-</span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="o">+</span><span class="m">3</span><span class="o">/</span><span class="m">2</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="nb">Sech</span><span class="p">[(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">3</span><span class="nb">I</span><span class="w"> </span><span class="nv">m</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nb">Tanh</span><span class="p">[(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">])</span><span class="o">;</span><span class="w">
</span><span class="nb">FourierTransform</span><span class="p">[</span><span class="nv">fkgk</span><span class="p">[</span><span class="nv">x</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">p</span><span class="o">,</span><span class="w"> </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">
</span></code></pre></div></div> <p>which gives us</p> \[\tilde{ \mathfrak{g} }_ {k}(p) = \frac{6\pi p}{\omega_ {k}\sqrt{ 4k^{2} + m^{2} }} \text{csch}\left(\frac{(k+p)\pi}{m}\right).\] <p>However, if we perform the Fourier transform term-by-term we get</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">In[1]:=</span><span class="w"> </span><span class="nb">FourierTransform</span><span class="p">[</span><span class="nb">E</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="nb">I</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nv">\[Omega]k</span><span class="w"> </span><span class="nb">Sqrt</span><span class="p">[</span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="p">])</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">p</span><span class="o">,</span><span class="w"> 
 </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">

</span><span class="gp">Out[1]=</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="nv">\[Pi]</span><span class="w"> </span><span class="nb">DiracDelta</span><span class="p">[</span><span class="nv">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">p</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="nb">Sqrt</span><span class="p">[</span><span class="m">4</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="nv">\[Omega]k</span><span class="p">)</span><span class="w">

</span><span class="gp">In[2]:=</span><span class="w"> </span><span class="nb">FourierTransform</span><span class="p">[</span><span class="nb">E</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="nb">I</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nv">\[Omega]k</span><span class="w"> </span><span class="nb">Sqrt</span><span class="p">[</span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="p">])</span><span class="w"> </span><span class="p">(</span><span class="m">3</span><span class="o">/</span><span class="m">2</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="nb">Sech</span><span class="p">[(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">]</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">p</span><span class="o">,</span><span class="w"> 
 </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">

</span><span class="gp">Out[2]=</span><span class="w"> </span><span class="p">(</span><span class="m">6</span><span class="w"> </span><span class="p">(</span><span class="nv">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">p</span><span class="p">)</span><span class="w"> </span><span class="nv">\[Pi]</span><span class="w"> </span><span class="nb">Csch</span><span class="p">[((</span><span class="nv">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">p</span><span class="p">)</span><span class="w"> </span><span class="nv">\[Pi]</span><span class="p">)</span><span class="o">/</span><span class="nv">m</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="nb">Sqrt</span><span class="p">[</span><span class="m">4</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="nv">\[Omega]k</span><span class="p">)</span><span class="w">

</span><span class="gp">In[3]:=</span><span class="w"> </span><span class="nb">FourierTransform</span><span class="p">[</span><span class="nb">E</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="nb">I</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nv">\[Omega]k</span><span class="w"> </span><span class="nb">Sqrt</span><span class="p">[</span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="p">])</span><span class="w"> </span><span class="p">(</span><span class="o">-</span><span class="m">3</span><span class="w"> </span><span class="nb">I</span><span class="w"> </span><span class="nv">m</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nb">Tanh</span><span class="p">[(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">])</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">p</span><span class="o">,</span><span class="w"> 
 </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">

</span><span class="gp">Out[3]=</span><span class="w"> </span><span class="o">-</span><span class="p">((</span><span class="m">6</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nv">\[Pi]</span><span class="w"> </span><span class="nb">Csch</span><span class="p">[((</span><span class="nv">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">p</span><span class="p">)</span><span class="w"> </span><span class="nv">\[Pi]</span><span class="p">)</span><span class="o">/</span><span class="nv">m</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="nb">Sqrt</span><span class="p">[</span><span class="m">4</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="nv">\[Omega]k</span><span class="p">))</span><span class="w">
</span></code></pre></div></div> <p>Putting them together we get</p> \[\tilde{ \mathfrak{g} }_ {k}(p) = \frac{6\pi p}{\omega_ {k}\sqrt{ 4k^{2} + m^{2} }} \text{csch}\left(\frac{(k+p)\pi}{m}\right)+\frac{2\pi(2k^{2}-m^{2})}{\omega_ {k}\sqrt{ 4k^{2} + m^{2} }} \delta(p+k).\] <p>I have no idea why this delta function disappeared. Any help here?</p> <hr/> <p>Let’s proceed to calculate $\rho_ {1}$. Since $k_ {x}$ is separated into three parts, i.e. zero mode, shape mode and continuum, we can separate $\rho_ {1}$ into three corresponding parts, writing</p> \[\rho_ {1} = \rho_ {1B} + \rho_ {1S} + \int \frac{dk_ {x}}{2\pi} \, \rho_ {1k_ {x}}\] <p>where</p> \[\begin{align*} \rho_ {1B} &amp;= - \frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, (\tilde{ \mathfrak{g} }_ {B}(p_ {x}))^{2} \int \frac{dp_ {y}}{2\pi}\, \frac{(\omega_ {k_ {B}p_ {y}}-\omega_ {p_ {x}p_ {y}})^{2}}{\omega_ {p_ {x}p_ {y}}}, \tag{6} \\ \rho_ {1S} &amp;= -\frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, (\tilde{ \mathfrak{g} }_ {S}(p_ {x}))^{2} \int \frac{dp_ {y}}{2\pi} \, \frac{(\omega_ {k_ {S}p_ {y}}-\omega_ {p_ {x}p_ {y}})^{2}}{\omega_ {p_ {x}p_ {y}}}, \tag{7} \\ \rho_ {1C} &amp;= \int \, \frac{dk_ {x}}{2\pi} \left( - \frac{1}{4} \right) \int \frac{dp_ {x}}{2\pi}\, \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \int \frac{dp_ {y}}{2\pi} \, \frac{(\omega_ {k_ {x}p_ {y}}-\omega_ {p_ {x}p_ {y}})^{2}}{\omega_ { p_ {x}p_ {y}}} \tag{8} \\ &amp;=: \int \frac{dk_ {x}}{2\pi} \, \rho_ {1k_ {x}}. \end{align*}\] <p>The subscript $C$ in $\rho_ {1C}$ stands for continuum.</p> <p>We have separated the integral over $d^{2}p$ into its two components for a reason, so we can perform the integral over the flat direction $p_ {y}$ analytically first. For the sake of convenience let’s define a general integral of form</p> \[\mathcal{I} (a,b) := \int_ {-\infty}^{\infty} \frac{d p}{2\pi} \, \frac{(\sqrt{ a+p^{2} }-\sqrt{ b+p^{2} })^{2}}{\sqrt{ b+p^{2} }} ,\] <p>With the help of mathematica again we get</p> \[\mathcal{I} (a,b) = \frac{a}{2\pi} \left( \frac{b}{a}-\ln \left\lvert \frac{b}{a} \right\rvert -1 \right).\] <p>This form shows that if $a=b$ then $\mathcal{I}_ {ab}=0$, as it should be by definition. If we allow the parameters of $\ln$ function to be dimensionful, another form is more useful to us,</p> \[\boxed { \mathcal{I} (a,b) = \frac{1}{2\pi} (b-a-a\ln \left\lvert b \right\rvert +a\ln \left\lvert a \right\rvert ). }\] <p>This form made obvious that $\mathcal{I}(a,b)$ behaviors nice at $a=0$.</p> <hr/> <p>To calculate Eq. (6), recall that (since $k_ {y}=-p_ {y}$)</p> \[\boxed { \omega_ {k_ {B}p_ {y}} = \left\lvert p_ {y} \right\rvert ,\quad \omega_ {k_ {S}p_ {y}} = \sqrt{ \frac{3m^{2}}{4}+p_ {y}^{2} },\quad \omega_ {k_ {x} p_ {y}} = \sqrt{ m^{2}+k_ {x}^{2}+p_ {y}^{2} }. }\] <p>We have</p> \[\begin{align*} \rho_ {1B} &amp;= - \frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, (\tilde{ \mathfrak{g} }_ {B}(p_ {x}))^{2} \int \frac{dp_ {y}}{2\pi}\, \frac{(\omega_ {k_ {B}p_ {y}}-\omega_ {p_ {x}p_ {y}})^{2}}{\omega_ {p_ {x}p_ {y}}}, \tag{6'} \\ &amp;= - \frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, (\tilde{ \mathfrak{g} }_ {B}(p_ {x}))^{2} \mathcal{I}(0,m^{2}+p^{2}_ {x}) \\ &amp;= - \frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, (\tilde{ \mathfrak{g} }_ {B}(p_ {x}))^{2} \frac{m^{2}+p^{2}_ {x}}{2\pi} \\ &amp;= - \frac{1}{8\pi} \int \frac{dp_ {x}}{2\pi} \, (\tilde{ \mathfrak{g} }_ {B}(p_ {x}))^{2} (m^{2}+p^{2}_ {x})\\ &amp;= -\frac{3 m^2}{20 \pi} \end{align*}\] <p>The last line is obtained by Mathematica code</p> <pre><code class="language-Mathematica">tldgB[p_] := -((Sqrt[6] \[Pi] p) /m^(3/2)) Csch[(p \[Pi])/m];
Integrate[-1/(8 \[Pi]) 1/(2 Pi) tldgB[px]^2 (m^2 + px^2), {px, -\[Infinity], \[Infinity]}, 
 Assumptions -&gt; {m &gt; 0, m \[Element] Reals}]
</code></pre> <hr/> <p>Next let’s move on to Eq. (7). We have</p> \[\begin{align*} \rho_ {1S} &amp;= -\frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, \left\lvert \tilde{ \mathfrak{g} }_ {S}(p_ {x}) \right\rvert ^{2} \int \frac{dp_ {y}}{2\pi} \, \frac{(\omega_ {k_ {S}p_ {y}}-\omega_ {p_ {x}p_ {y}})^{2}}{\omega_ {p_ {x}p_ {y}}} \\ &amp;= -\frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, \left\lvert \tilde{ \mathfrak{g} }_ {S}(p_ {x}) \right\rvert ^{2} \, \mathcal{I}\left( \frac{3}{4}m^{2},m^{2}+p_ {x}^{2} \right) \\ &amp;= -\frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, \left\lvert \tilde{ \mathfrak{g} }_ {S}(p_ {x}) \right\rvert ^{2} \, \mathcal{I}\left( \frac{3}{4}m^{2},m^{2}+p_ {x}^{2} \right) \end{align*}\] <p>where</p> \[\mathcal{I}\left( \frac{3}{4}m^{2},m^{2}+p_ {x}^{2} \right) = \frac{m^{2}}{2\pi} \left[ \frac{1}{4} + \frac{p_ {x}^{2}}{m^{2}} + \frac{3}{4}\ln\left( \frac{3}{4} \right) - \frac{3}{4}\ln\left( 1+\frac{p_ {x}^{2}}{m^{2}} \right) \right].\] <p>We’ll have to turn to numerical calculation now. For the sake of numerical calculation, we better change $p_ {x}$ to something without dimension. Set $t:= p_ {x} / m$, we have</p> \[\begin{align*} dp_ {x} &amp;= m dt, \\ \tilde{ \mathfrak{g} }_ {S}(t) &amp;= - \frac{2i\sqrt{3} \pi t}{\sqrt{ m }} \mathrm{sech}\,(\pi t), \\ \mathcal{I}(t) &amp;= \frac{m^{2}}{2\pi} \left[ \frac{1}{4} + t^{2} + \frac{3}{4}\ln\left( \frac{3}{4} \right) - \frac{3}{4}\ln\left( 1+ t^{2} \right) \right]. \end{align*}\] <p>All of this gives us</p> \[\rho_ {1S} = -\frac{1}{4} \int_ {\infty}^{\infty} \frac{m dt}{2\pi} \, \left\lvert \tilde{ \mathfrak{g} }_ {S}(t) \right\rvert^{2} \, \mathcal{I}(t) = -0.00725 m^{2}. \tag{7'}\] <hr/> <p>It is a little harder to do Eq. (8).</p> \[\begin{align*} \rho_ {1C} &amp;= - \frac{1}{4} \int \, \frac{dk_ {x}}{2\pi} \int \frac{dp_ {x}}{2\pi}\, \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \int \frac{dp_ {y}}{2\pi} \, \frac{(\omega_ {k_ {x}p_ {y}}-\omega_ {p_ {x}p_ {y}})^{2}}{\omega_ { p_ {x}p_ {y}}} \\ &amp;= - \frac{1}{4} \int \, \frac{dk_ {x}}{2\pi} \int \frac{dp_ {x}}{2\pi}\, \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \, \mathcal{I}(m^{2}+k_ {x}^{2},m^{2}+p_ {x}^{2}) \tag{8'} \end{align*}\] <p>where</p> \[\mathcal{I}(m^{2}+k_ {x}^{2},m^{2}+p_ {x}^{2}) = \frac{1}{2\pi} \left[ p_ {x}^{2}-k_ {x}^{2}+(m^{2}+k_ {x}^{2})\ln\left( \frac{m^{2}+k_ {x}^{2}}{m^{2}+p_ {x}^{2}} \right) \right].\] <p>This leaves</p> \[\rho_ {1k_ {x}} = - \frac{1}{8\pi} \int \frac{dp_ {x}}{2\pi}\, \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \, \left[ p_ {x}^{2}-k_ {x}^{2}+(m^{2}+k_ {x}^{2})\ln\left( \frac{m^{2}+k_ {x}^{2}}{m^{2}+p_ {x}^{2}} \right) \right]\] <p>Now we simply need to substitute</p> \[\tilde{ \mathfrak{g} }_ {k_ {x}}(p_ {x}) = \frac{6\pi p_ {x}}{\omega_ {k_ {x}}\sqrt{ 4k_ {x}^{2} + m^{2} }} \text{csch}\left(\frac{(k_ {x}+p_ {x})\pi}{m}\right)+\frac{2\pi(2k_ {x}^{2}-m^{2})}{\omega_ {k_ {x}}\sqrt{ 4k_ {x}^{2} + m^{2} }} \delta(p_ {x}+k_ {x}).\] <p>where $\omega_ {k_ {x}} = \sqrt{ m^{2}+k_ {x}^{2} }$. Note that since $\mathcal{I}(m^{2}+k_ {x}^{2},m^{2}+p_ {x}^{2})$ equals to zero when $p=-k$, hence we can discard the $\delta$ term and focus on the $\text{csch}$ term.</p> <p>Again we have to turn to numerical methods, and again we need to cook up some dimension-less variables from $k_ {x}$ and $p_ {x}$. Define $\kappa:= k_ {x} / m$ and $\rho := p_ {x} / m$, we have</p> \[\frac{\rho_ {1k_ {x}}(\kappa)}{m} = \int d \rho \, (-9 \rho ^2) \frac{ \left(\left(\kappa ^2+1\right) \log \left(\frac{\kappa ^2+1}{\rho ^2+1}\right)-\kappa ^2+\rho ^2\right) }{4 \left(4 \kappa ^4+5 \kappa ^2+1\right)} \, \text{csch}^2(\pi (\kappa +\rho )).\] <p>The figure of the integral is shown in the below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/rhokmarNew-480.webp 480w,/img/rhokmarNew-800.webp 800w,/img/rhokmarNew-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/rhokmarNew.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The contribution $\rho_ {1k_ x}$ to the one loop tension arising from each continuum normal mode $k_ x$. </div> <p>The numerical result shows that the total contribution is</p> \[\rho_ {1C} = -0.0312775 m^{2}\] <p>which is slightly different from that in the draft where $\rho_ {1C}=−0.03156 m^{2}$.</p> <hr/> <p>We can also directly perform the double integral over $dk_ {x}dp_ {x}$, based on Eq. (5.18) in the paper. Using the Mathematica code in the following</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">Module</span><span class="p">[{</span><span class="nv">integrandTem</span><span class="p">}</span><span class="o">,</span><span class="w"> 
 </span><span class="nv">integrandTem</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">_,</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="m">9</span><span class="o">/</span><span class="p">(</span><span class="m">2</span><span class="o">*</span><span class="m">4</span><span class="w"> </span><span class="nb">Pi</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="p">)))</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="p">(</span><span class="nb">Csch</span><span class="p">[</span><span class="nb">Pi</span><span class="w"> </span><span class="p">(</span><span class="nv">\[Kappa]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="p">)])</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="p">(</span><span class="o">-</span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="nb">Log</span><span class="p">[(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="p">)])</span><span class="o">;</span><span class="w"> 
 </span><span class="nb">NIntegrate</span><span class="p">[</span><span class="nv">integrandTem</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="nv">\[Infinity]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Infinity]</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Rho]</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="nv">\[Infinity]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Infinity]</span><span class="p">}]</span><span class="w">
 </span><span class="p">]</span><span class="w">
</span></code></pre></div></div> <p>where $\kappa := k_ {x} / m$ and $\rho := p_ {x} / m$, they are the reduced momenta.</p> <p>We get -0.0290278 without any error or warning message.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="kink"/><summary type="html"><![CDATA[Domain wall in phi-fourth model]]></summary></entry><entry><title type="html">Note on the kink mass correction in 3D Part I</title><link href="https://baiyangzhang.github.io/blog/2024/Note-on-Kink-Mass-Correction-in-3D-Part-I/" rel="alternate" type="text/html" title="Note on the kink mass correction in 3D Part I"/><published>2024-03-10T00:00:00+00:00</published><updated>2024-03-10T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Note-on-Kink-Mass-Correction-in-3D-Part-I</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Note-on-Kink-Mass-Correction-in-3D-Part-I/"><![CDATA[<h1 id="1-introduction">1 Introduction</h1> <h2 id="11-background">1.1 Background</h2> <p>We will establish the model and notation in this section. First, some nomenclatures.</p> <p><code class="language-plaintext highlighter-rouge">Shape modes</code>: In theories with a kink solution, the equation of motion in the background of the kink typically has discrete, bounded solutions. They are called <code class="language-plaintext highlighter-rouge">shape modes</code>. They are localized around the kink. Shape modes represent small fluctuations or deformations of the kink’s shape. Unlike the continuum of delocalized modes that represent free particle states, the shape modes are confined to the vicinity of the kink, with discrete energy levels.</p> <p><code class="language-plaintext highlighter-rouge">Oscillons</code>: They are localized, non-solitonic, and semi-stable field configurations that oscillate in time. They are interesting because they represent a form of partial stable, localized energy concentration that can persist for long times, even though they are not topologically protected like solitons or kinks. Oscillons are found in various nonlinear field theories and have been studied in different contexts, including cosmology and condensed matter physics. Their persistence and behaviors under various conditions are subjects of ongoing research in theoretical physics. However, it is believed that quantum correction will make oscillons rapidly decay into a pair of fundamental particles.</p> <p><code class="language-plaintext highlighter-rouge">The Unruh effect</code>: The Unruh effect is a prediction in quantum field theory, stating that an observer accelerating through a vacuum will perceive a thermal bath of particles, whereas an inertial observer would see none. This effect arises from the realization that <em>the concept of a vacuum state (or empty space) is observer-dependent</em>. For an accelerating observer, what appears as empty space to an inertial observer is seen as a warm gas of particles. This effect, named after physicist William Unruh, highlights the interplay between quantum theory and relativity, especially in contexts like black hole physics.</p> <p>In the frame work of the linearized soliton perturbation theory, we can systematically study the quantum corrections to both static and time-dependent solitonic solutions.</p> <hr/> <p>Firstly, we work in the Schrodinger picture where the operator are time-independent while the wave functions (vectors in Hilbert space) are time dependent. Secondly, we will start with the Hamiltonian formalism where the canonical momentum, $\pi$, is considered as a fundamental ingredient, rather than the time derivative of $\phi$, the field of the Lagrangian.</p> <p>In $2+1$ dimension, the Hamiltonian $H$ is the integral of the Hamiltonian density $\mathcal{H}$ over the 2D manifold of space alone,</p> \[H = \int_ {M} d^{2}x\, \mathcal{H}(\phi,\pi),\quad \mathcal{H}=\frac{1}{2}\pi^{2}(x)+\frac{1}{2} (\partial_ i \phi)^{2} + \frac{1}{\lambda}V(\sqrt{ \lambda }\phi(x))\] <p><strong>Dimensional analysis</strong></p> <p>In Lagrangian formalism, $[S]=[\hbar]$ where $S$ is the action, and $S = \int d^d x \, \mathcal{L}$, where $\mathcal{L}$ is the Lagrangian (density) and $d$ is the space-time dimension. Note that right now we are working with a partially “natural” units, where $c=1$ but $\hbar \neq 1$. Since $c=1$ we still have $[x] = [t]$, namely $L=T$ ($L$ is length and $T$ is time), but no longer do we have $T=E^{-1}$ since this is a result of $[\hbar]=ET=1$ (recall that $\hbar \omega=E$, meaning $[\hbar \omega]=E$, and $\omega$ is the frequency with dimension $1 / T$ ).</p> <p>Let $d$ be the dimension of spacetime, in the partial natural unit we have</p> \[\int d^{d}x \, \mathcal{L}\sim \hbar \implies [\mathcal{L}]=[\hbar] L^{-d}\] <p>Take a specific term, say $(\partial \phi)^{2}$, to continue the analysis,</p> \[[(\partial \phi)^{2}] = L^{-2} [\phi^{2}] = [\mathcal{L}] = [\hbar]L^{-d} \implies [\phi^{2}]=[\hbar] L^{2-d}\] <p>which is</p> \[[\phi] = [\sqrt{ \hbar }] \, L^{1-d / 2}.\tag{1}\] <p>This agrees with the convention that field operator $\phi$ scales as $\sqrt{ \hbar }$. Furthermore, this scaling property does not depend on the spacetime dimension $d$, it holds for any spacetime dimension.</p> <p>I don’t think the $\hbar$-dependence given by Eq. (1) is unique, apparently there exist other possibilities, we can move $\hbar$ around in the Lagrangian, just to make sure that the action altogether is of dimension $\hbar$. However it seems that $\phi\sim \hbar^{1/2}$ is indeed the most convenient option. Another way to see the advantage of this choice is from the action,</p> \[S \sim \hbar \sim \int d^{4}x \, (m^{2} \phi^{2}+(\partial \phi)^{2} +U(\phi)),\] <p>then</p> \[\int d^{4}x \, \left[ m^{2} \xi^{2}+(\partial \xi)^{2} + U(\xi) \right] \sim 1, \quad \xi:= \frac{\phi}{\sqrt{ \hbar } } .\] <p>In this way, we can absorb $\hbar$ into the definition of $\phi$, this is similar to absorbing the coupling $g$ into the field definition in gauge theory. We now <em>define</em> $\xi$ to be <em>independent</em> of $\hbar$.</p> <p>Having fixed the $\hbar$-dependence of $\phi$, we can substitute it to the potential $U$, for example the $\phi^{4}$ model to determine the $\hbar$ dependence of the coupling,</p> \[[S]\sim\int d^{d}x \, \lambda \phi^{4} \sim L^{d} [\lambda][\hbar^{2}] L^{4-2d} = L^{4-d}[\hbar^{2}][\lambda],\] <p>on the other hand we already know that $S\sim \hbar$ so</p> \[L^{4-d}[\hbar^{2}][\lambda] = [\hbar] \implies [\lambda \hbar] = L^{d-4}.\] <p>If we choose $L$ to be the fundamental unit instead of energy $E$, it is clear the $\lambda \hbar$ is independent of $\hbar$. Plus, we see that $d=4$ is special.</p> <p>For the sake of completeness, let’s consider the mass term in the Lagrangian. Similar to what we have for the kinetic term,</p> \[[m^{2} \phi^{2}] = E^{2}\, [\phi^{2}] = [\mathcal{L}] = [\hbar]\,L^{-d} \implies [\phi] = [\sqrt{ \hbar }]L^{-d/2}E^{-1} . \tag{2}\] <p><strong>Dimension and measurement</strong></p> <p>If two things are of the same dimension, for example, say</p> \[[A] \sim [a] = L\] <p>where $\sim$ means having the same dimension. Then we can use one of them as the unit to measure the other, say, use $a$ as the “ruler” to measure $A$, the result $\widetilde{A} :=A / a$ is a dimensionless number.</p> <p>Suppose $[\phi]=[\hbar]^{1/2}$, another way to say the same ting is $\phi \sim \sqrt{ \hbar }$, note that the tilde does not imply any relation between the <em>values</em> of $\phi$ and $\sqrt{ \hbar }$, it only means that they have the same dimension. The point is, since they have the same dimension, we can use one of them to measure the other, for example we can define</p> \[\tilde{\phi} = \phi / \sqrt{ \hbar }\] <p>which is a dimensionless number. Since $\hbar$ scales the “quantumness”, the more classical the world is, the smaller $\hbar$ (and $\sqrt{ \hbar }$), hence the bigger numeric value of $\tilde{\phi}$.</p> <hr/> <p>In the partial natural units, I’d like to think there are two fundamental “rulers” to measure all the quantities, such as mass, coupling, field, etc. One of them is the unit of energy, for example $\text{MeV}$, the other is $\hbar$ whose dimension is $ET$. To measure the length of something, we can use $\frac{\hbar}{\text{MeV} }$ as unit. The advantage of the partial natural unit is that it makes explicit the $\hbar$ factor, revealing the direct relations between quantities with $\hbar$, which is the scale of quantumness, this enables us to discern the importance of various quantities in the classical limit, making the analysis regarding semi-classical more straightforward.</p> <h2 id="12-digression-on-hbar-expansion">1.2 Digression on $\hbar$-expansion</h2> <p>To appreciate the importance of $\hbar$, just recall that in canonical quantization $[x,p]=i\hbar$, $\hbar$ enters explicitly in the commutation relation, providing the fundamental basis of quantum theory. This is also true in the case of quantum field theory. Furthermore, at each order of an expansion in $\hbar$, the physical symmetries (Lorentz invariance, $U(1)$ symmetry, etc.) must be satisfied, otherwise there will be some special value of $\hbar$ only at which the symmetries are preserved, which is just strange.</p> <p>In the unit where $c=1$, the Planck’s constant $\hbar$ has the unit of action, or rather the action has the unit of $[\hbar]=ET$, where $E$ is the energy scale and $T$ the time. It turns out that there are more than one way to assign $\hbar$-dependence for quantities such as the mass $m$, the coupling $g,\lambda$ etc. A criterion for the “right” choice is that, at $\hbar\to 0$ limit, the quantum theory agrees with the classical theory. As an example, Stanley Brodsky and Paul Hoyer in their <a href="https://arxiv.org/pdf/1009.2313.pdf">paper</a> used the quantum mechanical harmonic oscillator as an example in Eq.(1). The gist is that, you can rescale $x$ to $x / \sqrt{ \hbar }$, then the propagator is formally independent of $\hbar$. However, this will change how we view distance, in the $\hbar\to 0$ limit, for a fixed distance $L$, the “length” measure will increase as $1 / \sqrt{ \hbar }$, hence we are going to smaller and smaller area.</p> <p>It is generally understood that each loop contribution to amplitudes is associated with one factor of $\hbar$. However, to fully define the $\hbar\to 0$ limit one need to specify the $\hbar$ dependence of various quantities in the Lagrangian as mentioned before, such as the field operator, the mass, the coupling, etc. This is not as straightforward as one might think, for $\hbar$ not only appears in the action $iS / \hbar$ but also appears in the Lagrangian. In Brodsky’s paper mentioned above, the authors proposed a way to establish the $\hbar$ dependence such that the loop and $\hbar$ expansions are equivalent. We will go to more details in the following.</p> <p><strong>First, regard $\hbar$ as a constant of nature with certain dimension, use $\hbar$ to make terms in the Lagrangian dimensionless.</strong></p> <p>Again, let’s work with the assumption that $c = \epsilon_ {0} = 1$. Require $[S]=\hbar$, and $\alpha_ {s} = g^{2} / 4\pi \hbar$ is dimensionless, the latter implies that $[g]=\sqrt{ \hbar }=\sqrt{ ET }=\sqrt{ EL }$. From the self-energy of gluons $G_ {\mu \nu}G^{\mu \nu}$ where $G = \partial A - \partial A +ig / \hbar [A,A]$ we have</p> \[[A] = \sqrt{ \frac{E}{L} }.\] <p>For the same reasons, in the scalar QED the classical electric charge $e$ and mass $m$ are divided by $\hbar$,</p> \[S_ {\text{sQED} } = \int d^{4}x \, \left\lbrace \left\lvert D\phi \right\rvert ^{2}-\frac{m^{2} }{\hbar^{2} }\left\lvert \phi \right\rvert ^{2} \right\rbrace , \quad D = \partial +i \frac{e}{\hbar }A.\] <p>The boson field dimension</p> \[[\phi]=[A]= \sqrt{ \frac{E}{L} }.\] <p>Fermion fields are more complicated, since they have no classical counterparts, their dimensions are convention-dependent. We will deal with fermions in a different note perhaps.</p> <p><strong>Second step is to specify $\hbar$ dependence of all quantities appearing in the action.</strong></p> <p>The choice made by Brodsky and Hoyer is as following:</p> \[\widetilde{A}:= \frac{A}{\sqrt{ \hbar } },\quad \tilde{\phi}:= \frac{\phi}{\sqrt{ \hbar } }\] <p>where $\widetilde{A},\tilde{\phi}$ are $\hbar$-independent. Similarly, define the following $\hbar$-independent quantities</p> \[\widetilde{g}:= \frac{g}{\hbar},\quad \widetilde{e}:= \frac{e}{\hbar},\quad \widetilde{m}:= \frac{m}{\hbar}.\] <p>Then one can write the Lagrangian in terms of these $\hbar$-independent quantities to check the $\hbar$ dependence explicitly. It turns out that, at least in the simple models discussion in the paper, $\hbar$ always appears in the combination</p> \[\widetilde{g}\sqrt{ \hbar } \quad \text{and}\quad \widetilde{e}\sqrt{ \hbar }\] <p>that is, with the coupling. Hence loop correction of $\mathcal{O}(g^{2},e^{2})$ will be of order $\hbar$.</p> <p>This derivation is equivalent to the standard one of, for example, Mark Srednicki’s textbook, which associates a factor $\hbar$ to each propagator and $h^{-1}$ with each vertex, and assume the parameters appearing in the action to be independent of $\hbar$.</p> <p>Fore more details please refer to Brodsky and Hoyer’s paper mentioned above.</p> <h2 id="13-review-of-kink-mass-quantization">1.3 Review of Kink mass quantization</h2> <h1 id="2-kinks-in-3d">2 Kinks in 3D</h1> <p>In R. Jackiw’s <a href="https://www.sciencedirect.com/science/article/abs/pii/037015737690048X">1976 paper</a>, he made three assumption:</p> <ol> <li>The energy (mass) is finite;</li> <li>The energy is locally minimum, meaning the soliton is stable;</li> <li>The potential $U$ depends on a coupling constant $\lambda$ according to the scaling law</li> </ol> \[U(\phi;\lambda) = \frac{1}{\lambda}U(\sqrt{ \lambda }\,\phi;1).\] <p>The choice is such that all the $\lambda$ dependence are now moved to the pre-factor $1 / \lambda$. As for $\hbar$-expansion, we take the scheme such that $\hbar$-expansion agrees with $\lambda$-expansion, namely each loop brings in a factor of $\hbar$.</p> <p>Let the Hamiltonian be</p> \[H = \int d^{2}x \, : \mathcal{H} :_ {a},\quad \mathcal{H}(x) = \frac{\pi^{2} }{2} + \frac{(\partial_ {x}\phi(x))^{2} }{2} + \frac{1}{\lambda} V\left(\sqrt{ \lambda}\, \phi(x) \right).\] <p>Now, in order to get the equation of motion, we have two options: 1) Legendre-transform the equation to the Lagrangian formalism and adopt Euler-Lagrange equation, or 2) stick with the Hamiltonian formalism and adopt the Hamiltonian equations of motion (Hamilton equations) instead. Here we will take the second option.</p> <p>Recall that the Hamilton equations in classical field theory reads</p> \[\begin{align*} \frac{\delta \mathcal{H} }{\delta \phi_i} &amp;=-\dot{\pi}(x), \\ \frac{\delta \mathcal{H} }{\delta \pi_i} &amp;= \dot{\phi}_i. \end{align*}\] <p>This is a non-trivial generalization of the familiar Hamiltonian in classical mechanics, non-trivial since the connection between variational derivative and partial derivative is not as simple as one might think, we have</p> \[\frac{\delta \mathcal{H} }{\delta \phi} = \frac{\partial\mathcal{H} }{\partial \phi } - \left( \partial_ x\frac{\partial \mathcal{H} }{\partial(\partial_ {x}\phi)} \right).\] <p>Taking everything into consideration, we obtain the equation of motion by straightforward calculation. But before going there, let’s rewrite the Hamiltonian in a more compact form:</p> \[\boxed{ \lambda \, \mathcal{H} = \frac{1}{2} \tilde{\pi}^{2} + \frac{1}{2} \vec{\nabla}^{2}\,\tilde{\pi} + V(\tilde{\phi}),\quad \tilde{\pi} := \sqrt{ \lambda }\, \pi,\quad \tilde{\phi} := \sqrt{ \lambda }\,\phi. }\] <p>Then we can first obtain the EOM in terms of $\tilde{\phi}$ and $\tilde{\pi}$, the changing to un-tilded version is trivial. Finally we have</p> \[\ddot{\phi} - \vec{\nabla}^{2} \phi(x,t) + \frac{1}{\sqrt{ \lambda } } V^{(1)}(\sqrt{ \lambda }\phi) = 0\] <p>with definition</p> <p>\(V^{(n)} := \frac{ \partial^{n } V(\tilde{\phi})}{ \partial \tilde{\phi}^{n} }.\)</p> <h2 id="21-normal-modes-and-quantization">2.1. Normal modes and quantization</h2> <p>When kink solutions are placed in more than one spatial dimension, they become extended planar structures called “domain walls.”</p> <p>Now, how can we borrow the kink result form 2D directly to 3D? Consider a static kink solution in the $x$ direction</p> \[\phi(x,y) =: f(x,y)=: f_ {1}(x)\times f_ {2}(y) ,\] <p>where we have assumed the possibility of separation of variables. To say the kink is in the $x$ direction is to say the solution satisfies the equation of motion in the $x$ direction,</p> \[\frac{ \partial^{2} f(x,y) }{ \partial x^{2} } = \frac{1}{\lambda} \frac{ \partial V }{ \partial \phi } = \frac{1}{\sqrt{ \lambda } } \frac{ \partial V }{ \partial \tilde{\phi} } = \frac{1}{\sqrt{ \lambda } }V^{(1)}(\tilde{\phi}).\] <hr/> <p>First, consider the case in 2D. Writing the filed as a kink background plus fluctuation,</p> \[\phi(x,t) =: f(x) + {\mathfrak g}(x) e^{ -i\omega t }\] <p>where $f(x)$ is the 1-dimensional kink solution, the equation of motion in terms of ${\mathfrak g}$ reads</p> \[[-\omega^{2}-\partial_ {x}^{2}+V^{(2)}(\sqrt{ \lambda }f(x))]\, {\mathfrak g}(x) = 0.\] <p>As we mentioned before, there are three kinks of solutions: the zero mode, the shape mode and the continuum.</p> <hr/> <p>The equation of motion is the Sturm-Liouville equation. A general Sturm-Liouville problem is typically written in the form:</p> \[\frac{d}{dx}\left[ p(x) \frac{dy}{dx} \right] - q(x)y + \lambda r(x)y = 0\] <p>Here, $y$ is the function of the variable $x$ that we are solving for, and $p(x)$, $q(x)$, and $r(x)$ are known functions that specify the particular Sturm-Liouville problem. The parameter $\lambda$ is often referred to as the eigenvalue.</p> <p>Key characteristics and applications of the Sturm-Liouville equation include:</p> <ol> <li> <p><strong>Eigenvalue Problem</strong>: The Sturm-Liouville equation is an eigenvalue problem. The solutions $y(x)$ are eigenfunctions, and the associated values of $\lambda$ are eigenvalues. These eigenvalues are typically discrete and can be ordered as a sequence $\lambda_1, \lambda_2, \lambda_3, \ldots$, where each $\lambda_n$ corresponds to a particular eigenfunction $y_n(x)$.</p> </li> <li> <p><strong>Orthogonality and Completeness</strong>: The eigenfunctions of a Sturm-Liouville problem are orthogonal with respect to the weight function $r(x)$. This property is crucial in solving partial differential equations, as it allows the expansion of functions in terms of these eigenfunctions (similar to Fourier series).</p> </li> <li> <p><strong>Boundary Conditions</strong>: Sturm-Liouville problems are typically accompanied by boundary conditions that the solutions must satisfy. These conditions are usually specified at the endpoints of the interval in which the equation is defined.</p> </li> <li> <p><strong>Physical Applications</strong>: The Sturm-Liouville problem appears in various areas of physics, such as quantum mechanics (in solving the Schrödinger equation), heat conduction, wave propagation, and vibrations analysis. It is essential in the separation of variables technique for solving partial differential equations.</p> </li> <li> <p><strong>Self-Adjoint Form</strong>: The equation is often referred to as a self-adjoint form, which has important implications in the theory of linear operators and functional analysis.</p> </li> </ol> <p>In our case, the weight function is trivial.</p> <hr/> <p>We will denote the zero mode by ${\mathfrak g}_ {B}$ and the shape mode by ${\mathfrak g}_ {S}$. The $B$ in ${\mathfrak g}_ {B}$ has a historical reason, but in our note it is just part of the name. The normalization conditions are</p> \[\int dx \, {\mathfrak g}_ {S}^{2} = \int dx \, {\mathfrak g}_ {B}^{2} = 1 , \quad \int dx \, {\mathfrak g}_ {B}{\mathfrak g}_ {S} = 0 ,\quad \int dx \, {\mathfrak g}_ {k}(x){\mathfrak g}_ {p}(x) = 2\pi i\delta(p-k).\] <p>The sign of ${\mathfrak g}_ {B}$ is fixed using</p> \[{\mathfrak g}_ {B}(x) = - \frac{f'(x)}{\sqrt{ Q_ {0} } },\] <p>where $f$ is again the kink solution.</p> <p>We choose to expand in the $x$ direction in normal modes (in the kink background), while in the $y$ direction in plane waves. The 2D momentum $\vec{k}=\left\lbrace k_ {x},k_ {y} \right\rbrace$, where $k_ {x}=\left\lbrace B,S,k \right\rbrace$, $B$ for the zeromode (bounded solution), $S$ for the shape mode (also bounded) and $k$ for the continuum. A nice illustration of normal modes in the background of kink is shown in the figure below, which I shamelessly copied from Tanmay Vachaspati’s book, all the credits goes to Vachaspati.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kinkLevel-480.webp 480w,/img/kinkLevel-800.webp 800w,/img/kinkLevel-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kinkLevel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A trivial potential on a periodic space with period $L$ is shown on the left, while the normal modes in the background of a kink solution is shown on the right. What used to be the $n=0$ mode in the trivial potential (on the left) becomes the lowest bound state, the zero mode, in the non-trivial potential. Similarly a linear combination of the $n=\pm 1$ modes in the trivial case may become the second bound state ($n=+1$ in the illustration), and the other states remain unbounded but shift in form. </div> <p>We want to expand the static fluctuation field $\phi(r)$ (defined by $\phi=f_ {\text{kink} }+g$) in terms of normal modes. Since we have defined the indices $k$ in ${\mathfrak g}_ {k}(x)$ to include everything, we can conveniently write the field expansion as</p> \[\begin{align*} \phi(r) &amp;= \sum\!\!\!\!\!\!\!\!\int \frac{ d^{d}k}{(2\pi )^{d} } \, \left( B_ {k} ^{\ddagger} +\frac{B_ {-k} }{2\omega _ {k} } \right){\mathfrak g}_ {k} (r),\\ \pi(r) &amp;= \sum\!\!\!\!\!\!\!\!\int \frac{ d^{d}k}{(2\pi )^{d} } \, \left( B_ {k} ^{\ddagger} -\frac{B_ {-k} }{2} \right){\mathfrak g}_ {k} (r), \end{align*}\] <p>where $r = (x,y)$. We adopt the convention</p> \[B^{\ddagger}_ {k} = \frac{B^{\dagger}_ {k} }{2\omega _ {k} }.\] <p>This helps us to switch between different conventions for the field expansion.</p> <p>We have omitted the vector sign (or bold font) in $r$ since it would not raise any misunderstanding. We assume (quite reasonably) the separation of variables $x$ and $y$ for 2D normal modes ${\mathfrak g}(r)$,</p> \[{\mathfrak g}(r) = {\mathfrak g}_ {x}\times g_ {y},\quad {\mathfrak g}_ {x} = \text{kink normal modes},\, {\mathfrak g}_ {y} = \text{plane waves.}\] <p>The quantization in terms of $\phi$ and $\pi$ reads</p> \[[\phi(r),\pi(r')] = i\delta^{(d)}(r-r').\] <p>This represents the fundamental quantization relation, unaffected by the selection of sectors. We haven’t given a formal definition of sectors, roughly speaking, within each sector, there exists a distinct set of normal modes for expanding both $\phi$ and $\pi$. Each mode must conform to the aforementioned relation, namely the quantization relation given in space-time positions $r$. Ultimately, the difference across different sectors lies in the diverse backgrounds (regarded as classical functions) used for field expansion. However, as we are analyzing the same theory within the same space-time, the theory should be quantized only once, and, all sectors must consistently align with the same quantization process.</p> <h2 id="22-renormalization-methods-review">2.2 Renormalization methods review</h2> <p>Things does get more complicated when renormalization is included. The commonly used renormalization method include (but not limited to):</p> <ol> <li><code class="language-plaintext highlighter-rouge">Dimensional regularization</code>. This is particularly useful in 4D. This technique regularizes integrals by analytically continuing the number of dimensions of space $d$, usually reduce it by an infinitesimal quantity $\epsilon$. It preserves the gauge symmetry and Lorentz invariance.</li> <li><code class="language-plaintext highlighter-rouge">Cutoff regularization</code>. This method introduces a high-energy cutoff in the energy. By doing this we are making the momentum integrals finite by force, but as a price we would introduce a new parameter $\Lambda$ with the dimension of energy. This $\Lambda$ has profound physical consequence, such as in dimensional transmutation and, most importantly, the Wilsonian RG flow.</li> <li><code class="language-plaintext highlighter-rouge">Pauli-Villars Regularization.</code> This approach involves adding hypothetical heavy particles to the theory to cancel out the divergences. The mass of the heavy particles acts effectively as the cutoff in the integrals.</li> <li><code class="language-plaintext highlighter-rouge">Lattice reguglarization</code>. This method discretizes the spacetime into a lattice. Computations are performed on this lattice, the spacing between different sites, or the resolution of the lattice serves as a cutoff.</li> </ol> <p>There also exist tons of other renormalization methods, such as holographic renormalization, Hopf algebra renormalization, Connes-Kreimer renormalization, and so on. They sounds like fun but, unfortunately, they lie beyond the scope of our work and my comprehension.</p> <p>Different renormalization methods may yield different results depending on the specific sector they are applied to. For instance, the momentum cutoff method produces distinct outcomes when used in the trivial vacuum sector compared to the one-kink sector. From my perspective, it seems more appropriate to <em>consider the regularization method as an integral part of the quantization process</em>. Quantization itself is not a natural, free functor (just means a ); whether one opts for geometrical quantization or path integral quantization (among other methods), additional elements are always needed. For example, incorporating a momentum cutoff is often a crucial step.</p> <p>Different sectors also seem to give different perspectives regarding the relations between different regularization methods. For example, in trivial vacuum sector (which we will just call vacuum sector), the momentum cutoff method is closely connected to the lattice renormalization method, by roughly $\Lambda = 2\pi / a$ where $a$ is the lattice spacing. This very relation is harder to see in, for instance, one-kink sector (which we will just call kink sector).</p> <p>A natural question that follows is, if there exists a regularization method that applies to all the sectors (unlike the momentum cutoff renormalization) equally well? If so, which one? Apparently the space-time itself is the common ground of all sectors, since the lattice regularization is dependent on spacetime alone, it is independent of the specific mode-expansion we adopt for fields, so it seems reasonable to use it as the renormalization scheme. Other advantages of lattice renormalization includes, 1) it made obvious the Wilsonian RG flow, which is continuous change of various parameters in the theory depending on a continuous change of the lattice size. 2) The connection between the lattice quantization and momentum-cutoff renormalization is already know in the trivial vacuum sector. We just need to find a way to generalize it to other sectors.</p> <h2 id="23-lattice-quantization">2.3. Lattice quantization</h2> <p>In our work we will start with the lattice quantization. Recall that the canonical quantization relation in a continuous $d$-dimensional spacetime reads</p> \[[\phi(r),\pi(r')] = i \delta^{(d)} (r-r'), \tag{2.1}\] <p>In lattice quantization, the continuous spacetime of the theory is replaced by a discrete set of points, and the fields are defined only at these points. It is a powerful, comprehensive change of viewpoint, not only of numerical importance, but really alters our view of spacetime. In principal we can translate all concepts we have defined in spacetime continuum into the lattice spacetime, such as the gauge connection, gauge field strength, etc. Sometimes, it is convenient to think the lattices as “sample points” of a spacetime continuum. In this view, between the lattice sites there maybe exists something other than the pure void, but it is meaning less to talk about them anyway. This view is rather useful when discussing the connection between lattice quantization and momentum cutoff quantization.</p> <p>The commutation relation on a lattice should take the form</p> \[[\phi_ {i},\pi_ {j}] =i\delta_ {ij}, \tag{2.2}\] <p>where $i,j$ are the indices of different sites.</p> <p>Recall that in canonical quantization, Eq. (2.1) translate to the momentum space rather trivially, yielding</p> \[[a_ {p},a^{\dagger}_ {p'}] = (2\pi)^{d} \delta^{(d)}(p-p').\] <p>So the question is, how does the lattice commutation relation translates to different sectors? Particularly, in the trivial vacuum sector and the one-kink sector?</p> <h3 id="231-latticization-of-scalar-field">2.3.1 Latticization of scalar field</h3> <p>Write the scalar field $\phi$ in continuum as as function of spacetime position $\phi(x)$, and write the lattice position as a label of the field $\phi_ {a^{\mu} }$, where $a^{\mu}=m^{\mu} a$, $m^{\mu}\in \mathbb{N}^{d}$ is the $d$-dimensional count of the lattice site, $a$ is the lattice distance. The summation on lattices goes to integral in the continuous limit with the following dictionary,</p> \[\begin{align*} \sum_ {m^{d} } &amp;\to \int, \\ a^{d} &amp;\to d^{d}x,\\ f_ {a^{\mu} } &amp;\to f(x) \end{align*}\] <p>where $m^{d}$ indicates the $d$-dimensional lattice. Combined together we have the familiar formula</p> \[\sum_ {m^{d} } a^{d} f_ {a^{\mu} } \to \int d^{d}x \, f(x).\] <p>We can define two types of differences, corresponding to derivatives in the continuous case</p> \[\begin{align*} \partial_ {\mu} f_ {x} &amp;= \frac{1}{a} (f_ {x+\hat{a}^{\mu} }-f_ {x}),\\ \partial'_ {\mu} f_ {x} &amp;= \frac{1}{a} (f_ {x}-f_ {x-\hat{a}^{\mu} }), \end{align*}\] <p>where $\hat{a}^{\mu}$ is a vector in direction $x^{\mu}$ of length $a$, namely $\hat{a}$ moves to the next lattice site in the $x^{\mu}$ direction. These two differences are like differences “from right” and “from left”. For smooth functions of course their continuous limit all give the derivative.</p> <p>The interesting thing is that these two types of differences are kind of dual to each other. Define an inner product</p> \[(f_ {x},g_ {x}) := \sum_ {x} f_ {x} g_ {x},\] <p>similar to the case of differential forms, then</p> \[(f_ {x},\partial g_ {x}) = (-\partial' f_ {x},g_ {x}),\] <p>which corresponds to the integral by part</p> \[\int \, f\partial g = \int \,(-\partial f) g.\] <h3 id="232-second-quantization-in-the-vacuum-sector">2.3.2 Second quantization in the vacuum sector</h3> <p>First let’s review how the second quantization is achieved in the vacuum sector without latticization, namely on a continuum of spacetime of dimension $d+1$. This is what we learnt from textbooks.</p> <p>Note that we will adopt a slight change of variable here. Before we used $\vec{r}$ to denote the spatial vector where $\vec{r}=(x,y, \cdots)$ since in two dimensional space time, it is easier to write $x,y$ than $x_ {1},x_ {2}$. However, since now we are dealing with $d+1$ dimensional spacetime, we will also adopt a different convention, namely $x=(t,\vec{x})$ where $\vec{x}$ is a $d$-dimensional vector with components $x_ {1},\cdots,x_ {d}$. In summary, Latin letters without a vector sign $x$ are covariant $d+1$-vectors, the generalization of $4$-vector to arbitrary dimension.</p> <p>The simplest special relativistic equation of motion a field $\phi$ can satisfy is the massless Klein-Gordon equation,</p> \[\partial^{2}\phi=0,\quad \partial^{2}=\partial_ {\mu}\partial^{\mu}=\partial_ {t}^{2}-\nabla^{2}.\] <p>Decompose $\phi$ into a continuum of momentum mode, each mode can be written as</p> \[a_ {p} (t) e^{ i\vec{p}\cdot \vec{x} }\] <p>where we have assume the separation of variable $t$ and $\vec{x}$, and the time-dependent part of the Klein-Gordon equation gives</p> \[(\partial_ {t}^{2}+\vec{p}\cdot \vec{p})a_ {p} (t) = 0\] <p>with solutions</p> \[a_ {p} (t) =a_ {p} e^{ \pm i\omega t},\quad \omega^{2}= \vec{p}^{2}.\] <p>where $a_ {p}$ now is just some c-number, constant in time. The field can be expanded as a linear combination of all the momentum modes</p> \[\phi(t,\vec{r}) = \int \frac{d^{d}p}{(2\pi)^{d} } \, (a_ {p} e^{ -ipx }+a_ {p} ^{\ast }e^{ipx })\] <p>where the second term in the parenthesis is to make sure that $\phi$ is real. We have assembled $\omega$ and $\vec{p}$ into a Lorentzian vector $p=(\omega,\vec{p})$.</p> <p>Now the second quantization. This is usually first done in the momentum space, since we are treating each mode as a harmonic oscillator. You can already see one aspect of the difference between the previously define lattice quantization and the textbook second quantization. We introduce the <code class="language-plaintext highlighter-rouge">equal-time commutation relation</code></p> \[[a_ {k} ,a_ {p}^{\dagger}] = (2\pi)^{d} \,\delta^{d}(\vec{k}-\vec{p}),\] <p>where we have omitted the vector sign in the superscript $k,p$ to avoid overly cumbersome notation. The factors of $2\pi$ are a convention, stemming from our convention for Fourier transform, for the details see my other blog on conventions.</p> <p>We want the operators $a_ {p}^{\dagger}$ to create particles with momentum $\vec{p}$. Let $\left\lvert{\vec{p} }\right\rangle$ be a physical state with a single particle with momentum $\vec{p}$, <em>define</em></p> \[a_ {p} ^{\dagger}\left\lvert{\Omega}\right\rangle = \frac{1}{\sqrt{ 2\omega _ {p} } }\left\lvert{\vec{p} }\right\rangle ,\] <p>where $\left\lvert{\Omega}\right\rangle$ is the ground state in the vacuum sector. This factor of $1 / \sqrt{ 2\omega _ {p} }$ is just another convention.</p> <p>From the normalization $\left\langle \Omega \middle\vert \Omega \right\rangle=1$ and the commutation of $a,a^{\dagger}$ we get</p> \[\left\langle \vec{p} \middle\vert \vec{k} \right\rangle =2\omega _ {p} (2\pi)^{d}\delta^{d}(\vec{p}-\vec{k}).\] <p>As a result, the identity operator for one particle states is</p> \[\mathbb{1}=\int \frac{d^{d}p}{(2\pi)^{d} } \, \frac{1}{2\omega _ {p} }\left\lvert{\vec{p} }\right\rangle \left\langle{\vec{p} }\right\rvert .\] <p>We define the quantum field as integrals over $a_ {p}$ and $a_ {p}^{\dagger}$,</p> \[\phi_ {0}(\vec{x})= \int \frac{d^{d}p}{(2\pi)^{d} } \, \frac{1}{\sqrt{ 2\omega } } (a_ {p} e^{ i\vec{p} \cdot \vec{x} } + a_ {p} ^{\dagger}e^{ -i\vec{p}\cdot \vec{x} })\] <p>where the subscript $0$ indicates this is a free field. The traditional view is to take it as the definition of the field operator $\phi_ {0}$ constructed from the creation and annihilation operators $a_ {p}$ and $a_ {p}^{\dagger}$ (<em>Schwartz M.D.</em>), but we shall try an opposite viewpoint, as will be shown later.</p> <p>Later we will work with the Schrodinger picture which is less commonly used compared to the Heisenberg or interaction pictures. To finish the review on the second quantization, we just mentioned that in Heisenberg picture, all the time dependence is in operators such as $\phi$ and $a_ {p}$, the field operator reads</p> \[\phi_ {0}(\vec{x},t) = \int \frac{d^{d}p}{(2\pi)^{d} } \, \frac{1}{\sqrt{ 2\omega _ {p} } }(a_ {p} e^{ -ipx } + a_ {p} ^{\dagger}e^{ ipx }),\] <p>which is <em>not</em> Lorentz invariant.</p> <hr/> <p>Instead of the traditional view of harmonic oscillator quantization in the momentum space, let’s take Eq.(2) as the starting point. Adopt a somewhat un-conventional field expansion</p> \[\begin{align*} \phi(\vec{x}) &amp;= \int \frac{d^{d}p}{(2\pi)^{d} } \, e^{ -i\vec{x}\cdot \vec{p} }\phi_ {p},\\ \pi(\vec{x}) &amp;= \int \frac{d^{d}p}{(2\pi)^{d} } \, e^{ -i \vec{x}\cdot \vec{p} }\pi_ {p}. \end{align*}\] <p>The next question is how to inverse it on a lattice…</p> <h3 id="24-21-dimensional">2.4 (2+1)-dimensional</h3> <p>If we adopt (1) the ultraviolet cutoff $\Lambda$ and (2) the harmonic quantization <em>in the trivial vacuum sector</em> and translate it to the physical spacetime. As a result we get a non-local commutation relation in spacetime,</p> \[[\phi(x),\pi(y)]_ {\Lambda} = i \int_{-\Lambda}^{\Lambda} \frac{dp}{2\pi} \, e^{ -ip(x-y)} = i \frac{\sin(\Lambda x)}{\pi x} .\] <p>This is non-local in the sense that for certain $x\neq y$ the commutator is nonzero. However, this commutation relation does agree with the lattice picture, if we set the lattice spacing to be $\pi / \Lambda$! (check for yourself)</p> <p>But for now let’s forget about the momentum cutoff and work with a more general picture. We can define the normal ordering in the vacuum sector, then propagate it to the kink sector via the displacement operator, which we will talk about shortly.</p> <p>Recall the equation of motion reads</p> \[\partial^{2}\phi-\frac{1}{\lambda} \frac{dV(\sqrt{ \lambda }\phi)}{d\phi}=0,\] <p>separate $\phi(\vec{x},t)$ into the kink background $f(\vec{x})$ and the fluctuation (time-dependent) ${\mathfrak g}$,</p> \[\phi(\vec{x},t) = f(\vec{x})+{\mathfrak g}(\vec{x},t),\] <p>insert it into the equation of motion and use the fact that $f(\vec{x})$ satisfies the time-independent EoM, we get the EoM for the fluctuation field:</p> \[\begin{align*} 0 &amp;= \partial^{2} (f+{\mathfrak g}) -\frac{1}{\lambda} \frac{d V(\sqrt{ \lambda }(f+{\mathfrak g}))}{d\phi} \\ &amp;= \partial^{2} f + \partial^{2}{\mathfrak g}-\frac{1}{\lambda} \frac{d}{d\phi}\left( V( \sqrt{ \lambda } f)+{\mathfrak g} \frac{dV(\sqrt{ \lambda }\phi)}{d\phi} \mid_ {\phi=f} + \mathcal{O}({\mathfrak g^{2} }) \right) \\ &amp;= \partial^{2} f - \frac{1}{\lambda} \frac{dV(\sqrt{\lambda} f)}{df} + \partial^{2}{\mathfrak g}-\frac{ {\mathfrak g} } {\lambda} \frac{d^{2}V(\sqrt{\lambda}f)}{d f^{2} } +\mathcal{O}( {\mathfrak g}^{2} ) \\ &amp;= \partial^{2} {\mathfrak g}+V^{(2)}(\sqrt{ \lambda }f) + \mathcal{O}({\mathfrak g}^{2}), \end{align*}\] <p>where</p> \[V^{(n)} := \frac{d^{n}V(\tilde{\phi})}{d\tilde{\phi}^{n} },\quad \tilde{\phi}:=\sqrt{ \lambda }\phi.\] <hr/> <p>In the below are some results needed for the derivation, not carefully organized in a readable order. I will tidy it up later.</p> <p>The commutation relation reads</p> \[[\phi(\vec{x}_ {1}),\pi(\vec{x}_ {2})] = i \delta^{d}(\vec{x}_ {1}-\vec{x}_ {2}),\] <p>together with the decomposition we have</p> \[\begin{align*} [\phi_ {p_ {1} },\pi_ {p_ {2} }] &amp;= i(2\pi)^{d}\delta^{d}(p_ {1}+p_ {2}),\\ [\phi_ {k_ {1} },\pi_ {k_ {2} }] &amp;= i(2\pi)^{d}\delta^{d}(k_ {1}+k_ {2}), \end{align*}\] <p>note the plus sign instead of minus in the parenthesis.</p> <p>The decomposition into ladder operators reads</p> \[\begin{align*} \phi_ {p} &amp;= A^{\ddagger}_ {p}+\frac{A_ {-p} }{2\omega_ {p} },\quad \pi_ {p}=i\omega_ {p}A^{\ddagger}_ {p}- \frac{iA_ {-p} }{2},\quad A^{\ddagger}_ {p} = \frac{A^{\dagger}_ {p} }{2\omega _ {p} },\\ \phi_ {k} &amp;= B^{\ddagger}_ {k}+\frac{B_ {-k} }{2\omega_ {k} },\quad \pi_ {k}=i\omega_ {k}B^{\ddagger}_ {k}- \frac{iB_ {-k} }{2},\quad B^{\ddagger}_ {k} = \frac{B^{\dagger}_ {k} }{2\omega _ {k} }. \end{align*}\] <p>The commutation relation in terms of those reads</p> \[[B_ {k_ {1} },B^{\ddagger}_ {k_ {2} }] = (2\pi)^{d}\delta^{d}(k_ {1}-k_ {2}).\] <p>Note the minus sign. It is exactly the same as what we’ve got in the trivial vacuum sector where</p> \[[A_ {p_ {1} },A^{\ddagger}_ {p_ {2} }] = (2\pi)^{d}\delta^{d}(p_ {1}-p_ {2}).\] <p>Expand $A,A^{\ddagger}$ in terms of $B,B^{\ddagger}$ we have</p> \[\begin{align*} A^{\ddagger}_ {p} &amp;= \sum\!\!\!\!\!\!\!\!\int \; \frac{d^{d}k}{(2\pi)^{d} } \, \frac{\tilde{ {\mathfrak g} }_ {k}(-\vec{p})}{2\omega_ {p} } \left[ (\omega_ {p} +\omega _ {k} )B_ {k} ^{\ddagger}+(\omega _ {p} -\omega _ {k} )\frac{B_ {-k} }{2\omega_ {k} } \right] ,\\ A_ {-p}&amp;= \sum\!\!\!\!\!\!\!\!\int \; \frac{d^{d}k}{(2\pi)^{d} } \, \tilde{ {\mathfrak g} }_ {k}(-\vec{p}) \left[ (\omega _ {p} -\omega _ {k} ) B_ {k} ^{\ddagger}+(\omega _ {p} +\omega _ {k} )\frac{B_ {-k} }{2\omega _ {k} } \right]. \end{align*}\] <p>The idea is to</p> <ol> <li>write terms in the Hamiltonian in normal order with respect to $A,A^{\ddagger}$,</li> <li>use the above relation to rewrite it in terms of $B,B^{\ddagger}$,</li> <li>use the commutation relation to diagonalize $H$ in terms of $B,B^{\ddagger}$.</li> </ol> <p>The normal ordered leading order Hamiltonian (in kink sector) reads</p> <p>\(H'_ {2} = A+B+C\) where</p> \[\begin{align*} A &amp;= \frac{1}{2} \int d^{d}x \, :\pi^{2}(\vec{x}):_ {a}, \\ B &amp;= \frac{1}{2} \int d^{d}x \, :(\nabla\phi(\vec{x}))^{2}:_ {a}, \\ C &amp;= \frac{1}{2} \int d^{d}x \, V^{(2)} (\sqrt{ \lambda }f) :\phi^{2}(\vec{x}):_ {a}. \end{align*}\] <p>In plane-wave space and $\vec{k}$-space we have</p> \[\begin{align*} A &amp;= \frac{1}{2} \int\frac{d^{d}p}{(2\pi)^{d}} \, :\pi_ {p}\pi_ {-p}:_ {a} , \\ B+C &amp;= \frac{1}{2} \sum\!\!\!\!\!\!\!\!\int \;\frac{d^{d}k}{(2\pi)^{d}} \, \omega^{2}_ {k} \int \frac{d^{d}p_ {1}}{(2\pi)^{d}} \int \frac{d^{d}p_ {2}}{(2\pi)^{d}} \, \tilde{ {\mathfrak g}}_ {-k}(\vec{p}_ {1}) \tilde{ {\mathfrak g}}_ {k}(\vec{p}_ {2}) :\phi_ {p_ {1}}\phi_ {p_ {2}}:_ {a}. \end{align*}\] <hr/> <p>Define the Fourier transformation $\tilde{f}$ of function $f(x)$ to be</p> \[\tilde{f}(\vec{p}) := \int d^{d}x \, f(\vec{x})e^{ -i\vec{p}\cdot \vec{x} }.\] <p>The Fourier transformation of normal modes reads</p> \[\tilde{ {\mathfrak g} }_ {k}(p) = \int d^{d} x \, {\mathfrak g}_ {k}( \vec{x} ) e^{-i\vec{p} \cdot \vec{x} }\] <p>which satisfies relation</p> \[\tilde{ {\mathfrak g} }_ {k}^{\ast}(\vec{p}) = \tilde{ {\mathfrak g} }_ {-k}(-\vec{p}) .\] <p>Sometime this relation can help to make the numerical calculation easier.</p> <p>The normalization relations for ${\mathfrak g}$ reads</p> \[\begin{align*} \int d^{d}x \, {\mathfrak g}^{\ast }_ {k}(\vec{x}){\mathfrak g}_ {k'}(\vec{x}) &amp;=(2\pi)^{d}\delta ^{d}(\vec{k}-\vec{k}'), \\ \int \frac{d^{d}p}{(2\pi)^{d} } \tilde{ {\mathfrak g} }_ {k}(\vec{p}) \tilde{ {\mathfrak g} }_ {k'}(\vec{p}) &amp;= (2\pi)^{d}\delta ^{d}(\vec{k}+\vec{k}'), \\ \int \frac{d^{d}p}{(2\pi)^{d} } \tilde{ {\mathfrak g} }_ {k}(\vec{p}) \tilde{ {\mathfrak g} }^{\ast }_ {k'}(\vec{p}) &amp;= (2\pi)^{d}\delta ^{d}(\vec{k}-\vec{k}') , \end{align*}\] <p>and the completeness condition (in both spacetime and momentum space)</p> \[\begin{align*} \int \frac{d^{d}k}{(2\pi)^{2}} \, {\mathfrak g}_ {k}(\vec{x}) {\mathfrak g}^{\ast }_ {k}(\vec{y}) &amp;=\delta ^{d}(\vec{x}-\vec{y}), \\ \sum\!\!\!\!\!\!\!\!\int \; \frac{d^{d}k}{(2\pi)^{d}} \, \tilde{ {\frak g} }_ {k}(\vec{p}_ {1}) \tilde{ {\frak g} }^{\ast} _ {k}(\vec{p}_ {2}) &amp;= (2\pi)^{d} \delta^{d}(\vec{p}_ {1} - \vec{p}_ {2}). \end{align*}\] <p>Note that in our convention of Fourier transformation, instead of $+i\vec{p}\cdot \vec{x}$ we have minus sign. This is only the spatial part of the Fourier transformation (recall that our spacetime is $d+1$ dimensional).</p> <hr/> <p>We have divided the leading order, kink-sector Hamiltonian into $A+B+C$ three parts, in normal order we have</p> \[\begin{align*} A &amp;=\frac{1}{2} \int \frac{d^{d}p}{(2\pi)^{d} } \, \left( -\omega^{2}_ {p} A^{\ddagger}_ {p} A^{\ddagger}_ {-p}+ \omega _ {p} A^{\ddagger}_ {p} A_ {p} -\frac{1}{4} A_ {-p}A_ {p} \right), \\ B+C &amp;= \frac{1}{2} \sum\!\!\!\!\!\!\!\!\int \;\frac{d^{d}k}{(2\pi)^{d} } \int \frac{d^{d}p}{(2\pi)^{d} } \frac{d^{d}p'}{(2\pi)^{d} } \,\omega_ {k}^{2}\, \tilde{ {\mathfrak g} }_ {-k}(\vec{p} )\tilde{ {\mathfrak g} }_ {k}(\vec{p}') \\ &amp;\;\;\;\; \times \left[ A^{\ddagger}_ {p }A^{\ddagger}_ {p'} + \frac{A^{\ddagger}_ {p }A_ {-p'} }{2\omega_ {p'} } + \frac{A^{\ddagger}_ {p'}A_ {-p } }{2\omega_ {p } } + \frac{A_ {-p } }{2\omega_ {p } } \frac{A_ {-p'} }{2\omega_ {p'} } \right]. \end{align*}\] <p>In the calculation just keep in mind the property of $\tilde{ {\mathfrak g}}$ and invariance under combined exchange $p \leftrightarrow p’$ with $k \to -k$.</p> <p>The following relations are useful in derivation, with terms that do not annihilate the kink ground state $\left\lvert{0}\right\rangle$:</p> \[\begin{align*} A^{\ddagger}_ {p} A^{\ddagger}_ {p'} &amp;\cong \sum\!\!\!\!\!\!\!\!\int \frac{d^{d}k}{(2\pi)^{d} } \frac{d^{d}k'}{(2\pi)^{d} } \frac{\tilde{ {\mathfrak g} }_ {k}(-\vec{p}) \tilde{ {\mathfrak g} }_ {k'}(-\vec{p}')}{2\omega_ {p} 2\omega_ {p'} } \\ &amp;\;\;\;\;\; \times \left( (\omega _ {p} +\omega _ {k} )(\omega_ {p'}+\omega_ {k'})B^{\ddagger}_ {k} B^{\ddagger}_ {k'}+ \frac{1}{2\omega _ {k} }(\omega _ {p} -\omega _ {k} )(\omega_ {p'}+\omega_ { {k'} }) B_ {-k} B^{\ddagger}_ {k'}) \right), \\ A^{\ddagger}_ {p} A_ {-p'} &amp;\cong \sum\!\!\!\!\!\!\!\!\int \frac{d^{d}k}{(2\pi)^{d} } \frac{d^{d}k'}{(2\pi)^{d} } \frac{1}{2\omega_ {p} } \tilde{ {\mathfrak g} }_ {k}(-\vec{p})\tilde{ {\mathfrak g} }_ {k'}(-\vec{p}') \\ &amp;\;\;\;\;\; \times \left( (\omega _ {p} +\omega _ {k} )(\omega_ {p'}-\omega_ {k'})B^{\ddagger}_ {k} B^{\ddagger}_ {k'}+ \frac{1}{2\omega _ {k} } (\omega _ {p} -\omega _ {k} )(\omega_ {p’}-\omega_ { {k'} }) B_ {-k}B^{\ddagger}_ {k'}) \right), \\ A_ {-p} A_ {-p'} &amp;\cong \sum\!\!\!\!\!\!\!\!\int \frac{d^{d}k}{(2\pi)^{d} }\frac{d^{d}k'}{(2\pi)^{d} } \tilde{ {\mathfrak g} }_ {k}(-\vec{p})\tilde{ {\mathfrak g} }_ {k'}(-\vec{p}')\\ &amp;\;\;\;\;\; \times \left( (\omega _ {p} -\omega _ {k} )(\omega_ {p'}-\omega_ {k'})B^{\ddagger}_ {k} B^{\ddagger}_ {k'} + \frac{1}{2\omega _ {k} }(\omega _ {p} +\omega _ {k} )(\omega_ {p'}-\omega_ { {k'} }) B_ {-k}B^{\ddagger}_ {k'}) \right). \\ \end{align*}\] <p>We just need to keeps the terms surviving acting on $\left\lvert{0}\right\rangle$, namely the terms that are proportional to $B^{\ddagger} B^{\ddagger}$ and $B B^{\ddagger}$.</p> <p>The part in $A$ and $B+C$ that are proportional to $B^{\ddagger}B^{\ddagger}$ reads</p> \[\begin{align*} A &amp;\supset \frac{1}{8} \int \frac{d^{d}p}{(2\pi)^{d} } \sum\!\!\!\!\!\!\!\!\int \frac{\;d^{d}k}{(2\pi)^{d} } \frac{d^{d}k'}{(2\pi)^{d} } \tilde{ {\mathfrak g} }_ {k}(-\vec{p})\tilde{ {\mathfrak g} }_ {k'}(\vec{p}) B_ {k}^{^{\ddagger} } B_ {k'}^{\ddagger} \\ &amp;\;\;\;\;\; \times (-4\omega _ {k} \omega_ {k'} + 2\omega _ {p} \omega _ {k} -2\omega _ {p} \omega_ {k'}) \\ &amp;= - \frac{1}{2} \sum\!\!\!\!\!\!\!\!\int \, \frac{d^{d}k}{(2\pi)^{d} } \, \omega^{2}_ {k}B^{\ddagger}_ {k} B^{\ddagger}_ {-k}, \\ B+C &amp;\supset \frac{1}{2} \sum\!\!\!\!\!\!\!\!\int \; \frac{d^{d}k}{(2\pi)^{d} } \frac{d^{d}k_ {1} }{(2\pi)^{d} } \frac{d^{d}k_ {2} }{(2\pi)^{d} } \int \frac{d^{d}p_ {1} }{(2\pi)^{d} } \frac{d^{d}p_ {2} }{(2\pi)^{d} } \\ &amp; \;\;\;\;\; \times \omega^{2}_ {k} \, \tilde{\mathfrak g}_ {k_ {1} }(-\vec{p}) \tilde{\mathfrak g}_ {k_ {2} }(-\vec{p}') \tilde{\mathfrak g}_ {-k}(\vec{p}) \tilde{\mathfrak g}_ {k}(\vec{p}') B^{\ddagger}_ {k_ {1} }B^{\ddagger}_ {k_ {2} } \\ &amp;= \frac{1}{2} \sum\!\!\!\!\!\!\!\!\int \frac{d^{d}k}{(2\pi)^{d} } \,\omega _ {k} ^{2} B^{\ddagger}_ {k}B^{\ddagger}_ {-k} . \end{align*}\] <p>Note that the last two terms in the second line cancel each other since $k,k’$ are dummy indices, we can simply exchange them. We have used the normalization condition for $\tilde{ {\mathfrak g} }$ functions. Apparently, put together, in $A+B+C$ the terms proportions to $B^{\ddagger}B^{\ddagger}$ disappear! We are left with terms proportional to $B B^{\ddagger}$ only, which can be obtained similarly. We will omit the explicit result here since it can be easily found in other papers.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="kink"/><summary type="html"><![CDATA[1 Introduction]]></summary></entry><entry><title type="html">Domain Wall</title><link href="https://baiyangzhang.github.io/blog/2024/Domain-Wall/" rel="alternate" type="text/html" title="Domain Wall"/><published>2024-03-06T00:00:00+00:00</published><updated>2024-03-06T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Domain-Wall</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Domain-Wall/"><![CDATA[<h1 id="kink-in-11-dimension">Kink in 1+1 dimension</h1> <p>Domain walls in theoretical physics are fascinating phenomena that arise in a variety of contexts, from condensed matter physics to cosmology. They are a type of topological soliton that occurs when a discrete symmetry is spontaneously broken. In simpler terms, domain walls can be thought of as boundaries between different phases or domains where the order parameter (some scalar degree of freedom) differs on either side.</p> <p>When kink solutions are placed in more than one spatial dimension, they become extended planar structures called “domain walls.” For example, in $3+1$ dimension, a flat $\mathcal{Z}_ {2}$ domain wall in $yz$-plane is</p> \[\phi(t,x,y,z) = \eta\, \tanh\left( \sqrt{ \frac{\lambda}{2} } \, \eta x \right).\] <p>The energy density is given by that of $\mathcal{Z}_ {2}$ kink, the our convention it reads</p> \[\mathcal{E} = \frac{\lambda \eta^{4}}{2} \cosh^{-4}\left( \sqrt{ \frac{\lambda}{2} } \, \eta x \right)\] <p>since the Lagrangian is</p> \[\frac{1}{2} (\partial \phi)^{2} - \frac{\lambda}{4} (\phi^{2} - \eta^{2})^{2}\] <p>and the $\mathcal{Z} _2$ kink solution (with center at $x=0$) reads</p> \[\phi_ {k} (x,t) = \eta \tanh\left( \sqrt{ \frac{\lambda}{2} } \eta x \right).\] <p>The new aspects of domain walls are that they can be curved and deformations can propagate along them. Another feature of the planar domain wall is that it is invariant under boosts in the plane parallel to the wall. This is simply because the solution is independent of t, y and z and any transformations of these coordinates do not affect the solution.</p> <hr/> <p>The characteristic length of a kink is roughly speaking inverse to some mass, which turns out to be the mass of the fluctuating field in the kink sector $1 / \eta\sqrt{ \lambda }$. For distance scale much larger than that of a kink, we can regard it as a point particle. Recall that in the context of general relativity, the true trajectory of a particle through spacetime is such that it extremizes the proper time. In curved spacetime the trajectory (geodesic) followed by a free particle <em>maximizes the proper time</em> compared to nearby paths. Particles are “<em>lazy</em>” in the sense that they try to maximize the time spent to get to point B from point A. The action can be written as</p> \[S_ {1+1} = -M \int d\tau , \tag{1}\] <p>the superscript $1+1$ denotes the dimension.</p> <p>The proper time is also the line element of the world line of the particle, let $X^{\mu}$ be the coordinates of the kink center, we can write</p> \[d\tau^{2} = g_ {\mu \nu} \left( \frac{dX^{\mu}}{dt} \right) \left( \frac{dX^{\nu}}{dt} \right) dt^{2}.\] <hr/> <p>The question is, can we derive the action Eq. (1) from the original action for $\phi$ field, of which the kink is a solution? Such a derivation should lead to Eq. (1) plus corrections that depend on the internal structure of the kink. Then Eq. (1) would be the effective action of kink.</p> <p>The key assumption to derive the effective action is that, the field profile of a kink is well-approximated by that of the <em>static kink solution</em> in the <code class="language-plaintext highlighter-rouge">instantaneous rest frame</code>. Recall that, if the kink were to be given a velocity $v$, it would be Lorentz boosted to a moving frame. In that case, <em>the instantaneous rest frame would refer to the original frame before the Lorentz boost</em>, where the kink solution is static. Lorentz boosting the kink solution involves applying a Lorentz transformation to the coordinates, which would modify the form of kink solution $\phi_ {k}$ to account for the relativistic effects of motion, but in its own rest frame, the kink solution retains the static form given above.Generally speaking, the instantaneous rest frame of a moving object is a reference frame in which the object is at rest at a particular instant in time. This frame is also known as the object’s <code class="language-plaintext highlighter-rouge">comoving frame</code>or <code class="language-plaintext highlighter-rouge">proper frame</code>. In this frame, the laws of physics take their simplest form, just like they would in a frame where the particle was always at rest.</p> <p>Let’s work in the <em>kink frame coordinates</em> which are denoted by $y^{a} = (\tau,\xi)$, $a=0,1$. $\tau$ is also called the kink world-line coordinate. These coordinates are functions of the background coordinates that are denoted by $x^{\mu}=(t,x)$, $\mu=0,1$. The kink world line is the world line of the kink center, denoted by $X^{\mu}=(t,X(t))$. Therefore the tangent vector to the world line is</p> <p>\(T^{\mu} \propto \partial_ {t} X^{\mu} = \left( 1, V \right), \quad V:= \frac{ \partial X }{ \partial t }\) and $T$ is normalized to $T^{2}=1$.</p> <p>The unit vector $\hat{N} = N^{\mu}(\tau)$ orthogonal to $\hat{T} = T^{\mu}$ is found by solving</p> \[g_ {\mu \nu} T^{\mu} N^{\nu} = 0, \quad N^{\mu}N_ {\mu}=N^{2} = -1.\] <p>Note the normalization of $N$ is $-1$ since it is time-like. Since we are working in 2D, there is only one $N^{\mu}$. In the special case of a Minkowski background we choose $g_ {\mu \nu} = \eta_ {\mu \nu} = \text{diag}(1,-1)$. (This is gonna piss off some theorists I know) We find</p> \[T^{\mu} = \gamma(1,V),\quad N^{\mu} = \gamma(V,1),\quad \gamma = \frac{1}{\sqrt{ 1-V^{2} }}.\] <p>The coordinate axis $\tau$ is along $T^{\mu}$ and $\xi$ is along $N^{\mu}$. Therefore, if we arbitrarily choose a time $\tau_ {0}$ when the kink is located at spacetime point $X_ {0}$, then near the kink any other spacetime point coordinate can be written as</p> \[x^{\mu} = X_ {0}^{\mu} + \tau \hat{T}_ {0} + \xi \hat{N}_ {0},\] <p>where $\hat{T}_ {0}$ and $\hat{N}_ {0}$ are the tangent and normal unit vector at time $\tau_ {0}$. However, we can always choose a time $\tau$ at which the coordinate in $\hat{T}$ direction is zero (at least locally), when $x^{\mu}$ is not too far away from kink world line. It is illustrated in the following figure, which I shamelessly copied from Tanmay’s textbook.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/kinkWorldLine-480.webp 480w,/img/kink/kinkWorldLine-800.webp 800w,/img/kink/kinkWorldLine-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/kinkWorldLine.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Thick curve is the kink world line. The kink-frame coordinates $y^a=(\tau,xi)$ are defined in the instantaneous rest frame of the kink and functions of the background coordinates $x^\mu=(t,x)$. </div> <p>Generally speaking, given a curve $C:=p(t)$ parametrized by $t$ in manifold $\mathcal{M}$, the curve is a map from the parameter to the manifold,</p> \[C : \mathbb{R} \to \mathcal{M}.\] <p>Curve $p(t)$ defines a velocity vector $\dot{p}(t)$. Note that in a general manifold (including affine space), points and vectors are distinct concepts, each serving a different role, although they are closely related and often used together in geometric computations and theoretical discussions. The tangent vector really takes a set of parametrized points and turn them into something entirely different: vectors. Furthermore, vectors can be equivalently regarded as differentials operators. The philosophy can be roughly states as: we use functions to measure the space, and we use differential to measure vectors. For example, we use coordinate patches to denote different points on a manifold, but a coordinate patch is nothing but a map from (sub-) manifold to $\mathbb{R}^{n}$, hence is a real-valued function. A vector $V$ with components $V^{0},\cdots,V^{i}$ in basis $\left\lbrace x \right\rbrace$ can be regarded as a differential operator $V = V^{i} \partial_ {i}$.</p> <p>In our case, the curve $X(\tau)$ is parametrized by $\tau$, and the worldline of the kink (center) can be regarded as a <code class="language-plaintext highlighter-rouge">coordinate curve</code>, that is to say the tangent vector given by $\partial_ {\tau}$ is not the basis. Recall that the 2-dimensional coordinate system was defined as $y^{a}=(\tau,\xi)$, we can talk about the metric in terms of $y$’s, denoted as $h_ {ab}$, in contrast to $g_ {\mu \nu}$ in terms of $x$’s.</p> <p>However, the treatment of $(\tau,\xi)$ in Vachaspati’s textbook is a little weird, it seems to regard $X^{\mu}_ {0}$ (see the figure above) as some kink of origin, which is problematic in curved manifold. Maybe I missed something here? Anyways, here I will just treat $(\tau,\xi)$ as a (locally defined) coordinate system. We can also construct basis $\partial_ {\xi}$ such that their Lie bracket is zero,</p> \[[\partial_ {\tau},\partial_ {\xi}]=0\] <p>as is required for coordinate basis, this is locally equivalent to $\partial_ {\tau}$ being parallel to $\partial_ {\xi}$.</p> <p>We can go on and calculate the metric in terms of $y$ coordinates. Let $\left\langle X,Y \right\rangle$ be the inner product of vectors $X$ and $Y$, we have</p> \[h_ {00} := \left\langle \partial_ {\tau}, \partial_ {\tau} \right\rangle = \left\langle \partial_ {\tau}X,\partial_ {\tau}X \right\rangle = \left\langle (\partial_ {\tau}X^{\mu}) \partial_ {\mu},(\partial_ {\tau}X^{\nu}) \partial_ {\nu} \right\rangle = g_ {\mu \nu} (\partial_ {\tau}X^{\mu}) (\partial_ {\tau}X^{\nu}).\] <p>By construction we have</p> \[h_ {01} := \left\langle \partial_ {\tau},\partial_ {\xi} \right\rangle =0\] <p>and</p> \[h_ {11} := \left\langle \partial_ {\xi},\partial_ {\xi} \right\rangle = -1.\] <p>Next we need to write down the action. We assume the only import region is spacetime is that close to the kink world line, far from it the spacetime would be flat and the fluctuation would be a bunch of plane waves, it shouldn’t matter too much to the quantum correction of the domain wall. Thus we will only consider <em>the action given by a “band” around the kink world line.</em></p> <hr/> <p>In the context of a curved spacetime, particularly in a two-dimensional, the action for a scalar field can be described using the general form that incorporates the effects of curvature. The action $S$ for a scalar field $\phi$ in a curved spacetime can be written as:</p> \[S = \int d^2x \sqrt{-g} \left( -\frac{1}{2} g^{\mu\nu} \partial_\mu \phi \partial_\nu \phi - V(\phi) + \frac{1}{2} g R \phi^2 \right) \tag{2}\] <p>Here $d^2x \sqrt{-g}$ represents the <em>invariant differential volume element</em> in two-dimensional spacetime, $g^{\mu\nu}$ is the inverse metric tensor, used to raise indices. $g$ is a dimensionless coupling constant that describes the coupling of the scalar field to the Ricci scalar $R$, a scalar quantity that describes the curvature of spacetime and is obtained by contracting the Ricci tensor $R_{\mu\nu}$. The term $\frac{1}{2} \xi R \phi^2$ is known as the non-minimal coupling term. In two dimensions, the choice of $g$ can be particularly interesting due to the conformal properties of the spacetime. For now we will neglect the non-minimal coupling term.</p> <p>In our coordinates $y=(\tau,\xi)$ and metric is $h_ {ab}$, the determinant of $h_ {ab}$ reads</p> \[h := \det h = - g_ {\mu \nu} \partial_ {\tau}X^{\mu} \partial_ {\tau}X^{\nu}.\] <p>As before we want to study the effects of small fluctuation about the kink background, so we write</p> \[\phi = \phi_ {k} + \psi\] <p>where $\phi_ {k}$ is the classical kink solution, $\psi$ is the deviation from it. Upon quantization we will only quantize $\psi$ not $\phi_ {k}$. The action now follows from Eq. (2),</p> \[\begin{align*} S &amp;= \int d\tau d\xi \, \sqrt{ \left\lvert h \right\rvert } \mathcal{L}(\phi) \\ &amp;= \int d\tau d\xi \, \sqrt{ \left\lvert h \right\rvert } \mathcal{L}(\phi_ {k}+\psi) \\ &amp;= \int d\tau d\xi \, \sqrt{ \left\lvert h \right\rvert } \mathcal{L}(\phi_ {k}) + \int \frac{\delta^{2}S}{\delta \phi \delta \phi} \, \psi^{2} + \mathcal{O}(\psi^{3}) \\ &amp;= -M_ {\text{kink}} \int d\tau \, \sqrt{ \left\lvert h \right\rvert } + \mathcal{O}(\psi^{2}). \end{align*}\] <p>This is just we we have conjectured in Eq.(1), plus some higher order corrections! If the kink is moving relatively slowly, $\left\lvert h \right\rvert$ would be approximately $1$. Note that the negative kink mass comes from the integral</p> \[\int d \xi \, \mathcal{L} = \int d\xi \, (T-V) = -\int d\xi \, V =M_ {\text{kink}},\] <p>the first order of $\psi$ disappears due to the fact that $\phi_ {k}$ is a solution to the equation of motion.</p> <p>Note that the leading term in the effective action $\int d\tau$ is proportional to the world volume. This result can easily be extended to walls (and strings) propagating in higher dimensions. Even if the self-gravity of the domain wall is taken into account, the dominant contribution to the effective action is still the Nambu-Goto action.</p> <h1 id="walls-in-3--1-dimensions">Walls in 3 + 1 dimensions</h1> <p>In $3+1$ dimension, the position of domain wall, being a co-dimension one object in space, is described by $2+1$ dimensional coordinates. To be more specific, a domain wall in 3-dimensional space is a 2D plane, hence is parametrized by two parameters, call them $\xi^{1}$ and $\xi^{2}$. As a consequence the world-volume of such a domain wall is 2+1 dimensional hence should be parametrized by three parameters. This is reflected in how we describe the location of domain wall. Let the location of the domain wall be</p> \[X^{\mu} = X^{\mu}(y),\quad y = (\tau,\xi^{1},\xi^{2}),\] <p>$\tau$ is again the proper time, $\xi^{1,2}$ are spatial coordinates. $y$ is the coordinates on the domain-wall world-volume, similar to the case in $1+1$ dimension, with metric given by</p> \[h_ {ab} = g_ {\mu \nu}(X) \frac{ \partial X^{\mu} }{ \partial y^{a} } \frac{ \partial X^{\nu} }{ \partial y^{b} } .\] <p>The major difference between the kink in 1 + 1 dimensions and the domain wall is that the wall can be curved, and so the profile $\phi_ {\text{kink}}$ does not solve the equation of motion. The dynamics of a domain wall is much richer, for example, as the wall moves it accelerates and emits radiation.</p> <p>The Nambu-Goto action of domain wall can be derived in the same way as in 1+1 kink,</p> \[S_ {0} = - \sigma \int d^{3} y \, \sqrt{ \left\lvert h \right\rvert } \tag{3}\] <p>where $\sqrt{ \left\lvert h \right\rvert }$ comes from the invariant integral measure $\sqrt{ \left\lvert h \right\rvert } d^{3}y$, and $\sigma$ is the tension of the wall.</p> <p>From the effective action of the domain wall Eq.(3) we can derive the equation of motion for the wall. But before that we need to know how the variation of the determinant of a matrix $h$. There are two ways to achieve that. One can calculate $\delta \det h$ directly. Under the variation $h\to h+\delta h$ where $\delta h$ is assumed to be infinitesimal, we have</p> \[\begin{align*} \det (h+\delta h) &amp;= \det (h(\mathbb{1}+h^{-1} \delta h))\\ &amp;= \det h \times \det(\mathbb{1}+h^{-1} \delta h)\\ &amp;= \det h ( 1 + \mathrm{Tr}\,(h^{-1} \delta h) + \mathcal{O}(\delta h^{2}) )\\ &amp;= \det h + \det h \times \mathrm{Tr}\,(h^{-1} \delta h). \end{align*}\] <p>Hence</p> \[\delta \det h \equiv \det (h+\delta h)-\det(h) = \det h \times \mathrm{Tr}\,(h^{-1} \delta h).\] <p>We just mention on the fly that there is another formula</p> \[\det M = \exp(\mathrm{Tr}\,\ln M).\] <p>We could also start with this formula and arrive at the same result.</p> <p>Instead of giving the full derivation of the equation of notion, we only mention here that the equation is highly non-linear since $h_ {ab}$ itself is defined as a quadratic in derivative of $X^{\mu}$.</p> <h1 id="some-solutions">Some solutions</h1> <p>The effective action of domain wall, or Nambu-Goto action, is valid when the domain wall is not curved too much, and separate walls (or different parts of the same wall) are not placed too closely. Otherwise the single kink solution would not be a very good approximation. In addition, in the center of mass frame, the velocity of the wall should be relatively small. If these conditions are met, we can use Nambu-Goto effective action to study the properties of walls.</p> <p>Recall that we use $y^{a}, a=0,1,2$ for the parametrization of the world volume of the domain wall. (We will not dwell on the difference between parametrization and coordination here, but we should be aware.) In components $y=(\tau,\xi^{1},\xi^{2})$. The Minkowski metric is defined as $\eta=(1,-1,-1,-1)$.</p> <p>A domain wall flat in $x-y$ direction is described by</p> \[X^{\mu} = (X^{0}=t, X^{1}=\xi^{1}, X^{2} = \xi^{2}, X^{3}=0).\] <p>Next consider a planar domain wall with some “ripples” in $3$-direction, then $X^{3}$ is a function of $y$’s just as $X^{1}$ and $X^{2}$. Write</p> \[X^{\mu} = (X^{0}=t, X^{1}=\xi^{1}, X^{2} = \xi^{2}, X^{3}=X^{3}(y)).\] <p>The metric is given by</p> \[h_ {ab} = \eta_ {\mu \nu} \partial_ {a}X^{\mu} \partial_ {b}X^{\nu} = \eta_ {ab} - \partial_ {a} X^{3} \partial_ {b}X^{3}\] <p>Then we can calculate the inverse of $h_ {ab}$, insert it into the equation of motion, and solve it. Of course it is easier said then done, here we only mention some of the results. One class of solutions found by Friedlander etc. in 1976 corresponds to a pulse of arbitrary shape on a planar domain wall that propagates in the $+X^{1}$ direction <em>at the speed of light</em>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/movingWall-480.webp 480w,/img/kink/movingWall-800.webp 800w,/img/kink/movingWall-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/movingWall.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Sketch of a traveling wave on a planar domain wall. The pulse propagates at the speed of light along the wall. Figure taken from Tanmay Vachaspati, who is a physics professor at Case Western Reserve University. </div> <p>These solutions are known as <code class="language-plaintext highlighter-rouge">travelling waves</code>.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="kink"/><category term="domainWall"/><summary type="html"><![CDATA[Kink in 1+1 dimension]]></summary></entry><entry><title type="html">Regression Methods in Biostatistics</title><link href="https://baiyangzhang.github.io/blog/2024/Regression-Methods-in-Biostatistics/" rel="alternate" type="text/html" title="Regression Methods in Biostatistics"/><published>2024-03-05T00:00:00+00:00</published><updated>2024-03-05T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Regression-Methods-in-Biostatistics</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Regression-Methods-in-Biostatistics/"><![CDATA[<p><em>Disclaimer: Nothing in this note is original.</em></p> <h1 id="introduction">Introduction</h1> <p>In life some questions are too important to be left to opinion, superstition, or conjecture. For example, which drug should a doctor prescribe to treat an illness? What factors increase the risk of an individual developing coronary heart attack? To answer these questions (even remotely), we need <em>objective, evidence-based</em> decision making method.</p> <p><strong>Evidence-based practice:</strong> Using sound research findings based on observed or collected data to make decisions.</p> <p>In principle, people collect and process information every day of their lives. Since it’s something we do frequently, you might think we would be really good at it…but we’re not. We are not good at picking out patterns from a sea of noisy data. And, on the flip side, we are too good at picking out non-existent patterns from small numbers of observations.</p> <p>In order to mitigate any of our own personal biases when answering important questions about the way the world works (i.e., to do good science), we must be careful to be rigorous in the way we proceed. The <strong>scientific method</strong> is the process used to answer scientific questions.</p> <ol> <li>Ask the question</li> <li>Construct a hypothesis</li> <li>Test it with a study or an experiment</li> <li>Analyze data and draw conclusions (wish its as simple as that)</li> <li>Communicate results</li> </ol> <p>However, collecting and analyzing data can be complicated. Statistics helps us design studies, test hypotheses, and use data to make scientifically valid conclusions about the world. In general, scientists use the scientific method to make generalizations about classes of people on the basis of their studies. The class of people that they are trying to make generalizations about is called the <strong>population</strong>. Most of the time, it is impractical and expensive to study all individuals in a population, Instead of sampling everyone in the population, or taking a <strong>census</strong>, typically we study only a portion of the population called the <strong>sample</strong>. In order to determine how best to obtain a sample to answer the research questions, we must be cautious about the study design. Then, researchers make <strong>generalizations</strong>, or <strong>inference</strong>, about the entire population based on studying the sample.</p> <p>In this note we consider two broad categories of statistical analysis: <code class="language-plaintext highlighter-rouge">descriptive statistics</code>, which deals with methods for summarizing and/or presenting data and <code class="language-plaintext highlighter-rouge">inferential statistics</code>.</p> <p><strong>Descriptive statistics</strong>: Methods for summarizing and/or presenting data. <strong>Inferential statistics</strong>: Methods for making generalizations about a population using information contained in the sample.</p> <hr/> <p>It is difficult to sort through large streams of data and make any meaningful conclusions. Instead, we can better understand data by condensing it into human readable mediums through the use of data <code class="language-plaintext highlighter-rouge">summaries</code>, often displayed in the forms of tables and figures. However, in doing so, information is often be lost in the process. A good data summary will seek to strike a balance between clarity and completeness of information.</p> <p>There are two broad types of data that we may see in the wild, which we will call <code class="language-plaintext highlighter-rouge">categorical data</code> and <code class="language-plaintext highlighter-rouge">continuous data</code>. As the name suggests, categorical data (sometimes called <code class="language-plaintext highlighter-rouge">qualitative</code> or <code class="language-plaintext highlighter-rouge">discrete</code> data) are <em>data that fall into distinct categories</em>. Categorical data can further be classified into two distinct types:</p> <ul> <li>Nominal data: data that exists without any sort of natural or apparent ordering, e.g., colors (red, green, blue), gender (male, female), and type of motor vehicle (car, truck, SUV).</li> <li>Ordinal data: data that does have a natural ordering, e.g., education (high school, some college, college) and injury severity (low, medium, high).</li> </ul> <p><code class="language-plaintext highlighter-rouge">Continuous data</code> (sometimes called quantitative data), on the other hand, are data that can take on any numeric value on some interval or on a continuum. Examples of continuous data include height, weight, and temperature. Categorical and continuous data are summarized differently, and we’ll explore a number of ways to summarize both types of data.</p> <p>Below are some terminologies.</p> <p><code class="language-plaintext highlighter-rouge">Absolute frequency</code>: The number of observations in a category <code class="language-plaintext highlighter-rouge">Rate/Relative frequency</code>: The number of observations in a category relative to any other quantity <code class="language-plaintext highlighter-rouge">Percent/Proportion</code>: The number of observations per 100 <code class="language-plaintext highlighter-rouge">Bar plot</code>: Visualization of categorical data which uses bars to represent each category, with counts or percents represented by the height of each bar <code class="language-plaintext highlighter-rouge">Stratification</code>: The process of partitioning data into categories prior to summarizing</p> <hr/> <p>The two most common ways to describe the center are with the <code class="language-plaintext highlighter-rouge">mean</code> and the <code class="language-plaintext highlighter-rouge">median</code>. We all know what the mean is. The median is another common <em>measure of the center of a distribution</em>. In particular, for a set of observations, <em>the median is an observed value that is both larger than half of the observations, as well as smaller than half of the observations</em>.</p> <p>Sometimes there lies some data that are extremely different than the rest. Take the salaries of staff of some American university, the highest paid employee is usually the football coach, and much much higher then the rest. This one high salary, which is not representative of most of the salaries collected, is known as an <code class="language-plaintext highlighter-rouge">outlier</code>. In this particular case, the mean is highly sensitive to the presence of outliers while the <em>median is not</em>. Measures that are less sensitive to outliers are called <code class="language-plaintext highlighter-rouge">robust</code> measures. The median is a robust estimator of the center of the data.</p> <p>The shape of a distribution is often characterized by its <code class="language-plaintext highlighter-rouge">modality</code> and its <code class="language-plaintext highlighter-rouge">skew</code>. The modality of a distribution <em>is a statement about its modes, or “peaks.”</em> Distributions with a single peak are called <code class="language-plaintext highlighter-rouge">unimodal</code>, whereas distributions with two peaks are call <code class="language-plaintext highlighter-rouge">bimodal</code>. <code class="language-plaintext highlighter-rouge">Multimodal</code> distributions are those with three or more peaks. The <code class="language-plaintext highlighter-rouge">skew</code> on the other hand describes <em>how our data relates to those peaks</em>. Distributions in which the data is dispersed evenly on either side of a peak are called <code class="language-plaintext highlighter-rouge">symmetric distributions</code>; otherwise, the distribution is considered skewed. The direction of the skew is towards the side in which the tail is longest.</p> <hr/> <p>In addition to measuring the center of the distribution, we are also interested in the spread or dispersion of the data. Two distributions could have the same mean or median without necessarily having the same shape. Perhaps the most intuitive methods of describing the dispersion of our data are those associated with <code class="language-plaintext highlighter-rouge">percentile-based</code> summaries. Formally, the $p$-th percentile is some value $V_ {p}$ such that</p> <ol> <li>$p\%$ of observations are $\leq V_ {p}$;</li> <li>$1-p\%$ of observations are $\gg V_ {p}$.</li> </ol> <p>The <code class="language-plaintext highlighter-rouge">quartile</code> is made of</p> \[\begin{align*} Q_ {1} &amp;= 25\text{th} \text{ percentile} = 1\text{st} \text{ or lower quartile}\\ Q_ {2} &amp;= 50\text{th} \text{ percentile} = 2\text{nd} \text{ quartile or median}\\ Q_ {3} &amp;= 75\text{th} \text{ percentile} = 3\text{rd} \text{ or upper quartile}\\ \end{align*}\] <p>A commonly used percentile-based measure of spread combining these measures is the <strong>interquartile range (IQR)</strong>, defined as</p> <p>\(\text{IQR} := Q_ {3} - Q_ {1}.\) The IQR is not impacted by the presence of outliers, so it is considered a robust measure of the spread of the data. So, like the median, it enjoys the quality of being a robust measure of the data.</p> <p>Percentiles are also used to create another common visual representation of continuous data: the <code class="language-plaintext highlighter-rouge">boxplot</code>, also known as a <code class="language-plaintext highlighter-rouge">box-and-whisker plot</code>. A boxplot consist of the following elements:</p> <ul> <li>A box, indicating the Interquartile Range (IQR), bounded by the values $Q_ {1}$ and $Q_ {3}$;</li> <li>The median, or $Q_ {2}$, represented by the line drawn within the box;</li> <li>The “whiskers,” extending out of the box, which can be defined in a number of ways. Commonly, the whiskers are 1.5 times the length of the IQR from either $Q_ {1}$ or $Q_ {3}$;</li> <li>Outliers, presented as small circles or dots, and are values in the data that are not present within the bounds set by either the box or whiskers.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/boxPlot-480.webp 480w,/img/boxPlot-800.webp 800w,/img/boxPlot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/boxPlot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Just like histograms, boxplots can also illustrate the skew of a data. In a histogram, the skewed was named after the location of the tail and in a boxplot, this corresponds to the side with a longer whisker. Here we can see histograms and boxplots for various distributions of data. </div> <hr/> <p>The variance and the standard deviation are numerical summaries which quantify how spread out the distribution is around its mean.</p> <p>We have two kinds of variances: <code class="language-plaintext highlighter-rouge">sample variance</code> and <code class="language-plaintext highlighter-rouge">population variance</code>. The difference between sample variance and population variance lies in how they are calculated and what they represent in the context of statistical analysis.</p> <p><strong>Population Variance</strong>:</p> <p>Population variance measures how much the members of a population differ from the population mean. It is denoted by $\sigma^2$. If you have a population with $N$ members and population values $x_ 1, x_ 2, …, x_ N$, the population variance $\sigma^2$ is calculated as: \(\sigma^2 = \frac{1}{N} \sum_ {i=1}^{N} (x_ i - \mu)^2\) where $\mu$ is the population mean. Note that $\mu$ is not the mean of some measured data, it is supposed to be given by some theoretical model. Population variance is used when you have access to all the data points in the population.</p> <p><strong>Sample Variance</strong></p> <p>Sample variance measures how much the members of a sample (a subset of the population) differ from the sample mean. It is an estimator of the population variance. Sample variance is denoted by $s^2$. If you have a sample of size $n$ with values $x_ 1, x_ 2, …, x_ n$, the sample variance $s^2$ is calculated as: \(s^2 = \frac{1}{n-1} \sum_ {i=1}^{n} (x_ i - \overline{x})^2\) where $\overline{x}$ is the sample mean.</p> <p>Key Differences:</p> <ol> <li><strong>Purpose:</strong> Population variance describes the variability within an entire population, while sample variance estimates the population variance from a subset of the population.</li> <li><strong>Formula:</strong> The population variance formula divides by $n$ (the total number of population members), whereas the sample variance formula divides by $n-1$ (one less than the sample size).</li> <li><strong>Bias Correction:</strong> The use of $n-1$ in the sample variance formula, known as Bessel’s correction, corrects for the bias in the estimation of population variance from a sample.</li> </ol> <p>When we calculate the variance of a sample, we typically use the sample mean $\overline{x}$ as an estimate of the true population mean. However, using the sample mean introduces a bias because it is based on the same data points that we are using to calculate the variance. This means the sum of the squared deviations $(x_ i - \overline{x})^2$ tends to be smaller than it would be if we used the true population mean, leading to an underestimate of the true population variance.</p> <p>To correct for this bias, we use $n-1$ in the denominator instead of $n$. This adjustment is known as Bessel’s correction. The rationale behind it is that when estimating variance from a sample, we lose one degree of freedom because we have estimated the mean from the same data set. Using $n-1$ effectively compensates for this loss, making the sample variance an unbiased estimator of the true population variance.</p> <p>In summary, the factor $\frac{1}{n-1}$ is used instead of $\frac{1}{n}$ to make the sample variance an unbiased estimator of the population variance, accounting for the fact that the sample mean is used in the variance calculation.</p> <p>The standard deviation, denoted as $s$, is a function of variance. Recall that the mean is not a robust outlier and is highly sensitive to skew or the presence of outliers. Consequently, the variance and the standard deviation are also very sensitive.</p> <hr/> <p>The regression method is a statistical technique used to model and analyze the relationships between a dependent variable and one or more independent variables. The primary goal of regression is to predict the value of the dependent (or <code class="language-plaintext highlighter-rouge">response</code>) variable based on the values of the independent (or <code class="language-plaintext highlighter-rouge">predictor</code>) variables. It is widely used in various fields such as economics, finance, biological sciences, and social sciences for forecasting, estimating, and determining causal relationships.</p> <p>There are several types of regression methods, each suited to different types of data and relationships:</p> <ol> <li> <p><strong>Linear Regression</strong>: The simplest form of regression, linear regression uses a linear approach to model the relationship between the dependent variable and one or more independent variables. The model assumes that the relationship can be described by a straight line in the form $y = \beta_ 0 + \beta_ 1x_ 1 + \epsilon$, where $y$ is the dependent variable, $x_ 1$ is the independent variable, $\beta_ 0$ is the y-intercept, $\beta_ 1$ is the slope of the line, and $\epsilon$ represents the error term.</p> </li> <li> <p><strong>Multiple Linear Regression</strong>: An extension of simple linear regression, this method involves two or more independent variables. The model is expressed as $y = \beta_ 0 + \beta_ 1x_ 1 + \beta_ 2x_ 2 + … + \beta_ nx_ n + \epsilon$, where $x_ 1, x_ 2, …, x_ n$ are the independent variables.</p> </li> <li> <p><strong>Logistic Regression</strong>: Used when the dependent variable is categorical, typically binary. Logistic regression models the probability that the dependent variable belongs to a particular category, using a logistic function.</p> </li> <li> <p><strong>Polynomial Regression</strong>: A form of regression where the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y.</p> </li> <li> <p><strong>Ridge and Lasso Regression</strong>: These are types of linear regression that include regularization. Regularization adds a penalty on the size of coefficients to prevent overfitting. Ridge regression adds a squared magnitude of the coefficient as a penalty term to the loss function, while Lasso regression adds the absolute value of the magnitude of the coefficient as a penalty term.</p> </li> <li> <p><strong>Non-linear Regression</strong>: Used when the data cannot be modeled using linear methods due to a non-linear relationship between the dependent and independent variables. Non-linear regression can fit complex curves to data.</p> </li> </ol> <p>Regression analysis involves selecting the appropriate model for the data, estimating the model parameters (usually through methods like least squares), evaluating the model’s adequacy (checking for goodness-of-fit, residuals, etc.), and interpreting the results to make inferences or predictions.</p> <p>In practice, the choice of regression method depends on the nature of the dependent variable, the shape of the relationship, and the distribution of the residuals, among other factors. Proper model selection, diagnostic testing, and validation are crucial steps in ensuring that the regression model provides reliable and accurate predictions or insights.</p> <h1 id="basic-statistical-methods">Basic Statistical Methods</h1> <h2 id="t-test-and-anova-analysis-of-variance">t-Test and ANOVA (Analysis of Variance)</h2> <p>My time is really limited here so I’ll direct jump to a short review of some mostly commonly used statistical methods.</p> <p>The basic $t$-test is used to compare two independent samples. The t-statistic on which the test is based is the difference between the two sample averages, divided by the standard error of that difference. The t-test is designed to work in small samples, whereas Z-tests are not.</p> <h3 id="t-test">t-test</h3> <p>Below is the gist of the derivation of the t-distribution. Assume we have a population that follows a normal distribution with mean $\mu$ and standard deviation $\sigma$. We take a random sample of size $n$ from this population, and we calculate the sample mean $\bar{x}$. The sample mean $\bar{x}$ is also normally distributed (due to the Central Limit Theorem) with mean $\mu$ and standard deviation $\sigma / \sqrt{n}$. We standardize $\bar{x}$ to transform it into a standard normal variable $Z$:</p> \[Z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}}\] <p>In practice, $\sigma$ (the population standard deviation) is often unknown and must be estimated from the sample. We use the sample standard deviation $s$ as an estimate for $\sigma$, where $s$ is calculated from the sample data. We replace $\sigma$ with $s$ in the standardization formula, but this introduces additional variability because $s$ is an estimate:</p> \[T = \frac{\bar{x} - \mu}{s / \sqrt{n}}\] <p>This ratio $T$ does <strong>not</strong> follow a standard normal distribution anymore due to the uncertainty introduced by using $s$ instead of $\sigma$. Instead, the variable $T$ follows a distribution that is similar to the normal distribution but with heavier tails. This is the t-distribution. <em>The exact shape of the t-distribution depends on the sample size $n$ through the degrees of freedom, which is $n - 1$ in this context</em>. The degrees of freedom account for the additional uncertainty introduced by estimating the population standard deviation.</p> <p>The t-distribution is formally defined through its probability density function (pdf), which is more complex than that of the normal distribution and involves the Gamma function. The pdf of the t-distribution for a given $t$ value and $v$ degrees of freedom (where $v = n - 1$) is given by:</p> \[f(t; v) = \frac{\Gamma((v+1)/2)}{\sqrt{v\pi}\Gamma(v/2)} \left(1 + \frac{t^2}{v} \right)^{-(v+1)/2}\] <p>where $\Gamma$ is the Gamma function, a generalization of the factorial function to complex numbers.</p> <p>In short, remember the following:</p> <ul> <li>The t-distribution accounts for the additional uncertainty in estimating the population mean when the population standard deviation is unknown and the sample size is small.</li> <li>As the sample size increases, the t-distribution approaches the standard normal distribution because the estimate $s$ becomes more accurate, reducing the extra uncertainty.</li> </ul> <hr/> <p>In the context of the t-test and statistical hypothesis testing, “significance” refers to the degree to which the test results allow us to reject the null hypothesis. The null hypothesis typically proposes that there is no effect or no difference between groups or conditions. When we say a result is “statistically significant,” it means that the observed data are unlikely to have occurred under the null hypothesis, suggesting that there is a real effect or difference.</p> <p>The significance level, denoted as $\alpha$, is a threshold set by the researcher before conducting the test, which defines the probability of rejecting the null hypothesis when it is actually true (a type I error). Common values for $\alpha$ are 0.05, 0.01, and 0.10, with 0.05 being the most widely used. Setting $\alpha$ at 0.05 means that there is a 5% risk of concluding that a difference exists when there is no actual difference.</p> <p>The p-value is a key metric derived from the t-test that indicates the probability of observing the test results, or more extreme results, if the null hypothesis were true. A p-value that is less than or equal to the significance level ($p \leq \alpha$) indicates that the observed data are unlikely under the null hypothesis, leading to the rejection of the null hypothesis. In simpler terms, a low p-value (typically ≤ 0.05) suggests that the evidence against the null hypothesis is strong enough to consider the results statistically significant.</p> <p><strong>Interpretation of Significance</strong></p> <ul> <li> <p><strong>Statistically Significant</strong>: If the test result is statistically significant, it suggests that the evidence is strong enough to reject the null hypothesis. This typically means there is a meaningful difference between the groups being compared, which is not likely to have occurred by chance.</p> </li> <li> <p><strong>Not Statistically Significant</strong>: If the result is not statistically significant, it suggests that the evidence is not strong enough to reject the null hypothesis. This could mean that there is no meaningful difference between the groups, or that the study did not have enough power (e.g., sample size was too small) to detect a difference if one exists.</p> </li> </ul> <p>It’s important to note that <em>statistical significance does not necessarily imply practical or clinical significance</em>. A result can be statistically significant but still be of little practical value if the observed effect or difference is too small to be of interest or use in a practical context.</p> <hr/> <h3 id="two-sided-hypothesis-test">Two-sided Hypothesis Test</h3> <p>In biostatistics, the two-sided t-test (also known as the two-tailed t-test) is commonly used to determine whether there is a significant difference between the means of two groups, <em>without specifying the direction of the difference</em>. This type of test is employed when the research question is concerned with whether there is any difference at all, rather than predicting which group will have a higher or lower mean.</p> <p>Biostatistics often involves comparing biological measurements or outcomes across different groups. For instance, one might compare the efficacy of two different medications, the impact of a treatment versus a placebo, or physiological measurements (like blood pressure) between two groups with different dietary habits. In these cases, researchers might not have a strong hypothesis about which group will have higher or lower means, or they may wish to test for the possibility of a difference in either direction. The two-sided t-test is ideal for these scenarios because it allows for the detection of significant differences regardless of their direction.</p> <p>The formula for the t-statistic in a two-sided t-test is similar to that of a one-sided t-test, but the <em>interpretation of the p-value and the critical value from the t-distribution is different</em>.</p> <p>For an independent two-sample t-test, the formula for the t-statistic remains:</p> \[t = \frac{\bar{x}_ 1 - \bar{x}_ 2}{\sqrt{s^2 \left(\frac{1}{n_ 1} + \frac{1}{n_ 2}\right)}}\] <p>However, in a two-sided t-test, you’re interested in differences in both directions, so you consider both tails of the distribution when determining the critical t-value or when interpreting the p-value.</p> <p>The hypotheses for a two-sided t-test are formulated as follows:</p> <ul> <li>Null Hypothesis ($H_ 0$): There is no difference between the group means ($\mu_ 1 = \mu_ 2$).</li> <li>Alternative Hypothesis ($H_ a$): There is a difference between the group means ($\mu_ 1 \neq \mu_ 2$).</li> </ul> <p>In a two-sided t-test, the p-value represents the probability of observing a test statistic as extreme as, or more extreme than, the one observed, in either direction, assuming the null hypothesis is true. If this p-value is less than or equal to the chosen significance level ($\alpha$), the null hypothesis is rejected, indicating a statistically significant difference between the two group means.</p> <p>A significant result in a two-sided t-test suggests that there is enough evidence to conclude that a difference exists between the two group means, but it does not indicate which group has the higher mean. This approach is particularly useful in biostatistics, where establishing the existence of a difference can be crucial for further research, clinical decisions, or policy-making, even before the direction of the difference is fully understood.</p> <h3 id="f-test">F-test</h3> <p>Suppose that we need to compare sample averages across the arms of a clinical trial with multiple treatments, or more generally across more than two independent samples. For this purpose, one-way analysis of variance (ANOVA) and the F-test take the place of the t-test. F-test technique extends the t-test, which compares only two means, by allowing comparisons among multiple groups simultaneously, thus providing a holistic view of the data.</p> <p>In the context of $F$-test, the <em>null hypothesis is that the mean values of the outcomes from all the populations sampled from are the same</em>, against the alternative that the means differ in at least two of the samples.</p> <p>The F-test is based on the F-distribution and uses an F-statistic to test the null hypothesis. <em>The test essentially compares the variance between the groups to the variance within the groups</em>; a higher ratio suggests that the group means differ significantly.</p> <p>Key Concepts of One-Way ANOVA are</p> <ul> <li><code class="language-plaintext highlighter-rouge">Between-Group Variability</code>: Differences among the group means, which reflect the effect of the independent variable.</li> <li><code class="language-plaintext highlighter-rouge">Within-Group Variability</code>: Variations within each group, attributed to random error or individual differences not due to the treatment effect.</li> <li><code class="language-plaintext highlighter-rouge">F-Statistic</code>: A ratio of the between-group variability to the within-group variability. A larger F-statistic indicates a greater likelihood that significant differences exist among the group means.</li> </ul> <p>F-test assumes that:</p> <ol> <li><strong>Independence of Observations:</strong> The data in different groups should be independent of each other.</li> <li><strong>Normality:</strong> The distribution of the residuals (errors) should be approximately normally distributed for each group.</li> <li><strong>Homogeneity of Variances:</strong> The variances among the groups should be approximately equal.</li> </ol> <p>Next we will give the gist of the derivation of F-distribution, follow by an example of application.</p> <p>Roughly speaking, <em>the F-distribution arises when dividing one $\chi^{2}$ (chi-square) distributed variable by another,</em> each divided by their respective degrees of freedom. Here’s a step-by-step explanation:</p> <p>Consider two independent chi-square distributed variables, $X$ and $Y$, with degrees of freedom $d_ 1$ and $d_ 2$, respectively. These chi-square variables can be thought of as the sum of squares of $d_ 1$ and $d_ 2$ independent standard normal variables.</p> <p>The probability density functions (pdf) for $X$ and $Y$ are given by:</p> \[f_ X(x) = \frac{1}{2^{d_ 1/2}\Gamma(d_ 1/2)} x^{d_ 1/2 - 1} e^{-x/2}, \quad x &gt; 0\] \[f_ Y(y) = \frac{1}{2^{d_ 2/2}\Gamma(d_ 2/2)} y^{d_ 2/2 - 1} e^{-y/2}, \quad y &gt; 0\] <p>where $\Gamma$ denotes the Gamma function.</p> <p>The F-statistic is constructed by dividing $X/d_ 1$ by $Y/d_ 2$, each chi-square variable divided by its degrees of freedom, which normalizes them:</p> \[F = \frac{X/d_ 1}{Y/d_ 2}\] <p>To derive the pdf of the F-distribution, we need to find the distribution of the variable $F$. This involves some complex integration because we have to consider the joint distribution of $X$ and $Y$, and then transform it to the distribution of $F$. The transformation involves the Jacobian of the transformation from $(X, Y)$ to $(F, Y)$, and then integrating out $Y$ to get the marginal distribution of $F$. After performing the necessary mathematical manipulations, the pdf of the F-distribution for a given $f$ value, with degrees of freedom $d_ 1$ and $d_ 2$, is given by:</p> \[f(f; d_ 1, d_ 2) = \frac{\Gamma((d_ 1+d_ 2)/2)}{\Gamma(d_ 1/2)\Gamma(d_ 2/2)} \left(\frac{d_ 1}{d_ 2}\right)^{d_ 1/2} f^{d_ 1/2 - 1} \left(1 + \frac{d_ 1}{d_ 2}f\right)^{-(d_ 1+d_ 2)/2}, \quad f &gt; 0\] <p>This distribution is used to test hypotheses about the equality of variances of two normally distributed populations, among other applications.</p> <p>Next, an example:</p> <p>Imagine a researcher wants to investigate the effect of different teaching methods on student performance. The researcher divides a group of 90 students into three equal groups, each subjected to a different teaching method: Method A (traditional), Method B (interactive), and Method C (technology-assisted). After a semester, the researcher measures the performance of each student on a standardized test.</p> <p>The research question is: “Do the three teaching methods result in different student performance levels?”</p> <p>To address this question using one-way ANOVA, the researcher would:</p> <ol> <li><strong>Calculate the group means:</strong> Find the average performance score for students in each teaching method group.</li> <li><strong>Compute the ANOVA statistics:</strong> Determine the between-group and within-group variances and calculate the F-statistic.</li> <li><strong>Compare the F-statistic to a critical value from the F-distribution:</strong> The critical value depends on the level of significance (usually set at 0.05) and the degrees of freedom for the numerator (between-groups, $k - 1$, where $k$ is the number of groups) and the denominator (within-groups, $N - k$, where $N$ is the total number of observations).</li> </ol> <p>If the computed F-statistic is greater than the critical value, the null hypothesis is rejected, suggesting that there is a significant difference in student performance across at least two of the teaching methods. The researcher might then conduct post-hoc tests to identify specifically which groups differ from each other.</p> <hr/> <h3 id="robustness">Robustness</h3> <p>We have assumed normal distribution for the distribution of random variables. However, both the t-test and the F-test are pretty robust to violations of the normality assumption, especially in large samples, similar to the central limit theorem. <em>By robust we mean that the type-I error rate, which is the mistake of rejecting the null hypothesis when it actually holds, is not seriously affected.</em> They are, however, primarily sensitive to outliers, which will mess up the variation.</p> <p>Specifically for the independent two-sample t-test, there’s an important assumption known as the <strong>equal variance assumption</strong> or <strong>homoscedasticity</strong>. This assumption states that the variance within each of the groups being compared should be approximately equal. The t-test is less robust to violations to this assumption, which can seriously affect the type-I error rate (and not always in conservative direction). In contrast, the overall F-test in ANOVA loses efficiency, but the error rate of type-I is use seriously increases. If the assumption of equal variances is violated, adjustments to the t-test can be made to account for the difference in variances. One common approach is to use Welch’s t-test, which does not assume equal population variances. Welch’s t-test adjusts the degrees of freedom of the t-test based on the sample sizes and variances of the two groups, making it more reliable when the variances are unequal.</p> <h1 id="correlation">Correlation</h1> <p>Pearson correlation coefficient, often symbolized as $r$, is a measure of the linear correlation between two variables $X$ and $Y$. In biostatistics, it’s widely used to quantify the degree to which two biological or health-related variables are linearly related. The value of $r$ ranges from -1 to +1, where:</p> <ul> <li>$r = 1$ indicates a perfect positive linear relationship,</li> <li>$r = -1$ indicates a perfect negative linear relationship,</li> <li>$r = 0$ suggests no linear relationship.</li> </ul> <p>In biostatistics, Pearson correlation is used to explore relationships between various biological, clinical, or health-related variables. Some examples include:</p> <ol> <li> <p><strong>Gene Expression Studies</strong>: Researchers might use Pearson correlation to assess the relationship between the expression levels of two genes across various conditions or tissue types, helping to identify potentially co-regulated genes or gene pairs with opposing expression patterns.</p> </li> <li> <p><strong>Nutritional Epidemiology</strong>: It can be used to explore the relationship between dietary intake (like calorie intake) and health outcomes such as blood pressure or cholesterol levels. A positive correlation might suggest that higher intake is associated with higher blood pressure, while a negative correlation could indicate the opposite.</p> </li> <li> <p><strong>Clinical Trials</strong>: In trials, Pearson correlation might be applied to examine the relationship between the dose of a drug and its effect on a biomarker. A positive correlation would suggest that as the dose increases, the biomarker levels also increase, indicating a possible dose-response relationship.</p> </li> </ol> <p>The Pearson correlation coefficient is calculated as:</p> \[r = \frac{\left\langle (x-\overline{x})(y-\overline{y}) \right\rangle }{\sqrt{ \left\langle (x-\overline{x})^{2} \right\rangle \left\langle (y-\overline{y})^{2} \right\rangle }}\] <p>Where $\left\langle \bullet \right\rangle$ is the <em>sample mean</em> of $\bullet$, not the population mean.</p> <p>While the Pearson correlation coefficient is a powerful tool, it has limitations. It only measures <em>linear relationships</em>, so it may not capture more complex patterns. Additionally, <em>it is sensitive to outliers</em>, which can disproportionately affect the correlation coefficient. Finally, <em>a significant Pearson correlation does not imply causation; it only indicates that a linear relationship exists</em>.</p> <hr/> <p>Like the $t$-test and linear regression, the correlation coefficients are sensitive to outliers. In this case, a <em>robust</em> alternative is the Spearman correlation coefficient, which is equivalent to the Pearson coefficient applied to the <code class="language-plaintext highlighter-rouge">ranks</code> of $x$ and $y$. <em>By rank we mean position in the ordered sequence of the values of a variable</em>. For example, if $x$ takes on values $1.2,5.1,4.3,16.0$, then we first order them from small to large, then the so-called rank is the position; the rank of $1.2$ is 1, the rank of 4.3 is 2, the rank of the outlier 16.0 is 4. In the given order the outliers are by construction either on the smallest end or the largest end. Ranks are used in a range of nonparametric methods, in no small part because of their robustness when the data include outliers. Their disadvantage is that any information contained in the measured values of the outcome beyond the ranks is lost.</p> <p>To be more specific, here’s a step-by-step explanation of how ranking is done, along with an example:</p> <ol> <li> <p><strong>Sort the data</strong>: Arrange the data in ascending or descending order.</p> </li> <li> <p><strong>Assign ranks</strong>: Assign ranks to each observation based on its position in the sorted data. The smallest observation gets a rank of 1, the second smallest gets a rank of 2, and so on.</p> </li> <li> <p><strong>Handle tied ranks</strong>: If there are tied values (i.e., two or more observations with the same value), assign them the average of the ranks they would have received. For example, if two observations tie for the second smallest value, each would receive a rank of 2.5.</p> </li> </ol> <p>Let’s illustrate this with an example. Consider the following dataset: 10, 15, 8, 20, 25, 15, 30</p> <ol> <li> <p>Sort the data: 8, 10, 15, 15, 20, 25, 30</p> </li> <li> <p>Assign ranks:</p> <ul> <li>8 gets a rank of 1</li> <li>10 gets a rank of 2</li> <li>15 gets a rank of 3.5 (average of ranks 3 and 4)</li> <li>15 gets a rank of 3.5 (average of ranks 3 and 4)</li> <li>20 gets a rank of 5</li> <li>25 gets a rank of 6</li> <li>30 gets a rank of 7</li> </ul> </li> </ol> <hr/> <p>Kendall’s tau (often denoted as $\tau$) is a measure of association or correlation between two ranked variables. It’s a <code class="language-plaintext highlighter-rouge">non-parametric statistic</code>, meaning <em>it doesn’t assume any specific distribution for the variables involved</em>. Kendall’s tau is particularly useful when dealing with ranked or ordinal data, where the exact numerical values of the data points might not be as important as their relative ordering.</p> <p>The formula for Kendall’s tau for two variables $X$ and $Y$ with $n$ observations each is given by:</p> \[\tau = \frac{\text{Number of concordant pairs} - \text{Number of discordant pairs}}{\text{Number of possible pairs}}\] <p>Where:</p> <ul> <li>A pair of observations $X_ i, Y_ i$ and $X_ j, Y_ j$ is considered concordant if the ranks agree, i.e., if $(X_ i - X_ j)(Y_ i - Y_ j) &gt; 0$.</li> <li>A pair is discordant if the ranks disagree, i.e., if $(X_ i - X_ j)(Y_ i - Y_ j) &lt; 0$.</li> <li>The number of possible pairs is the total number of pairs of observations, which is $\frac{n(n-1)}{2}$ for n observations.</li> </ul> <p>Let’s go through an example to illustrate Kendall’s tau:</p> <p>Suppose we have the following ranked data for two variables X and Y:</p> <p>\(X: 5, 3, 1, 4, 2\) \(Y: 2, 4, 5, 1, 3\)</p> <p>Step 1: Calculate the number of concordant and discordant pairs.</p> <ul> <li>Concordant pairs: Count the pairs where the ranks agree.</li> <li>Discordant pairs: Count the pairs where the ranks disagree.</li> </ul> \[\text{Concordant pairs} = 4\] <p>pairs (5, 4), (5, 3), (4, 2), (3, 2)</p> \[\text{Discordant pairs} = 6\] <p>pairs (5, 2), (5, 1), (5, 3), (4, 1), (4, 3), (3, 1)</p> <p>Step 2: Calculate Kendall’s tau.</p> \[\tau = \frac{\text{Concordant pairs} - \text{Discordant pairs}}{\text{Number of possible pairs}}\] \[\tau = \frac{4 - 6}{\frac{2}} = \frac{-2}{10} = -0.2\] <p>So, Kendall’s tau for the given data is -0.2.</p> <p>Interpretation: Since Kendall’s tau is negative, it suggests a slight negative association between variables X and Y. This means that as the rank of X increases, the rank of Y tends to decrease slightly, and vice versa.</p> <p>Kendall’s tau is widely used in various fields, especially when dealing with ranked or ordinal data, as it provides a robust measure of association that is not sensitive to the specific values of the ranks.</p> <h1 id="linear-regression-method">Linear Regression Method</h1> <p>Linear regression methods in biostatistics are used to describe the relationship between one or more independent (predictor or explanatory) variables and a continuous dependent (outcome) variable. These methods are fundamental in biostatistical analysis for understanding associations, predicting outcomes, and identifying potential causal relationships in health sciences. The primary methods include:</p> <ol> <li>Simple Linear Regression: <ul> <li><strong>Description</strong>: Examines the relationship between a single independent variable (X) and a dependent variable (Y).</li> <li><strong>Model</strong>: The relationship is modeled as $Y = \beta_ 0 + \beta_ 1X + \epsilon$, where $\beta_ 0$ is the y-intercept, $\beta_ 1$ is the slope of the line (indicating the change in Y for a one-unit change in X), and $\epsilon$ represents the error term.</li> <li><strong>Use Cases</strong>: Used when you want to see how changes in one predictor variable influence changes in the outcome. For example, studying the effect of drug dosage on blood pressure levels.</li> </ul> </li> <li>Multiple Linear Regression (MLR): <ul> <li><strong>Description</strong>: Extends simple linear regression to include multiple independent variables.</li> <li><strong>Model</strong>: $Y = \beta_ 0 + \beta_ 1X_ 1 + \beta_ 2X_ 2 + … + \beta_ kX_ k + \epsilon$, where $\beta_ 0$ is the intercept, $\beta_ i$ are the coefficients for each predictor $X_ i$, and $\epsilon$ is the error term.</li> <li><strong>Use Cases</strong>: Useful when investigating the impact of several factors on an outcome. For instance, assessing how patient age, weight, and smoking status together influence the risk of developing cardiovascular diseases.</li> </ul> </li> <li>Polynomial Regression: <ul> <li><strong>Description</strong>: A form of regression analysis where the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial.</li> <li><strong>Model</strong>: $Y = \beta_ 0 + \beta_ 1X + \beta_ 2X^2 + … + \beta_ nX^n + \epsilon$.</li> <li><strong>Use Cases</strong>: Employed when the relationship between variables is not linear, allowing for a better fit to data that display curvature. For example, modeling the growth rate of bacteria at different temperatures might require a polynomial fit.</li> </ul> </li> <li>Ridge Regression (L2 Regularization): <ul> <li><strong>Description</strong>: Addresses multicollinearity (high correlation among independent variables) in MLR by adding a penalty term equal to the square of the magnitude of the coefficients.</li> <li> <table> <tbody> <tr> <td><strong>Model</strong>: The cost function is $\text{Cost} =</td> <td> </td> <td>Y - X\beta</td> <td> </td> <td>^2 + \lambda</td> <td> </td> <td>\beta</td> <td> </td> <td>^2$, where $\lambda$ is the penalty term.</td> </tr> </tbody> </table> </li> <li><strong>Use Cases</strong>: Useful in situations with many predictors, some of which might be correlated. It helps in reducing overfitting by shrinking the coefficients.</li> </ul> </li> <li>Lasso Regression (L1 Regularization): <ul> <li><strong>Description</strong>: Similar to ridge regression but uses an absolute value penalty for the size of coefficients, which can lead to some coefficients being exactly zero.</li> <li> <table> <tbody> <tr> <td><strong>Model</strong>: The cost function is $\text{Cost} =</td> <td> </td> <td>Y - X\beta</td> <td> </td> <td>^2 + \lambda</td> <td>\beta</td> <td>$.</td> </tr> </tbody> </table> </li> <li><strong>Use Cases</strong>: Used for variable selection and regularization to improve prediction accuracy and interpretability of the statistical model by excluding irrelevant variables.</li> </ul> </li> <li>Elastic Net Regression: <ul> <li><strong>Description</strong>: Combines penalties of ridge regression and lasso regression.</li> <li> <table> <tbody> <tr> <td><strong>Model</strong>: The cost function includes both L1 and L2 penalties, $\text{Cost} =</td> <td> </td> <td>Y - X\beta</td> <td> </td> <td>^2 + \lambda_ 1</td> <td>\beta</td> <td>+ \lambda_ 2</td> <td> </td> <td>\beta</td> <td> </td> <td>^2$.</td> </tr> </tbody> </table> </li> <li><strong>Use Cases</strong>: Effective when there are multiple correlated variables, providing a balance between ridge and lasso regression by including both sets of penalties.</li> </ul> </li> </ol> <p>Some comments. Ridge Regression is called L2 regularization because of the nature of the penalty applied to the coefficients in the regression model. In this context, “L2” refers to the L2 norm of the coefficient vector, which is used as the penalty term. The L2 norm is essentially the square root of the sum of the squared vector values, but in the context of ridge regression, the penalty term involves the square of the L2 norm (i.e., the sum of the squared values of the coefficients, not taking the square root).</p> <p>More mathematically, for a regression coefficient vector $\beta = [\beta_ 1, \beta_ 2, …, \beta_ n]$, the L2 norm is defined as:</p> \[||\beta||_ 2 = \sqrt{\beta_ 1^2 + \beta_ 2^2 + ... + \beta_ n^2}\] <p>In ridge regression, the penalty term added to the <code class="language-plaintext highlighter-rouge">cost function</code> (which is minimized during the training of the model) is the square of this L2 norm (hence the term “L2 regularization”), but it’s often just presented without the square root to begin with in the context of ridge regression.</p> <p>The rationale behind using L2 regularization (ridge regression) is to prevent overfitting by shrinking the coefficients of less important features towards zero (though not exactly zero, which is a characteristic of Lasso regression, or L1 regularization). This is particularly useful when dealing with multicollinearity or when the number of predictor variables is large relative to the number of observations. <em>The L2 regularization term penalizes large coefficients, thus enforcing a constraint on the size of coefficients, which can lead to more robust and better-generalized models.</em></p> <p>Overfitting occurs when a statistical model or machine learning algorithm captures the noise of the data rather than the underlying pattern. It happens when the model is too complex relative to the amount and noisiness of the input data. The overfitted model has high variance and low bias, making excellent predictions on the training data but performing poorly on new, unseen data because it has essentially memorized the training dataset rather than learning the general underlying patterns.</p> <hr/> <p>The name “Lasso regression” comes from the term “Least Absolute Shrinkage and Selection Operator.” It was introduced by Robert Tibshirani in 1996 as a new regression method that not only has the capability to shrink the coefficients toward zero, like Ridge regression, but also to set some coefficients exactly to zero. This latter property makes Lasso regression particularly useful for feature selection in addition to regularization.</p> <p>The term “Lasso” itself is a metaphor, likening the method to a cowboy’s lasso used to catch and select specific components (in this case, variables or features in a model). The lasso wraps around the most important features while discarding the less important ones, making it a valuable tool for models with a large number of features, many of which might be irrelevant or redundant.</p> <h1 id="logistic-regression-method">Logistic Regression method</h1> <p>Logistic regression in biostatistics is a statistical analysis method used to model the relationship between one or multiple independent variables and a dependent variable that is binary (i.e., it takes on two possible outcomes, often coded as 0 and 1). It’s particularly useful in the field of biostatistics for analyzing and predicting the probability of a binary outcome based on one or more risk factors or predictor variables.</p> <p>Logistic regression is a statistical method used for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). It’s used extensively in fields like medicine, biology, and social sciences, among others, for tasks like disease prediction, customer churn prediction, and spam detection.</p> <p>The logistic function, also called the sigmoid function, is an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.</p> <h1 id="entropy-in-statistics">Entropy in Statistics</h1> <p>In statistics and machine learning, entropy is a measure of uncertainty, randomness, or unpredictability in a set of outcomes. The concept of entropy, borrowed from thermodynamics and information theory, is particularly useful in model fitting and various statistical analyses for quantifying the amount of information, selecting models, and even in decision tree algorithms. Here’s how entropy is applied in these contexts:</p> <p><strong>Information Gain in Decision Trees:</strong></p> <p>In the context of decision trees, particularly in classification problems, entropy is used to measure the impurity or disorder in a set of examples. Information gain, which is based on the decrease in entropy, is then used to decide which feature to split on at each step in the tree.</p> <ul> <li><em>Entropy before Split</em>: Measures the degree of uncertainty or impurity in the dataset before it is divided.</li> <li><em>Entropy after Split</em>: Measures the weighted sum of the entropy of each subset created after splitting the dataset based on a specific feature.</li> <li><em>Information Gain</em>: The difference between the entropy before the split and the weighted entropy after the split. A feature with the highest information gain is chosen for the split because it best reduces uncertainty.</li> </ul> <p><strong>Model Selection and Regularization:</strong></p> <p>Entropy can also be used as a criterion for model selection and regularization, particularly in methods like Maximum Entropy (MaxEnt) modeling, which is used in various fields including natural language processing (NLP) and ecology.</p> <ul> <li><em>Maximum Entropy Modeling</em>: In MaxEnt, the principle of maximum entropy is used to select the probability distribution that best represents the current state of knowledge (the one with the maximum entropy), subject to the given constraints (e.g., the known moments or expectations of certain features). This approach ensures that no additional assumptions are made beyond what is justified by the data, leading to a model that is maximally non-committal with regard to missing information.</li> </ul> <p><strong>Feature Selection and Dimensionality Reduction:</strong></p> <p>Entropy can be used to evaluate the importance or relevance of features in a dataset. Features that do not contribute significantly to reducing uncertainty (or increasing the information gain) can be considered for removal, which is an essential aspect of feature selection and dimensionality reduction.</p> <ul> <li><em>Mutual Information</em>: A related concept, mutual information, measures the amount of information that one variable provides about another. In feature selection, mutual information can be used to quantify the relevance of each feature with respect to the target variable, where features with higher mutual information are preferred.</li> </ul> <p><strong>Quantifying Prediction Uncertainty:</strong></p> <p>In probabilistic modeling and Bayesian statistics, entropy can be used to quantify the uncertainty associated with predictions. Models that produce probability distributions as predictions can have their predictive entropy calculated to assess how uncertain the model is about its predictions.</p> <ul> <li><em>Predictive Entropy</em>: High entropy in the predicted probability distributions indicates high uncertainty in the predictions, which can be crucial for understanding the confidence of the model in its predictions and for decision-making processes where uncertainty needs to be minimized.</li> </ul> <p>Example in Decision Tree:</p> <p>Consider a dataset with two features (X1: Color, X2: Size) and a binary target variable (Y: Defective or Not Defective). The entropy of the target variable represents the uncertainty in the defective status. If a split based on the “Color” feature results in subsets with lower entropy (more purity in terms of the target variable), the information gain from this split is high, making “Color” a good candidate for splitting. The decision tree algorithm will use this entropy-based criterion to construct a tree that aims to reduce the uncertainty (entropy) in the target variable with each split.</p> <p>In summary, entropy serves as a foundational concept in model fitting and statistics, enabling more informed decisions about model structure, feature importance, and the uncertainty in predictions, thereby improving model interpretability and effectiveness.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="biostatistics"/><summary type="html"><![CDATA[Disclaimer: Nothing in this note is original. Introduction]]></summary></entry><entry><title type="html">Renormalization method in PDF</title><link href="https://baiyangzhang.github.io/blog/2024/Renormalization-method-in-PDF/" rel="alternate" type="text/html" title="Renormalization method in PDF"/><published>2024-03-02T00:00:00+00:00</published><updated>2024-03-02T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Renormalization-method-in-PDF</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Renormalization-method-in-PDF/"><![CDATA[<h1 id="abstract">Abstract</h1> <p>The application of renormalization group method in solving the differential equation in the momentum space, and the error estimation of the solution with finite sized lattice size.</p> <h1 id="introduction">Introduction</h1> <p>The question we want to answer here is very simple: If we solve the PDF in the momentum space with a given cutoff $\Lambda$, how does the solution change with respect to $\Lambda$?</p> <h1 id="first-order-homogeneous-linear-differential-equation">First Order Homogeneous Linear Differential Equation</h1> <p>We want a well behaved function $\mathbb{R} \to \mathbb{R}$ so we begin with Gaussian function $g(t) = e^{-\lambda t^2}$. The equation it solves is</p> \[\dot{g}(t) + 2 \lambda t g(t) = 0, \tag{1}\] <p>which is a homogeneous first order ODE.</p> <p>Imagine that the function is defined on a grid with lattice spacing $a$, then the equation takes the discrete form</p> \[\frac{g(t_ {i+1})-g(t_ i)}{a} + 2 \lambda t g(t_ i) = 0,\quad \forall i \in \text{ lattice},\] <p>the solution will naturally depend on the lattice size, in the $a\to 0$ limit we will recover the continuous solution. We want to know what explicitly the dependence on $a$ looks like, and how to perform the error estimate with small but finite $a$.</p> <p>As the first attempt, in our simplified example, we go to the frequency space by substitute</p> \[g(t) = \frac{1}{\sqrt{2\pi}}\int_ {-\infty}^{\infty} d\omega \tilde{g}(\omega) e^{-i\omega t}.\] <p>The Fourier transformed equation reads</p> \[\int_ {-\infty}^{\infty} \frac{d\omega}{\sqrt{2\pi}} e^{-i\omega t} \left\lbrace -i \omega \tilde{g}(\omega) - 2i\lambda \frac{d}{d\omega} \tilde{g}(\omega) \right\rbrace.\] <p>A lower bound in the lattice size $a$ corresponds to an upper bound $\Lambda$ in the momentum (exchangeable with frequency in our discussion) space, $a \sim 1/\Lambda$, thus we have</p> \[\begin{align} &amp;\int_ {-\Lambda}^{\Lambda} \frac{d\omega}{\sqrt{2\pi}} e^{-i\omega t} \left\lbrace -i \omega \tilde{g}(\omega) - 2i\lambda \frac{d}{d\omega} \tilde{g}(\omega) \right\rbrace = 0, \\ \implies &amp; \omega \tilde{g}(\omega) + 2\lambda \frac{d}{d\omega} \tilde{g}(\omega) = 0\, \forall \, |\omega| &lt; \Lambda, \quad 0 \text{ otherwise} \end{align}\] <p>The equation for $\tilde{g}(\omega)$ takes the same form (up to some multiplicative constants) as that for $g(t)$, reflecting the fact that the Fourier transformed Gaussian function takes the same form as the original function.</p> <p>Here comes the key point: what happens if we decrease the momentum cutoff by infinitesimal, $\Lambda \to b\Lambda$, $b = 1-\epsilon$. With new cutoff $b\Lambda$ the equation above still holds trivially, since different modes are entirely independent of each other, we get</p> \[\omega \tilde{g}(\omega) + 2\lambda \frac{d}{d\omega} \tilde{g}(\omega) = 0\, \forall \, |\omega| &lt; (1-\epsilon)\Lambda, \quad 0 \text{ otherwise}.\] <p>In other words, for the modes that survive the renormalization flow, we have the trivial RG equation</p> \[\frac{d}{d\Lambda}\tilde{g}(\omega) = 0, \quad |\omega| &lt; b\Lambda.\] <p>Switch back to the physics space from frequency space, the error estimate is most easily done by subtracting $g^{(\Lambda)}(t)$ from $g^{(\infty)}(t)$, where</p> \[g^{(\Lambda)}(t) \equiv \frac{1}{\sqrt{2\pi}}\int_ {-\Lambda}^{\Lambda} d\omega \tilde{g}(\omega) e^{-i\omega t}\] <p>where the Fourier transform of Gaussian function is</p> \[\mathcal{F}\left\lbrace e^{-\lambda x^2}\right\rbrace(\omega) = \frac{1}{\sqrt{2 \lambda }}e^{-\frac{\omega ^2}{4 \lambda }}\] <p>Define</p> \[\boxed{\Delta g(t) \equiv g^{(\infty)}(t) - g^{(\Lambda)}(t)},\] <p>we have</p> \[\begin{align} \notag \Delta g(t) &amp;= \left( \int_ {-\infty}^{-\Lambda} + \int_ {\Lambda}^\infty \right) \left( \frac{d\omega}{\sqrt{2\pi}} \frac{1}{\sqrt{2\lambda}} e^{-\omega^2/4\lambda} e^{-i\omega t} \right) \\\notag &amp;=\frac{1}{\sqrt{\pi\lambda}} \int_ {\Lambda}^\infty d\omega e^{-\omega^2/4\lambda} \cos{\omega t} \\\notag &amp;= \frac{i}{2} e^{-\lambda t^2} \left(-2 i +\text{erfi}\left(\frac{-2 \lambda t+i \Lambda }{2 \sqrt{\lambda }}\right)+\text{erfi}\left(\frac{2 \lambda t+i \Lambda }{2 \sqrt{\lambda }}\right)\right)\\ &amp;= e^{-\lambda t^2} - e^{-\lambda t^2}\text{Re}\,\text{erf}\left( \frac{\Lambda + 2i\lambda t}{2\sqrt{\lambda}} \right) \end{align}\] <p>where we have simplified notations, Re erf is the real part of the error function, and $\text{erfi}(z)$ is the so-called imaginary error function defined by $\text{erfi}(z) = -i \,\text{erf}(iz)$.</p> <p>Well, the last expression is not super helpful, we can do better by looking at $\Delta g^2$ as an estimate of the overall error. Square the second line in the previous equation we get</p> \[\begin{align*} \notag \Delta g(t)^2 &amp;=\frac{1}{4\pi\lambda} \int_ {\left\lvert \omega_ 1 \right\rvert &gt;\Lambda} d\omega_ 1 \int_ {\left\lvert \omega_ 2 \right\rvert &gt;\Lambda} d\omega_ 2 \, e^{-\omega_ 1^2/4\lambda - \omega_ 1^2/4\lambda} \cos(\omega_ 1 t) \cos(\omega_ 2 t) \\\notag &amp;=\frac{1}{4\pi\lambda} \int_ {\left\lvert \omega_ {1,2} \right\rvert &gt;\Lambda} d^2\omega e^{-\omega^2/4\lambda} \cos(\omega_ 1 t) \cos(\omega_ 2 t) \\ &amp; &lt; \frac{1}{4\pi\lambda} \int_ {\left\lvert \omega_ {1,2} \right\rvert &gt;\Lambda} d^2\omega e^{-\omega^2/4\lambda} \end{align*}\] <p>where $\omega^2 = \omega_ 1^2 + \omega_ 2^2$. The integral region is shown in the Figure below, the four corners where $\omega_ {1,2}&gt;\Lambda$ corresponds to the integral region. We can extend the region first to $(\mathbb{R}^2 - \text{square})$, then to $(\mathbb{R}^2 - \text{disk})$, each step will increase the error, thus we will get an upper bound. The reason for changing the square to circle is that so we can use the rotation symmetry. Continue with the integral,</p> \[\begin{align} \notag |\Delta g(t)|^2 &amp;&lt; \frac{1}{4\pi\lambda}\int_ {\omega^2&gt;\Lambda^2} d^2\omega e^{-\omega^2/4\lambda}\\\notag &amp;= \frac{1}{4\pi\lambda} \int_ {\omega^2&gt;\Lambda^2} d^2\omega e^{-\omega^2/4\lambda} \\\notag &amp;= \frac{1}{4\pi\lambda}2\pi \int_ {\Lambda}^\infty \int d\omega \, \omega e^{-\omega^2/4\lambda} \end{align}\] <p>where we have used $d^2 \omega = d\omega \omega d\theta$,</p> \[\begin{align} |\Delta g(t)|^2 &amp;&lt; \frac{1}{4\lambda} \int_ {\Lambda}^\infty \int d\omega^2 \, \omega e^{-\omega^2/4\lambda} \\ &amp;= e^{-\Lambda^2/4\lambda}. \end{align}\] <p>The conclusion is that at each $t$, the error $[g^{(\infty)}-g^{(\infty)}]^2 &lt; e^{-\Lambda^2/4\lambda}$.</p> <p><img src="/img/region.png" alt="region"/></p> <p>Out of a more differential point of view, let us consider the change of the function as we vary the cutoff $\Lambda$. We can calculate $\boxed{\frac{d}{d\Lambda} g^{(\Lambda)}(t)}$. Note that sometimes we will neglect the independent variable $t$ to save some space.</p> <p>If we increase $\Lambda$ infinitesimally, we have</p> \[\begin{align} \notag g^{(\Lambda+\epsilon)}(t) &amp; = g^{(\Lambda)} + \frac{1}{\sqrt{\pi\lambda}} \int_ \Lambda^{\Lambda+\epsilon} d\omega e^{-\omega^2/4\lambda} \cos(\omega t)\\ &amp;= g^{(\Lambda)} + \frac{\epsilon}{\sqrt{\pi\lambda}} e^{-\Lambda^2/4\lambda} \cos(\Lambda t), \end{align}\] <p>Thus</p> \[\boxed{ \frac{d}{d\Lambda} g^{(\Lambda)}(t) = \frac{e^{-\Lambda^2 / 4\lambda}}{\sqrt{\pi\lambda}} \cos(\Lambda t) = \text{Re}\,\frac{e^{-\Lambda^2 / 4\lambda}}{\sqrt{\pi\lambda}} e^{i\Lambda t} = \text{Re}\,\frac{e^{-\lambda t^2}}{\sqrt{\pi\lambda}} e^{-\frac{1}{4\lambda} (\Lambda-i2\lambda t)^2}. }\] <p>It can be solved to give,</p> \[g^{(\Lambda)}(t) = C_ 1 - e^{-\lambda t^2} \text{Re} \, \text{erf}\left(i\sqrt{\lambda} t - \frac{\Lambda}{2\sqrt{\lambda}}\right),\] <p>where $C_ 1$ is the constant of integration. To eliminate $C_ 1$, recall that $f^{\infty}(t) = e^{-\lambda t^2}$, thus we have $C_ 1 = 0$. By the end of the day we have</p> \[\boxed{ g^{(\Lambda)}(t) = - e^{-\lambda t^2} \text{Re} \, \text{erf}\left(i\sqrt{\lambda} t - \frac{\Lambda}{2\sqrt{\lambda}}\right) },\] <p>which agrees with the previous equations. The value of $\Delta g(t)$ can also be estimated with the help of Hans Heinrich Burmann’s theorem,</p> \[\text{erf}{x} = \frac{2}{\sqrt{\pi}}\text{sgn}\cdot \sqrt{1-e^{-x^2}} \left( \frac{\sqrt{\pi}}{2}+ \sum_ {k\in \mathbb{Z}^+} c_ k e^{-k x^2} \right), \, c_ 1 = \frac{31}{200},\, c_ 2 = -\frac{341}{8000},\, \cdots.\] <p>In summary, We have obtained the lattice spacing dependence, or equivalently the momentum cutoff dependence, both the error estimate and the RG flow are discussed.</p> <hr/> <h1 id="in-homogeneous-linear-differential-equation">In-homogeneous Linear Differential Equation</h1> <hr/> <h1 id="kink-equation">kink Equation</h1> <hr/> <h1 id="conventions">Conventions</h1> <p>The conventions are chose so be the same as that used by Mathematica.</p> <p>Given a function $f(t):\mathbb{R} \to \mathbb{R}$, the Fourier transform in the symmetrical form is</p> \[\begin{align} \tilde{f} (\omega) &amp;= \frac{1}{\sqrt{2\pi}}\int_ {-\infty}^\infty f(t) e^{i\omega t} dt, \\ f(t) &amp;= \frac{1}{\sqrt{2\pi}}\int_ {-\infty}^\infty \tilde{f}(\omega) e^{-i\omega t} dt \end{align}\] <p>where $\tilde{f}(\omega) \equiv \mathcal{F} \left\lbrace f \right\rbrace(\omega)$.</p> <p>Note the factor of $\frac{1}{\sqrt{2\pi}}$ and the signs in the exponent.</p> <hr/> <h1 id="appendix-a-error-function-in-the-complex-plane">Appendix A. Error function in the complex plane</h1> <p>The error function in the complex plane is defined to be</p> \[\operatorname* {erf}{z} = \frac{2}{\sqrt{\pi}} \int_ {\Gamma} d\zeta \, e^{-\zeta^2}\] <p>where $\Gamma$ is any path going from $0$ to $z$. The real and imaginary part of an error function can be estimated by Abramowitz and Stegun.</p> <p>The error function $\text{erf}(z),\, z \in \mathbb{C}$ satisfy symmetry relations</p> \[\begin{align} \text{erf}(z) &amp;= - \text{erf}(-z), \\ \text{erf}(\overline{z}) &amp;= \overline{\text{erf}(z)}. \end{align}\] <p>A possibly useful series expression for numerical calculation is</p> \[\begin{multline} \operatorname* {erf}(x+i y) = \operatorname* {erf}{x} + \frac{e^{-x^2}}{2 \pi x} [(1-\cos{2 x y})+i \sin{2 x y}]\\ + \frac{2}{\pi} e^{-x^2} \sum_ {k=1}^{\infty} \frac{e^{-k^2/4}}{k^2+4 x^2}[f_ k(x,y)+i g_ k(x,y)] + \epsilon(x,y) \end{multline}\] <p>where</p> \[\begin{align*} f_ k(x,y) &amp;= 2 x [1-\cos(2 x y) \cosh(k y)] + k\sin(2 x y) \sinh(k y), \\ g_ k(x,y) &amp;= 2 x \sin(2 x y) \cosh(k y) + k\cos(2 x y) \sinh(k y). \end{align*}\]]]></content><author><name>Baiyang Zhang</name></author><category term="Math"/><category term="Renormalization"/><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Excerpt From “On Anarchism” by Noam Chomsky</title><link href="https://baiyangzhang.github.io/blog/2024/Excerpt-From-On-Anarchism/" rel="alternate" type="text/html" title="Excerpt From “On Anarchism” by Noam Chomsky"/><published>2024-03-01T00:00:00+00:00</published><updated>2024-03-01T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Excerpt-From-On-Anarchism</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Excerpt-From-On-Anarchism/"><![CDATA[<h3 id="introduction">Introduction</h3> <p><strong>Ableism:</strong> A term used to describe discrimination, prejudice, or bias against individuals with disabilities.</p> <p><strong>Gender queerness:</strong> Gender queerness is a concept and identity that falls under the broader umbrella of non-binary gender identities. It challenges and rejects the traditional binary understanding of gender as solely male or female. Instead, genderqueer individuals may identify with both genders, neither gender, or a different gender entirely. Here are some key points to understand about gender queerness:</p> <ul> <li><em>Fluidity</em>: Some genderqueer people experience a fluid gender identity, meaning their gender identity can change over time or in different circumstances.</li> <li><em>Beyond the Binary</em>: Genderqueer identities are diverse and can include identities like bi-gender (identifying as two genders), agender (lacking a gender or being gender neutral), and more.</li> <li><em>Personal Expression</em>: The way genderqueer people express their gender can vary widely. Some may choose to present androgynously, some may present in ways that are traditionally associated with men or women, and others may choose an entirely different form of expression.</li> <li><em>Preferred Pronouns</em>: Genderqueer individuals may use a variety of pronouns. This includes he/him, she/her, they/them, or other neopronouns such as ze/hir.</li> </ul> <p><strong>Zapatistas:</strong> The Zapatistas, officially called the Zapatista Army of National Liberation (EZLN), is a revolutionary leftist group that originated in Mexico in 1994. The group is named after Emiliano Zapata, a leading figure in the Mexican Revolution who advocated for land reforms and the rights of peasants. Here are some key aspects of the Zapatistas:</p> <ul> <li><strong>Indigenous Rights</strong>: A significant aspect of the Zapatista movement is its emphasis on the rights and autonomy of indigenous peoples in Mexico. Much of the EZLN’s membership consists of indigenous people from various ethnic groups.</li> <li><strong>Form of Struggle</strong>: After the initial 1994 uprising, the Zapatistas transitioned from a traditional armed struggle to a more peaceful, grassroots, and civil resistance. They prioritize community-based initiatives and projects over direct military action.</li> <li><strong>Autonomous Municipalities</strong>: The Zapatistas established “caracoles” (snails) and autonomous municipalities in Chiapas. These areas are governed by local assemblies and function outside the traditional political system of Mexico.</li> <li><strong>Subcomandante Marcos</strong>: One of the most well-known figures of the Zapatista movement is Subcomandante Marcos (also known as Subcomandante Galeano since 2014). He served as the movement’s spokesperson and is recognized for his writings, which mix revolutionary theory with poetic and mythical elements.</li> </ul> <p><strong>Black blocs:</strong> The term “black bloc” refers to a tactic often used by protesters, rather than a specific group or organization. Participants in a black bloc wear all-black clothing and cover their faces with masks or bandanas to maintain anonymity, create a unified presence, and avoid identification by law enforcement or surveillance.</p> <p><strong>Occupy Wall Street:</strong> Occupy Wall Street (OWS) was a protest movement that began on September 17, 2011, in Zuccotti Park, located in New York City’s Wall Street financial district. It was initiated by the Canadian activist group Adbusters and subsequently spread to cities across the United States and around the world. The primary slogan of the movement was “We are the 99%,” which highlighted the increasing income and wealth inequality in the U.S., drawing attention to the concentration of wealth in the top 1% of the population.</p> <p>Here are some key aspects and outcomes of Occupy Wall Street:</p> <ul> <li><strong>Goals and Grievances</strong>: The movement was largely about economic inequality, corporate influence over the democratic process, and the perceived failures of Wall Street and big banks, especially in the wake of the 2007-2008 financial crisis. Protesters also voiced concerns about issues such as student loan debt, foreclosures, and the influence of money in politics.</li> <li><strong>Leaderless Structure</strong>: OWS was deliberately leaderless and used a consensus-based decision-making process. General Assemblies were held where participants could discuss issues, make decisions, and plan actions.</li> <li><strong>Encampments</strong>: Inspired by the initial occupation of Zuccotti Park, similar encampments sprang up in various cities around the U.S. and the world. These became sites of community, discussion, and often faced police evictions.</li> <li><strong>Impact on Discourse</strong>: While OWS did not have a specific set of demands or a unified platform, it significantly impacted public discourse. Concepts like income inequality and the “1%” became more mainstream topics of discussion in the media, among politicians, and in everyday conversations.</li> <li><strong>Criticism</strong>: The movement faced criticism on various fronts. Some felt that the lack of a clear set of demands or a centralized leadership made the movement ineffective. Others believed that certain actions or behaviors by protesters detracted from the movement’s broader goals.</li> <li><strong>Legacy</strong>: While the encampments and direct actions associated with OWS dwindled by 2012, the movement’s broader impacts can still be seen. It influenced subsequent social movements and brought attention to economic inequality, which remained a significant topic in subsequent political campaigns and policy discussions.</li> <li><strong>Connection to Other Movements</strong>: OWS shared tactics, ideas, and personnel with other movements and causes, such as Black Lives Matter, the Fight for $15 (a campaign for a higher minimum wage), and even the 2016 and 2020 presidential campaigns of Senator Bernie Sanders.</li> </ul> <p> <strong>corporatocracy:</strong> A system in which power effectively rests with a small, elite group of inside individuals, sometimes from a small group of educational institutions, or influential economic entities or devices, such as banks, commercial entities, lobbyists that act in complicity with, or at the whim of the oligarchy, often with little or no regard for constitutionally protected prerogative.</p> <hr/> <p><strong>Principal:</strong> Power that isn’t really justified by the will of the <em>governed</em> should be dismantled.</p> <p>The anarchist historian Rudolf Rocker (a German socialist, 1873-1953) said</p> <blockquote> <p>For the Anarchist, freedom is not an abstract philosophical concept, but the vital concrete possibility for every human being to bring to full development all the powers, capacities, and talents with which nature has endowed him, and turn them to social account.</p> </blockquote> <p>First, what is a syndicalist. A syndicalist is someone who supports or is involved in syndicalism, which is a type of economic and political theory that advocates for the control of industries and businesses by worker unions. The basic idea behind syndicalism is the belief that <em>workers, through their unions, should own and manage industries, rather than being controlled by private owners or the state</em>.</p> <p>Key aspects of syndicalism include:</p> <ol> <li> <p><strong>Direct Action</strong>: Syndicalists often believe in bypassing political systems and advocating for change through strikes, boycotts, and other forms of direct collective action.</p> </li> <li> <p><strong>Workers’ Self-Management</strong>: Workers should manage the workplaces, making decisions that affect them directly rather than having those decisions made by capitalists or distant bureaucrats.</p> </li> <li> <p><strong>Anti-Capitalism</strong>: Syndicalism is inherently anti-capitalist as it opposes the private ownership of the means of production for profit.</p> </li> <li> <p><strong>Solidarity</strong>: A strong emphasis on solidarity among workers, believing that all labor has a common interest against the exploitative nature of capitalist society.</p> </li> <li> <p><strong>Federalism</strong>: Syndicalism promotes a federated structure of organization, from local to regional to national and international levels, with decision-making power remaining as local as possible.</p> </li> </ol> <p>Syndicalism was particularly influential in the early 20th century and played a significant role in labor movements in Europe, notably in Italy, France, and Spain, where it was linked with large-scale social and political upheavals. It is less prominent today but still exists as a current in labor and leftist movements around the world.</p> <p>Rocker would take for granted that</p> <blockquote> <p>the serious, final, complete liberation of the workers is possible only upon one condition: that of the appropriation of capital, that is, of raw material and all the tools of labor, including land, by the whole body of the workers.</p> </blockquote> <p>Anarcho-syndicalist are convinced that a Socialist economic order cannot be created by the decrees and statues of government, but only by the solidaric collaboration of the workers with hand and brain in each special branch of production; that is, through the taking over of the management of all plants by the producers themselves under such form that the separate groups, plants, and branches of industry are independent members of the general economic organism and systematically carry on production and the distribution of the products in the interest of the community on the basis of free mutual agreements.</p> <p>Such ideas had been put into practice in a dramatic way in the <strong>Spanish Revolution</strong>.</p> <p>The Spanish Revolution of <code class="language-plaintext highlighter-rouge">1936</code> was a profound social upheaval triggered by a military coup against the <code class="language-plaintext highlighter-rouge">Second Spanish Republic</code>, which had been grappling with deep political instability and socioeconomic unrest. This period, marked by an audacious experiment in social and economic transformation, arose in the context of a country deeply divided. On one side were the <code class="language-plaintext highlighter-rouge">Nationalists</code>, consisting of conservative elements like monarchists, large landowners, the Catholic Church, and the fascist <em>Falange</em>, led by General <em>Francisco Franco</em>. On the other, stood the <code class="language-plaintext highlighter-rouge">Republicans</code>, a heterogeneous group that included liberal bourgeoisie, socialists, communists, and anarchists. Years of escalating tensions between the wealthy elite and the working classes, a succession of failed political reforms, and the influence of rising European fascist movements had primed Spain for conflict. The attempted coup fractured the nation, with various regions holding out as strongholds of resistance, particularly Catalonia and Aragon, where anarchist and socialist groups were most influential. As the Republic scrambled to contain the rebellion, a vacuum of power enabled radical left-wing elements to assume control of cities like Barcelona, implementing far-reaching collectivization policies that saw factories, utilities, and farms taken over by workers’ collectives.</p> <p>This radical restructuring of society unfolded amidst a brutal civil war that would last until 1939. Workers, armed and organized into militias, not only fought on the front lines against the Nationalist forces but also transformed the rear guard into a laboratory of social innovation. Land was redistributed, industries were self-managed by employees, and many societal structures were reorganized according to libertarian socialist principles, particularly those championed by the Confederación Nacional del Trabajo (CNT) and the Federación Anarquista Ibérica (FAI). However, the revolution was marred by internal conflicts, as the diverse Republican factions had conflicting goals and ideologies. The Communist Party of Spain, backed by Soviet aid, sought to stabilize the Republic and undermine the anarchists’ influence, leading to intra-Republican strife that culminated in the violent May Days of 1937 in Barcelona. Meanwhile, the Nationalists steadily gained ground, aided significantly by military support from Nazi Germany and Fascist Italy, while the policy of non-intervention adopted by democratic powers effectively isolated the Republican forces. The internal dissension, coupled with the steady advance of Franco’s forces, spelled doom for the Republican cause. By 1939, the Nationalist victory had snuffed out the revolution, and Franco established a dictatorship that would endure for decades, rolling back the revolution’s reforms and ruthlessly suppressing any remnants of opposition.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="noamChomsky"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Kink in Quantum Field Theory, A Broad Outline</title><link href="https://baiyangzhang.github.io/blog/2024/Quantum-Kinks/" rel="alternate" type="text/html" title="Kink in Quantum Field Theory, A Broad Outline"/><published>2024-02-28T00:00:00+00:00</published><updated>2024-02-28T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Quantum-Kinks</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Quantum-Kinks/"><![CDATA[<p>Since the details of calculation can be found in other notes, here I will only talk about the broad outline. I will use as few as formula as possible.</p> <p>We does it mean to <em>quantize</em> the kink? It is similar to what we mean by <em>quantizing the free theory</em>? Generally speaking, there exist two different but equivalent methods, the canonical quantization and path integral quantization. Regarding the canonical quantization,</p> <ol> <li>given a classical field theory, we first need to identify the equation of motion and its set of solutions, namely eigen functions.</li> <li>Expand the fields in these eigenfunctions, this is how we diagonalize the Hamiltonian.</li> <li>Introduce the canonical quantization relation.</li> </ol> <p>In textbooks we always start from some simple model for scalar fields $\phi$, based on which the above steps are illustrated. The vacuum of the field theory is conveniently chose as $\phi=0$. In a sense, we are expanding the field in the background of $0$, namely the vacuum of the model. Then to quantize the kink simply means we change the background about which we expand the scalar field, from zero to the specific kink solution.</p> <p>Using the picture of path integral things are much more obvious. Here, to consider the quantum correction to a kink solution is to include the quantum effects of fluctuation about the classical kink solution.</p> <p>The quantization we described above is akin to the familiar second quantization. Further, if one would like to describe the creation and annihilation of kinks themselves by suitable kink creation and annihilation operators, this would be what people call the <em>third quantization</em>.</p> <p>It turns out that quantum corrections always reduces the kink energy.</p> <hr/> <p>We will adopt perturbative methods to study the quantization of kinks. As we all know, perturbation stops working at strong coupling, $\lambda \gg 1$. What happens to $\mathcal{Z}_ {2}$ and sine-Gordon kinks when the coupling is large? From the classical kink solution we know that the mass of a kink is proportional to $1 / \lambda$, so the kink mass decreases as $\lambda$ increases. Eventually kinks will be even lighter than mesons. At large $\lambda$ we know much more in sine-Gordon model than $\mathcal{Z}_ {2}$ model. In sine-Gordon, we have a dual theory, namely the massive Thirring model.</p> <h1 id="quantization-procedure">Quantization procedure</h1> <p>The broad outline is as following.</p> <ol> <li>Consider the 2D QFT model with compact spatial dimension of size $L$. Let $\phi$ be the degree of freedom. We could adopt either periodic or anti-periodic boundary condition. Eventually we will take $L \to \infty$, but for now it is large but finite.</li> <li>Consider small quantum fluctuation $\psi$ about the kink ground $\phi_ {k}$, namely $\psi = \phi - \phi_ {k}$.</li> <li>Linearize the equation for the fluctuation field $\psi$ (not the original field). Find the solutions, also known as <code class="language-plaintext highlighter-rouge">eigenmodes</code> or <code class="language-plaintext highlighter-rouge">normal modes</code>. Expand the fluctuation field $\psi$ in normal modes.</li> <li><code class="language-plaintext highlighter-rouge">Quantization.</code> Each normal mode corresponds to a quantum harmonic oscillator, with zero point fluctuations. Sum up the zero point energies of all the normal modes. This is the quantum correction we were looking for, but without appropriate renormalization procedure the sum is divergent.</li> <li><code class="language-plaintext highlighter-rouge">Renormalization</code> must be performed. This is the subtle part. The zero point energy of the trivial vacuum (without kink background) must be subtracted from the zero point energy of the kink, since we want the energy of the trivial vacuum to be zero. Also, the energy must be expressed in terms of renormalized parameters.</li> </ol> <p>As we turn on the potential slowly, some of the low-lying modes in the trivial box become the bound states of the kink.</p> <p>Notice that in the trivial vacuum, the solutions to the equation of motion, a.k.a. the scattering states, are plane waves. They are eigenfunctions to both energy and momentum operator; in the presence of a kink, however, the scattering states are now normal modes, which are eigenstates of energy operator but <em>not eigenstates to momentum operator</em>.</p> <p>To consistently compare the energy difference between trivial vacuum sector (just vacuum sector from now on) and kink sector, we need to carefully match discrete modes (thanks to finite box size $L$) in these two.</p> <p>$\mathcal{Z}_ {2}$ kink has two bound states, sine-Gordon has only one, the translational zero mode. The leading order correction to kink mass are quite close in these two cases.</p> <h1 id="introduce-the-particles">Introduce the particles</h1> <p>As in the second quantization of a free quantum field theory, particle creation and annihilation operators are introduced for each of the excitation modes of the kink. This is straightforward, except for the zero mode. The final result is a quantum theory with both kinks and particles, which are sometimes referred to as mesons.</p> <p>The distinctive aspect of the zero mode in second quantization is associated with its time dependence and the reality of its eigenfunction, in contrast to the complex nature of the remaining eigenfunctions. To be more specific, the time dependence of the eigenfunction is $e^{ i\omega t}$, since zero mode has zero energy, the time dependence is trivial. A real eigenfunction means that in the normal mode expansion, instead of</p> \[a_ {k} f _ {k} + a_ {k}^{\dagger} f_ {k}^\ast\] <p>we have a single term, say</p> \[c_ {0} F,\quad F \text{ is the zero mode eigenfunction}\] <p>and $c_ {0}^{\dagger}=c_ {0}$. It is classical since it commutes with everything (that is $c_ {0}$ itself and all the other ladder operators).</p> <h1 id="sign-of-the-leading-order-correction">Sign of the leading order correction</h1> <p>In both $\mathcal{Z}_ {2}$ and sine-Gordon models, the leading order quantum correction reduces the energy of the kink. A argument by Coleman in his private communication show that this observation holds quite true in 1+1 dimension, with boson only. However I am not convinced by the argument. I am not gonna put the argument here, interested readers can refer to section 4.5 of Tanmay Vachaspati’s textbook.</p> <h1 id="boson-fermion-connection">Boson-fermion connection</h1> <p>Boson and fermion operators satisfy different (equal time) commutation relations. It is remarkable that in 1+1 dimension, it is possible to construct explicitly a fermionic operator from bosonic operators. Let $\phi$ be a scalar bosonic field and $\psi_ {1}, \psi_ {2}$ the two components of Dirac spinor (there are only two components allowed in 1D space), then</p> \[\psi_ {1}(x) = C : e^{ P_ {+}(x) } :_ {a}, \quad \psi_ {2} = -i C : e^{ P_ {-}(x) } :,\] <p>The normal ordering is defined in the trivial vacuum sector, justifying us to call it a “defining” sector. $C$ is a c-number coefficient depending on mass and other cutoffs, while $P_ {\pm}$ are q-number function of $\phi(x)$.</p> <p>Note that normal ordering should be treated carefully – normal ordering should be prior to commuting operators that occur within the string.</p> <p>This transformation between fermionic and bosonic operators hold on the level of quantum operators, not just on the level of expectation values. Furthermore, this transformation holds independent of interactions. However, when the bosonic model is sine-Gordon model, the dual fermionic model turn out to be another well-known model – the massive Thirring model.</p> <h1 id="equivalence-of-sine-gordon-and-massive-thirring-models">Equivalence of sine-Gordon and massive Thirring models</h1> <p>The sine-Gordon model is an important field theory model in both classical and quantum physics, known for its rich structure of soliton solutions. The difference between the classical and quantum versions of the sine-Gordon model, particularly in the context of ground states for various parameter values, can be understood in terms of quantization and the effects of quantum fluctuations.</p> <hr/> <p>The Lagrangian of sine-Gordon model is</p> \[L_ {sG} = \frac{1}{2} (\partial_ {\mu}\phi)^{2} - \frac{\alpha}{\beta^{2}}(1-\cos(\beta \phi)).\] <p>where $\alpha$ is a parameter related to the amplitude of the potential, and $\beta$ controls the periodicity of the potential. The term $(1 - \cos(\beta \phi))$ represents a periodic potential with minima occurring at $\phi = 2\pi n/\beta$ for integer $n$, which are the classical ground states of the system.</p> <p>The sine-Gordon model undergoes a change in behavior when quantized, particularly as the parameter $\beta$ is varied. This model does not have a well-defined ground state for</p> \[\beta^{2}&gt;8\pi.\] <p>The key to understanding the issue with having a well-defined ground state for $\beta^2 &gt; 8\pi$ lies in the renormalization group flow of the coupling constants and the concept of quantum fluctuations.</p> <p>In quantum field theory, quantum fluctuations can significantly affect the properties of a model. The coupling constants, such as the one associated with the $\beta$ parameter in the sine-Gordon model, can “run” or change their values at different energy scales due to renormalization effects. This running of the coupling constants is described by the renormalization group (RG) equations.</p> <p>The condition $\beta^2 &gt; 8\pi$ is significant because it marks a threshold beyond which the <em>quantum fluctuations in the sine-Gordon model become so strong that they destabilize the classical vacuum structure</em>. This can be understood in terms of the renormalization of the coupling constant $\beta$: as the energy scale changes, the effective $\beta$ can grow in such a way that the periodic potential becomes “flatter” at the quantum level, making it harder to define distinct vacuum states.</p> <p>For $\beta^2 \leq 8\pi$, the renormalization effects are such that the theory can maintain its integrability and the solitons (or topological excitations) of the sine-Gordon model have well-defined, finite masses. These solitons can be thought of as the “particles” of the quantum field theory, and their stability is crucial for the theory to have a well-defined ground state.</p> <p>When $\beta^2 &gt; 8\pi$, <strong>the quantum corrections make the mass of these solitons diverge</strong>, leading to a loss of particle-like excitations that could stabilize the ground state. This destabilization is related to the fact that the quantum theory no longer supports stable soliton solutions as it does in the classical case or in the quantum case for smaller $\beta$. <em>The vacuum structure becomes ambiguous due to the proliferation of vacuum fluctuations</em>, making it challenging to define a unique ground state. As a result, for $\beta^2 &gt; 8\pi$, the sine-Gordon model does not have a well-defined ground state due to the strong quantum fluctuations that destabilize the classical vacuum structure.</p> <p>In the range $0 &lt; \beta^{2} &lt; 8\pi$, the sine-Gordon model is dual to massive Thirring model whose Lagrangian reads</p> \[\mathcal{L}_ {mT} = i\overline{\psi} \partial\llap{/}\, \overline{\psi} - m \overline{\psi} \psi - \frac{g}{2} j^{\mu} j_ {\mu},\quad j^{\mu} := \overline{\psi} \partial^{\mu}\psi.\] <p>The relations between coupling is</p> \[\frac{g}{\pi} = 1- \frac{4\pi}{\beta^{2}}.\] <p>The fermionic field $\psi$ creates a fermion after quantization, it also creates a kink in terms of $\phi$ field. Hence the sine-Gordon kink is identified with the fermion in the massive Thirring model. The fermion in the massive Thirring model carries a topological charge in the sine-Gordon model.</p> <p>The massive Thirring model does not have soliton solutions (a solution for a Dirac field represents a state that a fermion can occupy, not a classical soliton or anything like that), however fermions can form bound states, for the interaction between a fermion and an anti-fermion is attractive. It can be shown that such a bound state corresponds to a scalar field created by $\phi$.</p> <p>We have a few options for the fundamental degree of freedom, including the boson $\phi$, the fermions $\psi$, the kink of $\phi$, the bound state of $\psi$. Which one is the fundamental dof depends on its <em>mass</em>. For example, in the massive Thirring model let mass of a single fermion be $m$, a pair of fermion-antifermion has mass less than $2m$ due to the interaction between them. As the coupling increases, the interaction strengthens and the mass further reduces. Eventually the bounded pair will be lighter than a single fermion, then the bound state should be regarded as the fundamental degree of freedom. We know that the bounded state corresponds to boson created by $\phi$ field, hence the fundamental description should be the sine-Gordon model. A dual picture exists for $\phi$ and kink, a lattice result is shown in the below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kinkMassvsCoupling-480.webp 480w,/img/kinkMassvsCoupling-800.webp 800w,/img/kinkMassvsCoupling-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kinkMassvsCoupling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The figure show how mass of the $Z_2$ kink depends on the inverse coupling $\beta_ {0}=1 / \lambda_ {0}$, where $\lambda_ {0}\equiv 6 \lambda a^{2}$ is the scaled coupling on the lattice ($48\times 48$) whose lattice spacing is $a$. From the figure we see that the classical value of the kink is always larger then that with quantum correction. When the coupling $\lambda$ is larger then certain value, that is when $\beta_ {0}$ is smaller then certain value, the kink become lighter than the boson, and takes the place of the fundamental dof. It is to say that the fundamental dof becomes the fermion in the massive Thirring model, due to the soliton-particle duality. </div> <p>The construction of fermion operators from boson operator and vise versa has been used extensively in condensed matter physics under the name of <code class="language-plaintext highlighter-rouge">bosonization</code>.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="kink"/><summary type="html"><![CDATA[Since the details of calculation can be found in other notes, here I will only talk about the broad outline. I will use as few as formula as possible.]]></summary></entry><entry><title type="html">Note on Classical Kinks</title><link href="https://baiyangzhang.github.io/blog/2024/Note-on-Classical-Kinks/" rel="alternate" type="text/html" title="Note on Classical Kinks"/><published>2024-02-26T00:00:00+00:00</published><updated>2024-02-26T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Note-on-Classical-Kinks</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Note-on-Classical-Kinks/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>The reason why kinks (and domain walls) are of interest to us is that,</p> <ol> <li>they are known to exist in many laboratory systems and may exist in other exotic settings such as the early universe,</li> <li>they provide a relatively simple setting for studying the non-linear, non-perturbative physics,</li> <li>they shed light on the dynamics of phase transition.</li> </ol> <p>Take the phi-4 model for example, the Lagrangian is (one of many different forms)</p> \[\mathcal{L} = \frac{1}{2}(\partial_ {\mu}\phi)^{2} - \frac{\lambda}{4} (\phi^{2} - \eta^{2})^{2}\] <p>Then we can write down the kink solution, so-called $\mathcal{Z}_ {2}$ kink. The nice thing about $\mathcal{Z}_ {2}$ kink is that almost everything, from the kink solution to its energy, can be written down in closed form, as we shall see in the next chapter.</p> <p>One can rescale the field $\phi$ and the coordinate $y$ such that the Lagrangian reads</p> \[\mathcal{L} = \eta^{2}\left[ \frac{1}{2} (\partial_ {\mu}\phi)^{2} - \frac{1}{4} (\phi^{2}-1)^{2}\right] .\] <p>From this form it is clear that $\eta^{2}$ does not enter the classical equation of motion. It plays the rule of $\frac{1}{\hbar}$.</p> <hr/> <p>Derrick’s theorem states that, broadly speaking, <em>in spatial dimensions greater than (or equal to) two, static, finite-energy, non-singular solutions to the equations of motion (like solitons) cannot exist for scalar field theories with purely local interactions.</em> The theorem, formulated by G.H. Derrick in 1964, implies that such field configurations would be unstable and either collapse to a singularity or disperse. However, in higher spatial space we could still have topological structure with infinite energy, one example of such constructs is <em>domain wall</em>. Domain walls are extended planar structures separating two different vacua.</p> <h1 id="the-model">The Model</h1> <h2 id="phi-fourth-kink">Phi-fourth kink</h2> <p>We will introduce the scalar field in (1+1) dimension. We regard the mass terms as a special type of self-interaction and put it in the interaction $U(\phi(x))$. The Lagrangian is</p> \[\mathcal{L} = \frac{1}{2} (\partial \phi)^{2} - U(\phi), \quad (\partial \phi)^{2} := \partial _ {\mu}\phi \partial ^{\mu}\phi.\] <p>The Euler-Lagrange equation, or the equation of motion, is</p> <p>\(\frac{\partial \mathcal{L}}{\partial \phi}= \partial_ {\mu} \frac{\partial \mathcal{L}}{\partial (\partial _ {\mu}\phi)}\) which in terms of $U(\phi)$ becomes</p> \[\partial _ {\mu}\partial ^{\mu}\phi + \frac{d U(\phi)}{d\phi} =0.\] <p>For a static solution this simplifies to</p> \[\frac{d^{2}\phi}{dx^{2}} = \frac{dU(\phi)}{d\phi}.\] <p>At a certain moment, the instantaneous energy is obtained by the Legendre transformation of the Lagrangian. Then the translational symmetry makes sure that the energy is a conserved quantity in time. Classically, given the field configuration, we just substitute it to the energy functional, then you have the total energy of the field configuration. After quantization, though, there will be infinite zero point energy. But for our concern we will just pretend it is not there and carry on to calculate the difference of energy. This is a common property to quantum field theory models, it is usually possible to calculate the <strong>difference of something</strong> rather than the things themselves, which will be divergent, as it often turn out to be.</p> <p>The vacuum manifold is the manifold given by the minimum of $U( \phi )$. If there exists non-trivial maps</p> \[\text{boundary of space} \to \text{vacuum manifold}\] <p>then we can construct soliton solutions. If there exists non-trivial maps from</p> \[\text{boundary of space-time} \to \text{vacuum manifold}\] <p>then we can construct instanton solutions.</p> <p>The energy functional can be further simplified if we introduce the so-called <code class="language-plaintext highlighter-rouge">superpotential</code> whose derivative gives the potential</p> \[U(\phi) = \frac{1}{2} \left( \frac{dW(\phi)}{d\phi} \right)^{2}.\] <p>Then the energy is given by</p> \[E[\phi] = \frac{1}{2} \int dx \, \left\{ \left( \partial _ {x}\phi \mp \frac{dW}{d\phi} \right) ^{2} \pm \boxed { 2(\partial _ {x}\phi)\left( \frac{dW}{d\phi} \right) }\right\},\] <p>the boxed term can be easily shown to be a total derivative! The integral over</p> \[\left( \frac{\partial\phi}{\partial x} \right) \frac{dW}{d\phi}= \frac{\partial W}{\partial x}\] <p>is a surface term, whose value only depends on the boundary condition of $\phi$. We have</p> \[E[\phi(x)] = \frac{1}{2} \int dx \, \left( \partial _ {x}\phi \mp \frac{dW}{d\phi} \right) ^{2} \pm W{\Large\mid}^{\phi(x=+\infty)}_ {\phi(x=-\infty)}\] <p>which is of the form $E=(\cdots)^{2}+\mathrm{eq}$, the minimum of which is given by $\text{eq}=0$.</p> <p>Recall that for the (anti-)kink solution, the boundary condition is taken such that the potential $U(x)$ takes different vacua at different space boundaries, then $W(x=\pm\infty)$ is fixed. The kink equation will just be</p> \[\boxed { \partial _ {x}\phi \mp \frac{dW}{d\phi} = 0, \quad U(\phi)=\frac{1}{2} \left( \frac{dW}{d\phi} \right)^{2}. }\] <p>where the minus sign is chosen for kinks and plus for anti-kinks. This first-order PDF is called the BPS equation, named after Bogomolny, Prasad, and Sommerfield, the corresponding minimum of energy is called the BPS energy</p> \[E_ {\text{BPS}} = W{\Large\mid}^{\phi(x=+\infty)}_ {\phi(x=-\infty)}.\] <p>Any field configuration with energy equal to BPS energy is said to <strong>saturate the BPS bound</strong>.</p> <p>In terms of the potential $U (x)$, the BPS equation becomes</p> \[\partial _ {x}\phi = \frac{dW}{d\phi} = \sqrt{ 2U(\phi) },\] <p>since the solution to it is a local minimum, the Euler-Lagrange equation is therefore automatically satisfied by solutions of the BPS equation, even though the latter is only a first order differential equation.</p> <hr/> <h2 id="kink-solution">Kink solution</h2> <p>Next we turn to a specific potential, the well-known phi-fourth model. The self-interaction is defined to be</p> \[U(\phi) = \frac{1}{4} (\phi^{2}-1 )^{2} ,\] <p>here we write the potential in a dimensionless and mathematically convenient form. The minimum of $U(\phi)$ is given by $\phi = \pm 1$. If you draw the potential, you’ll see that the potential has two symmetric minima and one maximal given by $\phi=0$.</p> <p>The topological non-trivial solution that connects one vacuum to another smoothly is the classical kink solution. Solve the BPS equation we find</p> \[\phi_ {K} = \tanh \frac{x-a}{l_ {k}},\quad l_ {k} =\sqrt{ 2 }.\] <p>The position of the kink is $a$ and $l_ {k}$ the characteristic size of it. To get a moving kink we just Lorentz boost it by velocity $v$,</p> <p>Besides the kink solution, there is another static solution given by the elliptic sine function. But first, a digression to elliptic functions.</p> <hr/> <p><strong>Jacobi elliptic functions</strong></p> <p>The circular functions arise from ratios of lengths in a circle. In a similar manner, the elliptic functions can be defined by means of ratios of lengths in an ellipse. Many of the key properties of the elliptic functions follow from simple geometric properties of the ellipse.</p> <p>The most general form of Jacobi elliptic functions take two input, the first input behaviors as variable and the second as a parameter that controls the behavior of the function. The first input, the variable, is usually written as either $u$ or $\phi$, related by</p> \[u=\int_{0}^{\phi} d\theta \, \frac{1}{\sqrt{ 1-m \sin ^{2}\theta} }.\] <p>The angle $\phi$ is called the amplitude, a rather confusing name to call an angle.</p> <p>The most general form of elliptic integral is</p> \[f(x)= \int dx \, \frac{A+B}{C+D\sqrt{ S }},\quad A,B,C,D,S\in \mathbb{R}[x], \,\text{deg}(S)=3\text{ or }4.\] <p><em>They can be considered as the generalization of the inverse of trigonometric functions.</em></p> <p>Define something called <code class="language-plaintext highlighter-rouge">modulus</code> $k$ satisfy $0&lt;k^{2}&lt;1$. This is sometimes written in terms of $m:=k^{2}$ or the <code class="language-plaintext highlighter-rouge">modulus angle</code> $k=\sin \alpha$. The <strong>incomplete</strong> <code class="language-plaintext highlighter-rouge">elliptic integral</code> <em>of the first kind</em> reads</p> \[F(\phi,k)=\int_{0}^{\sin \phi} dt \, \frac{1}{\sqrt{ (1-t^{2})(1-k^{2}t^{2}) }},\quad 1\leq k^{2}\leq 1.\] <p>If we let $t=\sin \theta$ then</p> \[F(\phi,k)=\int_{0}^{\phi} d\theta \, \frac{1}{\sqrt{ 1-k^{2}\sin ^{2} \theta }}, \quad 0\leq \phi\leq \frac{\pi}{2}\] <p>This is the <em>incomplete</em> <code class="language-plaintext highlighter-rouge">Legendre elliptic integral.</code> The complete Legendre elliptic integral is obtained by setting $\phi$ to its maximal value, i.e., $\phi=\pi / 2$ or $\sin \phi=1$.</p> <p>The incomplete elliptic integral of the <em>second kind</em> reads</p> \[E(\phi,k)=\int_{0}^{\phi} d\theta \, \sqrt{ 1-k^{2}\sin ^{2} \theta }.\] <p>or equivalently,</p> \[E(\phi,k)=\int_{0}^{\sin \phi} dt \, \sqrt{ \frac{1-k^{2}t^{2}}{1-t^{2}} } ,\quad 0\leq k^{2}\leq 1.\] <p>Similarly, the complete elliptic integral of the first kink can be obtained by setting $\phi=\pi / 2$.</p> <p>As an example of the Jacobian elliptic function $sn$ we can write</p> \[u(x=\sin \phi,k)=F(\phi,k)=\int_{0}^{x} dt \, \frac{1}{\sqrt{ (1-t^{2})(1-t^{2}k^{2}) }}\] <p>then the inverse of $u(x)$ is defined to be</p> \[x = sn (u,k)\] <p>or</p> \[u(x,k)=\int_{0}^{x} dt \, \frac{1}{\sqrt{ (1-t^{2})(1-t^{2}k^{2}) }} .\] <p>While there are 12 different types of Jacobian elliptic functions based on the number of poles and the upper limit on the elliptic integral, the three most popular are the co-polar trio of sine amplitude, $sn(u, k)$, cosine amplitude, $cn(u, k)$ and the delta amplitude elliptic function, $dn(u, k)$ satisfy</p> \[sn^{2}+cn^{2}=1,\quad k^{2}sn^{2}+dn^{2}=1.\] <p>Elliptic integral of the third kind reads</p> \[\Pi(\phi,n,k)=\int_{0}^{\sin \phi} dt \, \frac{1}{(1+nt)^{2}\sqrt{ (1-k^{2})(1-k^{2}t^{2}) }}\] <p>with the same range of $k$. Or equivalently</p> \[\Pi(\phi,n,k)=\int_{0}^{ \phi} d\theta \, \frac{1}{(1+n\sin ^{2}\theta)\sqrt{ 1-k^{2}\sin ^{2}\theta }}.\] <p>As mentioned before, the three standard forms of the Jacobi elliptic functions $sn,cn,dn$ are the sine, cosine and delta amplitude elliptic functions respectively. They are obtained by inverting the elliptic integral of the first kind</p> \[u\equiv u(\phi\mid k)\equiv u(\phi,k) \equiv F(\phi,k)=\int_{0}^{\phi} d \theta \, \frac{1}{\sqrt{ 1-k^{2}\sin ^{2} \theta }} .\] <p>The parameter before $\sin ^{2}\theta$, i.e., $k$ is called the <code class="language-plaintext highlighter-rouge">elliptic modulus</code>, and the upper bound of the integral is called the <code class="language-plaintext highlighter-rouge">Jacobi amplitude</code>, denoted $\text{amp}$. The inverse of $u(\phi)$ is</p> \[\phi = u^{-1}(\phi,k)=:\text{amp}(u,k)\] <p>and we can write the Jacobi elliptic functions in terms of $\phi$,</p> \[\begin{align} sn(u,k)&amp;=\sin \phi\equiv\sin(\text{amp}(u,k)), \\ cn(u,k)&amp;=\cos\phi=\cos(\text{amp}(u,k)), \\ dn(u,k)&amp;=\sqrt{ 1-k^{2}\sin ^{2}\phi }. \end{align}\] <p>These functions are <em>doubly periodic</em> generalizations of the trigonometric functions satisfying</p> \[\begin{align} sn(u,0)&amp;=\sin u, \\ cn(u,0)&amp;=\cos u, \\ dn(u,0)&amp;=1. \end{align}\] <p>since $u=\phi$ at $k=0$ .</p> <hr/> <p>The familiar kink solution in our notation is</p> \[\phi_ {K}(x) = \tanh\left( \frac{x-a}{l_ {K}} \right),\quad l_ {K}=\sqrt{ 2 }.\] <p>where $a$ is the center of the kink and $l_ {K}$ the characteristic size.</p> <p>It is easy using Mathematica to check that this solution satisfies both the second order equation of motion and the first order BPS equation. The anti-kink solution is just $-\phi_ {K}(x)$.</p> <p>We claim without proof that there exists another static solution to the kink equation,</p> \[\phi(t)=\phi_ {0}\, \text{sn}(bx,k),\quad k^{2}=\frac{\phi_ {0}^{2}}{2-\phi_ {0}^{2}},\quad b^{2}=1-\frac{\phi_ {0}^{2}}{2}.\] <hr/> <p>What about a moving kink then? Firstly, the center of the kink will move with velocity $v$ thus we should replace $x-a$ with $x-a-vt$ where $v$ is the kink velocity. Secondly, from special relativistic we know that a moving frame will experience space contraction, thus we should multiply $x-a-vt$ by $\gamma$ factor, which is $\gamma=\frac{1}{\sqrt{ 1-v^{2} }}$ in natural units. Then a right-moving kink can be written as</p> \[\phi_ {K,v}= \tanh\left( \frac{x-a-vt}{\sqrt{ 2(1-v^{2}) }} \right).\] <h3 id="kink-antikink-collisions">Kink-antikink collisions</h3> <p>When the kink and antikink are separated far away from each other, the interaction between them is negligible and the configuration with a kink and a antikink is simply the addition of kink and antikink solutions, up to some additive const to make sure that the field goes to the vacuum at the space boundaries. The center of the kink, for a right moving one, is $-a+vt$ where $v$ is the velocity. The kink-antikink configuration reads</p> \[\phi_ {K \overline{K}}(t,x)= \tanh\left( \frac{x-(-a+vt)}{\sqrt{ 2 }\sqrt{ 1-v^{2} }} \right) +\tanh\left( \frac{x-(a-vt)}{\sqrt{ 2 }\sqrt{ 1-v^{2} }} \right) -1,\tag{1}\] <p>Note the last term $-1$ which is there to make sure the correct boundary condition is satisfied.</p> <p>When the kink is too close to the antikink, the simple configuration Eq.(1) can no longer satisfy the equation of motion, in physical terms there will be non-linear interaction between the kink and the antikink. To find the solution to EOM we unfortunately have to rely on numerical calculation. To be specific, in numerical calculation we</p> <ol> <li>make space-time into 2-dimensional grid, the time grid is usually required to be finer than space grid to make the numerical results more reliable. Set up the initial condition, including the kink-antikink configuration and their initial speed.</li> <li>Time-evolve the initial spate using the equation of motion, make sure the position of the kink and antikink are not at the boundary of the space.</li> <li>Do some consistency checks, for example make sure that the total energy is (reasonably) conserved, or test that the numerical method we used, when applied to a kink at still, will remain a kink at still.</li> </ol> <p>Even without specific calculation we can tell that the kink and antikink can’t just pass each other and keep moving, since if we move the antikink to the left and kink to the right, the field in the between will take value</p> \[\phi(x){\Large\mid}_ {x=0} = -1-1-1=-3\] <p>where the first $-1$ comes from the antikink $\phi_ {\overline{K}}(\infty)=-1$, second from the kink and third form $-1$ in Eq.(1). However $-3$ is not a vacuum configuration! The energy will start to accumulate in between the kink and the antikink until the kinetic energy of them a re exhausted hence they have no other choice then to term back and follow the way they came. Thus they will “scatter” off each other.</p> <p>A closer study of the case reveals that kink and antikink don’t always scatter off each other. During the scatter, the energy will dissipate, so the speed after scatter will decrease, if the initial velocity is too low, below some critical velocity $v_ {\text{cr}}$, there will not be enough energy left for the kink and antikink to escape each other, after collision they will try to separate but can only separate by a finite distance, before they could fully reform into kink and antikink, the energy would be depleted and they would have no choice to return to each other and scatter again, forming an oscillation. This object is often referred to as a <code class="language-plaintext highlighter-rouge">bion</code> or sometimes <code class="language-plaintext highlighter-rouge">oscillon</code>. In this note we will adopt the term bion, referring to the bound state of kink-antikink pair. In $3+1$ dimension, bions are also called quasi-breathers.</p> <p>Note that bion is a quasi-long-lived state, which will eventually decay into trivial vacuum, but the time it takes is so long that we can safely treat it as a stable particle.</p> <p>For $v_ {\text{in}}&gt;v_ {\text{cr}}$, where $v_ {\text{in}}$ is the incoming velocity, kink and antikink always bounce and escape to infinity. Below $v_ {\text{cr}}$ things are more interesting, there exists windows where the kink-antikink pair will scatter once, deposit some energy between them, the deposited energy form a vibrational mode, the kink-antikink pair come back to each other after a little while and bounce again, this time retrieves the deposit energy, and move away from each other to the infinity. The time of bounce needed before they eventually escape each other could be two, three, etc., the corresponding incoming velocity windows are called two-bounce escape windows, three-bounce escape windows, and so on.</p> <h3 id="collective-coordinate-approximation">Collective Coordinate Approximation</h3> <p>Consider again the kink-antikink scatter. Work in the center of mass frame, the position and velocities are symmetric, we need only one parameter, namely the position of the kink as a function of time $t$, to uniquely fix the configuration. Use $a(t)$ to denote the position of the kink. Now the question is, can be write down a (low energy) effective theory in terms of $a(t)$ only? If we can, it would be the collective coordinate approximation (CCA) model.</p> <p>We can substitute the field configuration</p> \[\phi_ {K \overline{K}}=\tanh\left( \frac{x+a(t)}{\sqrt{ 2 }} \right) - \tanh\left( \frac{x-a(t)}{\sqrt{ 2 }} \right) - 1\] <p>into the phi-fourth Lagrangian for $\phi(x,t)$ and obtain an effective Lagrangian in terms of $a(t)$ then <em>integrate over the space</em>. It should adopt the form</p> \[L_ {\text{CCA}} = L_ {\text{CCA}}(a,\dot{a}) = \frac{1}{2} m_ {a}\dot{a}^{2}-V(a)\] <p>where $m_ {a}$ is the effective mass for the kink, it is <em>position-dependent</em> in general.</p> <p>At large separation, we find that the mass parameter $m(a)$ and the effective potential $V(a)$ both approach $2M_ {K}$, the mass of two isolated static kinks. To be specific, we have</p> \[m(a)=I_ {+}(a),\quad V(a) = \frac{1}{2} I_ {-}(a) +\frac{1}{4} \int_{-\infty}^{\infty} dx \, (1-\phi^{2}_ {K \overline{K}})^{2}\] <p>where</p> \[I_ {\pm }= 2M_ {K} \pm \int_{-\infty}^{\infty} dx \, \frac{1}{\cosh ^{2}((x+a) / \sqrt{ 2 })\cosh ^{2}((x-a) / \sqrt{ 2 })} ,\] <p>where the integral goes to zero exponentially as $a$ increases.</p> <p>The effective potential can more or less account for the free bounce and formation of a bion, but can not explain the escape window, or the relativistic effects that might happen when the speed of kinks are high.</p> <h3 id="gluing-static-solutions">Gluing static solutions</h3> <p>Another way to construct the bion configuration is by gluing together three piece, 1) the left half of the kink solution, 2) half a elliptic sine function and 3) the right half of the antikink. Then the numerical methods can be used to evolve the state. However, to start the numerical evolvement, we also need to know the field configuration at the next-to-start time-slice, we can fix it by hand, by shrink the elliptic sine function a little bit, let $\phi_ {0}$ decrease a little bit.</p> <h3 id="kink-impurity-interactions">Kink-impurity interactions</h3> <p>Roughly speaking we have two ways to study the evolution of kinks-antikink states,</p> <ol> <li>Take the superposition of two static solutions, i.e. the kink solution and the antikink solution. It is not strictly speaking the solution of the equation of motion, thus the equation of motion will evolve it in a non-trivial way.</li> <li>Instead, we can start with some exact solution of the equation of motion, but then use a slightly different equation of motion to evolve it. This is the situation encountered in the description of kink interacting with impurities.</li> </ol> <p>The impurities could appear for a couple of reasons. For example, the phi fourth theory is usually the low energy effective theory of some other theory defined at UV, and this “UV completion” of our phi fourth theory might have some impurities, or defects, which can then get passed on to the phi-fourth theory. This impurity could be some defect embedded in the crystal structure, for example. To describe such process, we can modify the phi-fourth potential by</p> \[\frac{1}{4}(\phi^{2}-1)^{2}\to \frac{1}{4} (\phi^{2}-1)^{2}(1-\epsilon \delta(x-x_ {0})).\] <p>When $\epsilon&gt;0$ the defect acts like a potential barrier, if $\epsilon&lt;0$ a well.</p> <p>With this modified potential we can then talk about its equation of motion. In numerical calculation we can approximate the delta function either by a Kronecker delta function, or a Gaussian shape high and narrow.</p> <p>To be more specific, consider a static kink solution starting from $a=6$, moving towards am impurity located at $x_ {0}=0$, with fixed impurity strength $\left\lvert \epsilon \right\rvert=0.5$. The figure below shows the phase diagram.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/phase-480.webp 480w,/img/phase-800.webp 800w,/img/phase-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/phase.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The right panel shows a attractive impurity. If the incoming velocity of the kink is large enough, it will just pass through the impurity and deposit some vibrational energy at the impurity, as shown by the read line. If the velocity is too low then it will be trapped by the attractive impurity, starts to oscillate about the impurity, shown by the blue line. If the velocity is in the resonance window, the kink will bounce back resonantly, shown by the green line. </div> <p>On the left panel a repulsive impurity $\epsilon=-0.5$ is shown, for low velocity the kink will bounce back, as shown by the blue line; for high velocity the kink can overcome the impurity barrier and keep propagating, as shown by the red line.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="QuantumFieldTheory"/><category term="Kink"/><category term="Meson"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Note on The Moral Foundations of Politics</title><link href="https://baiyangzhang.github.io/blog/2024/Note-on-The-Moral-Foundations-of-Politics/" rel="alternate" type="text/html" title="Note on The Moral Foundations of Politics"/><published>2024-02-25T00:00:00+00:00</published><updated>2024-02-25T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Note-on-The-Moral-Foundations-of-Politics</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Note-on-The-Moral-Foundations-of-Politics/"><![CDATA[<h1 id="enlightenment-politics">Enlightenment Politics</h1> <blockquote> <p>f there is a single overarching idea shared in common by adherents to different strands of Enlightenment thinking, it is faith in the power of human reason to understand the true nature of our circumstances and ourselves. Human improvement is measured by the yardstick of <strong>individual rights</strong> that embody, and protect, <strong>human freedom</strong>.</p> </blockquote> <blockquote> <p>Descartes announced that he was in search of propositions that are impossible to doubt. His famous example, known as the <code class="language-plaintext highlighter-rouge">cogito</code>, was ‘‘I think, therefore I am.’’</p> </blockquote> <blockquote> <p>Immanuel Kant defined in <code class="language-plaintext highlighter-rouge">The Critique of Pure Reason</code> (1781), of placing knowledge ‘‘on the secure path of a science.’’</p> </blockquote> <blockquote> <p>These developments in philosophy reflected and reinforced the emergence of modern scientific consciousness.</p> </blockquote> <p>Such ideas, as necessary conditions for the development of natural science (not merely technology), seems to never had appeared in China. Year 1781 is the year 乾隆四十六年 in China, one of the most closed, ignorant, and autocratic era in history.</p> <blockquote> <p>During the seventeenth and eighteenth centuries, when the hallmark of scientific knowledge was indubitable certainty, ethics, political philosophy, and the human sciences were regarded as superior to the natural sciences. This view seems strange from the vantage point of the twenty-first century, when fields like physics, chemistry, astronomy, geology, and biology have all advanced with astonishing speed to discoveries that would have been unimaginable in the eighteenth century.</p> </blockquote> <h2 id="the-workmanship-ideal-of-knowledge">The Workmanship Ideal of Knowledge</h2> <blockquote> <p>The first distinctive feature of the early Enlightenment concerns the range of <code class="language-plaintext highlighter-rouge">a priori knowledge</code>, the kind of knowledge that either follows from definitions or is otherwise deduced from covering principals. This is the kind of knowledge Descartes had in mind when he formulated his cogito and that Kant located in the realm of ‘‘analytic judgments.’</p> </blockquote> <p><strong>Epistemology</strong> is a branch of philosophy that studies the nature, origin, and limits of human knowledge. The term comes from the Greek words “episteme,” meaning knowledge or understanding, and “logos,” meaning study or discourse. Epistemology addresses questions such as:</p> <ul> <li>What is knowledge?</li> <li>How is knowledge acquired?</li> <li>What do people know?</li> <li>How do we know what we know?</li> <li>What are the limits of human knowledge?</li> <li>What makes beliefs justified or rational?</li> </ul> <p>In exploring these questions, epistemology deals with the definition of knowledge and its scope and limits. It often involves debating between different theories of knowledge, such as empiricism (the idea that knowledge comes primarily from sensory experience), rationalism (the idea that reason is the main source of knowledge), and constructivism (the idea that knowledge is constructed by individuals through their interactions with the world).</p> <p>Immanuel Kant distinguished between two types of judgments: <code class="language-plaintext highlighter-rouge">analytic</code> and <code class="language-plaintext highlighter-rouge">synthetic</code>. These distinctions are central to his philosophy, especially in his work “Critique of Pure Reason.”</p> <ol> <li> <p><strong>Analytic Judgments</strong>: An analytic judgment is one where the predicate (the part of the sentence that says something about the subject) is contained within the subject itself. The truth of an analytic judgment is derived from the meanings of the words involved and logical reasoning. They are tautological in nature and do not add any new information about the world. For example, the statement “All bachelors are unmarried” is analytic because the predicate “unmarried” is part of the definition of the subject “bachelor.”</p> </li> <li> <p><strong>Synthetic Judgments</strong>: A synthetic judgment, on the other hand, is one where the predicate adds something to the subject that is not contained within it. The truth of a synthetic judgment is determined through how our concepts relate to the world and cannot be known just by understanding the meanings of the words. They require empirical investigation or intuition. For instance, “The cat is on the mat” is a synthetic judgment because the concept of “the cat” does not inherently include the concept of “being on the mat.”</p> </li> </ol> <p>Kant’s distinction between analytic and synthetic judgments is fundamental to his epistemology, particularly in addressing the question of how human beings can have knowledge about the world. He further introduced the concept of “synthetic a priori” judgments, which are synthetic judgments that are known independently of experience (a priori), like mathematical truths.</p> <hr/> <p>The <code class="language-plaintext highlighter-rouge">creationist</code> or <code class="language-plaintext highlighter-rouge">workmanship</code> theory in political science, often associated with the work of John Locke, is a theory of political obligation. It suggests that political authority and legitimacy derive from the consent of the governed, likening the role of the government or ruler to that of a craftsman or creator who constructs a system with the consent and for the benefit of the people.</p> <p>This theory is rooted in the idea that political and social structures are artificial constructs, made by human beings, unlike natural phenomena. The “creationist” aspect implies that political structures are deliberately created or constructed, rather than organically evolved. The “workmanship” aspect emphasizes the idea that the creators or rulers of these structures have a responsibility to the people they govern, similar to how a craftsman is responsible for the quality and function of their creation.</p> <p>Locke’s theory was revolutionary at its time because it challenged the prevailing notion of the divine right of kings, suggesting instead that political authority is justified only when it serves the interests of the governed and respects their rights. This theory laid the groundwork for modern concepts of democracy, individual rights, and the social contract.</p> <hr/> <p>Thomas Hobbes and John Locke, two prominent philosophers, had distinct views on natural law, reflecting their differing perspectives on human nature and the ideal structure of society.</p> <p>Hobbes, in his work “Leviathan,” presented a rather pessimistic view of human nature. He believed that in the state of nature (a hypothetical condition without government or laws), humans are driven by self-interest and a desire for self-preservation, leading to a “war of all against all” (bellum omnium contra omnes). In this state, life would be “solitary, poor, nasty, brutish, and short.”</p> <p>For Hobbes, natural law is a set of precepts or general rules, <em>discovered by reason</em>, which prohibit anything destructive to one’s own life. <em>It’s based on the right of every individual to preserve their own life</em>, leading to the conclusion that humans should seek peace. This is where his famous concept of the social contract comes into play: individuals surrender some of their freedoms and submit to the authority of a ruler (or a ruling assembly) to ensure their own safety and peace. Thus, Hobbes’s natural law is fundamentally about self-preservation and the avoidance of harm to others as a means of securing one’s own safety.</p> <p>Locke’s view, as articulated in “Two Treatises of Government,” is more optimistic about human nature. He believed that in the state of nature, humans live in a state of equality and freedom, not inherently prone to violence or war. For Locke, the <em>law of nature is a moral guide based on the belief that God has given the world to all people in common</em>. It teaches that, since all are equal and independent, no one ought to harm another in their life, health, liberty, or possessions.</p> <p>Locke’s natural law is grounded in the rights to life, liberty, and property. It includes the idea that people have the obligation to respect the rights of others. His social contract theory suggests that people form governments to protect these natural rights. If a government fails to do so, citizens have the right to overthrow it. This view laid the groundwork for modern democracy and significantly influenced the development of political philosophy in the Western world.</p> <p>So, Hobbes saw natural law as a means of avoiding the brutal state of nature through self-preservation and peace, whereas Locke viewed natural law as a moral guide ensuring equality and the inherent rights of life, liberty, and property.</p> <hr/> <blockquote> <p>A basic issue for Locke and many of his contemporaries was the ontological status of natural law and in particular its relation to God’s will.</p> </blockquote> <p>In this sentence, “ontological status” refers to the fundamental nature or essence of natural law, especially in relation to its existence and its relationship to God’s will. Ontology, in philosophy, is the study of being or existence, and it deals with questions concerning what entities exist or can be said to exist, and how such entities can be grouped, related within a hierarchy, and subdivided according to similarities and differences.</p> <p>So, when discussing the “ontological status of natural law” in the context of John Locke and his contemporaries, the focus is on understanding the very essence of natural law: whether it exists as an objective reality independent of human beings, how it relates to or derives from God’s will, and what its fundamental characteristics are. This was a central topic in the philosophical and theological debates of that era, particularly in the context of determining the basis and legitimacy of moral and legal principles. Locke and many others were engaged in trying to understand whether natural laws were inherent aspects of the universe, ordained by God, or whether they were constructs of human reason and society.</p> <p>“Will-centered” refers to the philosophical position known as voluntarism. This is a theory that emphasizes the role of the will, either divine or human, in various philosophical contexts. In the context of Locke’s moral and political writings, being “will-centered” or a voluntarist means that Locke ultimately leaned towards the view that natural law and moral principles are determined by the will, particularly the will of God, rather than being inherent or objective truths that exist independently of any will.</p> <p>In Locke’s time, the debate about the nature of natural law often centered around whether natural laws were intrinsic to the universe (a position known as intellectualism or rationalism) or whether they were decrees of God’s will (voluntarism). A will-centered or voluntarist approach suggests that moral and legal norms derive their authority from an act of will, particularly the divine will, rather than from reason alone or from the inherent nature of reality. In this view, what is right or wrong, just or unjust, is so because God wills it to be that way, and human beings understand and follow these laws through revelation, religious teachings, or other means of discerning God’s will.</p> <blockquote> <p>Locke distinguished “ectype”’ from “archetype” ideas: ectypes are general ideas of substances, and archetypes are ideas constructed by man.</p> </blockquote> <p>John Locke’s distinction between “ectype” and “archetype” ideas is a crucial aspect of his epistemological theory, which he discusses in his work “An Essay Concerning Human Understanding.” This distinction is part of his broader inquiry into the nature of human knowledge and understanding.</p> <p>In Locke’s philosophy, archetypes are the original models or patterns from which copies are made. They are the fundamental, primary ideas that exist in the mind of God or, in a more secular interpretation, the perfect, abstract forms of things. When Locke refers to archetypes as ideas constructed by man, he means that these are the ideal standards or criteria we hold in our minds for categorizing and understanding the world. They represent our understanding of what the essential characteristics of a particular thing are.</p> <p>For instance, the archetype of a tree would be the idealized concept or mental representation of what a tree is supposed to be. This archetype is not derived from any particular tree but is a kind of composite or abstracted idea of “treeness” that we use to recognize and categorize individual trees.</p> <p>Ectypes, on the other hand, are derivative or secondary ideas. They are the imperfect copies or generalizations that we derive from our experience with individual instances in the world. Ectype ideas are more about the general ideas of substances we form based on our sensory experiences and observations. When we see many individual trees, for example, we form a general idea of what a tree is - this is an ectype. It’s a more practical, experiential idea based on the aggregation of real-world instances.</p> <p>In summary, Locke’s distinction between archetype and ectype ideas can be understood as a differentiation between the idealized, abstract concepts we hold in our minds as standards (archetypes) and the more practical, general ideas we form based on our sensory experience of the world (ectypes). Archetypes are about the essence or ideal form of things, while ectypes are about the general, often imperfect, concepts we derive from actual experiences.</p> <h2 id="the-preoccupation-with-certainty">The Preoccupation with Certainty</h2> <blockquote> <p>The post-Humean Enlightenment tradition has been marked by a fallibilist view of knowledge. All knowledge claims are fallible on this account, and science advances not by making knowledge more certain but by producing more knowledge. Recognizing the corrigibility of all knowledge claims and the possibility that one might always be wrong exemplifies the modern scientific attitude. As Karl Popper (1902-1994) noted, the most that we can say, when hypotheses survive empirical tests, is that they have not been falsified so that we can accept them provisionally.</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">Value judgments</code> are statements or opinions that express an evaluation, typically of something’s worth, beauty, goodness, or morality. Examples include statements like “Lying is wrong,” or “This painting is beautiful.” A.J. Ayer was a key figure in the logical positivist movement, which held that for a statement to be meaningful, it must be either empirically verifiable (i.e., testable by observation or experiment) or analytically true (true by definition, like mathematical or logical statements). In logical positivism, a <code class="language-plaintext highlighter-rouge">proposition</code> is a statement that can be either true or false. It’s a claim about the world that can, <em>at least in principle</em>, be tested and verified or falsified.</p> <p>The Logical Positivist movement, also known as Logical Empiricism, was a philosophical movement that emerged in the early 20th century. It primarily revolved around a group of philosophers associated with the Vienna Circle (<code class="language-plaintext highlighter-rouge">Moritz Schlick</code>, <code class="language-plaintext highlighter-rouge">Hans Hahn</code>, ), along with others like A.J. Ayer in Britain. This movement sought to apply the rigor of scientific methodology to philosophy, with a significant focus on the analysis of language and the verification of statements.</p> <p>Key Features of Logical Positivism include</p> <ol> <li> <p><strong>Verification Principle</strong>: The central tenet of Logical Positivism is the verification principle. This principle asserts that a statement is only meaningful if it can be empirically verified or is analytically true (true by virtue of its meaning, like “All bachelors are unmarried”). The idea was to eliminate metaphysical and abstract discussions that couldn’t be supported by empirical evidence or logical reasoning.</p> </li> <li> <p><strong>Empiricism and Science</strong>: Logical Positivists emphasized the importance of empirical evidence and scientific methods in acquiring knowledge. They viewed science as the model for all true knowledge.</p> </li> <li> <p><strong>Rejection of Metaphysics</strong>: They were critical of metaphysics and other traditional philosophical endeavors, which they saw as meaningless since such statements couldn’t be empirically verified. They believed that many philosophical problems arose from misunderstandings of language and could be resolved by clarifying the language used.</p> </li> <li> <p><strong>Language and Meaning</strong>: A significant focus was placed on the analysis of language, particularly the language of science. They aimed to clarify how language is used in scientific theories and to distinguish between meaningful and meaningless statements.</p> </li> <li> <p><strong>Influence of Wittgenstein</strong>: Although not officially part of the Vienna Circle, Ludwig Wittgenstein’s early work, especially his “Tractatus Logico-Philosophicus,” significantly influenced Logical Positivism. Wittgenstein argued that <em>much of philosophy consists of nonsensical propositions and that the role of philosophy should be to clarify thought and language</em>.</p> </li> <li> <p><strong>Ethical and Aesthetic Statements</strong>: Logical Positivists generally considered ethical and aesthetic statements to be expressions of emotions or subjective preferences, rather than statements that could be true or false.</p> </li> </ol> <p>The “positivism” component is linked to the movement’s commitment to a scientific and empirical approach to knowledge. Positivism, as a philosophical stance, argues that knowledge should be based on positive, observable facts and their logical and mathematical treatment. It rejects introspection and intuition as sources of knowledge and instead emphasizes empirical evidence obtained through observation and experimentation. Logical Positivists extended this approach by asserting that statements must be empirically verifiable (or analytically true) to be meaningful.</p> <hr/> <p>Somewhat to my surprise, Karl Popper is not a member of the Vienna circle even though they shared many intellectual engagements. Furthermore, Karl Popper is even critically oppositional. The Vienna Circle advocated for the verification principle, which held that a statement is meaningful only if it can be empirically <em>verified</em>. Popper challenged this view, proposing <em>falsificationism</em> instead. According to Popper, scientific theories cannot be conclusively verified but can be falsified. He argued that a theory is scientific if it is testable and can potentially be refuted by evidence. This approach places a greater emphasis on the role of empirical refutation rather than verification.</p> <p>Also, Popper was critical of what he called <code class="language-plaintext highlighter-rouge">historicism</code> – the belief that <em>history unfolds according to deterministic laws or principles</em>. He argued that such theories, which <em>were often used to justify authoritarian regimes</em>, are fundamentally flawed. He believed that historicism led to totalitarianism because it promoted the idea that certain individuals or groups had access to inevitable truths about societal development, thus justifying their absolute rule. Popper advocated for what he termed an <code class="language-plaintext highlighter-rouge">open society</code>. An open society, in his view, is characterized by a democratic government, individual freedoms, and a critical attitude towards tradition and authority. It allows for change and improvement through rational and critical discourse, as opposed to the unquestioning acceptance of dogmatic principles.</p> <p>Just as Popper applied the <em>principle of falsifiability</em> to scientific theories, he suggested that political policies should also be subjected to critical scrutiny and should be alterable in the face of new evidence or arguments. He was wary of any political theory or system that claimed to have absolute or final answers.</p> <hr/> <p>According to Ayer, the expression of a value judgment is not a proposition since it can not be judged by right and wrong, the question of truth or falsehood does not here arise.</p> <p>Regarding ethics, Ayer points out that many theorists in ethics tend to treat statements about the causes and characteristics of our ethical feelings as if these statements were definitions of ethical concepts. For example, a theory might claim that an action is good if it promotes happiness. Here, the cause of the ethical feeling (happiness) is used to define the ethical concept (good). Ayer argues that ethical concepts are <em>pseudo-concepts</em>, since ethical concepts, in his view, is neither empirically verifiable or analytically correct.</p> <p>Ayer’s stance is closely associated with <code class="language-plaintext highlighter-rouge">emotivism</code>, a meta-ethical view that suggests <em>ethical statements do not assert propositions but express emotional attitudes</em>. According to emotivism, saying “Stealing is wrong” is akin to expressing one’s disapproval of stealing, rather than making an objective claim about the nature of stealing.</p> <h2 id="the-centrality-of-individual-rights">The Centrality of Individual Rights</h2> <blockquote> <p>In addition to faith in science, the Enlightenment’s central focus on individual rights differentiates its political philosophy from the ancient and medieval commitments to order and hierarchy. This focus brings the freedom of the individual to the center of arguments about politics. This move was signaled in the natural law tradition by a shift in emphasis from the logic of law to the idea of natural right.</p> </blockquote> <p>Hobbes contended that it was customary to conflate “Jus and Lex, law and right”. Yet he made the distinction that right, consisted in liberty to do, or to forbeare, whereas law, determines and binds to one of them. Similarly by Locke.</p> <p>John Locke’s oppinion on natural law is as the following. In his work <em>Essays on the Law of Nature</em>, Locke argues a moral law inherent in the world and discoverable through reason.</p> <p>Key points of Locke’s argument include:</p> <ol> <li> <p><strong>Natural Law and Reason:</strong> Locke posits that natural law is an aspect of the natural world, similar to physical laws. According to him, this <em>moral law can be discovered through the use of reason, without the need for divine revelation</em>.</p> </li> <li> <p><strong>Moral Obligations:</strong> He argues that <em>natural law imposes moral obligations on individuals</em>. These moral principles are universal and apply to all people, regardless of their culture or society.</p> </li> <li> <p><strong>Rights and Duties:</strong> Locke’s view of natural law is closely tied to his ideas about individual rights and duties. He believes that natural law forms the basis for understanding human rights, especially the right to life, liberty, and property.</p> </li> <li> <p><strong>Foundation for Political Theory:</strong> These essays lay the groundwork for Locke’s later political theories, particularly those presented in his famous works, “Two Treatises of Government.” He uses the concept of natural law to argue for the rights of individuals and the limitations of governmental power.</p> </li> <li> <p><strong>Human Equality:</strong> Locke emphasizes the inherent equality of all human beings, derived from their natural state. This idea is a critical aspect of his argument against absolute monarchy and for the formation of governments based on the consent of the governed.</p> </li> <li> <p><strong>Religious Tolerance:</strong> Although not as explicitly developed in these essays as in his later works, Locke’s concept of natural law also leads to his advocacy for religious tolerance, seeing religious belief as a matter of individual conscience.</p> </li> </ol> <p>In summary, Locke’s “Essays on the Law of Nature” propose that there is a moral law inherent in the natural world, understandable through human reason, and that this law underpins human rights and forms the basis for just and ethical governance.</p> <hr/> <p>John Locke’s <code class="language-plaintext highlighter-rouge">voluntarist theology</code> reflects his views on the nature of God and the relationship between divine will and moral law. The emphasis is on <em>the will will (voluntas in Latin, hence the name) of God of God as the primary or sole source of moral law</em>. Locke’s voluntarism posits that <em>moral laws are decrees of God’s will</em>. In this view, what is morally right or wrong is so because God wills it, and not necessarily because it aligns with any intrinsic moral truths or rational principles independent of God’s will. Locke emphasizes the <em>absolute freedom</em> and <em>omnipotence</em> of God. He argues that God’s will is not bound by any external standards or principles. Therefore, moral laws are a product of God’s free choice.</p> <p>While Locke is a proponent of reason and believes that human beings can discover moral truths through rational inquiry, he also upholds the importance of divine revelation. In his voluntarist theology, revelation plays a crucial role in imparting knowledge of God’s will, which might not be entirely accessible through reason alone. Locke’s voluntarism is tied to his rejection of innate ideas, a concept he famously critiques in his “Essay Concerning Human Understanding.” He argues against the notion that <em>moral principles are innately known</em>, instead positing that our understanding of moral laws comes from experience, reason, and revelation. Locke’s voluntarist approach suggests that moral obligations are ultimately grounded in obedience to God’s will. This perspective can lead to a form of ethical subjectivism, where moral truths depend on the decrees of a divine authority.</p> <hr/> <blockquote> <p>In Locke’s formulation, natural law dictates that man is subject to divine imperatives to live in certain ways, but, within the limits set by the law of nature, men can act in a godlike fashion. Man as maker has a maker’s knowledge of his intentional actions, and a natural right to dominion over man’s products. … Provided we do not violate natural law, we stand in the same relation to the objects we create as God stands to us; we own them just as he owns us.</p> </blockquote> <h2 id="tensions-between-science-and-individual-rights">Tensions Between Science and Individual Rights</h2> <p>The two enlightenment values, the preoccupation of science and the commitment to individual rights, seem to be in contradiction with each other. Science is deterministic, concerned with discovering the laws that govern the universe, with human being included. This has potential for conflict with an ethic that emphasizes individual freedom, for now the freedom has to be subjugated to the laws (of nature, of God).</p> <p>In Locke’s theory, the freedom to comprehend natural law by one’s own lights supplied the basis of Locke’s right to resist, which could be invoked against the sovereign. No one is in a higher position to monopolize the right to interpret the scripture.</p> <blockquote> <p>We will see this tension surface repeatedly in the utilitarian, Marxist, and social contract traditions, without ever being fully resolved.</p> </blockquote> <h1 id="classical-utilitarianism">Classical Utilitarianism</h1> <p>Jeremy Bentham famously wrote that</p> <blockquote> <p>Nature has placed mankind under the governance of two sovereign masters, <em>pain</em> and <em>pleasure</em>. It is for them alone to point out what we ought to do, as well as to determine what we shall do. On the one hand the standard of right and wrong, on the other the chain of causes and effects, are fastened to their throne. They govern us in all we do, in all we say, in all we think: every effort we can make to throw off our subjection, will serve but to demonstrate and confirm it. In words a man may pretend to abjure their empire: but in reality he will remain subject to it all the while. The principle of utility recognizes this subjection, and assumes it for the foundation of that system, the object of which is to rear the fabric of felicity by the hands of reason and law. Systems which attempt to question it, deal in sounds instead of senses, in caprice instead of reason, in darkness instead of light.</p> </blockquote> <p>The <code class="language-plaintext highlighter-rouge">principle of utility</code>, as Bentham explains, ‘‘approves or disapproves of every action whatsoever, according to the tendency which it appears to have to augment or diminish the happiness of the party whose interest is in question: or, what is the same thing in other words, to promote or to oppose that happiness.’’</p>]]></content><author><name>Baiyang Zhang</name></author><category term="politics"/><summary type="html"><![CDATA[Enlightenment Politics]]></summary></entry><entry><title type="html">Representables in Category Theory</title><link href="https://baiyangzhang.github.io/blog/2024/Representables-in-Category-Theory/" rel="alternate" type="text/html" title="Representables in Category Theory"/><published>2024-01-10T00:00:00+00:00</published><updated>2024-01-10T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Representables-in-Category-Theory</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Representables-in-Category-Theory/"><![CDATA[<h3 id="representables">Representables</h3> <p>A category is a world of objects, all looking at one another. Each sees the world from a different viewpoint. Take the category of topological spaces for example, consider the object with only one point, denoted $\star$, given another topological space $T$, a map \(\star\to T\) can be regarded as $\star$ looking at $T$. What does $\star$ see? Since $\star$ itself is a point, the image given by a continuous map (by definitions the morphisms in the category of topological spaces are continuous maps) of $\star$ is another point in $T$, that’s to say, a point can only see points! It can’t see any other structures, limited by what it is. This is similar to what happens in a society with real people in it. A curve, on the other hand, could see much more. It can see a point if it wants, but it can also see another curve in other objects. In the language of category theory, all the things an object could see translates into <em>the set of arrow going out from it</em>.</p> <p>We can also ask the dual question: <em>fixing an object of a category, what are maps into it</em>? Take the category $\text{Set}$ of sets for example. Consider a set with only two elements. Given any set $X$, the maps from $X$ to the two element set is the subset of $X$!</p> <p>In the following we will talk about how each object sees and is seen by the category. This naturally leads to the notion of <code class="language-plaintext highlighter-rouge">representable functors</code>, or just <code class="language-plaintext highlighter-rouge">representables</code>.</p> <hr/> <p>Fix an object $A \in \mathcal{A}$. Consider the <em>totality</em> of maps <em>out of</em> $A$. To each $B\in \mathcal{A}$, there is assigned the set $\mathcal{A}(A,B)$ of maps from $A$ to $B$. This assignation is actually <strong>functorial</strong>, in the sense that to <em>each</em> $B\in\mathcal{A}$, there is a set $A(A,B)$, and to each arrow in $A$, there is another arrow in the codomain of this functorial, which we will define shortly.</p> <p><strong>Definition 1.</strong> Let $\mathcal{A}$ be a locally small (meaning that the arrows from one object to another actually form a set) category and $A\in \mathcal{A}$. Define a functor</p> \[H^{A}: B \mapsto \mathcal{A}(A,B)\] <p>where $\mathcal{A}(A,B)$ are the collection of arrows from $A$ to $B$, here $\mathcal{A}(A,B)$ is regarded as a set. Thus</p> \[H^{A}: \mathcal{A}\to \text{Set}\] <p>where $\text{Set}$ is the category of sets, of which the objects are all kinds of sets and the arrows are function from one set to another. Let $B\in \mathcal{A}$ be an object in $\mathcal{A}$. Obviously $H^{A}$ is a <em>set-valued functor</em> defined on $\mathcal{A}$. For $A’ \in \mathcal{A}$,</p> \[H^{A}(A') := \mathcal{A}(A \to A') \text{ regarded as a set}.\] <p>An easy way to remember the direction of arrow (at least for me) is to think of $A$ in $H^{A}$ standing up high, since it is a superscript, it is standing “upstairs”, as a result $A$ has a pretty good view, allowing it to “see” other objects (say, $B$) in $\mathcal{A}$, and the arrow $A \to B$ represents that $A$ is watching $B$. With this analogy, in novel <em>1984</em> by Orwell, the big brother is watching everyone in the nation, making him the <em>initial object.</em> Similarly, given $A \in \mathcal{A}$, when we need to talk about arrow coming from other objects <em>into</em> $A$ later, we will define the functor $H_ {A}$ where $H_ {A}(B)$ is the set of arrows from $B$ to $A$ this time, for $A$ sits at the bottom and everyone can see $A$.</p> <p>For a map $g: A’ \to A’’$ in $\mathcal{A}$, $H^{A}$ maps $g$ to another map from $H^{A}(A’)$ to $H^{A}(A’’)$ by composition, that is</p> \[H^{A}(A'): A \to A', \quad H^{A}(A'') : A\to A'', \quad H^{A}(g): (A\to A') \to (A\to A'')\] <p>where $H^{A}(g)$ is realized by</p> \[A\to A' \xrightarrow{g} A''\] <p>which is a map from $A$ to $A’’$, hence an element of $H^{A}(A’’)$.</p> <p>To explain it in a different way, consider object $X\in H^{A}(A’)$, $X$ by construction is a map from $A$ to $A’$. Let $Y \in H^{A}(A’’)$, then $Y$ is nothing but a map from $A$ to $A’’$. Now, what is an arrow from $X$ to $Y$? It is something that maps $A\to A’$ to $A \to A’’$, which can be achieved by the composition $g\,\circ\,X$, where $g: A’ \to A’’$.</p> <p>Sometimes $H^{A}(g)$ is written as $g\,\circ\,-$, where $-$ is a commonly used symbol as a place holder; or written as $g_ {\ast}$, reminding us that it is somehow “induced” by $g$. All are frequently used in publications.</p> <hr/> <p>Again, let $A$ be a locally small (the collection of morphisms between any two objects is a set) category. A functor</p> \[X: \mathcal{A} \to \text{Set}\] <p>is <code class="language-plaintext highlighter-rouge">representable</code> if</p> \[X \cong H^{A} \text{ for some } A \in \mathcal{A}.\] <p>$A$ is said to be a <code class="language-plaintext highlighter-rouge">representation</code> of $X$, together with an isomorphism between $H^{A}$ and $X$. In other words, when dealing with a representable functor $X$, identifying a representation for $X$ requires more than just specifying an object $A$; we must also precisely determine how $X$ is isomorphic to $H^A$.</p> <p>Representable functors are sometimes just called <code class="language-plaintext highlighter-rouge">representables</code>. Only set-valued functors (functors with codomain $\text{Set}$) can be representable.</p> <p>Representable functors are important for a few reason in category theory. The Yoneda Lemma, which is the topic of this note, is a fundamental result in category theory that involves representable functors. We will introduce the Yoneda lemma shortly, but shortly put it, it states that <em>every functor</em> is <em>naturally isomorphic</em> to a functor represented by some object in the category. This lemma provides a powerful tool for embedding any locally small category into a category of functors (where the arrows are natural transformations). The Yoneda embedding, which arises from this, embeds a category into a category of presheaves, preserving and reflecting properties of objects and morphisms. If you don’t know (or not interested in) what a presheave is, just ignore it.</p> <p>Representable functors are key in the study of natural transformations. The <code class="language-plaintext highlighter-rouge">naturality condition</code> in the definition of natural transformations can be better understood and characterized using representables. Representables also play a role in the study of adjoints and limits, which are some key-important concepts in category theory. For instance, adjoint functors can often be characterized using representables, and the existence of certain limits and colimits (limits where all the arrows are reversed) can be analyzed through representable functors. Representable functors are also indispensable to sheaf theory and topos theory, providing a link between geometric intuition and abstract category-theoretic formalism. Unfortunately these fascinating results lie outside the scope of this simple note, or for that matter lie outside of my comprehension.</p> <hr/> <p>Since we are already talking about the category of functors, it is perhaps timely to mention two similar but distinct definitions in category theory, namely the category of functors and the so-called $2$-category.</p> <ul> <li> <p><strong>Category of Functors</strong>: This is a construction where the objects are functors and the morphisms between these objects are natural transformations. Specifically, given two categories $\mathcal{C}$ and $\mathcal{D}$, the category of functors, often denoted $\text{Fun}(\mathcal{C}, \mathcal{D})$, has as objects all functors from $\mathcal{C}$ to $\mathcal{D}$ and as morphisms the natural transformations between these functors.</p> </li> <li> <p><strong>$2$-Category</strong>: A $2$-category is a more general and abstract concept. In a $2$-category, there are objects, morphisms between objects (also called <code class="language-plaintext highlighter-rouge">1-morphisms</code>), and morphisms between morphisms (called <code class="language-plaintext highlighter-rouge">2-morphisms</code>), or “arrows between arrows”. This structure introduces a new level of morphisms.</p> </li> </ul> <p>In a certain sense, the category of functors between two categories can be seen as a specific example of a 2-category, where the objects are the categories, the 1-morphisms are the functors between these categories, and the 2-morphisms are the natural transformations between these functors. However, the concept of a 2-category is broader and can be applied to many other contexts beyond just categories and functors.</p> <hr/> <p><strong>Ex.1</strong> Consider the category of sets, denoted $\text{Set}$, let $1$ be the set of only one element. Now, what would $H^{1}$ be? It can be written as</p> \[\text{Mor}(1,-) \text{ or } \text{Hom}(1,-) \text{ or } \text{Set}(1,-),\] <p>they all mean the same thing: the maps from $1$ to something else. Let $S \in Set$, $H^{1}(S)$ would be the collection of the maps from $1$ to $S$, which is itself another set, hence</p> \[H^{1}: \text{Set} \to \text{Set}.\] <p>Since a map from $1$ to a set $S$ amounts to an element of $S$, we have</p> \[H^{1}(S) \cong S \quad \;\forall\; S \in \text{Set}.\] <p>It can be shown (which I will not do here) that this isomorphism is <em>natural</em> in $S$, so $H^{1}$ is naturally isomorphic to the identity functor $\mathbb{1}_ {\text{Set}}$. Hence $\mathbb{1}_ {\text{Set}}$ is representable, by $1$.</p> <hr/> <p>Representability is not a property shared by just any set-valued functor. In fact, rather few functors are representable, consider the total amount of functors we could construct. However, forgetful functors <em>tend to be</em> representable. We state without proof the following proposition.</p> <p><strong>Proposition.</strong> Any set-valued functor with a left adjoint is representable.</p> <hr/> <p>We have defined, for each object $A$ of category $\mathcal{A}$ , a functor $H^{A}\in[\mathcal{A},\text{Set}]$ (given two categories $\mathcal{A}$ and $\mathcal{B}$, $[\mathcal{A},\mathcal{B}]$ is the functor category from the former to the latter). This describes how $A$ sees the world. As $A$ varies, so does the view. On the other hand, it is always the same world being seen, so the different views from different objects are somehow related. Generally speaking, whenever there is a map between objects $A$ and $A’$, there is also a map between $H^{A}$ and $H^{A’}$. Since $H^{A}$ are $H^{A’}$ are both functors (the “view” of $A,A’$), a map between them are potentially natural transformation, potentially since we need to show that this map satisfies the rule of naturality.</p> <p>Precisely, a map</p> \[f: A' \to A,\quad A,A'\in \mathcal{A}\] <p>induces a natural transformation</p> \[H^{f} : H^{A} \to H^{A'}.\] <p>An interactive diagram of this can be found <a href="https://q.uiver.app/#q=WzAsMixbMCwwLCJcXG1hdGhjYWx7QX0iXSxbMywwLCJcXHRleHR7U2V0fSJdLFswLDEsIkheQSIsMCx7ImN1cnZlIjotNH1dLFswLDEsIkhee0EnfSIsMix7ImN1cnZlIjo0fV0sWzIsMywiSF5mIiwwLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dXQ==">here</a>.</p> <p>Recall that a natural transformation $\alpha$ between two functors $F$ and $G$, both from category $\mathcal{C}$ to category $\mathcal{D}$, is made up of components. Each component is a morphism in category $\mathcal{D}$.</p> <p>For each object $X$ in category $\mathcal{C}$, the <code class="language-plaintext highlighter-rouge">component</code> of the natural transformation $\alpha$ at $X$ is a morphism in category $\mathcal{D}$:</p> \[\alpha_ X : F(X) \rightarrow G(X)\] <p>These components must satisfy a naturality condition, which states that for every morphism $f: X \rightarrow Y$ in category $\mathcal{C}$, the naturality diagram commutes. The naturality condition must hold for all objects and morphisms in category $C$, making the transformation “natural” in the sense that it works consistently across the entire category.</p> <p>Coming back to $H^{f}$. Let $B\in \mathcal{A}$, what would the component $H^{f}_ {B}$ be? Recall that $f$ maps from $A’$ to $A$, by construction $H^{f}$ maps in the opposite direction, it is the function</p> \[H^{A}(B) \equiv \mathcal{A}(A,B) \to H^{A'}(B)\equiv \mathcal{A}(A',B),\] <p>in terms of the elements, let $p \in \mathcal{A}(A,B)$ we have</p> \[H^{f}: p \mapsto p\,\circ\,f.\] <p>Notice that, each $H^{A}$ is <em>covariant</em> (meaning they preserve the direction of the arrows), however they come together to form a <em>contravariant</em> thing! What exactly is this thing then? To understand it, the following definition is important:</p> <p>Let $\mathcal{A}$ be a locally small category, the functor</p> \[H^{-}: \mathcal{A}^{\text{op}} \to [\mathcal{A},\text{Set}]\] <p>is defined on objects $A$ by $H^{-}(A)=H^{A}$, and on maps $f$ by $H^{-}(f)=H^{f}$.</p> <p>A lot of explain regarding the notations is in order. Apparently the dash $-$ in $H^{-}$ is a placeholder, to be filled by whatever it acts on, no matter if it is an object or an arrow. $\mathcal{A}^{\text{op}}$ is the opposite, or dual category of $\mathcal{A}$, with same objects but reversed arrows (all of them). $\mathcal{A}^{\text{op}}$ is a category, $[\mathcal{A},\text{Set}]$ is another category ( of functors from $\mathcal{A}$ to $\text{Set}$), thus $H^{-}$ is a functor. $H^{-}$ of $A$ is $H^{A}$, which is a set-valued functor on $\mathcal{A}$, hence $H^{A}$ is an object of $[\mathcal{A},\text{Set}]$.</p> <p>All of the definitions presented so far in this chapter can be dualized. At the formal level, this is trivial: just reverse all the arrows! But in our analogy, after dualize it, we are no longer asking what objects see, but <em>how they are seen</em>:</p> <p>Let $\mathcal{A}$ be a <em>locally small</em> category and $A\in \mathcal{A}$. We define a functor $H_ {A}$ as follows,</p> \[H_ {A} := \mathcal{A}(-,A): \mathcal{A}^{\text{op}}\to\text{Set}\] <p>where</p> <ul> <li>for objects $B \in \mathcal{A}$, put $H_ {A}(B) = \mathcal{A}(B,A)$;</li> <li>For maps $g:B’\to B$, define</li> <li> \[H_ {A}(g) = \mathcal{A}(g,A) = g^{\ast } = - \,\circ\,g: \mathcal{A}(B,A)\to\mathcal{A}(b',A)\] </li> </ul> <p>by</p> \[p\mapsto p\,\circ\,g \quad \;\forall\; p: B\to A.\] <hr/> <p>Note that a map $B’\to B$ induces a map in the opposite direction, $H_ {A}(B)\to H_ {A}(B’)$.</p> <p>We now define representability for <em>contravariant set-valued</em> functors.</p> <p>Let $\mathcal{A}$ be a locally small category, and $X$ be a set-valued contravariant functor,</p> \[X: \mathcal{A}^{\text{op}} \to \text{Set}.\] <p>We say $X$ is <strong>representable</strong> is $X \cong H_ {A}$ for some $A\in \mathcal{A}$. A <code class="language-plaintext highlighter-rouge">representation</code> of $X$ is a choice of $A$ and an isomorphism between $X$ and $H_ {A}$.</p> <p>As an example, take the power set (contravariant-)functor $\mathcal{P}$ for example. Recall that the power set $\mathcal{P}(S)$ of a set $S$ is the collection of subsets of $S$. Let $S,S’\in \text{Set}$ be two sets, and $f: S \to S’$ a map (morphism) in the category of sets. Since we are regarding $\mathcal{P}$ as a contravariant functor, we should define $\mathcal{P}(f)$, which is a map from $\mathcal{P}(S’)$ to $\mathcal{P}(S)$, notice the inversed direction. Well, turns out this can be done in a more or less direct way: we define $\mathcal{P}(f)$ to be in a sense the <em>inverse</em> of $f$! Take $U\in\mathcal{P}(S’)$, we define the action on $U$ by $\mathcal{P}(f)$ by $f^{-1}(U)$, where $f^{-1}$ is <strong>not</strong> the inverse of $f$, for we didn’t assume that $f$ is invertible at all, but rather the preimage of $f$,</p> \[f^{-1} (U) := \left\lbrace s\in S \,\middle\vert\, f(s)\in U \right\rbrace .\] <p>On the other hand, the subsets of $S$ can be regarded as a maps from $S$ to $2$, the set with two elements, which we call <code class="language-plaintext highlighter-rouge">true</code> and <code class="language-plaintext highlighter-rouge">false</code>. To see how it works, take $S=\left\lbrace 1,2 ,3\right\rbrace$, the subset $\left\lbrace 1,2 \right\rbrace$ can be regarded as a map $g: S\to \left\lbrace \text{true,false} \right\rbrace$ where $1,2$ are map to true and $3$ is mapped to false. Such maps are sometimes called the characteristic functions. This provides as another way to look at power sets, and confirms the idea that in category theory everything is a morphism!</p> <p>Note that $\chi:=\left\lbrace \text{true,false} \right\rbrace$ is itself an object in $\text{Set}$, so we can talk about the Hom-functor $H_ {\chi}$, defined by maps from other stuff into $\chi$. Maybe not too surprisingly, $H_ {\chi}$ is isomorphic to the power functor $\mathcal{P}$,</p> \[\mathcal{P} \cong H_ {\chi}.\] <p>Try to convince yourself with it, better with some simple examples. Thus we say the power functor $\mathcal{P}$ is representable, and the representation is $\chi$.</p> <hr/> <p>Just as we assembled the covariant representables $H^{A}$s into a big functor $H^{-}$, we can do the same for the contravariant representables. If $f: A \to A’$ is a map in $\mathcal{A}$, there is an induced natural transformation $H_ {f}$:</p> \[H_ {f}: H_ {A} \to H_ {A'},\quad H_ {A,A'}: \mathcal{A}\to\text{Set}.\] <hr/> <p>Let $\mathcal{A}$ be a locally small category. The <code class="language-plaintext highlighter-rouge">Yoneda embedding</code> of $\mathcal{A}$ is the functor</p> \[H_ {-}: \mathcal{A} \to [A^{\text{op}},\text{Set}]\] <p>defined on objects $A \in \mathcal{A}$ as $H_ {-}(A)=H_ {A}$ and arrows $H_ {-}(f)=H_ {f}$.</p> <p>Note that in $[\mathcal{A}^{\text{op}},\text{Set}]$, the objects are functors from $\mathcal{A}^{\text{op}}$ to $\text{Set}$ and the arrows are natural transformations.</p> <p>For each object $A\in\mathcal{A}$, the Yoneda embedding assigns a functor $H_ {A}$, it is sometimes called the Yoneda functor. In a sense, the Yoneda embedding allows as to regard each $A$ as arrows. Or, more precisely, it allows us to represent each $A$ as a set-valued functor. Yoneda embedding is a one-to-one correspondence between objects in $\mathcal{A}$ and functors in $[\mathcal{A},\text{Set}]$. In a sense to be explained, $H_ {-}$ embeds $\mathcal{A}$ into $[\mathcal{A},\text{Set}]$.</p> <hr/> <p>Let $\mathcal{A}$ be a locally small category, the functor \(\text{Hom}_ {\mathcal{A}}: \mathcal{A}^{\text{op}}\times \mathcal{A}\to\text{Set}\) is defined by the following diagram, <img src="/img/hom.png" alt="hom"/></p> <p>This is like a generalization of $\text{Hom}(A,B)$ that we have encountered before. Recall that $\text{Hom}(A,B)$ is the set of the morphisms from $A$ to $B$, now $\text{Hom}_ {\mathcal{A}}$ is generalized such that it can also take two maps $f,g$ as well.</p> <hr/> <p>Given an arbitrary object in an arbitrary category, it is in general meaningless to talk about the “element” of this object since it not necessarily a set. However, in the category of sets, an element is the same thing as a map $1\to A$. This inspires the following definition.</p> <p>Let $A$ be a locally small category. A <code class="language-plaintext highlighter-rouge">Generalized element</code> of $A$ is a map with codomain $A$. A map $S\to A$ is a generalized element of $A$ of shape $S$.</p> <p>For example, a generalized element of a set $S$ of shape $\mathbb{N}$ is nothing but a sequence in $S$. In the category of topological spaces, the generalized elements of shape $1$ (the one point space) are points, and the generalized elements of shape $\mathbb{S}^{1}$ are loops.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="categoryTheory"/><summary type="html"><![CDATA[Representables]]></summary></entry></feed>