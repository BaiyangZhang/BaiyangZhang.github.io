<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://baiyangzhang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://baiyangzhang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-10-23T12:11:34+00:00</updated><id>https://baiyangzhang.github.io/feed.xml</id><title type="html">Baiyang Zhang</title><subtitle>A place dedicated to sharing insights and reflections on mathematics, physics, and social sciences. </subtitle><entry><title type="html">Introduction to Higher Form Symmetry</title><link href="https://baiyangzhang.github.io/blog/2023/Introduction-to-Higher-Form-Symmetry-Lecture-3/" rel="alternate" type="text/html" title="Introduction to Higher Form Symmetry"/><published>2023-10-23T00:00:00+00:00</published><updated>2023-10-23T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Introduction-to-Higher-Form-Symmetry%20Lecture%203</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Introduction-to-Higher-Form-Symmetry-Lecture-3/"><![CDATA[<p>For conventions used in this note, see my blog <a href="http://www.mathlimbo.net/2022/07/17/Conventions-and-Formula/">here</a>.</p> <p>In lecture 2 we have talked about classic symmetry and their re-interpretation using the language of differential (exterior) form. We have made the connection between the so-called symmetry defect operator (SDO) and a charged, point operator. In this note we try to generalized this concept to charged operators defined on manifolds of dimension more than zero, that is, a line, a surface, etc.</p> <p>Let’s start with the ordinary symmetry once again. Recall that we have written the variation of the action as \(\delta S = \int d^{D-1}x \, J^{\mu}\partial_ {\mu}\epsilon(x) \tag{1}\) where $\xi$ is the parameter of the symmetry transformation, under which the charged operators transform as \(\phi(x) \to \epsilon(x) \Delta \phi(x).\) In our convention, $\Delta$ stands for a small but finite change while $\epsilon(x)$ stands for an infinitesimal function. Eq. (1) can be regarded as the definition of the Noether current $J^{\mu}$, which tells us how much the action changed under the transformation in question. But, being a symmetry of the system, of course the action remains unchanged, hence the conservation of the charge \(\partial_ {\mu}J^{\mu} = 0 \Longleftrightarrow d\star J=0.\)</p> <p>In the language of differential forms, we can regard $\epsilon$ as a $0$-form, which is (by definition) a function. Then the variation of action reads \(\delta S = \int_ {M^{(D)}} (\star J)\wedge d\epsilon, \tag{2}\) where $\star J$ is a $(D-1)$-form, $d \epsilon$ is a $1$-form (since $\epsilon$ is a zero form), hence their wedge product is a $D$-form, something can be integrated over $D$-dimensional manifold $M$, whose boundary is $\Sigma$, by the way.</p> <p>The advantage of Eq. (2) is that it can be generalized to higher forms.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="PureMath"/><category term="Notes"/><summary type="html"><![CDATA[For conventions used in this note, see my blog here.]]></summary></entry><entry><title type="html">Excerpt From “On Anarchism” by Noam Chomsky</title><link href="https://baiyangzhang.github.io/blog/2023/Excerpt-From-On-Anarchism/" rel="alternate" type="text/html" title="Excerpt From “On Anarchism” by Noam Chomsky"/><published>2023-10-20T00:00:00+00:00</published><updated>2023-10-20T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Excerpt-From-On-Anarchism</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Excerpt-From-On-Anarchism/"><![CDATA[<h3 id="introduction">Introduction</h3> <p><strong>Ableism:</strong> A term used to describe discrimination, prejudice, or bias against individuals with disabilities.</p> <p><strong>Gender queerness:</strong> Gender queerness is a concept and identity that falls under the broader umbrella of non-binary gender identities. It challenges and rejects the traditional binary understanding of gender as solely male or female. Instead, genderqueer individuals may identify with both genders, neither gender, or a different gender entirely. Here are some key points to understand about gender queerness:</p> <ul> <li><em>Fluidity</em>: Some genderqueer people experience a fluid gender identity, meaning their gender identity can change over time or in different circumstances.</li> <li><em>Beyond the Binary</em>: Genderqueer identities are diverse and can include identities like bi-gender (identifying as two genders), agender (lacking a gender or being gender neutral), and more.</li> <li><em>Personal Expression</em>: The way genderqueer people express their gender can vary widely. Some may choose to present androgynously, some may present in ways that are traditionally associated with men or women, and others may choose an entirely different form of expression.</li> <li><em>Preferred Pronouns</em>: Genderqueer individuals may use a variety of pronouns. This includes he/him, she/her, they/them, or other neopronouns such as ze/hir.</li> </ul> <p><strong>Zapatistas:</strong> The Zapatistas, officially called the Zapatista Army of National Liberation (EZLN), is a revolutionary leftist group that originated in Mexico in 1994. The group is named after Emiliano Zapata, a leading figure in the Mexican Revolution who advocated for land reforms and the rights of peasants. Here are some key aspects of the Zapatistas:</p> <ul> <li><strong>Indigenous Rights</strong>: A significant aspect of the Zapatista movement is its emphasis on the rights and autonomy of indigenous peoples in Mexico. Much of the EZLN’s membership consists of indigenous people from various ethnic groups.</li> <li><strong>Form of Struggle</strong>: After the initial 1994 uprising, the Zapatistas transitioned from a traditional armed struggle to a more peaceful, grassroots, and civil resistance. They prioritize community-based initiatives and projects over direct military action.</li> <li><strong>Autonomous Municipalities</strong>: The Zapatistas established “caracoles” (snails) and autonomous municipalities in Chiapas. These areas are governed by local assemblies and function outside the traditional political system of Mexico.</li> <li><strong>Subcomandante Marcos</strong>: One of the most well-known figures of the Zapatista movement is Subcomandante Marcos (also known as Subcomandante Galeano since 2014). He served as the movement’s spokesperson and is recognized for his writings, which mix revolutionary theory with poetic and mythical elements.</li> </ul> <p><strong>Black blocs:</strong> The term “black bloc” refers to a tactic often used by protesters, rather than a specific group or organization. Participants in a black bloc wear all-black clothing and cover their faces with masks or bandanas to maintain anonymity, create a unified presence, and avoid identification by law enforcement or surveillance.</p> <p><strong>Occupy Wall Street:</strong> Occupy Wall Street (OWS) was a protest movement that began on September 17, 2011, in Zuccotti Park, located in New York City’s Wall Street financial district. It was initiated by the Canadian activist group Adbusters and subsequently spread to cities across the United States and around the world. The primary slogan of the movement was “We are the 99%,” which highlighted the increasing income and wealth inequality in the U.S., drawing attention to the concentration of wealth in the top 1% of the population.</p> <p>Here are some key aspects and outcomes of Occupy Wall Street:</p> <ul> <li><strong>Goals and Grievances</strong>: The movement was largely about economic inequality, corporate influence over the democratic process, and the perceived failures of Wall Street and big banks, especially in the wake of the 2007-2008 financial crisis. Protesters also voiced concerns about issues such as student loan debt, foreclosures, and the influence of money in politics.</li> <li><strong>Leaderless Structure</strong>: OWS was deliberately leaderless and used a consensus-based decision-making process. General Assemblies were held where participants could discuss issues, make decisions, and plan actions.</li> <li><strong>Encampments</strong>: Inspired by the initial occupation of Zuccotti Park, similar encampments sprang up in various cities around the U.S. and the world. These became sites of community, discussion, and often faced police evictions.</li> <li><strong>Impact on Discourse</strong>: While OWS did not have a specific set of demands or a unified platform, it significantly impacted public discourse. Concepts like income inequality and the “1%” became more mainstream topics of discussion in the media, among politicians, and in everyday conversations.</li> <li><strong>Criticism</strong>: The movement faced criticism on various fronts. Some felt that the lack of a clear set of demands or a centralized leadership made the movement ineffective. Others believed that certain actions or behaviors by protesters detracted from the movement’s broader goals.</li> <li><strong>Legacy</strong>: While the encampments and direct actions associated with OWS dwindled by 2012, the movement’s broader impacts can still be seen. It influenced subsequent social movements and brought attention to economic inequality, which remained a significant topic in subsequent political campaigns and policy discussions.</li> <li><strong>Connection to Other Movements</strong>: OWS shared tactics, ideas, and personnel with other movements and causes, such as Black Lives Matter, the Fight for $15 (a campaign for a higher minimum wage), and even the 2016 and 2020 presidential campaigns of Senator Bernie Sanders.</li> </ul> <p> <strong>corporatocracy:</strong> A system in which power effectively rests with a small, elite group of inside individuals, sometimes from a small group of educational institutions, or influential economic entities or devices, such as banks, commercial entities, lobbyists that act in complicity with, or at the whim of the oligarchy, often with little or no regard for constitutionally protected prerogative.</p> <hr/> <p><strong>Principal:</strong> Power that isn’t really justified by the will of the <em>governed</em> should be dismantled.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="NoamChomsky"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Note on Numerical Methods in Solving ODE</title><link href="https://baiyangzhang.github.io/blog/2023/Notes-on-Numerical-Methods-in-Solving-ODE/" rel="alternate" type="text/html" title="Note on Numerical Methods in Solving ODE"/><published>2023-10-17T00:00:00+00:00</published><updated>2023-10-17T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Notes-on-Numerical-Methods-in-Solving-ODE</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Notes-on-Numerical-Methods-in-Solving-ODE/"><![CDATA[<p>In my line of work I constantly need to solve unsolvable ODEs and PDEs, unsolvable in the sense that it is impossible to get an analytical solution. What we do is to turn to numerical methods for help. For instance, just recently I need to solve a modified version of the Skyrme equation (the equation of motion resulted from the Skyrme model). I thought it would be helpful to summarize what I’ve learnt here.</p> <p>Various numerical methods have been developed to tackle different types of ODEs (initial value problems, boundary value problems, linear, nonlinear, etc.). Here are some of the most popular numerical methods for solving ODEs:</p> <ol> <li><strong>Euler’s Method</strong>: <ul> <li>This is the simplest one-step method.</li> <li>It’s based on a linear approximation of the solution.</li> <li>While straightforward and instructive for educational purposes, it’s rarely used in practice due to its low accuracy and stability issues.</li> </ul> </li> <li><strong>Runge-Kutta Methods</strong>: <ul> <li>These are a family of iterative methods.</li> <li>The 4th order Runge-Kutta (often called RK4) is particularly popular due to its balance between accuracy and computational cost.</li> </ul> </li> <li><strong>Leapfrog (or Midpoint) Method</strong>: <ul> <li>A second-order method that is particularly useful in cases where energy conservation is crucial, such as in molecular dynamics simulations.</li> </ul> </li> <li><strong>Predictor-Corrector Methods</strong>: <ul> <li>These methods predict a solution using an explicit method and then correct it with an implicit method.</li> <li>Examples include the Adams-Bashforth (predictor) and Adams-Moulton (corrector) methods.</li> </ul> </li> <li><strong>Backward Differentiation Formulas (BDF)</strong>: <ul> <li>These are implicit multi-step methods.</li> <li>Commonly used for stiff ODEs.</li> </ul> </li> <li><strong>Multistep Methods</strong>: <ul> <li>These methods use values at multiple previous time steps.</li> <li>Examples include the Adams methods.</li> </ul> </li> <li><strong>Symplectic Integrators</strong>: <ul> <li>These are used for Hamiltonian systems where preserving the symplectic structure (related to conservation of energy) is essential.</li> </ul> </li> <li><strong>Implicit Methods</strong>: <ul> <li>Used frequently for stiff equations where explicit methods require prohibitively small time steps.</li> <li>Examples include the backward Euler method and the trapezoidal rule.</li> </ul> </li> <li><strong>Shooting Method</strong>: <ul> <li>Primarily used for boundary value problems (BVPs).</li> <li>Converts a BVP into an initial value problem (IVP) and then solves the IVP.</li> </ul> </li> <li><strong>Relaxation Methods</strong>: <ul> <li>Also for boundary value problems.</li> <li>Iteratively refines an initial guess to the solution.</li> </ul> </li> <li><strong>Finite Difference Method</strong>: <ul> <li>Converts differential equations into difference equations, which can then be solved algebraically.</li> <li>Often used for both ODEs and PDEs.</li> </ul> </li> <li><strong>Collocation Methods</strong>: <ul> <li>This approach seeks an approximate solution by considering values at specific points (collocation points).</li> </ul> </li> <li>Continuation method, which we will go to detail later.</li> </ol> <hr/> <p>These methods can be adapted or combined in various ways depending on the specific problem at hand. Moreover, the choice of method often depends on the nature of the ODE (e.g., stiffness), desired accuracy, computational cost considerations, and the specific properties that need to be preserved (e.g., conservation laws).</p> <p>Many modern computational packages and software (like MATLAB, Mathematica, and SciPy in Python) provide built-in functions that implement these methods, which makes it easier for users to solve ODEs without delving deeply into the numerical intricacies of each method.</p> <p><strong>Stiffness.</strong></p> <p>Imagine you’re on a winding road with both smooth curves and sharp turns. If you’re driving a car along this road at a constant speed, the smooth curves can be navigated quite easily, but the sharp turns require more caution and precision.</p> <p>Similarly, in the context of differential equations, there can be parts of the solution that change very slowly (smooth curves) and others that change extremely rapidly (sharp turns). When a differential equation has solutions with widely differing rates of change over its domain, we say that the equation is “stiff.” When you’re solving a stiff differential equation using numerical methods (like the Euler method or the Runge-Kutta method), you’ll notice that the rapid changes require very small step sizes for accurate solutions. However, the slow-changing parts don’t need such small steps. If you choose a step size suitable for the rapidly changing sections (very small), the computation can become inefficient because you’re using more steps than necessary for the slow-changing sections. On the other hand, if you choose a larger step size suitable for the slow-changing sections, the solution can become unstable or highly inaccurate in the rapidly changing sections.</p> <p>To efficiently and accurately solve stiff differential equations, specialized numerical methods have been developed, known as “stiff solvers.” These solvers are designed to adaptively handle the challenges posed by stiffness, allowing for stable and efficient computation.</p> <h3 id="numerical-values-of-parameters">numerical values of parameters</h3> <p>I collected the following values from the Adkins:Nappi:1984 paper<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>,<br/> \(m_ {\pi} = 108 \text{ MeV}, \quad e=4.82,\quad F_ {\pi} = \frac{m_ {\pi}}{0.263 e}\) which gives us \(\begin{align} m_ {1} &amp;= 0.526, \\ m_ {2} &amp;= 1.052. \end{align}\)</p> <p>In the chiral case, pion is massless and we have \(\begin{align} m_ {1} &amp;= 0, \\ m_ {2} &amp;= 1.052. \end{align}\)</p> <h3 id="the-shooting-method">The shooting method</h3> <p>The shooting method is a numerical technique used to solve boundary value problems (BVPs) for ordinary differential equations (ODEs). <em>It’s especially useful for second-order ODEs, but can be applied to higher-order equations as well</em>.</p> <p>Here’s a basic overview of the shooting method:</p> <p><strong>The Problem:</strong> Suppose you have a second-order ODE given as: \(y''(x) = f(x, y, y'),\) with boundary conditions: \(y(a) = y_ a\) \(y(b) = y_ b\)</p> <p><strong>The Challenge:</strong> Directly solving the BVP using typical ODE solvers is difficult because standard solvers require initial conditions (values of $y$ and $y’$ at a starting point), rather than boundary conditions at two separate points.</p> <p><strong>The Shooting Method’s Approach:</strong></p> <ol> <li> <p><strong>Guess an Initial Slope</strong>: Choose an initial guess for the derivative $y’(a)$, let’s call it $y’_ a$.</p> </li> <li> <p><strong>Solve as an IVP</strong>: Using the known value $y(a) = y_ a$ and the guessed $y’(a) = y’_ a$ then solve the ODE as an initial value problem (IVP) over the interval $[a, b]$ using standard techniques, like the Runge-Kutta method.</p> </li> <li> <p><strong>Check the Endpoint</strong>: Once you’ve solved the ODE using your initial guess, check the value of $y(b)$ from this solution. Compare it to the desired boundary condition $y_ b$.</p> </li> <li> <p><strong>Adjust the Guess</strong>: If $y(b)$ from your solution is close to $y_ b$, then you’re done. If not, adjust your guess for $y’(a)$ and solve the IVP again. This is typically done using a root-finding algorithm like Newton’s method or the secant method.</p> </li> <li> <p><strong>Iterate</strong>: Repeat steps 2-4 until $y(b)$ from your solution is sufficiently close to $y_ b$, or until a set number of iterations have been reached.</p> </li> </ol> <p>The method’s name comes from the idea that you’re “shooting” from one boundary towards the other. Your first “shot” might miss the target (the second boundary condition). By adjusting your aim (the initial derivative guess) and shoot again, you try to hit the target. The process is repeated until you’re close enough to the target, similar to adjusting one’s aim when firing at a target in marksmanship.</p> <p>While the shooting method can be effective, it’s not guaranteed to work for all BVPs, especially when the underlying ODEs are highly nonlinear or when appropriate initial guesses are hard to ascertain. Unfortunately, the solving of the modified Skyrme equation seems to fall in the category, as I am about to go to details right now.</p> <p>With the parameters listed in the previous chapter, I tried to solve the equation of motion using shooting method with the following codes</p> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bc1</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">Pi</span><span class="p">;</span>
<span class="n">bc2</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="mi">50</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>

<span class="n">approximateSolution</span> <span class="o">=</span> <span class="n">Pi</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Tanh</span><span class="p">[</span><span class="n">r</span><span class="p">]);</span>

<span class="n">initialGuessY</span> <span class="o">=</span> 
  <span class="n">approximateSolution</span> <span class="p">/</span><span class="o">.</span> 
   <span class="n">r</span> <span class="o">-&gt;</span> <span class="mi">0</span><span class="p">;</span>  <span class="p">(</span><span class="o">*</span><span class="n">Evaluate</span> <span class="n">approximate</span> <span class="n">solution</span> <span class="n">at</span> <span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="p">)</span>
<span class="n">initialGuessYPrime</span> <span class="o">=</span> 
  <span class="n">D</span><span class="p">[</span><span class="n">approximateSolution</span><span class="p">,</span> <span class="n">r</span><span class="p">]</span> <span class="p">/</span><span class="o">.</span> <span class="n">r</span> <span class="o">-&gt;</span> <span class="mi">0</span><span class="p">;</span>  <span class="p">(</span><span class="o">*</span><span class="n">Evaluate</span> <span class="n">derivative</span> <span class="n">at</span> <span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="p">)</span>

<span class="n">shootingMethod</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"Shooting"</span><span class="p">,</span> 
   <span class="s2">"StartingInitialConditions"</span> <span class="o">-&gt;</span> <span class="p">{</span><span class="n">f</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">initialGuessY</span><span class="p">,</span> 
   <span class="n">f</span><span class="o">'</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">initialGuessYPrime</span><span class="p">}};</span>

<span class="n">solutionTest1</span> <span class="o">=</span> <span class="n">Module</span><span class="p">[{\[</span><span class="n">Eta</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m1</span> <span class="o">=</span> <span class="mf">0.526</span><span class="err">`</span><span class="p">,</span> <span class="n">m2</span> <span class="o">=</span> <span class="mf">1.052</span><span class="err">`</span><span class="p">},</span>
	 <span class="n">shootingMethod</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"Shooting"</span><span class="p">,</span> 
    <span class="s2">"StartingInitialConditions"</span> <span class="o">-&gt;</span> <span class="p">{</span><span class="n">f</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">Pi</span><span class="p">,</span> <span class="n">f</span><span class="o">'</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">6</span><span class="p">}};</span>
  <span class="n">NDSolve</span><span class="p">[{</span><span class="n">eom</span><span class="p">,</span> <span class="n">bc1</span><span class="p">,</span> <span class="n">bc2</span><span class="p">},</span> <span class="n">f</span><span class="p">,</span> <span class="p">{</span><span class="n">r</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">},</span> <span class="n">PrecisionGoal</span> <span class="o">-&gt;</span> <span class="mi">7</span><span class="p">,</span> 
   <span class="n">AccuracyGoal</span> <span class="o">-&gt;</span> <span class="mi">7</span><span class="p">]]</span>
</code></pre></div></div> <p>where eom is short for the equation of motion, given by \(\begin{align} \text{eom} =&amp;-2 r^4 f''(r)-4 \eta r^2 f''(r) \sin ^2(f(r))+4 \eta r^3 f'(r)^3-4 r^3 f'(r)^3-4 r^3 f'(r)-2 \eta r^2 f'(r)^2 \sin (2 f(r)) \\ &amp;+6 \eta r^4 f'(r)^2 f''(r)-6 r^4 f'(r)^2 f''(r)+2 \text{m1}^2 r^4 \sin (f(r))+2 \text{m2}^2 r^4 \sin (f(r)) \\ &amp;-\text{m2}^2 r^4 \sin (2 f(r))+2 r^2 \sin (2 f(r))+\sin (2 f(r))-\sin (2 f(r)) \cos (2 f(r)) \\ &amp;==0. \end{align}\)</p> <p>However the computation takes a long time and yields a nonsensical result, <img src="/img/eom.png" alt=""/></p> <p>which clearly doesn’t make any sense.</p> <p>Maybe we can make it work by providing a super accurate initial condition? With this hope I try to solve the equation at the origin, namely $r=0$. Expand $f(r)$ about the origin we get \(f(r) = f(0) + r f'(r) = \pi + rg(r),\quad g(r) := f'(r)\) where we have made use of the initial condition that $f(0)=\pi$, and $r$ is supposed to be very small. Take this to the equation of motion, with some manipulation we get \(\left(-4 r^4 g^{2}(r)-2 r^4\right) g'(r)-2 m_ 1^2 r^5 g(r)-4 m_ 2^2 r^5 g(r)-4 r^3 g^{3}(r) =0\) keep the leading order and NLO in $r$ we have \(\left(2 r g^{2}(r)+r\right) g'(r)+2 g^{3}(r)=0\) In paper arXiv:hep-ph/0106150v2, Ponchiano etc. adopted Pade approximation and it seems to be working good. But it’s not directly useful to me.</p> <p>Well let’s move on to the next method.</p> <h3 id="the-continuation-homotopy-embedding-method">The continuation (homotopy, embedding) method</h3> <p>The core idea behind the “continuation method” is that, instead of trying to solve a super-hard problem right away, we start with a simpler version of it that we can solve. Then, we “continue” from that solution, making small changes step by step, until we reach the solution of the original, harder problem.</p> <ol> <li><strong>Start Simple</strong>: Begin with a version of the problem that’s easy to solve.</li> <li><strong>Make Small Changes</strong>: Adjust the problem little by little, using the solution from the last step as the starting point for the next.</li> <li><strong>Reach the Target</strong>: Continue this process until you’ve transformed your simple problem’s solution into a solution for your original, harder problem.</li> </ol> <p>Let us apply the aforementioned philosophical ideas into practice. Suppose we wish to solve a system of $N$ non-linear equations in $N$ variables, say \(F(x) = 0,\quad F: \mathbb{R}^{n} \to \mathbb{R}^{n}.\) We assume $F$ is $C^{\infty}$. Suppose that we don’t know a lot about the initial value of the derivative, then we can’t effectively adopt the shooting method. As a possible remedy, define a homotopy or deformation $H(x,t)$ which deforms from some simpler equations $G(x)$ to $F(x)$ when $t$ smoothly changes, to be specific define \(H(x,0) = G(x),\quad H(x,1) = F(x).\) Everything is required to be smooth here. Typically, one can choose a so-called <code class="language-plaintext highlighter-rouge">convex homotopy</code> such as \(H(x,t) = t\,F(x) + (1-t)\, G(x).\) $H(x,t)$ is the function we are trying to solve. Our job is to find $G(x)$ with known solution, then the PDE that $H(x,t)$ satisfies, offer the initial condition, then try to solve it.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Nuclear Physics B233 (1984) 109-115, doi: 10.1016/0550-3213(84)90172-x <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[In my line of work I constantly need to solve unsolvable ODEs and PDEs, unsolvable in the sense that it is impossible to get an analytical solution. What we do is to turn to numerical methods for help. For instance, just recently I need to solve a modified version of the Skyrme equation (the equation of motion resulted from the Skyrme model). I thought it would be helpful to summarize what I’ve learnt here.]]></summary></entry><entry><title type="html">The Moral Foundations of Politics</title><link href="https://baiyangzhang.github.io/blog/2023/The-Moral-Foundations-of-Politics/" rel="alternate" type="text/html" title="The Moral Foundations of Politics"/><published>2023-10-12T00:00:00+00:00</published><updated>2023-10-12T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/The-Moral-Foundations-of-Politics</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/The-Moral-Foundations-of-Politics/"><![CDATA[<h3 id="chapter-1">Chapter 1</h3> <p>2023-10-12</p>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[Chapter 1]]></summary></entry><entry><title type="html">Lecture Notes on Mathematics in Economics</title><link href="https://baiyangzhang.github.io/blog/2023/Mathematical-Economics/" rel="alternate" type="text/html" title="Lecture Notes on Mathematics in Economics"/><published>2023-10-09T00:00:00+00:00</published><updated>2023-10-09T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Mathematical-Economics</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Mathematical-Economics/"><![CDATA[<h3 id="syllabus">Syllabus</h3> <p><strong>Semester</strong>: Fall 2023</p> <p><strong>Duration:</strong> 40 Real hours (54 teaching hours), 3 real hours per class, 14 classes / 7 weeks</p> <p><strong>Lecturer</strong>: Dr. Baiyang Zhang</p> <p><strong>Office Address:</strong> N/A</p> <p><strong>Email</strong>: <a href="mailto:james.fisher@henu.edu.cn">byzhang@henu.edu.cn</a></p> <p><strong>Lecture Schedule</strong>: Monday and Wednesday, 2:30 - 5:30</p> <p><strong>Classroom:</strong> Monday at Teaching building Room 3502, Wednesday at Room 109 at the School of Economics</p> <p><strong>Textbooks and References:</strong></p> <ol> <li> <p>“<em>Fundamental Methods of Mathematical Economics</em>” by Chiang and Wainwright</p> </li> <li> <p>“<em>Introduction to Probability</em>” by Grinstead and Snell</p> </li> <li> <p>“<em>Introduction to Linear Algebra</em>” by Strang.</p> </li> </ol> <p><strong>Course Objectives:</strong>  This course will include the basics of analysis, derivatives and integration, linear algebra, optimization, and probability, with the goal of preparing students for further course work within the School of Economics.</p> <p><strong>Assessment Policy:</strong> The assessments for this course will one final, in addition to several homeworks. Each item is scored on a percentage basis. The final score for the class is the weighted sum of the items’ scores.  The weights are as follows: final accounts for 70% of the final grade, and the homeworks account for the remaining 30% of the final grade.</p> <p>In general, the final grade is an A when the final score is 85% or better, a B when the final score is between 70% and 84.9%, a C when the final score is between 60% and 69.9%, a D when the final score is between 50% and 59.9%, and an F when the final score is below 50%.  Assessment and final grades, however, may be curved to the benefit of the students.</p> <p><strong>Tentative Weekly Schedule:</strong></p> <p>CW = Chiang and Wainwright, GS = Grinstead and Snell, W = Wooldridge.</p> <p>Additional review sessions may be scheduled in advance of exams.</p> <table> <thead> <tr> <th><strong>Lecture</strong></th> <th><strong>Topics</strong></th> <th><strong>Reading</strong></th> </tr> </thead> <tbody> <tr> <td>1</td> <td>Introduction and Basics of Analysis</td> <td>CW, Ch. 1 and 2</td> </tr> <tr> <td>2-4</td> <td>Linear Algebra</td> <td>CW, Ch. 4 and 5</td> </tr> <tr> <td>5-6</td> <td>Derivatives</td> <td>CW, Ch. 6,7 and 8</td> </tr> <tr> <td>7</td> <td>Integrals</td> <td>CW, Ch. 13</td> </tr> <tr> <td>8-10</td> <td>Unconstrained Optimization</td> <td>CW, Ch. 9, 10, and 11</td> </tr> <tr> <td>11</td> <td>Constrained Optimization with Equality Constraints</td> <td>CW, Ch. 12</td> </tr> <tr> <td>12</td> <td>Probability Distributions and Combinatorics</td> <td>GS, Ch. 1, 2 and 3</td> </tr> <tr> <td>13</td> <td>Common Distributions and Conditional Probability</td> <td>GS, Ch. 4 and 5</td> </tr> <tr> <td>14</td> <td>Expected Values</td> <td>GS, Ch. 6</td> </tr> </tbody> </table> <hr/> <h2 id="lecture-1">Lecture 1</h2> <p><strong>Mathematical Economics versus Econometrics</strong></p> <p>Econometrics is concerned mainly with the measurement of economic data. Hence it deals with the study of empirical observations using statistical methods of estimation and hypothesis testing. Indeed, empirical studies and theoretical analyses are often complementary and mutually reinforcing. On the one hand, theories must be tested against empirical data for validity before they can be applied with confidence. On the other, statistical work needs economic theory as a guide, in order to determine the most relevant and fruitful direction of research.</p> <p><strong>Economic Models</strong></p> <p>A model is essentially and necessarily an abstraction from the real world. The sensible procedure is to pick out what appeals to our reason to be the primary factors and relationships relevant to our problem and to focus our attention on these alone.</p> <p><strong>Mathematics from a bird’s eye view</strong></p> <p>Explain: Algebra, Geometry, and Analysis.</p> <p>Most people who have done some high school mathematics will think of algebra as the sort of mathematics that results when you substitute letters for numbers. Algebra will often be contrasted with arithmetic, which is a more direct study of the numbers themselves.</p> <p>There is, however, a different contrast, between algebra and geometry, which is much more important at an advanced level. The high school conception of geometry is that it is the study of <code class="language-plaintext highlighter-rouge">shapes</code> such as circles, triangles, cubes, and spheres together with concepts such as rotations, reflections, symmetries, and so on. Thus, the objects of geometry, and the processes that they undergo, have a much more visual character than the equations of algebra.</p> <p>Some parts of mathematics involve manipulating symbols according to certain rules: for example, a true equation remains true if you “do the same to both sides.” These parts would typically be thought of as algebraic, whereas other parts are concerned with concepts that can be visualized, and these are typically thought of as geometrical.</p> <p>One is more symbolic and the other more pictorial.</p> <p>The word “analysis,” used to denote a branch of mathematics, is not one that features at high school level. However, the word “calculus” is much more familiar, and differentiation and integration are good examples of mathematics that would be classified as analysis rather than algebra or geometry. The reason for this is that they involve limiting processes. For example, the derivative of a function f at a point x is the limit of the gradients of a sequence of chords of the graph of $f$ , and the area of a shape with a curved boundary is defined to be the limit of the areas of rectilinear regions that fill up more and more of the shape.</p> <p>Thus, as a first approximation, one might say that a branch of mathematics belongs to analysis if it involves limiting processes, whereas it belongs to algebra if you can get to the answer after just a finite sequence of steps.</p> <p><strong>Branches of Mathematics</strong></p> <ul> <li>Algebra. Deals with number systems, polynomials, and more abstract structures such as groups, fields, vector spaces, and rings.</li> <li>Number theory.</li> <li>Algebraic geometry</li> <li>Analysis <ul> <li>The study of PDE, ODE.</li> <li>Dynamics. What happens when you take a simple process and do it over and over again?</li> </ul> </li> <li>Logic <ul> <li>Set theory</li> <li>Category theory</li> </ul> </li> <li>Combinatorics</li> <li>Theoretical Computer Science</li> <li>Probability</li> <li>Mathematical Physics</li> </ul> <hr/> <ol> <li><em>Math as a language with its own vocabulary and syntax.</em></li> <li><em>Introduction of set theory, including the basic concepts and operations that can be done to them.</em></li> <li><em>Introduce the concept of function and functional. They are nothing but various maps from one set to another.</em></li> <li>The number system. Explain integers, rational numbers, real numbers and complex numbers. $\mathbb{R},\mathbb{N}, \mathbb{Z}, \mathbb{Q}$.</li> </ol> <h2 id="lecture-2">Lecture 2</h2> <p>“You can’t add apples and oranges.” In a strange way, this is the reason for vectors. We have two separate numbers $v_ {1}$ and $v_ {2}$. The pair produces a two-dimensional vector $\vec{v}$. Explain the following concepts:</p> <ul> <li>column,</li> <li>components.</li> </ul> <p>We don’t add $v_ {1}$ and $v_ {2}$, but we do add vectors of the same type. Explain vector addition. We want to add apples with apples.</p> <p>Explain what is a scalar, and scalar multiplication.</p> <p>Given two vectors $\vec{v}$ and $\vec{w}$, explain the linear combination of them.</p> <p>This big view, taking all the combinations of $\vec{v}$ and $\vec{w}$, is linear algebra at work.</p> <p>Illustrate the addition of vectors using arrows.</p> <p>Introduce</p> <ul> <li>dot product,</li> <li>length.</li> </ul> <p>The dot product is gonna be needed when defining the action of a matrix on a vector.</p> <p>After introducing the product rules in two different ways, we introduce the linear equations.</p> <h3 id="lecture-3">Lecture 3</h3> <p><strong>Linear combination:</strong></p> <p>Imagine you have a collection of building blocks, and each block represents a different item. A “linear combination” is like creating a new structure using these blocks, where you decide:</p> <ol> <li><strong>How many of each block to use</strong>: This is similar to multiplying the block (or item) by a number.</li> <li><strong>How to combine them</strong>: Essentially, you’re just adding these multiplied blocks together.</li> </ol> <p>Let’s use a simpler example:</p> <p>Imagine you have two types of fruit: apples and bananas.</p> <p>A “linear combination” of apples and bananas could be:</p> <ul> <li>3 apples + 2 bananas</li> <li>5 apples + 1 banana</li> <li>2 apples - 4 bananas (Yes, in mathematics, you can have negative bananas! Just think of it as owing bananas to someone.)</li> </ul> <p>In each of these cases, the number of apples and bananas you decide to use (3, 2, 5, 1, etc.) are called “coefficients”.</p> <p>When it comes to mathematics and vectors, the idea is the same. You’re combining different vectors using certain coefficients to produce a new vector. But the basic idea is just like combining apples and bananas!</p> <hr/> <p>There are many ways to look at a matrix.</p> <ol> <li> <p><strong>Table of Numbers</strong>: At its core, a matrix is like a table or grid filled with numbers. Think of it like a spreadsheet or a bingo card. Each number sits in its own little box, and these boxes are organized into rows and columns.</p> </li> <li> <p><strong>Collection of Column Vectors</strong>: Imagine each column in that table as a list of numbers. This list can be seen as a “column vector”. So, a matrix can be thought of as a collection of these column vectors, standing side by side. For example, a matrix with three columns is like having three lists (or column vectors) put together.</p> </li> <li> <p><strong>Collection of Row Vectors</strong>: Similarly, you can think of each row in the matrix as a separate “row vector”. So, another way to view a matrix is as a stack of these row vectors, one on top of the other.</p> </li> <li> <p><strong>Transformation Machine</strong>(we will go to more details in this class): This is a more advanced way to think about matrices, especially in linear algebra. Imagine you have a point on a graph. A matrix can act as a “machine” where you input your point, and out comes a new point. This new point might be stretched, squished, rotated, or even flipped compared to the original. In essence, the matrix transformed it!</p> </li> <li> <p><strong>System of Equations</strong>(topic of this class too): If you’ve ever dealt with multiple equations at once (like trying to figure out both the price of a burger and fries when given combined costs), matrices can represent these systems. Each row could represent a different equation, and the numbers in that row represent the coefficients of variables in that equation.</p> </li> <li> <p><strong>Storage and Organization</strong>: In computer science and data analysis, matrices can be used to store data. For instance, consider ratings given by users to movies on a streaming platform. Each row might represent a user, each column might represent a movie, and the number in a specific box represents the rating that user gave to that movie.</p> </li> </ol> <p>These are just some of the many ways to look at matrices. Depending on the subject (like physics, computer graphics, or economics), matrices might take on other interesting interpretations!</p> <hr/> <p><strong>The multiplication of matrices</strong></p> <p><strong>The Basics</strong>:</p> <p>Matrix multiplication is not just multiplying numbers. Instead, it’s a combination of multiplication and addition. Remember, the way you multiply matrices is quite different from multiplying regular numbers, so it’s essential to understand the steps and rules.</p> <p><strong>The Key Rule</strong>:</p> <p>For two matrices to be multiplied, the number of columns in the first matrix must be equal to the number of rows in the second matrix. This is a crucial rule.</p> <p>If Matrix A has dimensions of $m \times n$ (meaning $m$ rows and $n$ columns) and Matrix B has dimensions of $p \times q$ (meaning $p$ rows and $q$ columns), then for A and B to be multipliable, $n$ must equal $p$. The resulting matrix will have dimensions $m \times q$.</p> <p><strong>How to Multiply</strong>:</p> <p>Let’s consider two simple matrices:</p> <p>Matrix A: \(\begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\) Matrix B: \(\begin{pmatrix} 2 &amp; 1 \\ 0 &amp; 3 \end{pmatrix}\)</p> <p>To multiply them:</p> <ol> <li><strong>First element of the result (top-left corner)</strong>: <ul> <li>Take the first row of Matrix A: (1, 2).</li> <li>Take the first column of Matrix B: (2, 0).</li> <li>Multiply corresponding elements and add them up: (1×2) + (2×0) = 2.</li> </ul> </li> <li><strong>Second element in the first row (top-right corner)</strong>: <ul> <li>Take the first row of Matrix A: (1, 2).</li> <li>Take the second column of Matrix B: (1, 3).</li> <li>Multiply corresponding elements and add them up: (1×1) + (2×3) = 7.</li> </ul> </li> <li><strong>First element in the second row (bottom-left corner)</strong>: <ul> <li>Take the second row of Matrix A: (3, 4).</li> <li>Take the first column of Matrix B: (2, 0).</li> <li>Multiply and add: (3×2) + (4×0) = 6.</li> </ul> </li> <li><strong>Second element in the second row (bottom-right corner)</strong>: <ul> <li>Take the second row of Matrix A: (3, 4).</li> <li>Take the second column of Matrix B: (1, 3).</li> <li>Multiply and add: (3×1) + (4×3) = 15.</li> </ul> </li> </ol> <p>The resulting matrix is:</p> <p>\(\begin{pmatrix} 2 &amp; 7 \\ 6 &amp; 15 \end{pmatrix}\) <strong>Visualization</strong>:</p> <p>Imagine Matrix A’s rows as horizontal hands reaching out, and Matrix B’s columns as vertical hands reaching up. When these hands “high-five”, they form the elements of the resulting matrix by the rule we just discussed.</p> <p><strong>Practice</strong>:</p> <p>The best way to get comfortable with matrix multiplication is to practice. Start with smaller matrices, understand the patterns, and then work with larger ones.</p> <p>Remember, the rule of matching columns of the first matrix to rows of the second is crucial. If they don’t match, the matrices can’t be multiplied.</p> <hr/> <h3 id="example-production-in-a-shoe-factory">Example: Production in a Shoe Factory</h3> <p>Imagine you run a small shoe factory. You produce two types of shoes: sneakers and boots.</p> <p><strong>Vectors</strong>:</p> <ol> <li><strong>Production Vector</strong> for a given week: <ul> <li>Sneakers: 100 pairs</li> <li>Boots: 50 pairs</li> </ul> <p>We can represent this as: \(\text{Shoes} = \begin{bmatrix} 100 \\ 50 \end{bmatrix}\)</p> </li> <li><strong>Cost Vector</strong> for producing each type of shoe: <ul> <li>Cost to produce one pair of sneakers: $20</li> <li>Cost to produce one pair of boots: $40</li> </ul> <p>This can be represented as: \(\text{Cost} = \begin{bmatrix} 20 \\ 40 \end{bmatrix}\)</p> </li> </ol> <p><strong>Matrix</strong>: Let’s say, to produce each shoe, you need two main raw materials: leather and rubber. We can create a <strong>Material Requirement Matrix</strong> that tells us how much of each material is required to produce one unit of each shoe type.</p> <p>For example:</p> <ul> <li>Each pair of sneakers requires 1 unit of leather and 2 units of rubber.</li> <li>Each pair of boots requires 3 units of leather and 1 unit of rubber.</li> </ul> <p>This matrix is: \(\text{Materials} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 1 \end{bmatrix}\) Where the first column corresponds to the requirements for sneakers and the second column to boots.</p> <p><strong>Matrix Multiplication</strong>:</p> <p>Now, suppose you want to find out how much raw material (leather and rubber) you’ll need for the entire week’s production.</p> <p>To do this, you’d multiply the Material Requirement Matrix by the Production Vector: \(\text{Total Materials} = \text{Materials} \times \text{Shoes}\)</p> <p>Multiplying, we get: \(\text{Total Materials} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 1 \end{bmatrix} \times \begin{bmatrix} 100 \\ 50 \end{bmatrix} = \begin{bmatrix} 200 \\ 350 \end{bmatrix}\)</p> <p>So, you’ll need:</p> <ul> <li>200 units of leather (100 for the sneakers and 150 for the boots)</li> <li>350 units of rubber (200 for the sneakers and 150 for the boots)</li> </ul> <p>This simple example demonstrates the power of vectors and matrices in understanding and organizing economic production.</p> <hr/> <p>Apply the multiplication of matrices to the product of:</p> <ol> <li>row vector times column vector,</li> <li>column vector times row vector (this one is strange).</li> </ol> <hr/> <p>Certainly! Let’s embark on this journey to understand transposes and inverses using clear examples and relatable analogies tailored for students stepping into the realm of mathematical economics.</p> <hr/> <p><strong>Transposes</strong></p> <p><strong>What is a Transpose?</strong> The transpose of a matrix is obtained by flipping the matrix over its main diagonal (the diagonal from the top-left to the bottom-right). In simpler terms, the rows of the matrix become the columns, and the columns become the rows.</p> <p><strong>Visual Analogy</strong>: Imagine you have a bookshelf full of books (your matrix). If you were to tip that bookshelf onto its side (so that it’s lying down), the rows of books would now appear as columns. That’s the transpose!</p> <p><strong>Example</strong>: Given the matrix: \(A = \begin{bmatrix} 2 &amp; 5 \\ 3 &amp; 7 \\ 1 &amp; 4 \\ \end{bmatrix}\)</p> <p>The transpose, denoted as $A^T$, is: \(A^T = \begin{bmatrix} 2 &amp; 3 &amp; 1 \\ 5 &amp; 7 &amp; 4 \\ \end{bmatrix}\)</p> <p><strong>In Mathematical Economics</strong>: Transposing can be useful for various reasons, such as making certain operations or calculations easier or more intuitive. For instance, when working with data sets or in regression analysis, transposes come in handy.</p> <p><strong>Inverses</strong></p> <p><strong>What is an Inverse?</strong> The inverse of a matrix, if it exists, is a matrix that, when multiplied with the original matrix, results in the identity matrix. The identity matrix is a special square matrix with ones on the main diagonal and zeros elsewhere.</p> <p>In symbols, for a matrix $A$, its inverse is denoted $A^{-1}$, such that: \(A \times A^{-1} = I\) where $I$ is the identity matrix.</p> <p><strong>Real-life Analogy</strong>: Think of the process of multiplication and its inverse, division. When you multiply a number by its reciprocal, you get 1. Similarly, in the world of matrices, when you multiply a matrix by its inverse, you get the identity matrix.</p> <p><strong>Properties</strong>:</p> <ol> <li>Not all matrices have inverses. Only square matrices (matrices with the same number of rows and columns) have the potential to have an inverse, and even among them, not all do.</li> <li>A matrix that does not have an inverse is called “singular” or “non-invertible”.</li> </ol> <p><strong>Example</strong>: For a 2x2 matrix: \(A = \begin{bmatrix} a &amp; b \\ c &amp; d \\ \end{bmatrix}\)</p> <p>Its inverse is: \(A^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d &amp; -b \\ -c &amp; a \\ \end{bmatrix}\)</p> <p>However, this inverse exists only if $ad-bc$ is not zero. If $ad-bc = 0$, then the matrix is singular and does not have an inverse.</p> <p><strong>In Mathematical Economics</strong>: The concept of an inverse matrix is fundamental when solving systems of linear equations, which frequently appear in economics. For example, determining equilibrium in markets, analyzing input-output models, or finding solutions to optimization problems often involve the use of matrix inverses.</p> <p>Both transposes and inverses are fundamental tools in the toolbox of mathematical economics. Just as we learn to add, subtract, multiply, and divide with numbers, we learn operations and manipulations with matrices to understand and solve intricate economic phenomena. As students progress, they’ll witness the power and elegance of linear algebra in analyzing economic systems.</p> <hr/> <p><strong>Square Matrix vs. Non-Square Matrix</strong></p> <p><strong>1. Square Matrix</strong>: A matrix is called a “square matrix” if it has the same number of rows and columns. In other words, its dimensions look like $n \times n$, where $n$ is a positive integer. You can visualize it as a perfect square filled with numbers, just like a chess or checkerboard.</p> <p><strong>Example</strong>: A 2x2 matrix: \(\begin{bmatrix} 2 &amp; 5 \\ 3 &amp; 7 \\ \end{bmatrix}\)</p> <p><strong>2. Non-Square Matrix</strong>: Any matrix that doesn’t have the same number of rows and columns is a “non-square matrix”. Its dimensions might look like $m \times n$, where $m$ and $n$ are positive integers, and $m \neq n$.</p> <p><strong>Example</strong>: A 2x3 matrix: \(\begin{bmatrix} 1 &amp; 4 &amp; 7 \\ 2 &amp; 5 &amp; 8 \\ \end{bmatrix}\)</p> <hr/> <p><strong>Square Matrices are Special in Multiplication.</strong></p> <p>When we talk about multiplication in the world of matrices, square matrices have a unique property: they’re “closed under multiplication”. This might sound fancy, but let’s break it down:</p> <p><strong>Closed Under Multiplication</strong>: This means that if you multiply two square matrices of the same size, you’ll get another square matrix of that same size as the result.</p> <p>Let’s say you have two square matrices, both of size $2 \times 2$. When you multiply them, the resulting matrix will also be $2 \times 2$. This property will hold true no matter how big or small the matrices are, as long as they’re square.</p> <p><strong>Economic Analogy</strong>: Imagine each square matrix as a factory machine. When a factory machine (a square matrix) processes another machine of the same size (another square matrix), the result is always a new machine of the same dimensions. This predictable outcome allows for consistent planning and operation, making these “machines” reliable and preferred in many scenarios.</p> <p><strong>Forming a Nice Algebra</strong>: The fact that square matrices are closed under multiplication means they form a consistent system, or a “nice algebra”. In this system, you can perform operations, like multiplication, and always know what kind of result to expect (another square matrix). This consistency is useful in mathematical economics because it provides a stable framework for analysis and predictions.</p> <hr/> <p><strong>What is a Linear Equation?</strong></p> <p><strong>Definition</strong>: A linear equation is an equation of the form: \(a_1x_1 + a_2x_2 + ... + a_nx_n = b\) where $x_1, x_2, … x_n$ are the variables, $a_1, a_2, … a_n$ are constants (known as coefficients), and $b$ is another constant.</p> <p><strong>Key Features</strong>:</p> <ol> <li>Each term consists of a variable multiplied by a constant.</li> <li>No term has a variable raised to a power higher than one.</li> <li>There are no products of variables (e.g., $x_1 \times x_2$).</li> </ol> <p><strong>Simple Example</strong>: Consider the equation $3x + 2y = 12$. Here, $x$ and $y$ are the variables, and the numbers 3 and 2 are their respective coefficients.</p> <p>Imagine you’re graphing this equation on a coordinate plane. For an equation with two variables, the graph would be a straight line. That’s why it’s called “linear” – the graph is a line.</p> <p><strong>What is a System of Linear Equations?</strong></p> <p><strong>Definition</strong>: A system of linear equations is just a collection of two or more linear equations that involve the <em>same set of variables</em>.</p> <p><strong>Simple Example</strong>: \(\begin{align*} 3x + 2y &amp;= 12 \quad \text{(Equation 1)} \\ x - y &amp;= 5 \quad \text{(Equation 2)} \end{align*}\)</p> <p>In this system, you have two linear equations, and you’d typically try to find values for $x$ and $y$ that satisfy both equations simultaneously.</p> <p><strong>Graphical Interpretation</strong>: When you plot both equations on a graph:</p> <ol> <li>If they intersect at a point, that point is the solution to the system (i.e., the values of $x$ and $y$ at that point satisfy both equations).</li> <li>If they never meet (parallel lines), the system has no solution.</li> <li>If the two equations represent the same line, then there are infinitely many solutions - any point on that line is a solution.</li> </ol> <p>Such systems help in understanding multiple interdependencies. For instance, if you have a market with two goods, and each equation represents how demand or supply changes based on the price of both goods, the system helps find an equilibrium where both goods’ demands are satisfied.</p> <p>Think of a linear equation as a single straight path (line) and a system of linear equations as multiple paths. Our goal is often to find where these paths meet or if they never do. In the context of economics, these meeting points can represent equilibrium states, optimal solutions, or any scenario where multiple conditions are satisfied at once. As students dive deeper into mathematical economics, they’ll see that these simple linear systems can be powerful tools for understanding complex economic relationships.</p> <hr/> <p>Certainly! Let’s simplify the concept and lay it out for students transitioning from a high school math background.</p> <hr/> <h3 id="using-matrices-to-represent-equations"><strong>Using Matrices to Represent Equations</strong></h3> <p>Let’s say we have the following system of equations:</p> \[\begin{align*} 2x + 3y &amp;= 8 \\ x - 4y &amp;= -3 \end{align*}\] <p>This can be represented in a matrix format as $AX = B$:</p> <p>Where: \(A = \begin{bmatrix} 2 &amp; 3 \\ 1 &amp; -4 \end{bmatrix}\) (Coefficients of the variables)</p> <p>\(X = \begin{bmatrix} x \\ y \end{bmatrix}\) (Our unknowns)</p> <p>\(B = \begin{bmatrix} 8 \\ -3 \end{bmatrix}\) (Results of the equations)</p> <h2 id="lecture-4">Lecture 4</h2> <h3 id="solving-using-the-inverse"><strong>Solving Using the Inverse</strong></h3> <p>Here’s the magic part: If we multiply both sides of our matrix equation $AX = B$ by the inverse of matrix $A$, which we’ll call $A^{-1}$, we can isolate $X$ (our unknowns).</p> <p>Doing the math: \(A^{-1}AX = X = A^{-1}B\)</p> <p>So, if we can find the inverse of $A$ (remember, not all matrices have inverses!), then we can multiply it with $B$ to get our solution, $X$.</p> <p>In economics, we often deal with many variables and relationships at the same time. Instead of trying to solve each relationship individually, matrices allow us to represent these complex relationships together and solve them in a more streamlined way.</p> <p>For example, imagine you’re studying how the price of one product affects the demand for another, and vice versa. Instead of solving each relationship individually, we can group them in a system of equations, represent them as matrices, and solve them all at once.</p> <p>Using the inverse matrix to solve a system of linear equations is like having a secret decoder ring. It’s a powerful tool that can make solving complex problems more manageable. As students dive deeper into mathematical economics, they’ll find that these tools, while initially seeming abstract, can be invaluable in understanding and analyzing economic relationships and behaviors.</p> <hr/> <p>Let’s think about 2D space for a moment. We’ve all seen the classic X-Y coordinate plane. Imagine you’ve got two vectors (think of them as arrows) on this plane. Sometimes, these two arrows will point in completely different directions. But occasionally, they might just lay flat on top of one another or be exactly opposite.</p> <p>Now, if we use these vectors as rows or columns in a matrix, the question becomes: Does this matrix have a unique way to revert any transformation it causes? Or in other words, can we find its inverse?</p> <p>This is where the idea of a matrix being “singular” comes in. A <strong>singular matrix</strong> doesn’t have an inverse. Visually, if you were to transform the entire 2D space using a singular matrix, some areas would scrunch up so much that they’d be impossible to revert to their original form.</p> <p>To figure out if a matrix is singular, we need a tool, and that tool is the <strong>determinant</strong>.</p> <p>Think of the determinant as a special number associated with a matrix. If the determinant is zero, our matrix is singular (it can’t be inverted). If the determinant isn’t zero, then the matrix can be inverted.</p> <p>For our 2x2 matrices (which are often the starting point in learning), the determinant gives us a sense of the “area scaling factor” when the matrix is used for a transformation. If the determinant is zero, it means the matrix squishes everything down to a line or a point, losing all the original area, making it impossible to revert.</p> <p>For a matrix to be nonsingular (i.e., to have an inverse), each row (like our detectives) has to bring something unique to the table. If even one row is just a repeat or combination of others, it’s like missing out on crucial information. And without that unique contribution from every row, we can’t find an inverse for our matrix.</p> <p><strong>**Rank of a Matrix</strong>**</p> <p>The rank of a matrix is a measure of the “dimension” of the linear space spanned by its rows or columns. In simpler terms, it tells us the number of linearly independent rows or columns in the matrix.</p> <p><em>The Library Analogy:</em></p> <ul> <li>Imagine you have a library of books. Some books might be exactly the same, and some might be different.</li> <li>If you were asked, “How many unique books do you have?”, you would ignore all duplicates and count only the distinct ones.</li> <li>The rank of a matrix is similar: It tells us how many “unique” rows (or columns) there are, ignoring any that can be made by combining others.</li> </ul> <p><em>Determining Rank:</em></p> <ol> <li>If a matrix has all zeros, its rank is 0.</li> <li>If a matrix has some non-zero elements but some rows (or columns) are just scalar multiples or combinations of other rows (or columns), its rank will be less than the total number of rows (or columns).</li> <li>If no row (or column) can be expressed as a combination of any other rows (or columns), the matrix is said to have full rank, meaning its rank is equal to the smaller of the number of rows or columns.</li> </ol> <hr/> <p><strong>Rank of a Matrix:</strong></p> <p>The rank of a matrix is a measure of the “dimension” of the linear space spanned by its rows or columns. In simpler terms, it tells us the number of linearly independent rows or columns in the matrix.</p> <p><strong>Analogies &amp; Insights:</strong></p> <ol> <li> <p><strong>The Library Analogy:</strong></p> <ul> <li>Imagine you have a library of books. Some books might be exactly the same, and some might be different.</li> <li>If you were asked, “How many unique books do you have?”, you would ignore all duplicates and count only the distinct ones.</li> <li>The rank of a matrix is similar: It tells us how many “unique” rows (or columns) there are, ignoring any that can be made by combining others.</li> </ul> </li> </ol> <hr/> <p><strong>Rank of a Matrix:</strong></p> <p>The rank of a matrix is a measure of the “dimension” of the linear space spanned by its rows or columns. In simpler terms, it tells us the number of linearly independent rows or columns in the matrix.</p> <p><strong>Analogies &amp; Insights:</strong></p> <ol> <li> <p><strong>The Library Analogy:</strong></p> <ul> <li>Imagine you have a library of books. Some books might be exactly the same, and some might be different.</li> <li>If you were asked, “How many unique books do you have?”, you would ignore all duplicates and count only the distinct ones.</li> <li>The rank of a matrix is similar: It tells us how many “unique” rows (or columns) there are, ignoring any that can be made by combining others.</li> </ul> </li> </ol> <p><strong>Determinant</strong>:</p> <p>The term “determinant” was used because it can “determine” whether or not a matrix has an inverse.</p> <p>Let’s say you have a 2x2 matrix: \(A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\)</p> <table> <tbody> <tr> <td>The determinant, often denoted as</td> <td>A</td> <td>or det(A), is calculated as:</td> </tr> <tr> <td>$$</td> <td>A</td> <td>= ad - bc $$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>If</td> <td>A</td> <td>equals zero, then A is singular.</td> </tr> </tbody> </table> <p>Introduce the Levi-Civita tensor. Use it to define the determinant.</p> <p>Of course! The Laplace Expansion is an important technique for calculating the determinant of a matrix. It’s especially useful when we have a matrix larger than $3 \times 3$, though it can be used for smaller matrices as well. The method is essentially a recursive process that breaks down a larger matrix into smaller ones.</p> <p><strong>Steps for Laplace Expansion:</strong></p> <ol> <li> <p><strong>Choosing a Row or Column:</strong> You can choose any row or column to expand upon. For the sake of simplicity, we often choose a row or column with the most zeros because it reduces the number of calculations we have to make (since any term multiplied by zero is zero).</p> </li> <li> <p><strong>Calculate Minors:</strong> For each element $a_{ij}$ of the matrix, remove the i-th row and the j-th column, and compute the determinant of the resulting $(n-1) \times (n-1)$ matrix. This determinant is called the “<code class="language-plaintext highlighter-rouge">minor</code>” of the element, often denoted $M_{ij}$.</p> </li> <li> <p><strong>Calculate Cofactors:</strong> Associated with each minor is a cofactor, which is defined as: $C_{ij} = (-1)^{i+j} \times M_{ij}$. This alternating sign pattern helps ensure the determinant computation is accurate.</p> </li> <li> <p><strong>Compute the Determinant:</strong> The determinant of the matrix is the sum of the products of the elements of your chosen row or column with their respective cofactors. Mathematically, if you chose the i-th row, this can be written as: \(\text{det}(A) = \sum_{j=1}^{n} a_{ij} C_{ij}\)</p> </li> </ol> <p>Alternatively, if you chose the j-th column, it is: \(\text{det}(A) = \sum_{i=1}^{n} a_{ij} C_{ij}\)</p> <p><strong>Example: Determinant of a $3 \times 3$ Matrix using Laplace Expansion:</strong></p> <p>Given matrix A: \(\begin{pmatrix} 1 &amp; 3 &amp; 2 \\ 4 &amp; 1 &amp; 3 \\ 2 &amp; 2 &amp; 1 \\ \end{pmatrix}\)</p> <p>To compute its determinant, let’s expand using the first row:</p> <ol> <li>For the element $a_{11} = 1$: <ul> <li>Minor $M_{11}$ is the determinant of: \(\begin{pmatrix} 1 &amp; 3 \\ 2 &amp; 1 \\ \end{pmatrix}\)</li> <li>$M_{11} = 1 - 6 = -5$</li> <li>Cofactor $C_{11} = (-1)^{1+1} \times (-5) = 5$</li> </ul> </li> <li>For the element $a_{12} = 3$: <ul> <li>Minor $M_{12}$ is the determinant of: \(\begin{pmatrix} 4 &amp; 3 \\ 2 &amp; 1 \\ \end{pmatrix}\)</li> <li>$M_{12} = 4 - 6 = -2$</li> <li>Cofactor $C_{12} = (-1)^{1+2} \times (-2) = 2$</li> </ul> </li> <li>For the element $a_{13} = 2$: <ul> <li>Minor $M_{13}$ is the determinant of: \(\begin{pmatrix} 4 &amp; 1 \\ 2 &amp; 2 \\ \end{pmatrix}\)</li> <li>$M_{13} = 8 - 2 = 6$</li> <li>Cofactor $C_{13} = (-1)^{1+3} \times 6 = -6$</li> </ul> </li> </ol> <p>Combining the results, \(\text{det}(A) = 1 \times 5 + 3 \times 2 + 2 \times (-6) = 5 + 6 - 12 = -1\)</p> <p>And that’s how you can use the Laplace Expansion to compute the determinant of a matrix! This method becomes more cumbersome for larger matrices, but the principles remain the same.</p> <p><strong>Properties of determinants</strong></p> <p>The addition (subtraction) of a multiple of any row to (from) another row will leave the value of the determinant unaltered. The same holds true if we replace the word row by column in the previous statement.</p> <p><em>It preserves the multiplication of matrices.</em> $\left\lvert A \cdot B \right\rvert = \left\lvert A \right\rvert \times \left\lvert B \right\rvert$.</p> <p><strong>Finding the Inverse Matrix</strong></p> <p>Introduce the adjoint of a matrix, and then the inverse.</p> <p><strong>Cramer’s rule</strong></p> <p>Let’s break down Cramer’s rule into a simple-to-understand explanation.</p> <p>Imagine you’re trying to solve a system of equations. This system might represent different scenarios. For instance, let’s say you and a friend are buying apples and bananas. Two different days, two different scenarios:</p> <ol> <li>On Monday, you bought 3 apples and 2 bananas, and it cost you $13.</li> <li>On Tuesday, you bought 4 apples and 5 bananas, and it cost you $31.</li> </ol> <p>From this, you have: 1) 3A + 2B = 13 2) 4A + 5B = 31</p> <p>Where A represents the cost of an apple and B represents the cost of a banana.</p> <p>Cramer’s rule helps you find the cost of A and B using determinants of matrices.</p> <p><strong>Step-by-step with Cramer’s Rule:</strong></p> <ol> <li> <p><strong>Main Determinant (D)</strong>: First, make a matrix of the coefficients of A and B: \(\begin{pmatrix} 3 &amp; 2 \\ 4 &amp; 5 \\ \end{pmatrix}\) Calculate its determinant (D). This determinant represents the “base scenario” of our system.</p> </li> <li> <p><strong>Determinant with respect to A:</strong> Replace the first column (which represents apples) with the numbers on the right side of our equations (13 and 31): \(\begin{pmatrix} 13 &amp; 2 \\ 31 &amp; 5 \\ \end{pmatrix}\) Calculate its determinant. This determinant represents the scenario when we’re focusing just on the apples.</p> </li> <li> <p><strong>Determinant with respect to B</strong>: Replace the second column (which represents bananas) with the numbers on the right side: \(\begin{pmatrix} 3 &amp; 13 \\ 4 &amp; 31 \\ \end{pmatrix}\) Calculate its determinant. This represents the scenario when we’re focusing just on the bananas.</p> </li> <li> <p><strong>Solving for A and B</strong>:</p> <ul> <li>The cost of an apple (A) is found by $A = D_A / D$</li> <li>The cost of a banana (B) is found by $B = D_B / D$</li> </ul> </li> </ol> <p>This gives you the individual prices of apples and bananas!</p> <p><strong>In simple words:</strong> Cramer’s rule lets you focus on one variable at a time (like just apples or just bananas) and then combine the results to find out the cost of each. It does this using determinants, which are a special number for matrices, like a fingerprint for the matrix.</p> <p>Remember, Cramer’s rule works best for systems where the number of equations matches the number of unknowns, and the main determinant (D) is not zero. If D were zero, it would be like trying to divide by zero, which we can’t do.</p> <p><strong>Application</strong></p> <p>Let’s walk through a simplified example of Input-Output Analysis using a hypothetical economy with just three industries: Agriculture, Manufacturing, and Services.</p> <p>Imagine the following table showing how each industry’s output is used as input by the others:</p> <table> <thead> <tr> <th> </th> <th>To Agriculture</th> <th>To Manufacturing</th> <th>To Services</th> <th>Final Demand</th> </tr> </thead> <tbody> <tr> <td><strong>From Agriculture</strong></td> <td>10</td> <td>30</td> <td>10</td> <td>50</td> </tr> <tr> <td><strong>From Manufacturing</strong></td> <td>20</td> <td>40</td> <td>20</td> <td>20</td> </tr> <tr> <td><strong>From Services</strong></td> <td>10</td> <td>10</td> <td>30</td> <td>50</td> </tr> </tbody> </table> <p>Each row represents the output of an industry, and each column (excluding the Final Demand column) represents the input to an industry. For example, the number 30 in the ‘From Agriculture’ row and ‘To Manufacturing’ column means that the Manufacturing sector uses 30 units of the Agriculture sector’s output.</p> <ol> <li> <p><strong>Creating the A matrix (Input-Output Coefficient Matrix):</strong> This matrix is obtained by dividing each element of the table by the total output of the corresponding industry. The total output for each industry is the sum of its outputs to all industries plus its final demand.</p> <p>Total output for each sector:</p> <ul> <li>Agriculture: 10 + 30 + 10 + 50 = 100</li> <li>Manufacturing: 20 + 40 + 20 + 20 = 100</li> <li>Services: 10 + 10 + 30 + 50 = 100</li> </ul> <p>Now, construct the A matrix:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|  0.1  0.3  0.1 |
|  0.2  0.4  0.2 |
|  0.1  0.1  0.3 |
</code></pre></div> </div> </li> <li><strong>The Leontief Inverse</strong>: To find the total output required to satisfy a given final demand, we use: \(X = (I - A)^{-1} Y\) Where: <ul> <li>$X$ is the total output vector.</li> <li>$Y$ is the final demand vector.</li> <li>$I$ is the identity matrix.</li> </ul> <p>In our case, $Y$ is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| 50 |
| 20 |
| 50 |
</code></pre></div> </div> <p>And $I$ is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| 1  0  0 |
| 0  1  0 |
| 0  0  1 |
</code></pre></div> </div> <p>Calculating $(I - A)$, and then finding its inverse can be done using a tool or software that supports matrix operations, such as MATLAB, Python (using NumPy), or even specialized calculators.</p> </li> <li><strong>Computing the Result</strong>: Once you have the Leontief inverse, you multiply it by the final demand vector $Y$ to get the total output vector $X$.</li> </ol> <p>The resulting $X$ vector will tell you how much each industry needs to produce in total to meet the given final demand, taking into account not just the direct demand for each industry’s products, but also the indirect demand generated by the need for inputs from other industries.</p> <p>In practice, real-world Input-Output tables are much larger and more complex, often involving hundreds of industries. Still, the basic principles and steps remain the same. Modern software tools make handling and analyzing these large matrices feasible.</p> <h2 id="lecture-5">Lecture 5</h2> <p>Imagine you have a seesaw in a playground. When two people of the same weight sit on each end, the seesaw will be balanced and level. This is similar to equilibrium in economics.</p> <p>In economics, equilibrium is like a balanced seesaw, but instead of people and weight, we are balancing supply and demand. When the amount of goods people want to buy (demand) is equal to the amount of goods available for sale (supply), we have what is called a market equilibrium. This balance determines the price of the good.</p> <p>For example, let’s say you and your friends want to buy lemonade on a hot day. If there’s a lot of lemonade available and not many people want to buy it, the price will likely be low. But if there’s only a little lemonade and a lot of people want it, the price will be high. The point at which the amount of lemonade people want to buy is equal to the amount available, and everyone is happy with the price, is the equilibrium.</p> <hr/> <p>Calculus is a branch of mathematics that deals with the study of change (differential calculus) and accumulation (integral calculus). It provides us with the tools to analyze and understand dynamic processes and systems that change continuously.</p> <p>When we talk about differential calculus, we are primarily concerned with the concept of a derivative, which represents the rate at which a function changes as its input changes. In simpler terms, derivatives help us find the slope of a curve at any given point. This is crucial in economics when we want to understand how one variable responds to changes in another variable, such as the relationship between price and quantity demanded in the market.</p> <p>On the other hand, integral calculus is concerned with the concept of an integral, which represents the accumulation of quantities. Integrals allow us to calculate areas under curves and can be used to find total cost, total revenue, or consumer surplus, given their respective density functions.</p> <p>Together, these concepts from calculus are fundamental tools in mathematical economics, helping us model and analyze the dynamic and complex relationships between different economic variables.</p> <hr/> <p>Review the concept of maps, functions. Then the concept of the slope (of the graph of a function). Then finite change and infinitesimal change. Then the derivative.</p>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[Syllabus]]></summary></entry><entry><title type="html">Note of Nonlinear Quantization</title><link href="https://baiyangzhang.github.io/blog/2023/Note-of-Nonlinear-quantization/" rel="alternate" type="text/html" title="Note of Nonlinear Quantization"/><published>2023-09-26T00:00:00+00:00</published><updated>2023-09-26T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Note-of-Nonlinear-quantization</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Note-of-Nonlinear-quantization/"><![CDATA[<h3 id="the-lagrangian">The Lagrangian</h3> <p>The rescaling works as follows. The length is measured by a length scale $\lambda$ as $x^{\mu} := \lambda x’^{\mu}$ where $x’$ is now dimensionless. I suspect the energy integral is measured by an energy unit $\mu$ by \(E = \int d^{3}x \, \mathcal{E} := \mu\, \int d^{3}x \, \mathcal{E} / \mu =\mu\, \int d^{3}x' \, \lambda^{3} \mathcal{E} / \mu.\) Then $\mathcal{E} / \mu$ is a dimensionless parameter.</p> <p>The term \(\mathcal{L}_ {k} := \frac{F_ {\pi}^{2}}{16}\mathrm{Tr}\,(R_ {\mu}^{2})\) (k for kinetic) is re-expressed as \(\mathcal{L}_ {k} = \frac{F_ {\pi}^{2} \lambda^{-2}}{16}\mathrm{Tr}\,(R_ {\mu'}^{2})\) since $R_ {\mu} \sim \partial_ {\mu}(-) = \lambda ^{-1}\partial_ {\mu’}(-)$, where $\partial_ {\mu’}$ is the derivative with respect to $x’$. Then the according contribution to the energy (Denoted by $E_ {k}$) \(E_ {k} = \int d^{3}x \, -\mathcal{L}_ {k}\) takes the new form \(E_ {k} =\mu \int d^{3}x' \, \mu ^{-1} \lambda^{3} \frac{F_ {\pi}^{2} \lambda^{-2}}{16}\mathrm{Tr}\,(R_ {\mu'}^{2})\) and I guess we want this to give \(E_ {k} =\mu \int d^{3}x' \, \frac{1}{2} \mathrm{Tr}\,(R_ {\mu'}^{2})\) thus we have the relation \(\boxed { 8\mu = F_ {\pi}^{2}\lambda. }\tag{relation 1}\)</p> <p>Now turn the the Skyrme term $\mathcal{L}_ {s}$. Do the same trick, in new unites we have \(\begin{align} E_ {s} &amp;= \mu \int d^{3}x' \, \mu ^{-1} \lambda^{3}\mathcal{L}_ {s} \\ &amp;=\mu \int d^{3}x' \, \mu ^{-1} \lambda^{3} \frac{\lambda^{-4}}{32e^{2}}\mathrm{Tr}\,([R_ {\mu'},R_ {\nu'}]^{2}) \end{align}\) where the factor $\lambda^{-4}$ again comes from re-express the derivative in $R_ {\mu}$. If we identify it with \(E_ {s} = \mu\,\int d^{3}x' \, \frac{2\eta-1}{16}\mathrm{Tr}\,[R_ {\mu'},R_ {\nu'}]^{2}\) where \(\boxed { \frac{1}{e^{2}} := \alpha^{2}(2\eta-1),\quad \beta := 2\alpha^{2}(1-\eta) }\) then we have another relation, \(\boxed { \frac{\alpha^{2}}{2}=\mu \lambda .} \tag{relation 2}\)</p> <p>From relation 1 and 2 we can solve for $\lambda$ and $\mu$ in terms of $\alpha$ and $F_ {\pi}$, \(\begin{cases} \mu = \frac{F_ {\pi}\alpha}{4}, \\ \lambda =\frac{2\alpha}{F_ {\pi}}. \end{cases}\)</p> <p>Then follow the paper draft we can also get \(m_ {1} = \frac{2\alpha m_ {\pi}}{F_ {\pi}}\) and \(m_ {2}=\frac{2\alpha M}{F_ {\pi}}.\)</p> <hr/> <h3 id="the-kinetic-term">The kinetic term</h3> <p>The kinetic terms are the terms in the Lagrangian that contains time derivatives.</p> <p>The most general rotation of a Skyrmion include both spatial rotation and iso-rotation, \(U(x,t) = A_ {1}(t) U_ {0}(D(A_ {2})(x-x_ {0})) A_ {1}(t)^{\dagger}\) where $U_ {0}$ is the static (time-independent) solution, $x_ {0}$ is the center of the Skyrmion, $A_ {1,2}$ are both $SU(2)$ matrices, $A_ {1}$ denotes the iso-rotation and $D(A_ {2})$ the spatial rotation. However, when we consider <em>the special Skyrmion configuration</em>, namely the hedgehog Skyrmion, the iso-rotation is equivalent to spatial rotation (for more details see the other note I made on Skyrmion), so we can just take one $A$ matrix. The convention is that we take $A_ {1}$, neglect the subscript $1$ we have \(U(x,t) = A(t)U_ {0}A(t)^{\dagger}.\) Making use of the specific form of the hedgehog solution, we can translate the time-dependent in $A(t)$ to \(U(\mathbf{x},t) = U_ {0}(R(t) \mathbf{x}),\quad R_ {ij}(t) = \frac{1}{2}\mathrm{Tr}\,(\sigma_ {i} A \sigma_ {j} A^{\dagger}).\)</p> <hr/> <p>Since $A(t)$ can be written as \(A(t) = \alpha_ {0}(t) + i\alpha_ {i}(t)\sigma^{i}, \quad \alpha \in \mathbb{R}\) and $\det(A)=1$ requires that \(\alpha_ {0}^{2}+\alpha_ {i}\alpha_ {i} = 1,\) thus the trajectory of $A(t)$ can be regarded as a curve on $\mathbb{S}^{3}$. Then $\dot{A}(t)$ is perpendicular to $A(t)$, thus we have \(\begin{align} \mathrm{Tr}\,(A ^{\dagger}(t)\dot{A}(t))&amp;=\mathrm{Tr}\,[(\alpha_ {0}\mathbb{1}_ {2}-i\alpha_ {i}\sigma^{i})\cdot(\dot{\alpha}_ {0}\mathbb{1}_ {2}+i \dot{\alpha}_ {j}\sigma^{j})] \\ &amp;=2\alpha_ {0}\dot{\alpha}_ {0}+\alpha_ {i}\dot{\alpha}_ {j}2\delta_ {ij} \\ &amp;= 2(\alpha_ {0},\alpha_ {1},\alpha_ {2},\alpha_ {3})\cdot(\dot{\alpha}_ {0},\dot{\alpha}_ {1},\dot{\alpha}_ {2},\dot{\alpha}_ {3}) \\ &amp;=0. \end{align}\)</p> <p>Furthermore, $A^{\dagger}\dot{A}$ is also skew-hermitian since \((A^{\dagger}\dot{A})^{\dagger}=(\dot{A}^{\dagger} A) = -A^{\dagger}\dot{A}.\)</p> <p>As a traceless anti-hermitian $2\times 2$ matrix, it can be expanded in the basis of three Pauli matrices with pure imaginary coefficients. Writing \(A^{\dagger}\dot{A} = \frac{i}{2} a_ {i} \sigma^{i}\) then the inverse is easily given by \(a_ {i} = -i \mathrm{Tr}\,\left\{ \sigma _ {i} A^{\dagger}\dot{A} \right\} ,\quad t\text{-dependent.}\)</p> <p>To avoid confusion, let us denote the time dependent rignt-currents as $\widetilde{R}_ {\mu}$ and keep $R_ {\mu}$ for time independent versions, that is $\widetilde{R}_ {\mu}:= A R_ {\mu}A^{\dagger}$.</p> <p>Straightforward calculation shows that \(\mathrm{Tr}\,\widetilde{R}_ {0}^{2}=2\mathrm{Tr}\,(M\cdot M-MU_ {0}MU_ {0}^{\dagger}),\quad M:=A^{\dagger}\dot{A}\) and if we define \(T_ {i} =\frac{i}{2}[\sigma _ {i} ,U_ {0}]U_ {0}^{\dagger}\) we have \(\mathrm{Tr}\,\widetilde{R}_ {0}^{2}=\mathrm{Tr}\,(T_ {i} a_ {i} T_ {j} a_ {j} ) = \mathrm{Tr}(T_ {i} T_ {j} ) \,a_ {i} a_ {j} .\)</p> <p>The kinetic energy reads \(T = \int d^3x \left[-\frac12\mathrm{Tr}\,(\widetilde{R}_0^2)-\frac{2\eta-1}{8}\mathrm{Tr}\,\left([\widetilde{R}_0,\widetilde{R}_i]^2\right)-\frac{1-\eta}{8}\left(\mathrm{Tr}\,(\widetilde{R}_0^2)\right)^2+\frac{1-\eta}{4}\mathrm{Tr}\,(\widetilde{R}_0^2)\mathrm{Tr}\,(\widetilde{R}_i^2)\right]\) We can rewrite it in terms of $a_ {i}$’s and regard $a_ {i}$’s as some kind of <strong>angular velocity</strong>. We have \(\boxed { T_ {i} a_ {i} = [A^{\dagger}\dot{A},U_ {0}]U_ {0}^{\dagger} = A^{\dagger}\widetilde{R}_ {0}A }\) and \(\widetilde{R}_ {i} = A (\partial_ {i}U_ {0})U_ {0}^{\dagger}A^{\dagger} = A R_ {i} A^{\dagger},\) Then we can rewrite the Lagrangian as following (to isolate-out the time dependent part).</p> \[\begin{align} \mathrm{Tr}\,\widetilde{R}_ {0}^{2}&amp; = \mathrm{Tr}\,(T_ {i} T_ {j} )a_ {i} a_ {j} , \\ \mathrm{Tr}\,\left([\widetilde{R}_0,\widetilde{R}_i]^2\right)&amp;= \mathrm{Tr}\,([T_ {i},R_ {i} ][T_ {j},R_ {i} ])a_ {i} a_ {j} , \\ \left(\mathrm{Tr}\,(\widetilde{R}_0^2)\right)^2&amp;= \mathrm{Tr}\,(T_ {i} T_ {j} ) \mathrm{Tr}\,(T_ {m} T_ {n} ) a_ {i} a_ {j} a_ {m} a_ {n} \end{align}\] <p>This agrees with Eq.(2.16) in the paper draft, which is \(\begin{align} T &amp;= \int d^3x\left[-\frac12\mathrm{Tr}\,(T_i T_j)-\frac{2\eta-1}{8}\mathrm{Tr}\,\left([T_i,R_k][T_j,R_k]\right)+\frac{1-\eta}{4}\mathrm{Tr}\,(T_i T_j)\mathrm{Tr}\,(R_k^2)\right]a_i a_j \\ &amp;\phantom{=\ }+\int d^3x\left[-\frac{1-\eta}{8}\mathrm{Tr}\,(T_i T_j)\mathrm{Tr}\,(T_k T_l)\right]a_i a_j a_k a_l, \end{align}\)</p> <p>Now what we need to do is to substitute the Hedgehog solution into the above expression. It’s done with the help of Mathematica.</p> <hr/> <p>By the end of the day, we have the expression for kinetic energy where the Hedgehog ansatz is adopted, \(T = \frac{1}{2} \Lambda_ {1} a ^{2} - \frac{1}{4} \Lambda_ {2} a^{4},\quad a^{2}:=a_ {i} a_ {i} , \,a^{4}:= a_ {i} a_ {i} a_ {j} a_ {j} ,\) where $\Lambda_ {1}$ is something like $\text{positive}+ \eta(\text{positive})$, while $\Lambda_ {2}$ is something like $(1-\eta)\text{positive}$.</p> <p>From this we can get the canonical momentum of $a_ {i}$, call it $J_ {i}$ \(J_ {i} = \Lambda_ {1}a_ {i} -\Lambda_ {2}a^{2} a_ {i}\) which squares to \(J^{2} = \Lambda_ {2}^{2}a^{6}-2\Lambda_ {1}\Lambda_ {2}a^{4}+\Lambda_ {1}^{2}a^{2}.\) Define $x:=a^{2}$ to simplify the notations, we want to solve the equation \(y(x) = \Lambda_ {2}^{2} x^{3}-2\Lambda_ {1}\Lambda_ {2}x^{2}+\Lambda_ {1}^{2}x-J^{2}=0,\) the solution will be something like $x=x(J)$.</p> <p>Being a cubic equation there exists three solutions, for details see the other note <a href="http://www.mathlimbo.net/2023/07/07/Quartic-Time-Derivative/">here</a>.</p> <p>In the limit $\eta\to 1$, for a finite positive $J^{2}$, there will always be three roots, all positive. The smallest one goes to a fixed finite value, while the other two goes to infinite. The root that stays finite in this limit is the root that goes to the original Skyrme result. So it makes sense to focus on it and regard it as the correct starting point.</p> <p>Now the question is, what are the kinetic energies given by the three non-degenerate roots, when $\eta&lt;1$. The kinetic energy in terms of $x$ reads \(T = \frac{1}{2} \Lambda_ {1} x - \frac{1}{4} \Lambda_ {2} x^{2},\quad x:=a_ {i} a_ {i}.\)</p> <p>Our goal is to substitute $x$ in kinetic energy from $x$ to $J^{2}$. The problem is that, for each value of $J^{2}$ we have three different values of $x$. Given three real roots $x_ {\text{min}}$, $x_ {\text{mid}}$ and $x_ {\text{max}}$, they will give different value of $T$, however in terms of $J^{2}$ they will give the same $T$ since they give the same $J^{2}$. It is reasonable to adopt $x_ {\text{min}}$ as the correct solution since it is what reproduces the Skyrme result when $\eta\to 0$. Let’s go back to the original equation \(y(x) = \Lambda_ {2}^{2} x^{3}-2\Lambda_ {1}\Lambda_ {2}x^{2}+\Lambda_ {1}^{2}x-J^{2}=0,\) and see if we can express $x$ in terms of $J^{2}$ explicitly. The <code class="language-plaintext highlighter-rouge">discriminant</code> of above equation is \(\Delta = -J^{2} \beta^3 \left(4 \alpha ^3+27 \beta J^{2}\right).\) where \(\alpha :=\Lambda_ {1}&gt;0,\quad \beta:=-\Lambda_ {2}&lt;0\) and $J^{2}&gt;0$. (Note the $\alpha,\beta$ defined here are not the ones appeared earlier.) If the discriminant $\Delta&gt;0$ then we have three non-degenerate roots, the positivity of $\Delta$ means \(\alpha^{3}&gt;- \frac{27}{4} \beta J^{2}.\) In the limit $\eta\to 1$, $\beta\to 0$ so we require $\alpha^{3}&gt;0$, which is automatically satisfied. This confirms that in this limit there will always be three degenerate roots.</p> <p>Define \(x=t-\frac{2\alpha}{3\beta}=:t+\frac{2\sqrt{ -p }}{\sqrt{ 3 }}\) where \(\begin{align} p&amp;= -\frac{\alpha^{2}}{3\beta^{2}} = - \frac{\Lambda_ {1}^{2}}{3\Lambda_ {2}^{2}}, \\ q&amp;= - \frac{2\alpha^{3}}{27\beta^{3}}-\frac{J^{2}}{\beta^{2}}= \frac{2\Lambda_ {1}^{3}}{27\Lambda_ {2}^{3}}-\frac{J^{2}}{\Lambda_ {2}^{2}}, \end{align}\) then the cubic equation adopts the depressed form, \(t^{3}+p t +q=0.\) The discriminant in terms of $p$ and $q$ is \(\Delta = -4 p^3 - 27 q^2, \quad \Delta &gt; 0 \implies 4 p^3 + 27 q^2&lt;0.\) We can see from the definition that $p&lt;0$ but $q$ is not necessarily so. If we solve for the minimum root $x$, we get (from mathematica) \(x_ {\text{min}}=\frac{i \left(\sqrt{3}+i\right) \sqrt[3]{\sqrt{12 p^3+81 q^2}-9 q}}{2 \sqrt[3]{2} 3^{2/3}}+\frac{p+i \sqrt{3} p}{2^{2/3} \sqrt[3]{3} \sqrt[3]{\sqrt{12 p^3+81 q^2}-9 q}}+\frac{2}{3} \sqrt{ -3p } \tag{3}\) which simplifies to (by hand mostly) \(\boxed { x_ {\text{min}} = \frac{4}{\sqrt{ 3 }}\sqrt{ -p }\,\sin ^{2}\left( \frac{\theta}{6}\right)} ,\quad \theta=\arctan \left( \frac{\sqrt{-12 p^3-81 q^2}}{-9q} \right) \tag{4}.\) This is as simple as I can get. I check it numerically, it works out. A natural question to ask is, what is the range of $\theta$? We could choose the principal value of the Arctan function, but the most reliable method to fix the ambiguities is still to compare the numerical results.</p> <hr/> <p><strong>Behavior at $\eta\to 1$</strong></p> <p>Write \(\eta := 1-\epsilon\) where $\epsilon$ is positive and infinitesimal. In this limit, at leading order we have \(\begin{align} \Lambda_ {1} &amp;\equiv\alpha= \frac{16\pi}{3}\int d r\; r^2\left[ \sin^2f +\sin^2(f)(f')^2 +\frac{\sin^4f}{r^2} \right], \\ \Lambda_ {2} &amp;\equiv-\beta= \boxed { \epsilon\; \frac{64\pi}{15}\int d r\;r^2\sin^4f=:-\epsilon K}. \end{align}\) $K$ goes to a constant in the classical Skyrme limit. We have (at $\epsilon\to 0$) \(K&lt;0,\quad \alpha&gt;0, \quad \beta&lt;0.\)</p> <p>Substitute $\beta=\epsilon K$ and leave $\alpha$ as it is, we get \(\begin{align} p&amp;= -\frac{\alpha^{2}}{3\beta^{2}} = \frac{1}{\epsilon^{2}}\left( -\frac{\alpha^{2}}{3K^{2}} \right) , \\ q&amp;= - \frac{2\alpha^{3}}{27\beta^{3}}-\frac{J^{2}}{\beta^{2}}= \frac{2\Lambda_ {1}^{3}}{27\Lambda_ {2}^{3}}-\frac{J^{2}}{\Lambda_ {2}^{2}}=\frac{1}{\epsilon^{3}}\left( -\frac{2\alpha^{3}}{27K^{3}} \right)+\frac{1}{\epsilon^{2}}\left( -\frac{J^{2}}{K^{2}} \right), \end{align}\) up to the NLO we have \(\begin{align} \theta &amp;= \arctan \left(-\sqrt{\epsilon } \sqrt{-\frac{27 J^2 K}{\alpha ^3}}\right)\\ &amp;\approx -\sqrt{\epsilon } \sqrt{-\frac{27 J^2 K}{\alpha ^3}}+ \frac{1}{3} \epsilon ^{3/2} \left(-\frac{27 J^2 k}{\alpha ^3}\right)^{3/2} +\mathcal{O}(\epsilon^{5/2}), \end{align}\) Consequently, up to NLO \(\begin{align} x_ {\text{min}} &amp;= \frac{4}{\sqrt{ 3 }}\sqrt{ -p }\,\sin ^{2}\left( \frac{\theta}{6}\right) \\ &amp;=-\frac{1}{\epsilon} \frac{4\alpha}{ 3K } \left( -\frac{3 J^2 K \epsilon }{4 \alpha ^3}-\frac{219 J^4 K^2 \epsilon ^2}{16 \alpha ^6} \right) \\ &amp;= \frac{J^{2}}{\alpha ^2} +\boxed { \epsilon \frac{73 K J^{4} }{4 \alpha ^5}}. \end{align}\) The boxed term is the correction resulting from the cubic terms. As for how useful this linearized expression is, I don not know, for in theory we already have a analytical expression, which we could use to fit $\eta$ according to various data.</p> <p><strong>The other two roots</strong></p> <p>Now let’s turn to the remaining two roots of the cubic equation. They have no correspondence in the classical Skyrme model, since as $\epsilon\to 0$ both of them goes to infinity, but it is helpful having their closed form written down for $\eta &lt; 1$.</p> <p>Regarding the three positive roots, let’s call them $x_ {\text{min}}, x_ {\text{mid}}$ and $x_ {\text{max}}$. We have already studied $x_ {\text{min}}$ in length in the previous section, some calculation shows that \(\boxed { x_ {\text{mid}} = \frac{4 \sqrt{-p} }{\sqrt{3}}\cos ^2\left(\frac{\theta-\pi }{6}\right), }\) at $\eta=1-\epsilon$ the asymptotic behavior is \(x_ {\text{mid}} =\frac{1}{\epsilon}\frac{\alpha }{K} -\frac{1}{\sqrt{ \epsilon }} \sqrt{-\frac{J^{2}}{\alpha K}} -\frac{27 J^2 K}{\alpha ^3}.\) As we can see, in the Skyrme limit the root diverges.</p> <p>Similarly, the greatest root is \(\boxed { x_ {\text{max}} = \frac{4 \sqrt{-p} }{\sqrt{3}}\cos ^2\left(\frac{\theta+\pi }{6}\right) .}\) Compare with $x_ {\text{mid}}$, we find that the $\theta-\pi$ term under $\cos$ is replace by $\theta+\pi$.</p> <p>The difference between $x_ {\text{max}}$ and $x_ {\text{mid}}$ is \(\boxed { x_ {\text{max}} - x_ {\text{mid}} = -2 \sqrt{-p}\, \sin \left(\frac{\theta }{3}\right), }\) at $\eta=1-\epsilon$ the asymptotic behavior is \(x_ {\text{max}} - x_ {\text{mid}} = \frac{1}{\sqrt{\epsilon }} \frac{-2 \sqrt{ J^2 }}{ \sqrt{-K \alpha}}-\sqrt{\epsilon } \frac{ 13 J^4\, \sqrt{-K}}{4 \,\alpha ^{7/2} \sqrt{J^2}}.\) It shows that close to the Skyrme limit, the distance between those two roots also becomes divergent.</p>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[The Lagrangian]]></summary></entry><entry><title type="html">Electrodynamics in Terms of Differential Forms</title><link href="https://baiyangzhang.github.io/blog/2023/Electrodynamics-in-Terms-of-Forms/" rel="alternate" type="text/html" title="Electrodynamics in Terms of Differential Forms"/><published>2023-09-25T00:00:00+00:00</published><updated>2023-09-25T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Electrodynamics-in-Terms-of-Forms</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Electrodynamics-in-Terms-of-Forms/"><![CDATA[<p><em>Disclaimer: Nothing in this note is original.</em></p> <h3 id="some-conventions">Some Conventions</h3> <p>We will use capital Latin letters such as $I,J,$ etc. to denote sets of indices, for example $a_ {I}$ is the shorthand notation for $a_ {i_ {1}\dots i_ {p}}$. To get rid of annoying factorial factors that frequently appear in differential form calculation, we require the indices to be arranged in increasing order, the indices are denoted $I_ {&lt;}$, \(I_ {&lt;} = \left\{ i_ {1}&lt;i_ {2}&lt;\dots&lt;i_ {p} \right\} .\) We also adopt the so-called generalized Kronecker delta symbol, \(\delta^{I}_ {J}:= \begin{cases} 1 &amp; J \text{ is an even permutation of }I \\ -1 &amp; J \text{ is an odd permutation of }I \\ 0 &amp; \text{ otherwise} \end{cases}\) for example, we have \(\delta^{123}_ {132}=-1,\quad \delta^{123}_ {231}=1, \quad \delta^{123}_ {124}=0.\)</p> <p>We can then define the familiar Levi-Civita symbol as \(\epsilon_ {I} = \epsilon_ {i_ {1}\dots i_ {n}} =\epsilon^{I}= \delta^{I}_ {12\dots n}.\) <strong>Note that it is called a symbol and it is not a tensor!</strong> So don’t use the metric to raise or lower the indices! Calculation-wise, the biggest difference between the <code class="language-plaintext highlighter-rouge">Levi-Civita symbol</code> and <code class="language-plaintext highlighter-rouge">Levi-Civita tensor</code> is that, for the symbol $\epsilon_ {I} = \epsilon^{I}$ <em>by definition</em>, however for the tensor what people usually do is to define $\epsilon_ {I}$ first, then use the metric to raise its indices, as for all tensors. It usually yields a factor involving $\det g$ and the signature of $g$, which is $-1$ for Minkowski metric.</p> <p>In the above-mentioned notations, the wedge product of two differential forms $\alpha,\beta$ can be written as (in components) \((\alpha \wedge \beta)_ {I} = \delta_ {I}^{MN} \alpha_ {M_ {&lt;}} \beta_ {N_ {&lt;}}.\) where repeated indices are supposed to be summed over, as usual. In functional form \((\alpha \wedge \beta)(V_ {I}) = \delta^{MN}_ {I} \alpha(V_ {M_ {&lt;}})\beta(V_ {N_ {&lt;}}).\) As an example of the last definition, take $\alpha,\beta$ to be two 1-forms acting on vectors $V_ {1},V_ {2}$, then by the definition \((\alpha \wedge \beta)(V_ {1}V_ {2}) = \delta^{MN}_ {12} \alpha(V_ {M_ {&lt;}}) \beta(V_ {N_ {&lt;}}) = \alpha(V_ {1}) \beta(V_ {2}) - \alpha(V_ {1})\beta(V_ {2}),\) which reproduced our familiar definition for the wedge product.</p> <p>As another example, let $\alpha$ be a 2-form and $\beta$ be a 1-form, then \((\alpha \wedge \beta)_ {523} = \delta_ {523}^{IJ}\; \alpha_ {I_ {&lt;}}\beta_ {J_ {&lt;}} = \delta^{253}_ {523}\alpha_ {25}\beta_ {3}+ \delta^{352}_ {523}\alpha_ {35}\beta_ {2} +\delta^{235}_ {523}\alpha_ {23}\beta_ {5}\) which is \(-\alpha_ {25}\beta_ {3} +\alpha_ {35}\beta_ {2} + \alpha_ {23}\beta_ {5}.\) The notations we adopt here greatly simplifies the calculation.</p> <p>The wedge product of three differential forms are also beautifully expressible in our notation, \((\alpha \wedge \beta \wedge \gamma)_ {I} = \delta^{ABC}_ {I} \alpha_ {A} \beta_ {B} \gamma_ {C},\quad A,B,C \text{ in increasing order}.\)</p> <hr/> <p>The contraction of indices also works in a simple way,</p> <p><strong>Lemma.</strong> \(\sum_ {J_ {&lt;}}\delta^{IJ}_ {M}\delta^{KL}_ {J} = \delta^{IKL}_ {J}.\)</p> <h3 id="a-little-bit-more-about-differential-forms">A little bit more about differential forms</h3> <p>Although forms can be expanded in any bases, the geometric meaning of differential forms is most obvious in terms of Cartesian coordinates. By Cartesian coordinates we just mean that the metric is that of the Euclidean space $\mathbb{R}^{n}$. In this case, the bases for the tangent vector space is $\partial_ {i}$ and the dual bases are given by $dx^{i}$. We already know that $dx^{i}$ reads off the i-th component of a vector $\mathbf{v}$, \(\left\langle dx^{i},\mathbf{v} \right\rangle = v^{i}.\) Note that there will be problem if you think of $dx^{i}$ as an infinitesimal quantity, as is the case in the classical real analysis! Because then on the left hand side we have the inner product between an infinitesimal thing and one regular-sized thing, the final product should be infinitesimal of order one, but on the right hand side we clearly have a regular-sized thing, the i-th component of a finite-sized vector. In fact, $dx^{i}$ is a special case of $d\mathbf{v}$ for arbitrary vector $\mathbf{v}$, and $d\mathbf{v}$ should be regarded as a <strong>vector-valued 1-form</strong>! That is, something that takes an vector and spits out an vector. For more details, please refer to Frankel etc.</p> <p>Let’s start with the 2-dimensional Euclidean space. Given two vectors $\mathbf{v,w}$ we have \(dx^{i}\wedge dx^{j}(\mathbf{v,w}) = \begin{vmatrix} v^{i} &amp; w^{j} \\ v^{j} &amp; w^{i} \end{vmatrix} = \pm \text{ area projected on the } x^{i}x^{j} \text{ plane.}\)</p> <hr/> <p>We will show that exterior products yield a coordinate-free expression for the determinant! Consider an n-tuple of 1-forms $\tau^{1},\dots,\tau^{n}$, given in terms of bases \(\tau^{i} = T^{i}_ {j} \sigma^{j}\) where $\sigma^{i}$ are the bases. Then \(\begin{align} \tau^{1}\wedge \dots \wedge \tau^{n} &amp;= T^{1}_ {i_ {1}}\sigma^{i_ {1}}\wedge \dots \wedge T^{n}_ {i_ {n} } \sigma^{i_ {n}} \\ &amp;= T^{1}_ {i_ {1}}\dots T^{n}_ {i_ {n}} \sigma^{i_ {1}}\wedge \dots \wedge \sigma^{i_ {n}} \\ &amp;= T^{1}_ {i_ {1}}\dots T^{n}_ {i_ {n}} \sigma^{1}\wedge \dots \wedge \sigma^{n} \epsilon^{i_ {1}\dots i_ {n}} \\ &amp;= (\det T) \;\sigma^{1}\wedge \dots \wedge \sigma^{n} \end{align}\) For this reason the wedge product is very convenient for discussing linear dependence. For example, using form it is quite easy to tell if a set of forms are linearly independent, see the theorem below.</p> <p><strong>Theorem.</strong> The set of 1-forms $\tau^{i}$ are <strong>linearly dependent</strong> iff \(\tau^{1} \wedge \tau^{2} \wedge \dots \wedge \tau^{n} = 0\)</p> <hr/> <p>Many formulas given in the form of vectors are more natural in terms of differential forms, here we shall briefly discuss the computations of differential forms and its relation with vector analysis.</p> <p>Consider $\mathbb{R}^{3}$ as a 3-manifold with any (perhaps curvilinear) coordinate system $x^{1,2,3}$. Let $f$ be a 0-form, that is a function on $\mathbb{R}^{3}$. The 1-forms are written as \(\alpha = \alpha_ {1}dx^{1} + a_ {2} dx^{2} +a_ {3}dx^{3}\) and 2-forms are written as \(\begin{align} \gamma &amp;= \gamma_ {12} dx^{1}\wedge dx^{2} + \gamma_ {23}\, dx^{2}\wedge dx^{3} + \gamma_ {13} dx^{1}\wedge dx^{3} \\ &amp;:=\gamma_ {3} dx^{2}\wedge dx^{1} + \gamma_ {1}\, dx^{2}\wedge dx^{3} + \gamma_ {2} dx^{3}\wedge dx^{1} \end{align}\) where we used the convention that the indices are always arrange in increasing order in the first line, and indices are cyclic in the second line.</p> <p>The 3-forms are the simplest for they only have one component, \(\omega = \omega(x^{1},x^{2},x^{3})dx^{1}\wedge dx^{2}\wedge dx^{3}.\)</p> <p>There are familiar expressions used in vector analysis in the case when the coordinates are Cartesian, involving line, surface, and volume integrals. Let’s look at some examples.</p> <p>Given 1-forms $\alpha,\beta$, let $\mathbf{a,b}$ be the corresponding vectors defined components-wise, namely \(\mathbf{a} = a^{i} \partial _ {i},\quad a^{i} \equiv \alpha_ {i}, \text{the same for }b \text{ and }\beta\) then we have \(\alpha \wedge \beta = \mathbf{a}\times \mathbf{b} \,dS^{i} \text{ component-wise}\) where $ds^{1} \equiv dx^{2}\wedge dx^{3}$ and etc. Note that this correspondence in general does not hold in curvilinear coordinates.</p> <hr/> <p>There has already been a lot of discussions about the theoretical importance and philosophy of the exterior differential, I will not dive in them here, interested readers can refer to various textbooks and lecture notes. Here instead I’ll just focus on the calculation methods, starting with some examples in $\mathbb{R}^{3}$.</p> <p>Given a function $f(x)$ which is by definition also a 0-form. The differential of $f$ is \(df = \partial_ {x}f dx + \partial _ {y}f dy + \partial _ {z}f dz\) If the coordinates are Cartesian, then the components are the components of the gradient of $f$, \(\nabla_ {i} f = \partial _ {i}f,\quad df = \nabla f \cdot d\mathbf{x}\) where $d\mathbf{x} = (dx^{1},dx^{2},dx^{3})$.</p> <p>That is for 0-form. Given a 1-form $\alpha$ and the corresponding vector $a$ whose components are given by that by $\alpha$, we have \(d \alpha = (\text{curl }a) \cdot d\mathbf{x} \text{ component wise.}\)</p> <p>Finally, for a 2-form $\beta$ and corresponding vector $b$, namely $b_ {i} =\epsilon_ {ijk}\beta_ {jk}$, we have \(d\beta = (\partial _ {1}b_ {1}+\partial _ {2}b_ {2}+\partial _ {3}b_ {3})dx\wedge dy\wedge dz,\) in Cartesian coordinates $d\beta$ is the divergence of the vector $b$.</p> <p>$d^{2}=0$ in any coordinate system; in Cartesian coordinates this yields the famous $\text{curl }\text{grad }=0$ and $\text{grad }\text{curl }=0$.</p> <p>It is important to realize that it is no more difficult to compute d in a curvilinear coordinate system than in a Cartesian one. For example, in spherical coordinates, for 1-form \(\alpha =\alpha_ {r} dr + \alpha_ {\theta}d\theta+ \alpha_ {\phi}d\phi\) and the orientation is given by $dr\wedge d\theta \wedge d\phi$, then $d\alpha$ is simply \(d\alpha = d\alpha_ {r}\wedge dr + d \alpha_ {\theta}\wedge d\theta + d\alpha_ {\phi} \wedge d\phi\) where \(d\alpha_ {r}\wedge dr = (\partial _ {\theta}\alpha_ {r}) d\theta \wedge dr + (\partial _ {\phi}\alpha _ {r}) d \phi \wedge dr, \text{ etc. }\)</p> <p>In general, in situations where the “curl “ of a “vector” is required, the “vector” will most naturally appear in covariant form, i.e., it will be a 1-form.</p> <p>Next we give the coordinate expression for $d$. Let $\alpha \in \Omega^{p}(\mathbb{R}^{n})$ and explicitly \(\alpha = \alpha_ {I_ {&lt;}} dx^{I}.\) Then \(d\alpha = (d \alpha_ {I_ {&lt;}}) \wedge dx^{I} = (\partial _ {j}\alpha_ {I_ {&lt;}}) dx^{j} \wedge dx^{I}.\) In component form we have \((d\alpha)_ {I} = \delta_ {I}^{jK_ {&lt;}} \;\partial _ {j}\alpha_ {K}.\)</p> <h3 id="orientation-and-pseudo-forms">Orientation and Pseudo-forms</h3> <p>Let $\mathbf{e}=(\mathbf{e_ {1},\dots,e_ {n}})$ and $\mathbf{f} = (\mathbf{f_ {1},\dots,f_ {n}})$ be two sets of bases, with transition matrix $P:\mathbf{e}\mapsto \mathbf{f}$ acting from the right, according to our convention, namely $\mathbf{f}=\mathbf{e}P$. They are said to be of the same <code class="language-plaintext highlighter-rouge">orientation</code> iff $\det P&gt;0$. Thus orientation can be seen as equivalences classes of bases, with only two elements. <strong>We orient a vector space by declaring one of the two classes of bases to be positive</strong>. In our 3-space it is usual to declare the right-handed bases to be positively oriented, but we could just as well have the left-handed bases as positive.</p> <p>Now consider a manifold $M$, at each point $p$ there is a tangent vector space $T_ {p}M$ on which we need to define an orientation. We shall define the orientation in a continuous manner. Given a parametrization $\mathbb{R}^{n}\to M$ of $M$ in terms of $x$, we have a set of natural bases given by \(\left\{ \partial_ {1},\dots,\partial_ {n} \right\}\) and we can simply define the orientation by saying that the order $\partial_ {1},\partial_ {2},\dots,\partial_ {n}$ is positive. Then any even permutation of it is still even, and odd permutation would be odd, simple as that.</p> <p>We shall say that a manifold $M$ is orientable if we can cover $M$ by coordinate patches having positive Jacobians in each overlap. We can then declare the given coordinate bases to be positively oriented, and we then say that we have oriented the manifold. Briefly speaking, if a manifold is orientable it is then possible to pick out, in a continuous fashion, an orientation for each tangent space $T_ {x}M$ of $M$. Conversely, if it is possible to pick out continuously an orientation in each tangent space, we can assume that the coordinate frames in each coordinate patch have the chosen orientation and Mil must be orientable. <strong>An interesting fact is that complex manifolds are always orientable.</strong></p> <p>Next we introduce so-called pseudoforms. Consider ordinary $\mathbb{R}^{3}$ with its euclidean metric. We would like to define the “volume 3-form” Vol$^{3}$ to be the form that assigns to any triple of vectors the volume of the parallelopiped spanned by the vectors; in particular $\text{Vol}(X, Y, Z)$ should be $1$ if $X, Y$, and $Z$ are orthonormal. But if $\text{vol}$ is to be a form we must then have $\text{vol}(Y, X, Z) = -1$, and yet $Y, X$, and $Z$ are orthonormal. We have asked too much of vol. In some books they get around this by taking absolute value $\left\lvert \text{Vol} \right\rvert$, but it this does great harm to the machinery of forms that we have labored to develop. What we could do is require that $\text{Vol}(X, Y, Z) = 1$ if the triple is an orthonormal right-handed system. This makes the volume form orientation-dependent. There is a serious drawback to this definition; what if we are dealing with a space that is not orientable? The physical space in which we live is, according to general relativity, curved and perhaps not orientable.</p> <p>We compromise by defining a new type of form (called “<code class="language-plaintext highlighter-rouge">form of odd kind</code>” by its inventor Georges de Rham) differing from our usual forms (of “<code class="language-plaintext highlighter-rouge">even kind</code>”) in a way that will not seriously harm our machinery.</p> <p>First note that the assignment of an orientation to a vector space $E$ is the same as the assignment of a function $o$ on bases of $E$ whose values are the two integers $\pm {1}$. If $(x)$ is a coordinate system, we should write $o(x)$ rather than $o(\partial_ {x})$.</p> <p><strong>Definition.</strong> A <code class="language-plaintext highlighter-rouge">pseudo-p-form</code> $\alpha$ on a vector space $E$ assigns,for each orientation $o(E)$, an exterior p-form $\alpha_ {o}$ such that if the orientation is reversed the exterior form is replaced by its negative, \(\alpha_ {-o} = - \alpha_ {0}.\) An example is the volume form of $\mathbb{R}^{3}$. Let $(x,y,z)$ be the Cartesian coordinate system in $\mathbb{R}^{3}$, we don’t endow it with an orientation yet, then \(\text{Vol} := o(x,y,z) dx\wedge dy\wedge dz.\) Note that the order in the orientation $o(x,y,z)$ is matched with the order in the differential form $dx,dy,dz$. If we adopt the familiar right-handed orientation then $\text{Vol}$ is $dx\wedge dy \wedge dz$, but if we adopt the left-handed orientation then $\text{Vol}$ would be $-dx \wedge dy \wedge dz$.</p> <p>So roughly speaking \(\text{pseudo-forms} = \text{orientation}\times \text{forms}.\)</p> <p>Similarly we can define pseudovectors, pseudoscalars, and so on, pseudo always referring to a change of sign with a change of orientation.</p> <p>The volume form in a Riemannian manifold is very similar to that in $\mathbb{R}^{3}$. The <code class="language-plaintext highlighter-rouge">volume (pseudo)-n-form</code> $\text{Vol}^{n}$ is by definition the unique n-form that 1) assigns to an orientation $o$ of the tangent space $M$ and 2) a positively oriented orthonormal bases $\mathbf{e}$ the value $+1$. Based on that, given an arbitrary coordinate system denoted by $(y)$, after some calculation we find that \(\boxed{ \text{Vol}^{n} = o(y) \sqrt{ \left\lvert g \right\rvert }\; dy^{1} \wedge \dots dy^{n}. }\)</p> <p>It is traditional to omit the orientation function $o(y)$, and we shall do so when no confusion can arise.</p> <h3 id="interior-products-and-vector-analysis">Interior Products and Vector Analysis</h3> <p>We know that the contraction between a contravariant index and a covariant index gives us a scalar, the two indices cancel each other. The interior product between a form and a vector is the application of that idea.</p> <p>Let $v$ be a vector and $\alpha$ a p-form. Their <code class="language-plaintext highlighter-rouge">interior product</code> is a $(p-1)$-form $i_ {v}\alpha$ defined by \(i_ {v}\alpha = 0 \text{ if } \alpha \text{ is a 0-form},\) \(i_ {v}\alpha = \alpha(v) \text{ if }\alpha \text{ is a }1\text{-form},\) and \(i_ {v}\alpha(w_ {2},\dots,w_ {p}) := \alpha(v,w_ {1},\dots,w_ {p}).\)</p> <p>Sometimes we shall write $i(v)$.</p> <p><strong>Theorem.</strong> The interior product $i_ {v}: \Omega^{p} \to \Omega^{p-1}$ is an <code class="language-plaintext highlighter-rouge">antiderivation</code>, that is, for two forms $\alpha \in \Omega^{p}$ and $\beta \in \Omega^{q}$ \(i_ {v}(\alpha \wedge \beta) = (i_ {v}\alpha)\wedge \beta + (-1)^{p} \alpha \wedge i_ {v}\beta.\)</p> <p>Pushing $i_ {v}$ through $\alpha$ gives us an extra factor $(-1)^{p}$, just like the exterior differentiation.</p> <p>In components, we have \(i_ {v}\alpha = \sum_ {I_ {&lt;}}v^{j}\alpha_ {jI}\; dx^{I}.\)</p> <hr/> <p>Given a specific coordinate system, we can associate a 1-form to a vector by identifying the components. Of course this wouldn’t work under a change of coordinates, since a covariant vector (1-form) and a contravariant vector (regular vectors) have different transformation rules, but we can forget about the transformation rule for now. Writing the contraction as left-right angle brackets, i.e. for a vector $v$ and a 1-form $\alpha$ \(v(\alpha) = \alpha(v) =: \left\langle \alpha,v \right\rangle\) where we adopted the convention that in $\left\langle -,- \right\rangle$ the vector appears on the right. The above mentioned associating to a vector a 1-form is just dualization, namely, given the 1-form $\nu$, which acts on any vector as $\left\langle \nu,- \right\rangle$, the associated vector is the unique vector $v$ that acts on any 1-forms as $\left\langle -,v \right\rangle$. We will write \(\nu = \left\langle -,v \right\rangle .\) For example, \(\left\langle \nu, X \right\rangle \equiv \left\langle X,v \right\rangle,\) where $\left\langle -,- \right\rangle$ is the inner product between two vectors on the right hand side. We use the same notation for all kind of “inner products”, may that be inner products between vectors, or between a vector and a 1-form.</p> <p>In 3D Euclidean space, a vector can also be associated to a 2-form given by $v_ {1}dx^{2}\wedge dx^{3}+\dots$, so how does this work in a general coordinate system? For example, given a curved 3D Riemannian manifold, is there a universal way to associate a vector to a 2-form? To to that, we need the interior product we just introduced.</p> <p>We claim that \(\mathbf{v} \longleftrightarrow \tilde{\nu} := i_ {\mathbf{v}}\text{Vol}^{3}.\) In curvilinear coordinates $(u)$, omitting the orientation $o(u)$, we have \(i_ {v}\sqrt{ \left\lvert g \right\rvert }\; du^{1}\wedge du^{2}\wedge du^{3}=\sqrt{ \left\lvert g \right\rvert }(v^{1} du^{2}\wedge du^{3}+u^{2}du^{3}\wedge du^{1}+v^{3} du^{2}\wedge du^{1}).\)</p> <p>Conversely, if $\beta$ is a 2-form, we can associate to it a vector $b$ with components \(\mathbf{b} := \frac{1}{\sqrt{ \left\lvert g \right\rvert }}(b_ {23},b_ {31},b_ {12}) .\) The same procedure will work with in any manifold $M^{n}$ having some distinguished volume form (not necessarily coming from a Riemannian metric). To the vector $v$ we may associate the (pseudo-)$(n-1)$-form $i_ {v}\text{Vol}^{n}$. It is pseudo since the volume is pseudo and $v$ is not.</p> <p>Let $v,w$ be two vectors and $\nu,\omega$ be the corresponding 1-forms. What about the cross product of the vectors? We know that $v\times w$ is antisymmetric and so is $\nu \wedge\omega$ , thus we would guess that $\nu \wedge\omega$ is the 2-form associated to the so-called vector $v\times w$. It turns out to be correct. Recall that the 2-form associated to vector $v\times w$ is $i_ {v\times w}\text{Vol}^{3}$ , we have \(\nu \wedge \omega=i_ {v\times w}\text{Vol}^{3}.\)</p> <p>Although not usually mentioned in elementary books, the vector product is defined in $\mathbb{R}^{3}$ as follows: $v\times w$ is the unique pseudovector such that \(\text{Vol}(v,w,c) = \left\langle v\times w, c \right\rangle \text{ for each vector }c.\)</p> <hr/> <p>Vector algebra in $\mathbb{R}^{3}$ is easily handled by use of interior and exterior products; the only question is, should one associate to a vector $\mathbf{B}$ its 1-form $\beta : = \left\langle -,\mathbf{B} \right\rangle$ or its 2-form $\beta^{(2)} = i_ {\mathbf{B}} \text{vol}^{3}$?</p> <p>We already know that $df$ is associated to $\nabla f$.</p> <p>We <strong>define</strong> the <code class="language-plaintext highlighter-rouge">curl</code> of $A$ by using $\mathbf{A}\leftrightarrow \alpha \in\Omega^{1}$ and then \(\text{curl } \mathbf{A} = d\alpha,\quad d\alpha = i_ {\text{curl A}}\text{Vol}^{3}.\) We <strong>define</strong> $\text{div }\mathbf{B}$ by using $\mathbf{B} \leftrightarrow \beta^{(2)}$ and \(d \beta^{(2)} := \text{div }\mathbf{B} \;\text{Vol}^{3}.\)</p> <p>Given in coordinates, we have \(\text{div }\mathbf{B} = \frac{1}{\sqrt{ \left\lvert g \right\rvert }} \frac{ \partial }{ \partial u^{i} } \left( \sqrt{ \left\lvert g \right\rvert }b^{i} \right).\)</p> <p>We define the <code class="language-plaintext highlighter-rouge">Laplacian</code> of a function f by \(\nabla^{2}f = \Delta f := \text{div }\text{grad }f= \frac{1}{\sqrt{ \left\lvert g \right\rvert }} \frac{ \partial }{ \partial u^{i} } \left( \sqrt{ \left\lvert g \right\rvert } \partial ^{i}f \right),\quad \partial ^{i} = g^{ij} \partial _ {j}.\) To continue with vector identities it is useful to associate a pseudo-3-form to each scalar $f$, namely \(f \longleftrightarrow f(x)\,\text{Vol}^{n}.\) Then we can use forms to calculate $\text{div }\mathbf{A}\times \mathbf{B}$. Recall that in $\mathbb{R}^{3}$ we have $(i_ {A}\text{Vol}^{3})\wedge \beta = \left\langle \mathbf{A,B} \right\rangle\text{Vol}^{3}$, we have \(\begin{align} \text{div }(\mathbf{A\times B }) \longleftrightarrow d(\alpha \wedge \beta) &amp;= (d\alpha)\wedge \beta-\alpha \wedge d\beta \\ &amp;= i_ {\text{curl} A}\text{Vol} \wedge \beta - \alpha \wedge i_ {\text{curl B}}\text{Vol} \\ &amp;= \left\langle \text{curl }\mathbf{A},\mathbf{B} \right\rangle \text{Vol} - \left\langle \mathbf{A},\text{curl }\mathbf{B} \right\rangle\text{Vol} \\ \longleftrightarrow &amp;\left\langle \text{curl }\mathbf{A},\mathbf{B} \right\rangle - \left\langle \mathbf{A},\text{curl }\mathbf{B} \right\rangle. \end{align}\)</p> <h3 id="dictionary">Dictionary</h3> <p>Let \(\begin{align} \text{Vol}^{3} &amp;= dx\wedge dy\wedge dz = \text{ volume form} \\ 0\text{-form } F &amp;= \text{functions f} \\ 1\text{-form } \alpha,\gamma &amp;= \text{covariant expression for vectors } \mathbf{A},\mathbf{C} \\ 2\text{-form }\beta &amp;=\text{vector }B \text{ through }i_ {B}\text{Vol}. \end{align}\)</p> <p>Recall that we use the interior product and the volume form to convert between vectors and 2-forms. Then we have the following rough, symbolic identifications.</p> \[\begin{align} \mathbf{A}\times \mathbf{C}&amp;\longleftrightarrow \alpha \wedge \gamma \text{ through }i_ {A\times C}\text{Vol} \\ \mathbf{A}\cdot \mathbf{B} &amp;\longleftrightarrow \alpha \wedge \beta^{(2)} = A\cdot B\text{Vol} \\ \mathbf{A}\cdot \mathbf{C} &amp;\longleftrightarrow i_ {\mathbf{C}}A \\ \text{grad }f &amp;\longleftrightarrow df \\ \text{curl }\mathbf{A} &amp;\longleftrightarrow d\alpha = i_ {\text{curl }A}\text{Vol} \\ \text{div }\mathbf{B}&amp;\longleftrightarrow d\beta^{(2)}=\text{div }\mathbf{B}\text{Vol} \\ \nabla^{2}f &amp;\longleftrightarrow d\,i_ {\text{grad }f}\text{Vol} = \nabla^{2}f \,\text{Vol} \end{align}\] <hr/> <h3 id="charge-and-current-in-classical-electromagnetism">Charge and Current in Classical Electromagnetism</h3> <p>Electromagnetic field is best described in the language of differential forms. However, <em>electric field and magnetic field are given by forms of different ranks</em>.</p> <p>When it comes to counting the rank of forms, there are usually two simple ways to do it,</p> <ol> <li>differential forms are what we integrate on a manifold, the dimension of the form must be the same as the dimension of the manifold on which it is integrated. For example, a form which we integrate on $\mathbb{R}^{3}$ is a 3-form.</li> <li>Forms also are maps that turns vectors into numbers. We can count the rank of a form by counting how may vectors it acts on. For example, if a form maps $V\otimes V$ to real numbers, then it must be a 2-form.</li> </ol> <p>In our case, it is easiest to adopt the first method. For example, given a test charge of charge $q$, if it moves along a curve $C$ in electric field $E$, the work done to the charge is given by \(W = q \int_{C} E,\) thus $E$ must be a 1-form. Similarly, the magnetic flux is given by the integral of magnetic field on some surface $S$ \(\Phi=\int_{S} \, B\) thus $B$ must be a 2-form. Next we will dive into details.</p> <p>We accept as a primitive notion the charge Q on a particle and we assume that there is a 3-form defined on the 3D space whose integral over a region gives us the total charge. Write \(Q = \int \sigma,\quad \sigma = \rho(x) \text{Vol},\) where $\rho$ is a charge density 0-form.</p> <p>As for the <strong>spatial current 2-form</strong>, we have \(\mathcal{j} = \sqrt{ \left\lvert g \right\rvert } \rho (v^{1}du^{2}\wedge du^{3}+\dots).\)</p> <p>The electric and magnetic fields are defined operationally. In the following we shall use the euclidean metric and Cartesian coordinates of $\mathbb{R}^{3}$ (where there is no blatant distinction between covariant and contravariant vectors) and then we shall put the results in a form independent of the metric.</p> <p>The electromagnetic force on a point mass of charge $q$ moving with velocity $v$ is given and measured by the Lorentz force law \(\mathbf{F} = q(\mathbf{E}+\mathbf{v}\times \mathbf{B}).\) You could say the fields $\mathbf{B,E}$ are <strong>defined</strong> by the Lorentz force law!</p> <p>The 2-form $\mathcal{B}$ for magnetic field is defined from the pseudo vector $\mathbf{B}$ via \(\mathcal{B} := i_ {B}\text{Vol}.\)</p> <p>We then have for the <code class="language-plaintext highlighter-rouge">Lorentz force covector</code> \(f^{1} = q(\mathcal{E}+i_ {v}\mathcal{B})\) where $\mathcal{E}$ is the electric field 1-form and $v$ the velocity of the test charge. This equation is independent of any metric or orientation.</p> <p>In <strong>any</strong> coordinates we have \(\mathcal{E} = E_ {1} dx^{1}+\dots,\quad \mathcal{B} = B_ {23} dx^{2}\wedge dx^{3}+\dots\)</p> <p>If we introduce a metric, then we may consider the associated vector field $\mathbf{E}$ and the pseudovector $\mathbf{B}$. The pseudovector B has components $B_ {}1 = B_ {23} / \sqrt{ \left\lvert g \right\rvert }$, and so on.</p> <p>Given a Riemannian metric, we can use it to raise or lower the indices, and we can define two new types of forms \(\star \mathcal{E} := i_ {E}\text{Vol} = \sqrt{ \left\lvert g \right\rvert }\;(E^{1}dx^{2}\wedge dx^{3}+E^{2}dx^{3}\wedge dx^{1}+E^{3}dx^{1}\wedge dx^{2})\) and \(\star \mathcal{B} := \left\langle -,\mathbf{B} \right\rangle =B_ {1}dx^{1}+B_ {2}dx^{2}+B_ {3}dx^{3}.\) For now, the star symbol can be simply regarded as a part of the name, later we will see that the star is a well-defined operation on forms.</p> <p><strong>Gauss’s Law.</strong> If $U$ is any compact 3D region, \(\int _ {U} \, \star \mathcal{E} = 4\pi Q(U), \quad Q(u) \text{ is the total charge in U}.\) We again conclude, when $\star \mathcal{E}$ is continuously differentiable, that \(d \star \mathcal{E} = 4\pi \sigma \longleftrightarrow \text{div} \mathbf{E} = 4\pi\rho.\)</p> <p>Faraday’s law in terms of forms reads \(d\mathcal{E} = - \frac{d}{d t} \int_ {U} \, \mathcal{B},\quad U \text{ constant.}\)</p> <p><strong>Ampere-Maxwell law.</strong> If $M$ is a compact 2-sided surface with prescribed normal, then \(\int _ {\partial M} \, \star \mathcal{B} = \int _ {M} \, 4\pi \mathbf{j} +\frac{ \partial \star \mathcal{E} }{ \partial t },\) thus \(d\star \mathcal{B} = 4\pi \mathbf{j} +\frac{ \partial \star \mathcal{E} }{ \partial t }.\) (assuming $\mathcal{B}$ continuously differentiable) with vector expression $\text{curl }B=4\pi \mathbf{J}+\partial\mathbf{E} / \partial t$.</p> <p>Note that the integral versions of Maxwell’s equations are more general than the partial differential equation versions since spatial derivatives do not appear in the equations. In particular, their continuity is of no concern !</p> <hr/> <p>However, that is what we have in 3D space. In 3D space the bases of forms are $dx,dy$ and $dz$, turning to relativistic 4D spacetime, we have an extra bases, i.e., $dt$. The expressions for electromagnetic field also needs some modifications.</p> <p>Let’s start with the Lorentz force law. In non-relativistic case, the 3-dimensional Lorentz force is given by \(\mathbf{f}=q(\mathbf{E}+\mathbf{v}\times \mathbf{B})\) where the speed of light is set to $1$. We need to <code class="language-plaintext highlighter-rouge">complete</code> this spatial force vector to Minkowski 4-vector force. Lucky for us there is a trick to do that. The 3-force can be written as \(\mathbf{f} = \frac{d\mathbf{p}}{dt} = \frac{d m_{0}\mathbf{u}}{dt}\) where $\mathbf{u}$ is the three component of the four velocity $u$, $\mathbf{p}$ is the three momentum. As discussed before, a natural completion of the three momentum is the four momentum so in order to complete $\mathbf{f}$ we should replace $\mathbf{p}$ by the four momentum $P$, and we need to substitute $t$ for the Lorentz scalar $\tau$. Thus the four-force become \(f^{\mu} = \frac{d}{d\tau}(m,\mathbf{p})=\left( \frac{dm}{dt},\gamma \frac{d\mathbf{p}}{dt} \right) = (f^{0},\gamma \mathbf{f}).\) To obtain $f^{0}$, notice that the inner product of $P$ and itself is a constant, \(P^{2} = \left\langle P,P \right\rangle =\text{const} = m_ {0}^{2}\) and as a consequence, the four momentum $P$ is perpendicular to $dP / d\tau$. Since $P=m_{0} u$ and $m_{0}$ is a Lorentz scalar, $\mathbf{P}$ is parallel to $\mathbf{u}$, we have \(\left\langle f,u \right\rangle =0= -\gamma f^{0} +\gamma^{2} \mathbf{f}\cdot \mathbf{v},\) from where we can work out $f^{0}$, \(f^{0} = \gamma \mathbf{f} \cdot \mathbf{v}.\)</p> <p>Putting everything together, we have complete the three-force to the Lorentz contravariant four-force, \(\boxed{ f = \gamma (\mathbf{f}\cdot \mathbf{v}, \mathbf{f}). }\) Substitute the expression for $\mathbf{f}$ as the Lorentz force, in terms of classical vector form of electro-magnetic field, we have \(f = \gamma q (\mathbf{E}\cdot \mathbf{v}, \mathbf{E}+\mathbf{v}\times \mathbf{B}).\) On the other hand, the force itself is a 1-form since we can integrate it along some path $C$ to evaluate the work done, \(W = \int_{C} \, f.\)</p> <hr/> <p>Coming back to the case of Lorentz force, to simplify the notations we write $E = E_{i}dx^{i}$ and $B= B_{i&lt;j}\,dx^{i}\wedge dx^{j}$. The 1-form expression of the Lorentz force reads \(f = -\gamma q\, i_{\mathbf{v}}E dt + \gamma q \, (E_{i}dx^{i}-i_{\mathbf{v}}B)\) where $i_{\mathbf{v}}$ is the inner product with vector $\mathbf{v}$.</p> <p>The Lorentz 1-form $f$ can be written as \(f = -q i_{\mathbf{v}} F\) where \(\boxed{ F=E \wedge dt+B }\) is the electromagnetic field strength 2-form. It was first introduced by Minkowski in 1907.</p> <h3 id="maxwells-equations">Maxwell’s Equations</h3> <p>In Minkowski space we define a 4-dimensional exterior differentiation $d$ in terms of the 3-dimensional exterior differentiation $\mathbf{d} = \sum_ {i}\partial_ {i}d^{i},i=x,y,z$ as \(d = \mathbf{d}+dt\wedge \partial _{t}\) then half of the Maxwell equations are given by \(dF = 0\) Following the Poincare’s lemma, if the spacetime manifold is simply connected, \(F = dA,\quad A = -\phi dt+A_ {i}dx^{i}\) for some 1-form $A$. Then we can further write down $E,B$ in terms of $A$, we would recover the familiar expressions.</p> <p>To account for the other half of the Maxwell equation, we need to introduce the <code class="language-plaintext highlighter-rouge">charged flux</code>. Consider a charged fluid with local velocity $\mathbf{v}$, the current vector then is $\mathbf{j}=\rho \mathbf{v}$, $\rho$ is the density <strong>measured in the inertial system $x$</strong>. Let $\rho_{0}$ be the <code class="language-plaintext highlighter-rouge">rest charge density</code>, that is, the <strong>density measured by someone moving instantaneously with the fluid</strong>, then \(\rho=\rho_{0} \gamma,\) this is due to the contraction of the space. Thus \(\mathbf{j} = \rho_{0}\gamma \mathbf{v}.\)</p> <p>Since $\rho_{0}$ is independent of the observer by definition, we can construct the so-called <code class="language-plaintext highlighter-rouge">current 4-vector</code> \(J:= \rho_{0} u = (\rho,\rho \mathbf{v}) := (\rho,\mathbf{J}).\) We may then construct the associated <code class="language-plaintext highlighter-rouge">current 3-form</code> \(\begin{align} \mathcal{J}&amp;=i_{\mathbf{J}}\text{Vol}^{4} = \sigma^{(3)} - j^{(2)}\wedge dt \\ \sigma^{(3)}&amp;= \rho dx\wedge dy\wedge dz, \\ j^{(2)}&amp;= j_ {1}dy\wedge dz+j_ {2}dz\wedge dx+j_ {3}dx\wedge dy. \end{align}\) where $\text{Vol}^{4}$ is the 4-volume form, $\text{Vol}=dt\wedge dx \wedge dy \wedge dz$. Then the other half of the Maxwell’s equations can be written as \(d\star F = 4\pi \mathcal{J}.\)</p> <h3 id="hodge-operator">Hodge Operator</h3> <p>On a (pseudo-)Riemannian manifold $M$ we will first introduce a <code class="language-plaintext highlighter-rouge">pointwise scalar product</code> between $p$-forms, denoted by pointy brackets, then use it to define a <code class="language-plaintext highlighter-rouge">global</code> scalar product, denoted by parenthesis.</p> <p>The local scalar product of two $p$-forms is defined to be \(\left\langle \alpha,\beta \right\rangle := \alpha_ {I_ {&lt;}} \beta^{I}\) where again $I={i_ {1},\dots,i_ {p}}$ is the generalized index and $I_ {&lt;}$ denotes that in the implied sum we have $i_ {1}&lt;i_ {2}&lt;\dots&lt;i_ {p}$. We can denote the orthonormal bases of 1-forms by \(\sigma^{1}, \dots,\sigma^{p}.\)</p> <p>The <code class="language-plaintext highlighter-rouge">global</code> or <code class="language-plaintext highlighter-rouge">Hilbert space scalar product</code> is defined by \(\left( \alpha,\beta \right) := \int _ {M} \, \left\langle \alpha,\beta \right\rangle \text{Vol} ^{n}.\) whenever this makes sense. This will be the case when $M$ is compact, or, more generally, when the integrand has compact support.</p> <p>Note that the space of smooth $p$-forms on a Riemannian $M$ that satisfy $\left( \alpha,\alpha \right)&lt;\infty$ form only a <code class="language-plaintext highlighter-rouge">pre-Hilbert space</code> since it is not <strong>complete</strong>; a limit of square integrable smooth forms need not even be continuous. To get a Hilbert space we must “complete” this space. We shall not be concerned here with such matters, and we shall continue to use the inaccurate description “Hilbert space.” We shall even go a step further and use this denomination even in the pseudo-Riemannian case, where $\left( -,- \right)$ is not even positive definite.</p> <p>If $\alpha \in\Omega^{1}(M)$, we may look at its contravariant version $A$ and define a $(n-1)$-form $i_ {A}\text{Vol}^{n}$. We can generalize this procedure, associate to each $p$-form a $(n-p)$-form $\star \alpha$, the <code class="language-plaintext highlighter-rouge">Hodge-dual</code> of $\alpha$, as follows. If \(\alpha = \alpha_ {I_ {&lt;}} dx^{I}\) then \(\boxed { \star \alpha = (\star\alpha) _ {J_ {&lt;}} dx^{J}, \quad (\star\alpha)_ {J} = \sqrt{ \left\lvert g \right\rvert } \alpha^{K} \epsilon_ {K_ {&lt;}J_ {&lt;}.} }\) and where the upper indices $K$ in $\alpha^{K}$ indicate that all of the covariant indices in $\alpha$ have been raised by the metric tensor. Note again that here $\epsilon_ {KJ}$ is <strong>not</strong> the Levi-Civita tensor but the Levi-Civita symbol, it simplifies the calculation but the price to pay is that $\alpha^{K}\epsilon_ {K_ {&lt;}J_ {&lt;}}$ can no longer be carelessly rewritten as $\alpha_ {K}\epsilon^{K_ {&lt;}}_ {\;\;\;\; J_ {&lt;}}$.</p> <p>For an important special case, the 0-form that is the constant function $f=1$ has \(\star 1 = \sqrt{ \left\lvert g \right\rvert } \epsilon_ {12..d} dx^{1}\wedge dx^{2}\dots \wedge dx^{d} = \text{Vol}^{d}.\)</p> <p>We have \(\alpha \wedge \star \beta=(\alpha \wedge \star \beta)_ {12\dots n} dx^{1}\wedge \dots \wedge dx^{n},\) and then \((\alpha \wedge \star \beta)_ {12\dots n} = \alpha_ {A_ {&lt;}}\beta_ {A} \sqrt{ \left\lvert g \right\rvert }dx^{1}\wedge \dots \wedge dx^{n}= \left\langle \alpha ,\beta \right\rangle\text{Vol}^{n} .\)</p> <p>We have claimed that $\star$ generalized the interior product $\alpha\to i_ {A}\text{Vol}^{n}$. To see this, notice that \(i_ {A} \text{Vol}^{n} = i_ {A}\sqrt{ \left\lvert g \right\rvert }\epsilon_ {I_ {&lt;}}dx^{I} = \sqrt{ \left\lvert g \right\rvert } A^{i}\epsilon_ {iJ_ {&lt;}}dx^{J} = \star \alpha,\quad \alpha_ {i}\equiv A^{i}.\)</p> <p>Let $\mathbf{e} = (\mathbf{e}_ {1},\mathbf{e}_ {2},\dots,\mathbf{e}_ {n})$ be an orthonormal frame of vectors. Then $\sigma^{i}$ corresponding to $\mathbf{e}_ {i}$ are also orthonormal and \(\sigma^{}\wedge \dots \wedge \sigma^{n} = \pm \text{Vol}^{n},\) and \(\star\sigma^{I} = \pm\sigma^{J}, \quad J\text{ is the complement of }I.\)</p> <p>For example, look at the electromagnetic field in a perhaps curved space-time manifold $M^{4}$. Using the space-time metric, we have \(\star F = \star(E\wedge dt)+\star B.\) We know $E = E_ {i} dx^{1}$ so $\star (E\wedge dt)= \star (E_ {i}dx^{i}\wedge dt)= E_ {i}\star(dx^{i}\wedge dt)$ but what is, for example, $\star (dx^{1}\wedge dt)$? We usually don’t need to resort to the original definition which can be pretty cumbersome in calculations. Instead we notice that the <strong>Hodge dual is closed related to the inner product and volume form</strong>. In Minkowski space we have $\left\lvert g \right\rvert=1$ thus we can neglect it. Say, we want to calculate $\star(dx^{2}\wedge dx^{3})$, it has the property that \((dx^{2}\wedge dx^{3})\wedge \star (dx^{2}\wedge dx^{3}) = \left\langle dx^{2}\wedge dx^{3},dx^{2}\wedge dx^{3} \right\rangle\; \text{Vol}^{4},\) which can be calculated using a relation which says that, given 1-forms $\alpha_ {i},\beta_ {j}$ we have \(\left\langle \alpha_ {i_ {1}}\wedge \dots \wedge \alpha_ {i_ {d}}, \beta_ {j_ {1}} \wedge \dots \wedge \beta_ {j_ {d}}\right\rangle = \det \left\langle \alpha_ {i_ {m}},\beta_ {j_ {n}} \right\rangle\) where the right hand side is a matrix with entry $(m,n)$ given by the inner product. Equipped with above relation and the fact that $dx$ are orthonormal, with convention $g=\text{diag} (1,-1,-1,-1)$ we have \(\begin{align} (dx^{2}\wedge dx^{3})\wedge \star (dx^{2}\wedge dx^{3}) &amp;= \left\langle dx^{2}\wedge dx^{3},dx^{2}\wedge dx^{3} \right\rangle \text{Vol}^{4} = \left\langle dx^{2},dx^{2} \right\rangle \times \left\langle dx^{3},dx^{3} \right\rangle \text{Vol}^{4} \\ &amp;=\text{Vol}^{4} = dt\wedge dx^{1}\wedge dx^{2}\wedge dx^{3} \end{align}\) thus we can read-off that \(\star(dx^{2}\wedge dx^{3}) = dt\wedge dx^{1}.\) likewise we have \(\star(dt\wedge dx^{1}) = -dx^{2}\wedge dx^{3}.\) To summarize, we have \(\boxed { \begin{align} \star(dx^{i}\wedge dx^{j} ) &amp;= -\epsilon^{ijk} \, dx^{k}\wedge dt, \\ \star(dt \wedge dx^{i} ) &amp;= -\frac{1}{2} \epsilon^{ijk}\, dx^{j}\wedge dx^{k}. \end{align} }\)</p> <p>It can be shown that, given $\alpha \in\Omega^{p}(M^{n})$, \(\star^{2} = \text{det}(g) (-1)^{p(n-p)}\)</p> <p>It is sufficient to verify these for terms of the form $\sigma^{I}$ and to assume these are orthonormal. Remember that $\star\sigma^{I_ {&lt;}} = \pm \sigma^{J_ {&lt;}}$ where $J$ is the compliment of $I$, and the sign dependent on the nature of the metric.</p> <h4 id="the-codifferential-operator-ddaggerdstardelta">The Codifferential Operator $d^{\dagger}=d^{\star}=\delta$</h4> <p>The codifferential operator $d^{\dagger}$ is the dual of exterior differential $d$ in the sense that, in the global inner product, \((d \alpha^{p-1},\beta^{p} ) \equiv (\alpha^{p-1},d^{\dagger}\beta^{p-1}).\) where the superscript denotes the dimension of the form. Thus $d^{\dagger}$ must send a $p$-form to a $(p-1)$-form.</p> <p>Not recall that \((d \alpha^{p-1},\beta^{p} ) = \int_ {M} \, (d\alpha^{p-1})\wedge \star \beta^{p}=(-1)^{p-1}\int_ {M} \, \alpha^{p-1}\wedge d\star \beta^{p}\) and \((\alpha^{p-1},d^{\dagger}\beta^{p-1}) = \int_ {M} \, \alpha^{p-1}\wedge \star d^{\dagger}\beta^{p}\) then it can be shown that given $\beta \in\Omega^{p}(M)$ if we define \(\boxed { d^{\dagger}\beta := \text{det}(g)\, (-1)^{n(p+1)+1}\star d\, \star \beta = (-1)^{p}\star^{-1}d \star \beta }\) then we would have, given $\alpha \in\Omega^{p-1}(M)$, \((d\alpha,\beta) - (\alpha,d^{\dagger}\beta) = \int_ {M} \, d(\alpha \wedge \star \beta).\) at least when $\alpha,\beta$ has <strong>compact support</strong>. If $M^{n}$ is closed then $d^{\dagger}$ is indeed the dual of $d$ in the pre-Hilbert space. If $M$ has a boundary, then the statement still holds if either of $\alpha$ and $\beta$ is zero on the boundary.</p> <p>The operator $d^{^{\dagger}}$ is called the <code class="language-plaintext highlighter-rouge">codifferential</code>. The traditional notation is $\delta$ but we will not use it, since we want to keep $\delta$ for variation. Instead we will use $d^{\dagger}$.</p> <p>A consequence of the definition for exterior derivative \(d\alpha = (\partial _ {\mu}\alpha_ {I_ {&lt;}})dx^{\mu}\wedge dx^{I}\) is that, <em>in a spacetime with a symmetric connection</em> $\Gamma^{\alpha}_ {\;\mu \nu}=\Gamma^{\alpha}_ {\;\nu \mu}$, the partial derivative $\partial_ {\mu}$ can always be replaced by the <strong>covariant derivative</strong> $\nabla_ {\mu}$, as the readers can verify, the covariant derivatives introduce new terms concerning the connections, and these terms identically vanish due to the anti-symmetric nature of differential forms. Some calculation shows that a coordinate expression for the $(p-1)$-form $d^{\dagger}\beta$ is \((d^{\dagger}\beta)_ {K} = - \beta^{i}_ {\;K;j}.\)</p> <h4 id="maxwells-equations-in-curved-space-time">Maxwell’s Equations in Curved Space-Time</h4> <p>We shall assume that the electromagnetic field is again described by an electromagnetic 2-form $F\in\Omega^{2}(M)$. In any local coordinates $(t,\mathbf{x})=(x^{0},x^{1},\dots,x^{4})$ we may decompose $F$ into part contain and doesn’t contain $dt$. The relation between $F$ and the electric 1-form $E\in\Omega^{1}(M)$ and magnetic 2-form $B\in\Omega^{2}(M)$ is given by \(F = E\wedge dt + B.\)</p> <p>Half of the Maxwell equation is given by $dF=0$. Define the charge density 3-form $\rho$ and current 2-form $j$, we have the 4D current 3-form \(J = \rho - j \wedge dt\) which is related to the 4-vector form of the current $J^{\mu}$ by \(J = i_ {J^{\mu}}\text{Vol}^{4}.\) where $\mu$ is just part of the name, not to be contracted with anything. Note that <strong>The current 4-vector depends on the metric!</strong> <strong>It is for this reason that the current 3-form is more basic than the 4-vector current</strong>.</p> <p>The other half of the Maxwell equation can be written as \(d\star F= 4\pi J^{(3)},\quad J^{(3)} \text{ is the current 3-form.}\)</p>]]></content><author><name>Baiyang Zhang</name></author><category term="Geometry"/><category term="Minkowski"/><category term="Frankel"/><summary type="html"><![CDATA[Disclaimer: Nothing in this note is original.]]></summary></entry><entry><title type="html">The Dirac Equation</title><link href="https://baiyangzhang.github.io/blog/2023/The-Dirac-Equation/" rel="alternate" type="text/html" title="The Dirac Equation"/><published>2023-09-21T00:00:00+00:00</published><updated>2023-09-21T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/The-Dirac-Equation</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/The-Dirac-Equation/"><![CDATA[<p><em>Disclaimer: Nothing in this note is original.</em></p> <p>Consider the three Lie algebra generators of $SU(2)$ as three basis, then ${\frak su}(2)$ is a rank three vector space with real coefficients, thus an element of $SU(2)$ can be regarded as a rotation in ${\frak su}(2)$, namely a $SO(3)$ matrix. The adjoint representation of $g\in SU(2)$ acts on ${\frak su}(2)$, \(\text{Ad}: SU(2)\to SO(3)\) which is double valued. Inversely, each $SO(3)$ has a double valued representation by $2\times 2$ complex matrices acting on $\mathbb{C}^{2}$. The complex vector $(\psi_ {1},\psi_ {2})^{T}$ on which this $2\times 2$ matrix acts on are called <code class="language-plaintext highlighter-rouge">spinors</code>.</p> <p>Mathematicians don’t like double-valued anythings since you can’t treat them as regular functions. So, we can say that $SU(2)$ matrices naturally furnish a <em>spinor representation</em> of the <code class="language-plaintext highlighter-rouge">2-fold cover</code> of $SO(3)$. When $SU(2)$ is thought of as the 2-fold cover of $SO(3)$, it’s called the <code class="language-plaintext highlighter-rouge">spinor group </code>$\text{Spin}(3)$.</p> <p>The topological reason that $SO(3)$ can admit a nontrivial double-cover is that $SO(3)$ is not simply connected, it is instead a projective manifold, \(SO(2) \cong \mathbb{R}P^{3}.\)</p> <p>An mysterious consequence of the projective nature of $SO(3)$ is that, <strong>one full rotation is different from two full rotations</strong>! A full rotation is by $2\pi$. The 1-parameter subgroup of $SO(3)$ defined by rotation along a certain axis (say $\hat{z}$) is given matrix \(\theta \mapsto \begin{pmatrix} \cos \theta &amp; -\sin \theta &amp; 0 \\ \sin \theta &amp; \cos \theta &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}\) and for $\theta \in (0,2\pi)$ the trajectory is a closed curve in $\mathbb{R} P^{3}$, which can not be shrunk to a point. (This statement is a little abstract) In the mean while, for $\theta \in(0,4\pi)$ the trajectory <strong>can</strong> be shrunk to a point.</p> <p>There are three ways to understand it more intuitively. The first example is the waiter with a platter picture. The second is Wyle’s double cone interpretation, which shows that a rotation by $4\pi$ is indeed continuously connected to doing nothing. A third interpretation will be shown later, until we have introduced the Dirac equation.</p> <h3 id="hamilton-on-composing-two-rotations">Hamilton on composing two rotations</h3> <p>The $SU(2)$ double cover is a power tool for investigating the product of two rotations, thanks to the nice algebraic properties of Pauli matrices. For example, given two rotations \(R_ {1} = \exp \left\{ -i T^{a} \theta^{a} \right\} ,\quad R_ {2} = \exp \left\{ -i T^{a}\theta'^{a} \right\}\) where \(T^{a} := \frac{\sigma^{a}}{2}\) is the generator for rotation, then we can write down the explicit expression of $R_ {1}R_ {2}$ in terms of $\theta$ and $\theta’$.</p> <p>Note that $\sigma^{1,2,3}$ span a real vector space $V^{3}$. It is the space of traceless hermitian matrices. In this space there is a quadratic form given by \(\left\langle h,h' \right\rangle := \frac{1}{2} \mathrm{Tr}\,(h h'), \quad h,h'\in V^{3}.\) Then the basis satisfy a very nice relation \(\left\langle \sigma_ {i},\sigma _ {j} \right\rangle = \delta_ {ij}.\)</p> <p>Next we will expand $V^{3}$ by imposing on it further algebraic structure. We can introduce <strong>multiplication</strong> of the basis $\sigma_ {i}$. This can be easily done by adopting the multiplication of matrices. However, $\sigma_ {i}\sigma_ {2}=i\sigma_ {3}$ is not in $V^{3}$. The easy remedy is to expand $V^{3}$ simply to include it, say define $e_ {4}:=i\sigma_ {3}$. Let’s define \(e_ {1}:=\sigma_ {1},\; e_ {2}:= \sigma_ {2}, \; e_ {3}:=\sigma_ {3},\; e_ {4}:=i\sigma_ {3}=\sigma_ {1}\sigma_ {2}.\) But that wouldn’t be enough, to make the algebra closed under multiplication we also need to include \(e_ {5}:= e_ {2}e_ {3}, \; e_ {6}:= e_ {1}e_ {3},\; e_ {7}:=e_ {1}e_ {2}e_ {3}=i I\) where $I=\mathbb{1}_ {2}$ is the two dimensional identity matrix, and \(e_ {8}:= I.\) The algebra (over $\mathbb{R}$) given by all the eight basis is called the <code class="language-plaintext highlighter-rouge">Pauli algebra</code>, it is said to be <strong>generated by the Pauli matrices.</strong> It is an <strong>associative algebra</strong>, that is, a vector space with a composition (called <code class="language-plaintext highlighter-rouge">product</code>) that is associative and is distribution with respect to addition. In fact, in this case, this $8$-dimensional vector space is nothing but the space of <em>all</em> $2\times 2$ complex matrices!</p> <p>The way we constructed the Pauli algebra can be generalized to Clifford algebra.</p> <p><strong>Clifford algebra.</strong> Let $C_ {n}$ be an associative algebra (over $\mathbb{R}$) with identity $I$, generated by an $n$-dimensional vector subspace $V^{n}$. Define a real quadratic form $\left\langle -,- \right\rangle$ over $V^{n}$. Let $e_ {1},\dots,e_ {n}$ be the basis of $V^{n}$, satisfying certain anti-commutation relation \(\left\{ e_ {i} ,e_ {j} \right\} =2 g_ {ij} \,I\) where $g_ {ij}$ is given by the quadratic form \(g_ {ij} := \left\langle e_ {i},e_ {j} \right\rangle .\) Then $C_ {n}$ is called the <code class="language-plaintext highlighter-rouge">Clifford algebra</code> generated by $V^{n}$.</p> <p>Note that the quadratic form could be anything, and it is part of the definition of Clifford algebra. Pauli algebra is a Clifford algebra. If we set $g_ {ij}$ to be identically zero, then we recover the exterior algebra.</p> <p>An important example of Clifford algebra are quaternions. Let $C_ {2}$ be the Clifford algebra generated by two basis, $e_ {1}$ and $e_ {2}$. Let the quadratic form be \(\left\langle e_ {i} ,e_ {j} \right\rangle =-\delta_ {ij},\) then we have \(\left\langle e_ {1},e_ {1} \right\rangle =\left\langle e_ {2},e_ {2} \right\rangle =-1.\) The product is also given by the anti-commutation relations, for example \(\left\{ e_ {1},e_ {1} \right\} = 2 e_ {1}e_ {1}=2g_ {11}I = -2\implies e_ {1}e_ {1}=-1.\) The same goes for $e_ {2}$. The product $e_ {1}e_ {2}$ defines a new basis of the algebra, \(e_ {3}:=e_ {1}e_ {2}, \quad e_ {3}e_ {3}=e_ {1}e_ {2}e_ {1}e_ {2}=-1.\) Rewrite the basis as $j=e_ {1},k=e_ {2}$ and $i=e_ {3}$, the resulting algebra is the quaternion, a quaternion in general can be expanded (over $\mathbb{R}$) in terms of these basis, \(a+bi+cj+dk,\quad a,b,c,d \in \mathbb{R}.\)</p> <hr/> <h3 id="the-dirac-algebra">The Dirac algebra</h3> <h4 id="the-lorentz-group-and-its-geometry">The Lorentz group and its geometry</h4> <p>Given the Minkowski metric $g=\text{diag}(-1,1,1,1)$, the <code class="language-plaintext highlighter-rouge">Lorentz group</code> $L$ is by definition the isometry group, \(L := \left\{ 4\times 4 \text{ real matrices }B \,\middle\vert\, \left\langle Bx,By \right\rangle=\left\langle x,y \right\rangle \right\}\) acting on the 4-dimensional Minkowski space $M$. In metric notation, \(\left\langle x,y \right\rangle := g_ {\mu \nu} \, x^{\mu}y^{\nu}.\) It can be shown that $L$ breaks down to four connected components, depending on the sign of the determinant and weather $Bx$ changes the sign of the time component of $x$. The so-called proper Lorentz group, whose elements has determinant $1$ and doesn’t change the sign in the time-component, is denoted as $L_ {0}$, this corresponds to the physical Lorentz transforms.</p> <p>Similar to $SO(3)$ is double-covered by $SU(2)$, $L_ {0}$ turns out to be double covered by $SL(2,\mathbb{C})$, the special linear $2\times 2$ complex matrix.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="Geometry"/><category term="Minkowski"/><category term="Frankel"/><summary type="html"><![CDATA[Disclaimer: Nothing in this note is original.]]></summary></entry><entry><title type="html">Introduction to Higher Form Symmetry</title><link href="https://baiyangzhang.github.io/blog/2023/Introduction-to-Higher-Form-Symmetry-Lecture-2/" rel="alternate" type="text/html" title="Introduction to Higher Form Symmetry"/><published>2023-09-18T00:00:00+00:00</published><updated>2023-09-18T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Introduction-to-Higher-Form-Symmetry%20Lecture%202</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Introduction-to-Higher-Form-Symmetry-Lecture-2/"><![CDATA[<h2 id="ordinary-symmetries">Ordinary symmetries</h2> <p>Before going to the generalized symmetries, let’s first review some aspects of ordinary symmetries in quantum field theory (QFT).</p> <h3 id="noether-current">Noether current</h3> <p>Noether theorem says that each continuous, physical symmetry (physical symmetry, as opposed to nonphysical symmetry, such as gauge symmetry) gives rise to a conserved current. The derivation of Noether current is most familiar to every physicist, so I won’t waste time to derive it here. The traditional, or standard way to obtain it is to follow the below given steps.</p> <ol> <li>The action is invariant under an <em>infinitesimal, continuous, global</em> transformation. However it doesn’t mean the the Lagrangian has to be strictly invariant under the same transformation, it can still vary by a total derivative, since its contribution is a surface terms, and for most theoretical setups, everything is set to be zero at the boundary of space time, so the surface terms contributes nothing.</li> <li>Substitute the varied Lagrangian into the action, with the help of various mathematical tools, such as integral by part, etc., we can extract the variation of the actions that is proportional to the variation of the fields, which must be identically zero, since the field variation is arbitrary. Along the process the <em>equation of motion</em> must be applied.</li> <li>Eventually we arrived at the conserved current $\partial_ {\mu}J^{\mu}=0$.</li> </ol> <p>An particularly clever way to derive the Noether current is to <em>gauge</em> the global symmetry. By gauging something it is meant that making some global symmetry local, for example, the wavefunction has a global symmetry $\psi\to e^{ i\theta }\psi$, where $\theta$ is a constant, hence this transformation is global; we can gauge this transformation by making $\theta$ position-dependent, namely $\psi\to e^{ i\theta(x) }\psi$. Gauging a symmetry is the same thing as gauging a transformation, we will use them interchangeably. The gauging method to derive the Noether current works as follows.</p> <ol> <li>Under the infinitesimal global symmetry transformation $\phi\to \phi+\epsilon \phi$, the action is still invariant (otherwise it won’t be a symmetry would it), $S\to S$. Now we gauge $\epsilon$, making it spacetime-dependent, $\epsilon\to\epsilon(x)$.</li> <li>Under a gauged symmetry transformation, the action will receive extra contribution coming from the derivative of $\epsilon$, at the leading order this contribution should be proportional to (linear in) $\partial_ {\mu}\epsilon(x)$, that is $\delta S =\int \, J^{\mu}\partial_ {\mu}\epsilon(x)$. Here $J^{\mu}$ is just the proportional coefficient, and the upper index $\mu$ is needed to contract with the lower index $\mu$ in $\partial_ {\mu}$.</li> <li>Now let us take the field configuration to be a physical one, that is, the configuration satisfies the equation of motion. Such configurations has an crucial advantage: it is a saddle point, any field variation about it will leave the action invariant, including the before defined gauged symmetry transformation. Thus we have something like $\delta S = \int \, J^{\mu}\partial_ {\mu}\epsilon(x)=0$. Apply the integral by part and neglect the surface term, we arrive at $\partial_ {\mu}J^{\mu}=0$, we have found the conserved current $J^{\mu}$!</li> </ol> <hr/> <p>As an excursion, I just like to mention some of my own philosophical understandings about the connection between a symmetry transformation and conserved current, feel free to skip this paragraph. They are like the two sides of the same coin, but what is this coin? A conserved quantity <em>shows certain blindness to the physical system</em>, <em>the incompetent of identifying the situation</em>. For example, conservation of energy means that the total energy is the same at any minute, so we can not use the total energy to identify time, as a clock. Interesting enough, the corresponding symmetry transformation, namely the time translation, has the same effect, it says that we can perform our experiment at any time, as long as it is the same experiment then we will get the same results, we are blind to the time at which the experiment is performed. In mathematics, we meet various obstructions to do things, for example it is the case in homology, and things become more interesting when such obstructions are present, maybe here it is similar, the conserved current, or symmetry transformation, can be regarded as a obstruction of identifying certain property of the physical system, for instance, the pair (time translation symmetry, energy conservation) is an obstruction for us to use it to tell the time of a system, simply because it is not time-dependent; the pair (space translation symmetry, momentum conservation) both suggests the blindness to the absolute position. But this picture also has lots of shortcomings, for example we not only have a conserved quantity but a conserved current, the latter is a stronger condition, it says that the way the conserved quantity moves is actually continuous, it can not just disappear somewhere and at the same second re-appear somewhere else, in other words the causality is preserved, I don’t know how to fit the importance of conserved currents into the “obstruction” language. Anyway, it is interesting to think of symmetry this way.</p> <hr/> <p>Having found the conserved current, we instantly have the conserved charge, \(Q = \int d^{D-1}x \, J^{0}\) where the spacetime is supposed to be $D$-dimensional. The conserved charge in conserved in one spacetime dimension, namely time, thus it must be integrated over <em>all the other dimensions</em>, namely the full space.</p> <p>Note that contrary to naïve speculations, the conserved current is in fact <em>not</em> uniquely defined, for we can add to $J^{\mu}$ any four-divergence of some anti-symmetric tensor $\partial_ {\nu}\Omega^{\mu \nu}$, then obviously \(\partial_ {\mu}(J^{\mu} + \partial_ {\nu}\Omega^{\mu \nu}) = 0 + \partial_ {\mu}\partial_ {\nu}\Omega^{\mu \nu} = 0\) since $\Omega^{\mu \nu}= -\Omega^{\nu \mu}$. In physics, we sometimes make use of this property to render the energy-momentum tensor $T^{\mu \nu}$ to be symmetric in $\mu$ and $\nu$. Define $J’ = J + \partial_ {\nu}\Omega^{\mu \nu}$, $J’$ and $J$ leads to the same conserved quantity (with sensible boundary condition).</p> <p>The quantization, to be specific the canonical quantization upgrade the Noether charge to a operator and bestows to it a profound meaning. The conserved charge $Q$, now being an operator, becomes the generator of the symmetry operator! For example, the momentum in the $x$-direction, $p_ {x}$, was classically a function, a so-called classical number or c-number. After quantization $\hat{p}_ {x}$ becomes an operator with certain commutation relation with $\hat{x}$, which is the position operator. Furthermore the translation in the $x$-direction is generated by $\hat{p}_ {x}$, to be specific the translation by distance $l$ is realized by the operator $\exp(-il\hat{p}_ {x})$. I could be wrong by a minus sign in the exponent, never could remember it.</p> <h3 id="ward-identity">Ward identity</h3> <p>The idea is as follows.</p> <ol> <li> <p>Under the symmetry transformation, the fields transform as \(\phi\to \phi' = R(g) \phi := 1+\delta \phi = 1+ \varepsilon(x) \Delta \phi(x),\) where we used $\delta \phi$ to denote infinitesimal variation of field while $\Delta \phi$ the finite change. $g \in G$ is an element of the symmetry group $G$, which could be either the gauge transformation or some physical transformation. We have made $\varepsilon(x)$ a function of $D$-dimensional space(-time).</p> </li> <li> <p>Regard the transformed field $\phi’$ as the dummy variable in the path integral, then use it to express the partition function <em>with a source $K$</em>. Usually the source is denoted by $J$ but we have already used it to denote the conserved current, so to avoid confusion we use $K(x)$. In Euclidean measure \(Z[K] = \int D\phi' \, e^{ -S_ {E}[\phi']+\int K\phi'(x) \, }.\) Then substitute the expression of the transformed field, namely $\phi’=\phi+\epsilon(x)\Delta \phi$, we get a new expression of $Z[K]$ in terms of $\phi,\Delta \phi$ and $\epsilon(x)$. Assume the measure is invariant under $\phi\to\phi’$, namely assume the <em>symmetry is not anomalous</em>, the new partition function $Z[K]$ is equivalent to the old one. Then we would get \(\left\langle e^{ \int d^{D}x' \, K(x')\phi(x') }(\partial_ {\mu}J^{\mu}-K(x)\Delta\phi(x)) \right\rangle=0\) where as usual the vev is given by \(\left\langle \bullet \right\rangle := \frac{1}{Z} \int D\phi \, e^{ -S_ {E} }(\bullet).\) Then we can take the functional derivative w.r.t. $K(x)$ then set it to zero, what we get are collectively called the Ward identities. For example, take no derivative and set $K=0$ we get \(\left\langle \partial_ {\mu}J^{\mu} \right\rangle =0.\) Take one derivative $\delta / \delta K(y)$ and set $K=0$ we get \(\left\langle \partial_ {\mu}J^{\mu}(x) \phi(y)\right\rangle-\delta(x-y)\left\langle \Delta\phi(y) \right\rangle =0.\) The second term, the so-called contact term, is a quantum correction to the classical result.</p> </li> </ol> <h3 id="symmetries-in-differential-form">Symmetries in differential form</h3> <p>In this part we will use the Language of differential forms a lot, for a beginner-friendly introduction please refer to my other <a href="http://www.mathlimbo.net/2023/04/10/Electrodynamics-in-Terms-of-Forms/">blog</a>.</p> <p>Next let’s rewrite what we already know about ordinary symmetry in the language of differential form. Given a conserved current $J_ {\mu}$, we have $\partial_ {\mu}J^{\mu}=0$. We can regard it as a 1-form $J\in \Omega^{1}$, \(J := J_ {\mu} dx^{\mu}, \quad J_ {\mu} = g_ {\mu \nu}J^{\nu}\) then the conservation law is \(d\star J =0.\)</p> <p>Let the space-time be a $D$ dimensional manifold with Euclidean metric, with $D-1$ dimensional submanifold denoted as $\Sigma$. $\Sigma$ has co-dimension one. The Noether charge on $\Sigma$ can be written as \(Q(\Sigma)= \int _ {\Sigma} \, \star J\) as one can easily verify that this indeed gives the familiar integral of charge when applied on Minkowski spacetime. Now we can generalize Minkowski spacetime to any manifold with arbitrary metric, but for the sake of simplicity we first confine ourselves to Euclidean spacetime, Minkowski results can be obtained from Euclidean spacetime via Wick rotation and path integral.</p> <p>The quantum consequence of symmetry is the Ward identity, so let’s see what it gives us when we combine (up to a minus sign on the tight hand side) \(\left\langle \partial_ {\mu}J^{\mu}(x) \phi(y)\right\rangle = \delta(x-y)\left\langle \Delta\phi(y) \right\rangle. \tag{0}\) with \(Q(\Sigma)= \int _ {\Sigma} \, \star J.\) Let $\Omega$ be a $D$-dimensional manifold whose boundary is $\Sigma$, $\partial \Omega=\Sigma$.</p> <p>In term of differential form, \(\left\langle \partial_ {\mu}J^{\mu}(x)\phi(y) \right\rangle = \left\langle d\star J\phi(y) \right\rangle = \left\langle d[(\star J) \phi(y)] \right\rangle = d\left\langle (\star J)\phi(y) \right\rangle \tag{1}\) where $d$ only acts on coordinate $x$, $\phi(y)$ is to be regarded as a constant to $d_ {x}$. That is why we can write $(d \star J)\phi(y)$ as $d[(\star J)\phi(y)]$.</p> <p>Integrate Eq. (1) over $\Sigma$, we have \(\int_ {\Omega} \, d\left\langle \star J \phi(y) \right\rangle = \int_ {\partial \Omega=\Sigma} \, \left\langle \star J \, \phi(y) \right\rangle = \left\langle Q(\Sigma)\,\phi(y) \right\rangle ,\) where we have used the Stokes theorem. But we also have the contact term in the Ward identity, namely the RHS of Eq.(0), integrate it over $\Omega$ we get \(\int_ {\Omega}d^{D}x \, \delta(x-y)\left\langle \phi(y) \right\rangle = \begin{cases} \left\langle \Delta\phi(x) \right\rangle &amp; x\in \Omega, \\ 0 &amp; x \notin \Omega. \end{cases}\) In summary, \(\left\langle Q(\Sigma)\,\phi(y) \right\rangle =\int_ {\Omega} d^{D}x\, \delta(x-y) \left\langle\Delta \phi(y) \right\rangle,\) Note that $\Omega$ has coordinate $x$ while $y$ can be regarded as part of the name of the operator $\phi(y)$. Since $x$ is the coordinate which varies on $\Omega$, the exterior differential $d$ acts on it only, not $y$.</p> <p>There is a topological interpretation of $\int_ {\Omega}d^{D}x \,\delta(x-y)$. If $y$ is inside $\Omega$ then the integral gives $1$, if not then the integral gives zero, we call it the <code class="language-plaintext highlighter-rouge">intersection number</code> of $\Omega$ and $y$. It can be regarded as the <code class="language-plaintext highlighter-rouge">link number</code> between the boundary of $\Omega$, i.e. $\Sigma$ and the point $y$. This link number between a point and a co-dimensional one object can be regarded as the generalization of the link number between two loops (see <em>Theodore Frankel</em>). We have \(\boxed { \text{Link}(\Sigma,y):= \int_ {\Omega} d^{D}x\, \delta(x-y). }\) The topological picture of link number is more or less intuitive, it tells us if we move a point infinitely far away from a co-dimensional one object, the least number of times that the former has to intersect the latter. Then \(\left\langle Q(\Sigma)\,\phi(y) \right\rangle =\text{Link}(\Sigma,y) \left\langle \Delta \phi(y) \right\rangle .\) Note that the above relation is <em>linear with respect to the manifold</em> $\Sigma$, since it is based on integrals and integrals are linear upon the integral domain. Suppose $\Sigma’$ is another co-dimensional one object which does not contain $y$, then \(\left\langle Q(\Sigma+\Sigma')\,\phi(y) \right\rangle =\text{Link}(\Sigma+\Sigma',y) \left\langle \Delta \phi(y) \right\rangle =\text{Link}(\Sigma,y) \left\langle \Delta \phi(y) \right\rangle +\text{Link}(\Sigma',y) \left\langle \Delta \phi(y) \right\rangle\) since $\text{Link}(\Sigma’,y)=0$, we have \(\left\langle Q(\Sigma+\Sigma')\,\phi(y) \right\rangle = \left\langle Q(\Sigma)\,\phi(y) \right\rangle\) if $\Sigma’$ has zero link number with $y$. This tells us the topological nature of this operator.</p> <p>Follow the ideas of the construction of symmetry operators in a Hilbert space, we are encouraged to consider \(\boxed { U(g,\Sigma^{(d-1)}) := e^{ i \theta(g) Q(\Sigma) }, }\) the generator $Q$ carries the information of the support of the “operator”, the parameter $\theta(g)$ carries the information of the group element, it can be simply written as $g$ if you like. This is usually by construction a <em>unitary</em> operator, since $Q(\Sigma)$ almost always corresponds to some physical observable hence are hermitian. Assume under the group action $g$ the field operator transforms as $\Delta \phi = T\phi$, where $R$ is certain representation of the generator. Assume the linking number between $\Sigma$ and $y$ is $1$, this will greatly simplify our notation. Then \(\left\langle Q(\Sigma)\,\phi(y) \right\rangle =T \left\langle \phi(y) \right\rangle .\) Since $R$ is the generator of the group action, it inspires us to treat it as an infinitesimal form of something of finite size. Since \(\left\langle \{1+id\theta \,Q(\Sigma)\}\phi(y) \right\rangle = (1+id\theta T)\left\langle \phi(y) \right\rangle\) we have \(\begin{align} \left\langle U(g,\Sigma)\phi(y) \right\rangle &amp;= \left\langle e^{ i\theta(g)Q(\Sigma)} \phi(y)\right\rangle \\ &amp;=\left\langle \lim_{ N \to \infty }\left( 1+i \frac{\theta(g)}{N} Q(\Sigma) \right)^{N} \times \phi(y) \right\rangle \\ &amp;=\lim_{ N \to \infty } \left\langle (1+id\theta Q(\Sigma))^{N}\times \phi(y) \right\rangle \\ &amp;=\lim_{ N \to \infty } \left\langle (1+i d\theta T)^{N} \phi(y) \right\rangle \\ &amp;= \left\langle \exp(i \theta(g)T) \phi(y) \right\rangle \\ &amp;=e^{i \theta(g)T} \left\langle \phi(y) \right\rangle \\ &amp;=: R(g) \left\langle \phi(y) \right\rangle. \end{align}\)</p> <p>Since the group action $R(g)$ acts on an operator $\phi(y)$ whose support is a point, we call this a 0-form symmetry. The operator $U(g,\Sigma)$ is sometimes called a <code class="language-plaintext highlighter-rouge">symmetry defect operator</code>.</p> <p>In order for $U(g,\Sigma)$ to enact a symmetry, it must represent a group structure, \(U(g,\Sigma) \,\circ\,U(g',\Sigma) = U(g\,\circ\,g',\Sigma).\) Indeed it can be shown that \(\left\langle U(g,\Sigma)U(g',\Sigma)\phi(y) \right\rangle = R(g)R(g')\phi(y)=R(g g')\phi(y).\)</p> <p>We have talked about the topological nature of charge $Q(\Sigma)$ when put in the vev. The topological nature of $U(g,\Sigma)$ can also be shown as a consequence of the conservation of the current. Let $\Sigma’$ be a homotopic deformation of $\Sigma$, meaning $\Sigma-\Sigma’$ is the boundary of some $D$-dimensional bulk which does not contain $\phi(y)$ or any charged operator. Then \(\begin{align} U(g,\Sigma)U(g^{-1} \Sigma') &amp;= e^{ i\theta(g) Q(\Sigma) }e^{ i \theta(g^{-1} )Q(\Sigma') } \\ &amp;= e^{ i\theta(g)\int_ {\Sigma} \, \star J }e^{ i(-\theta(g))\int_ {\Sigma'} \, \star J } \\ &amp;= \exp \left\{ i\theta(g)\int_ {\hat{\Sigma}} \,\star J \right\} \\ &amp;= \exp \left\{ i\theta(g)\int_ {\partial(\Sigma-\Sigma')} \,\star J \right\} \\ &amp;= \exp \left\{ i\theta(g)\int_ {\Sigma-\Sigma'} \,d\star J \right\} \\ &amp;=1. \end{align}\)</p> <p>Thus \(U(g,\Sigma)U(g^{-1} ,\Sigma')=1 \implies U(g,\Sigma)=U(g,\Sigma').\)</p> <p>This tool even generalized to discrete symmetries. Discrete symmetries, as gauge symmetry (which is <em>not</em> a real symmetry), has no Noether current, but the action of such symmetries on Hilbert space can nevertheless be represented as an operator, call it $U(g)$. This time there is no continuous parameter $\theta(g)$ involved. However, there are extra difficulties involving the topology of the spacetime. We will not dwell on this topic here.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="PureMath"/><category term="GeneralizedSymmetry"/><category term="Notes"/><summary type="html"><![CDATA[Ordinary symmetries]]></summary></entry><entry><title type="html">Connections in Associated Bundles</title><link href="https://baiyangzhang.github.io/blog/2023/Connections-in-Associated-Bundles/" rel="alternate" type="text/html" title="Connections in Associated Bundles"/><published>2023-09-02T00:00:00+00:00</published><updated>2023-09-02T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Connections-in-Associated-Bundles</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Connections-in-Associated-Bundles/"><![CDATA[<h3 id="associated-adjoint-bundle">Associated Adjoint Bundle</h3> <p>We may let $G$ act as a general linear transformation on its own Lie algebra ${\frak g}$, given a $Y\in{\frak g}$ and $g\in G$ we define \(\text{Ad}_ {g}(Y) = g Y g^{-1} = L_ {g\ast } \,\circ\,R_ {g^{-1} \ast } Y.\) Obviously if $G$ is abelian then $\text{Ad}_ {g}$ does not do anything to $Y$. This map $\text{Ad}$ takes an element of the group $G$, then uses it to “distort” a tangent vector in $T_ {e}G$, thus \(\text{Ad}: G \to GL({\frak g}), \quad g\mapsto \text{Ad}_ {g}\) where $\text{GL}({\frak g})$ is the group of general linear transformations of ${\frak g}$.</p> <p>One can check that $\text{Ad}_ {g}$ is an representation of $G$, which is essentially the <code class="language-plaintext highlighter-rouge">adjoint representation</code> of $G$. You might have already encountered different definition of adjoint representation of $G$ before, then you should be able to show that these definitions indeed agrees with each other. The subgroup $\text{Ad}_ {G}\subset GL({\frak g})$ is called the <code class="language-plaintext highlighter-rouge">adjoint group</code> of $G$.</p> <p>Furthermore, we can talk about the induced man, or differential, or “push-forward” of $\text{Ad}$. Now, $\text{Ad}_ {\ast}$ would map $TG$ to $TGL({\frak g})$. But how does it work?</p> <p>Given $g\in G$, we have the one-parameter subgroup $g(t)=\exp(tX)$ where $t$ is the parameter, and the corresponding velocity vector at the identity is \(X =\frac{d}{d t} g(t) = \frac{d}{d t} \exp(tX) {\Large\mid}_ {t=0}, \quad X \in {\frak g}.\) Write $g(\varepsilon)$ to denote the group element infinitesimally away from the identity $e$. $\text{Ad}$ maps $e$ to the identity linear transform \(\text{Ad}_ {e} Y = e Y e^{-1} =Y\) and maps $g(\varepsilon)$ to some other linear transform \(\text{Ad}_ {g(\varepsilon)} = g(\varepsilon) Y g(\varepsilon)^{-1} = : Y'\) which is not the same as $Y$. The induced map $\text{Ad}_ {\ast}$ should give the difference between $Y’$ and $Y$, which are both element of ${\frak g}$, hence so should be their difference, \(\text{Ad}_ : X \mapsto \text{some linear transform of } Y.\)Specific calculation shows that \(\text{Ad}_ {\ast }(X) Y = \frac{d}{d t} e^{ tX } Y e^{ -tX }{\Large\mid}_ {t=0}=XY-YX = [X,Y].\) We can also write $\text{Ad}_ {\ast}(X)$ as $\text{Ad}_ {X}$.</p> <p>Let us write $\text{ad}(X)$ or $\text{ad}_ {X}$ for the above linear transformation. Thus \(\text{ad}: {\frak g}\to{\frak g},\quad X\mapsto[X,-].\)</p> <p>In summary, \(\text{Ad}_ {\exp(tX)} Y =e^{ tX } Y e^{ -tX }\) is a curve in ${\frak g}$, its velocity is also in ${\frak g}$. For fixed $X$, the one parameter group \(\text{Ad}_ {\exp(tX)}\) is generated by $\text{ad}_ {X}$.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="Frankel"/><summary type="html"><![CDATA[Associated Adjoint Bundle]]></summary></entry></feed>