<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://baiyangzhang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://baiyangzhang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-11T08:04:42+00:00</updated><id>https://baiyangzhang.github.io/feed.xml</id><title type="html">Baiyang Zhang</title><subtitle>A place dedicated to sharing insights and reflections on mathematics, physics, and social sciences. </subtitle><entry><title type="html">Tsallis Statistics in Logistic Regression</title><link href="https://baiyangzhang.github.io/blog/2024/Tsallis-Statistics-in-Logistic-Regression/" rel="alternate" type="text/html" title="Tsallis Statistics in Logistic Regression"/><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Tsallis-Statistics-in-Logistic-Regression</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Tsallis-Statistics-in-Logistic-Regression/"><![CDATA[<ul> <li><a href="#1-introduction">1. Introduction</a> <ul> <li><a href="#11-the-basics-of-boltzmann-gibbs-extensive-entropy">1.1. The Basics of Boltzmann-Gibbs Extensive Entropy</a></li> <li><a href="#12-generalization-to-non-extensive-entropy">1.2. Generalization to non-Extensive entropy</a></li> </ul> </li> <li><a href="#2-boltzmann-gibbs-statistical-mechanics">2. Boltzmann-Gibbs Statistical Mechanics</a> <ul> <li><a href="#21-three-different-forms-of-bg-entropy">2.1. Three different forms of BG entropy</a></li> <li><a href="#22-properties-of-bg-entropy">2.2. Properties of BG entropy</a></li> <li><a href="#23-constraints-and-entropy-optimization">2.3. Constraints and Entropy Optimization</a></li> </ul> </li> <li><a href="#3-nonextensive-statistical-mechanics">3. Nonextensive Statistical Mechanics</a> <ul> <li><a href="#31-mean-value-in-tsallis-statistics">3.1. Mean Value in Tsallis Statistics</a></li> </ul> </li> <li><a href="#4-tsallis-in-logistic-regression-methods">4. Tsallis in Logistic Regression Methods</a> <ul> <li><a href="#41-traditional-logistic-regression-method">4.1 Traditional logistic regression method</a></li> <li><a href="#with-tsallis-statistics">With Tsallis statistics</a></li> </ul> </li> <li><a href="#appendix-useful-mathematical-formulae">Appendix. Useful Mathematical Formulae</a></li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p><code class="language-plaintext highlighter-rouge">Tsallis statistics</code> is a generalization of traditional statistical mechanics, devised by <em>Constantino Tsallis</em>, to better characterize complex systems. It involves a collection of mathematical functions and associated probability distributions that can be derived by optimizing the <code class="language-plaintext highlighter-rouge">Tsallis entropic form</code>, a generalization of familiar Boltzmann entropy. A key aspect of Tsallis statistics is the introduction of a real parameter $q$, which adjusts the distributions to exhibit properties intermediate between Gaussian and Levy distributions, reflecting the degree of non-extensivity of the system.</p> <p>Tsallis distributions include various families like the $q$-Gaussian, $q$-exponential, and $q$-Weibull distributions. These distributions are notable for their heavy tails and have been applied across diverse fields such as statistical mechanics, geology, astronomy, economics, and machine learning, among others.</p> <p>The adaptation of Tsallis statistics to these varied fields underscores its versatility in dealing with systems where traditional Boltzmann-Gibbs statistics might not be adequate, particularly in scenarios involving long-range interactions, memory effects, or multifractal structures. Tsallis statistics is particularly useful in the analysis of non-extensive systems, to <code class="language-plaintext highlighter-rouge">biostatistics</code> could offer a novel perspective on analyzing complex biological data. Tsallis statistics has been successfully applied in various complex physical systems, such as space plasmas, atmospheric dynamics, and seismogenesis, as well as in the analysis of brain and cardiac activity, showing excellent agreement between theoretical predictions and experimental data. This demonstrates the versatility and potential of Tsallis statistics in capturing the dynamics of complex systems, which could be beneficial in biostatistical applications.</p> <p>Given the interdisciplinary nature of biostatistics, which often deals with complex, high-dimensional data, the non-extensive framework of Tsallis statistics might offer new methodologies for data analysis. For instance, it could be useful in understanding the dynamics of ecosystems, population genetics, or the spread of diseases, where traditional models might not fully capture the underlying processes due to their complexity and the presence of long-range interactions.</p> <p>To explore this possibility further, one could start by investigating specific biostatistical problems where the assumptions of traditional statistical mechanics are not met, and then applying Tsallis statistics to see if it offers better predictive power or insights. It would also be beneficial to collaborate with experts in biostatistics to identify the most pressing challenges where Tsallis statistics could be applied.</p> <p>While the application of Tsallis statistics to biostatistics is an intriguing prospect, it is an emerging area that would require substantial interdisciplinary research to fully understand its potential and limitations. Below is a list of potential applications, take it with a grain of salt.</p> <ol> <li> <p><strong>Epidemiological Modeling</strong>: Tsallis statistics could be used to model the spread of diseases, especially in cases where traditional models fail to capture the long-range correlations between individuals in a population.</p> </li> <li> <p><strong>Genetic Data Analysis</strong>: Analysis of genetic sequences and variations, where non-extensive entropy might better capture the complexity and long-range dependencies within genetic information.</p> </li> <li> <p><strong>Protein Folding Dynamics</strong>: Investigating the non-linear dynamics of protein folding, where Tsallis statistics may offer insights into the anomalous diffusion processes involved.</p> </li> <li> <p><strong>Neural Network Analysis</strong>: Modeling the complex interactions within neural networks, particularly in understanding the non-linear dynamics of brain activities and signal transmissions.</p> </li> <li> <p><strong>Ecological Systems</strong>: Applying Tsallis statistics to model the complexity of ecological systems, where interactions can span vast spatial and temporal scales.</p> </li> <li> <p><strong>Cancer Growth Modeling</strong>: Understanding the anomalous growth patterns of tumors, where traditional models might not accurately capture the underlying dynamics.</p> </li> <li> <p><strong>Drug Response Modeling</strong>: Analyzing the variability in drug responses among populations, which may exhibit non-standard distribution patterns that Tsallis statistics could elucidate.</p> </li> <li> <p><strong>Heart Rate Variability Analysis</strong>: Investigating the complex, non-linear dynamics of heart rate variability, potentially uncovering new insights into cardiovascular health.</p> </li> <li> <p><strong>Analysis of Medical Imaging Data</strong>: Enhancing the interpretation of complex patterns in medical imaging, such as MRI or CT scans, through non-extensive statistical models.</p> </li> <li> <p><strong>Public Health Data Analysis</strong>: Applying Tsallis statistics to public health data, potentially uncovering new patterns or correlations in large-scale health trends.</p> </li> </ol> <hr/> <h2 id="11-the-basics-of-boltzmann-gibbs-extensive-entropy">1.1. The Basics of Boltzmann-Gibbs Extensive Entropy</h2> <p><em>The whole theory of Tsallis statistics is based on a single concept: the modified Boltzmann-Gibbs (B-G) entropy</em> $S_ {q}$. $q$ is some index show how much $S_ {q}$ differs from the $BG$ entropy, if $q=1$ then there is no difference.</p> <p>To explain why the Boltzmann-Gibbs entropy is said to be additive, let’s first clarify what we mean by entropy in this context. In statistical mechanics, the Boltzmann-Gibbs entropy is a measure of the number of microstates that correspond to a given macrostate, providing a quantification of the system’s disorder or randomness.</p> <p>The formula for Boltzmann-Gibbs entropy, $S$, for a system in a particular <strong>macrostate</strong> is given by:</p> \[S = -k_B \sum_i p_i \ln p_i\] <p>where $p_i$ is the probability of the system being in the $i$-th microstate, and $k_B$ is the Boltzmann constant.</p> <p>Entropy is said to be additive when, for two independent systems $A$ and $B$, the total entropy $S_{AB}$ of the combined system is the sum of their individual entropies:</p> \[S_{AB} = S_A + S_B\] <p>This additivity property stems from the assumption of statistical independence of the two systems, which implies that the probability of the combined system $AB$ being in a particular microstate is the product of the probabilities of $A$ and $B$ being in their respective microstates. If $A$ is in a microstate with probability $p_i$ and $B$ is in a microstate with probability $q_j$, then the probability of the combined system being in the microstate characterized by both $p_i$ and $q_j$ is $p_i \cdot q_j$.</p> <p>As an example, consider two independent systems $A$ and $B$, each with two possible microstates. For system $A$, let the probabilities of the microstates be $p_1$ and $p_2$, and for system $B$, let them be $q_1$ and $q_2$. The entropies of systems $A$ and $B$ are:</p> <p>\(S_A = -k_B (p_1 \ln p_1 + p_2 \ln p_2)\) \(S_B = -k_B (q_1 \ln q_1 + q_2 \ln q_2)\)</p> <p>For the combined system $AB$, there are four possible microstates, with probabilities $p_1q_1, p_1q_2, p_2q_1,$ and $p_2q_2$. The entropy of the combined system is:</p> \[S _{AB} = -k_B [(p_1q_1) \ln (p_1q_1) + (p_1q_2) \ln (p_1q_2) + (p_2q_1) \ln (p_2q_1) + (p_2q_2) \ln (p_2q_2)]\] <p>With some algebra, you can show that:</p> \[S_{AB} = S_A + S_B .\] <p>This demonstrates the additivity of entropy for independent systems. <em>The crucial point here is the assumption of independence</em>, which allows the probabilities of the combined system to be expressed as products of the individual systems’ probabilities, leading directly to the additivity of entropy.</p> <p>Tsallis in his book compared his generalization of B-G statistics to $q$-statistics to the generalization of a circle to ellipses in explaining the motion of celestial objects. In both cases a single parameter changes everything. However I would argue that in the case of Kepler and others, more physics was revealed then in Tsallis’ case.</p> <h2 id="12-generalization-to-non-extensive-entropy">1.2. Generalization to non-Extensive entropy</h2> <p>In his book Tsallis listed some reasons for considering non-extensive, non-Boltzmann-Gibbs entropy, which can be roughly translated into:</p> <ol> <li>There is no (mathematical or physical) reason not to.</li> <li>A statistical description of a system should be based on the dynamics of the system, the macroscopic theory should come from a microscopic one. This opens the way, especially for complex systems, for other than Boltzmann statistics.</li> <li>The existence of long-range interactions on the microscopic level.</li> </ol> <h1 id="2-boltzmann-gibbs-statistical-mechanics">2. Boltzmann-Gibbs Statistical Mechanics</h1> <h2 id="21-three-different-forms-of-bg-entropy">2.1. Three different forms of BG entropy</h2> <p>No we need to come back to one of the most important concept in physics, statistics and information theory: <strong>entropy</strong>. It appears in various fields, each with its unique perspective but underlying similarities in concept.</p> <p>Generally, <em>entropy represents a measure of disorder, randomness, or uncertainty</em>. In statistics, entropy is a measure of the <strong>unpredictability</strong> or the <strong>randomness</strong> of a distribution. <em>The higher the entropy, the more unpredictable the outcome</em>. For example, in a perfectly uniform distribution where all outcomes are equally likely, entropy is at its maximum, indicating maximum uncertainty or disorder. In contrast, a distribution where one outcome is certain has zero entropy, representing complete order. This concept is used in various statistical methods and models to quantify uncertainty or variability within a dataset. In information theory, entropy is a fundamental concept introduced by <code class="language-plaintext highlighter-rouge">Claude Shannon</code>. It quantifies the average amount of information produced by a stochastic source of data. The more uncertain or random the source, the higher the entropy. In practical terms, entropy helps in understanding the limits of data compression and the efficiency of communication systems. For instance, a message composed of completely random bits has higher entropy and cannot be compressed beyond a certain limit without losing information. On the other hand, a message with a lot of repetitive or predictable parts has lower entropy and can be compressed more effectively.</p> <p>In each of these fields, entropy helps us understand systems’ behavior in terms of unpredictability, disorder, and efficiency. While the context and applications may vary, the core idea revolves around the concepts of uncertainty and the distribution of states or outcomes.</p> <p>In the previous section we showed the definition of entropy without much justification, because there is none! Not from the first principal at least. The programme to derive the expression of entropy that we are using today is sometimes called the Boltzmann program, since that was what Boltzmann was trying to do, before he strangled himself to death using a curtain or something.</p> <p>However, if the possibilities is a continuous distribution, the BG entropy must be modified accordingly, discrete probability $p_ {i}$ must be replaced by probability density $p(x)$ where $x$ is the variable. As a naively guess, I would say that we can write the entropy as</p> \[-k \sum p _ {i} \ln p _ {i} \to -k \int dx \, p(x) \ln(p(x)),\] <p>where the probability distribution function (or probability density) is normalized,</p> \[\int dx \, p(x) =1.\] <p>However, a difference between discrete and continuous probability lies in its dimension! Normalized discrete probabilities $p_ {i}$ sums to 1, $\sum_ {i} p_ {i}=1$, since $1$ is dimensionless, so is $p_ {i}$. This is not true for continuous probability density $p(x)$, since now the normalization condition tells us that $\int dx \, p(x)$ should be dimensionless, and $dx$ has the dimension of length, so $p(x)$ must have dimension of length inversed! Thus, wo need to introduce another parameter, call it $\sigma$, with dimension of length. Then we can define the BG entropy in the continuous scenario:</p> \[S_ {BG} = -k\int dx \, p(x) \ln(\sigma\, p(x)).\] <p>For the case of equal probabilities, that is, $p= 1 / \Omega$ where $\Omega$ is the total number of allowed microscopic states, we have</p> \[S_ {BG} = k \ln\left( \frac{\Omega}{\sigma} \right).\] <p>We just mention on the fly that, in quantum mechanics, the probabilistic distribution of a mixed state in terms of pure states is described using the density matrix $\rho$, and the BG entropy is generalized to</p> \[S_ {BF} = -k\,\mathrm{Tr}\, (\rho \ln \rho).\] <h2 id="22-properties-of-bg-entropy">2.2. Properties of BG entropy</h2> <p>We will list without proof some of the key properties of BG entropy.</p> <ul> <li><strong>Non-negativity</strong>. $S\geq 0$ always. $S = k\ln \Omega$ might help to convince you of it.</li> <li><strong>BG entropy is maximized at equal probability</strong>. Anything that drives the system away from it will decrease the entropy.</li> <li><strong>Expansibility</strong>. Adding to a system new possible states with zero probability should not modify the entropy.</li> <li><strong>Additivity</strong>. Let $A,B$ be two systems with entropy $S(A)$ and $S(B)$, putting them together will result in a new system $A+B$ with entropy $S(A+B)=S(A)+S(B)$.</li> <li><strong>Concavity</strong>. Given two different probability distributions $\left\lbrace p_ {i} \right\rbrace$ and $\left\lbrace p’_ {i} \right\rbrace$, we can define an intermediate probability distribution</li> </ul> \[\widetilde{p}:= \lambda p + (1-\lambda)p',\quad \lambda \in (0,1).\] <p>Then we have</p> \[S(\widetilde{p})\equiv S(\lambda)&gt; \lambda S(p)+(1-\lambda)S(p').\] <p><strong>Shannon Uniqueness Theorem</strong>.</p> <p>In his work, Shannon was interested in finding a measure that could quantitatively capture the information content of a message source. He proposed several properties that this measure (which we now call entropy) should satisfy to be a useful and consistent measure of information. These properties included:</p> <ol> <li><strong>Additivity</strong>: The entropy of two independent sources should be the sum of their individual entropies.</li> <li><strong>Continuity</strong>: The measure should change continuously as the message probabilities change.</li> <li><strong>Symmetry</strong>: The measure should not depend on the order of the messages.</li> <li><strong>Maximum</strong>: The measure should be maximal for a uniform distribution, where all messages are equally likely.</li> </ol> <p>Shannon’s Uniqueness Theorem essentially states that, given these properties (along with a few others), the entropy of a discrete random variable is unique and is given by the now-familiar formula:</p> \[S = -\sum_{i} p(x_i) \ln p(x_i)\] <p>The theorem’s significance lies in its establishment of entropy as a <strong>unique measure</strong> that satisfies these intuitive and necessary properties for quantifying information. It solidified the concept of entropy as the foundational metric in information theory, leading to profound implications for communication, coding theory, and even other disciplines like statistics and thermodynamics.</p> <h2 id="23-constraints-and-entropy-optimization">2.3. Constraints and Entropy Optimization</h2> <p><strong>Imposing the Mean Value</strong></p> <p>We might know a priori the mean value of a variable $x$, i.e.</p> \[\left\langle x \right\rangle := \int dx \, x \, p(x) = \overline{x} \quad \text{ is known.}\] <p>We can apply the Lagrange multiplier method to find the optimizing distribution with the constraint, together with the normalization condition $\int dx \, p(x)=1$. The Lagrangian functional that we want to extremize reads now</p> \[\Phi[p(x)] = S_ {BG} - \alpha \int dx \, p(x) - \beta \int dx \, x \, p(x)\] <p>where we have neglected some constant terms since they don’t appear in the Euler-Lagrange equation, that is to say, they don’t affect the final result; and</p> \[S_ {BG} = -\int dx \, p(x)\ln p(x).\] <p>Using the method of variation, we get</p> \[p(x) = \frac{1}{\overline{x}} e^{ -x / \overline{x} }.\] <p><strong>Imposing the Mean value and the Mean Squared Value</strong></p> <p>Supposed that we not only know the mean value $\overline{x}=\left\langle x \right\rangle$, but also the mean square value $\left\langle x^{2} \right\rangle$:</p> \[\left\langle x^{2} \right\rangle \equiv \int dx \, (x-\left\langle x \right\rangle )^{2} p(x)\] <p>This time the Lagrangian reads</p> \[\Phi[p(x)] = S_ {BG} - \alpha \int dx \, p(x) - \beta_ {1}\int dx \, xp(x) - \beta_ {2} \int dx \, (x-\left\langle x \right\rangle )^{2}p(x).\] <p>Exactly as before, the variational method gives us</p> \[p(x) = \sqrt{ \frac{\beta_ {2}}{\pi} } \exp \left\lbrace -\beta_ {2}(x-\left\langle x \right\rangle )^{2} \right\rbrace\] <p>which is just the Gaussian distribution! This tells us that the Gaussian distribution maximizes the entropy with fixed mean and variance.</p> <h1 id="3-nonextensive-statistical-mechanics">3. Nonextensive Statistical Mechanics</h1> <p>Tsallis is convinced that there exists no logical-deductive procedure for generalizing any physical theory. As a possible motivation to generalize the exponential function $e^{ x }$, he started from the equation that $e^{ x }$ satisfies, which is fairly simple:</p> \[\frac{dy}{dx} = y.\] <p>A possible generalization of the equation is to writhe the RHS as $a+by$, we have</p> \[\frac{dy}{dx} = a+by \implies \frac{dy}{a+by}=dx\implies y=\frac{1}{b}(e^{ b(x+c) }-a),\] <p>which does look very promising. Then Tsallis considered a non-linear generalization:</p> \[\frac{dy}{dx} = y^{q}, \quad q\in \mathbb{R},\quad y(0)=1.\] <p>The boundary condition $y(0)=1$ is such that is agrees with the usual exponential function $e^{ 0 }=1$. Solving it we get</p> <p>\((1-q) (x+c) = y^{1-q} ,\) the boundary condition translates to</p> \[c = \frac{1}{1-q},\] <p>thus</p> \[\boxed{ y = (1+(1-q)x)^{1/(1-q)} =: e_ {q}^{x}. }\] <p>When $1+(1-q)x&lt;0$, $e_ {q}(x)$ is defined to be zero (<strong>why don’t complexify it</strong>?). Also note that $e^{ x }_ {q}$ goes to $e^{ x }$ at $q\to 1$ since, writing $q = 1-\epsilon$, we have</p> \[\lim_ { q \to 1 } e^{ x }_ {q} = \lim_ { \epsilon \to 0 } (1+\epsilon x)^{1/\epsilon} = e^{ x }.\] <p>From the same equation that we got the definition of $e^{ x }_ {q}$, we also get the inverse function of $x$ in terms of $y$:</p> \[\boxed{ x = \frac{y^{1-q}-1}{1-q} =: \log_ {q} y. }\] <p>They are referred to as $q$-exponential functions and $q$-logarithmic functions respectively.</p> <p>Recall that logarithmic functions turns multiplication into addition, $\log(AB)=\log(A)+\log(B)$, for $q$-logarithmic functions we have something similar,</p> \[\log_ {q}(AB) = \log_ {q}(A) + \log_ {q}(B) + (1-q) \log_ {q}(A)\log_ {q}(A).\] <h2 id="31-mean-value-in-tsallis-statistics">3.1. Mean Value in Tsallis Statistics</h2> <p>There are three types of Tsallis statistics, depending on how they take the mean value. Next we will discuss each of them in chronological order.</p> <p>Throughout the note we will assume that probabilities are normalized in the usual way,</p> \[\sum_ {i} p_ {i} = 1.\] <p>Given an observable $\mathcal{O}$, what could be the expected value $\left\langle \mathcal{O} \right\rangle$? The most naive guess, which is also the default definition, is to define</p> \[\left\langle \mathcal{O} \right\rangle := \sum_ {i} p _ {i} O_ {i}\] <p>where $\mathcal{O}_ {i}$ is the $i$-th possible value of $\mathcal{O}$ with probability $p_ {i}$. However, this definition yields and ill-defined thermodynamic distribution, <em>some energy states will not be allowed due to mathematical rather than physical reasons, and the distribution is not invariant under an overall shift in energy</em>. Normally only the energy difference matter, the only situation that I know of where the absolute energy matters is from gravity, which is clearly not the case here. Thus it is not a good definition for taking average.</p> <hr/> <p>Another way to define the average, known as Tsallis type II, is</p> <p>\(\left\langle \mathcal{O} \right\rangle := \sum_ {i} p_ {i}^{q} \mathcal{O}_ i.\) The problem is similar with type I, the sample space is constraint due to some un-natural reason, which I tend to interpret as the evidence of an ill-defined theory. Some divergence that occurs in type I does not occur here, but it introduces new problems, most of all the expected value of unity $\mathbb{1}$ is not $1$.</p> <p>However there is a remedy. Arguing from the point of view of information theory on incomplete probability distributions, Q. A. Wang suggested modifying the normalization of probability as</p> \[\sum_ {i} p_ {i}^{q} = 1.\] <p>This can be rewritten by defining $P_ {i}:= p_ {i}^{1}$, then</p> \[\left\langle \mathcal{O} \right\rangle := \sum_ {i} P_ {i} \mathcal{O}_ {i} .\] <hr/> <p>Type III assumes that the average is defined as type II but with an normalization factor:</p> \[\left\langle \mathcal{O} \right\rangle := N \sum_ {i} p_ {i}^{q}\mathcal{O}_ {i},\quad N = \sum_ {i}p_ {i}.\] <p>This solves the problem that the expectation value of identity $1$ is not $1$. The probability derived from is also becomes invariant under an overall shift.</p> <h1 id="4-tsallis-in-logistic-regression-methods">4. Tsallis in Logistic Regression Methods</h1> <p>This part will be presented in a much less pedagogical manner, we will just introduce the functions whenever we encounter them along the way, with little or none explanation. Before I could apply the Tsallis statistics, I need to first figure out how the traditional method works.</p> <h2 id="41-traditional-logistic-regression-method">4.1 Traditional logistic regression method</h2> <p>Say we are given a matrix $X_ {n\times p}$ of the expression of $p$ genes in $n$ samples, measured by microarray or RNA-seq or some other fancy technologies. It is assumed that $p\gg n$, this sets the stage for so-called <em>big $p$, small $n$ problem</em>. Denote the response (or dependent variables, observables) as a $n$-vector $\vec{y} = (y_ {1},\cdots,y_ {n})$. $y_ {i}$ can only take value from $1$ and $0$, hence is called <code class="language-plaintext highlighter-rouge">binary variable</code>, or <code class="language-plaintext highlighter-rouge">logistic variable</code>. Think of each sample as a patient, $y_ {i}=1$ if patient $i$ is cured of certain disease, or has a certain disease; $y_ {i}=0$ otherwise. For now we are talking about a mathematical model, not yet a biostatistics model, so we don’t need to know what $y=1$ actually means in real life, the meaning of it remains to be defined. The value of $\vec{y}$ is modeled by the Bernoulli function, as we will see shortly.</p> <p>We can organize the $n$-observations, each with $p$ genes, into a matrix $X$,</p> \[X = \begin{pmatrix} x_ {11} &amp; \cdots &amp; x_ {1p} \\ x_ {21} &amp; \cdots &amp; x_ {2p} \\ \vdots &amp; \ddots &amp; \vdots \\ x_ {n1} &amp; \cdots &amp; x_ {np} \end{pmatrix}.\] <p>Each row is a vector of different genes of one patient, each column is vector of different patients with one type of gene. We use $X_ {i}$ to denote the $i$-th gene observed in $n$ samples, namely the $i$-th column of $X$; we use $X_ {(j)}$ to denote the genes observed in $j$-th sample (one patient $j$), namely the $j$-th row. Note that the notational difference is just a pair of parenthesis. In general, $X$ is our data and $\vec{y}$ the dependent variable. In practice, it is known which patients are treated and which are not, hence we know the observed value of $\vec{y}$. The problem is to find a way to <strong>predict</strong> $y_ {a}$ from an observed vector of genes expressions $X_ {(a)}$. But first, we need to know the role played by each gene expression in different patients, that is we need to know how to interpret $X_ {a}$ for gene $a$. For example, if $X_ {a}$ is a constant vector for all the patients, then we know gene $a$ most likely has nothing to do with the disease of study.</p> <p>Let us now delve into the mathematical foundations of the model.</p> <p>Let $\pi_ {i}$ be the probability of $y_ {i}=1$, which we aim to predict. Following the standard logistic regression method, we define the logit (log here is used as a verb, <em>log</em>-it) function of $\pi_ {i}$,</p> \[\text{logit}(\pi_ {i} ) := \ln\left( \frac{\pi_ {i}}{1-\pi_ {i}} \right), \quad \text{logit}: [0,1]\to \mathbb{R}.\] <p>$\pi$ is the probability, and $\pi / (1-\pi)$ is called <code class="language-plaintext highlighter-rouge">odd</code> whose range is $\mathbb{R}^{+}:=[0,\infty)$. Note that in our convention zero is included in $\mathbb{R}^{+}$.</p> <p>Since we have constructed a continuous function $\text{logit}(\pi)$ from a probability function $\pi$, we can now adopt familiar <em>linear regression method</em> and apply it to the logit function. Parametrize the logit function in a linear form</p> \[\text{logit}(\pi_ {i}) := \beta_ {0}+X_ {(i)}\cdot\beta, \quad \beta=(\beta_ {1},\cdots,\beta_ {p}).\] <p>$\beta$ is the parameter vector to be fixed later. Note that $X \cdot \beta:= X^{T} \beta$, the dot denote multiplication between vectors.</p> <p>Direct derivation shows that</p> \[\pi_ {i} = \frac{1}{1+\exp(-\beta_ {0}-X_ {(i)}\cdot \beta)}=:\text{sigm}(\beta_ {0}+X_ {(i)}\cdot \beta),\] <p>where $\text{sigm}$ stands for sigmoid function, which literally means a function that has a $S$-shape. In our particular case the sigmoid function is give by</p> \[\text{sigm}(t) := \frac{1}{1+e^{ -t }}.\] <p>Note that $\text{sigm}$ is the inverse of $\text{logit}$,</p> \[\text{logit}(\pi)=x \Longleftrightarrow \pi = \text{logit}^{-1}(x) = \text{sigm}(x).\] <p>I am not sure if there exists other widely-applied definitions for sigmoid function except for that given above. There surely are other functions with an S-shape, such as tanh and arctan, but I am not sure if they can be called sigmoids? Or maybe difference in the predictive power by introducing different choices of sigmoid functions are negligible, hence it only makes sense that we stuck with the simplest option?</p> <p>Now, the question is how can we fix the parameters? The natural answer is: by maximizing the likelihood, or equivalent by minimizing the loss function. The likelihood function, by definition, is the probability (likelihood) for a certain observation $\vec{y}$. This function measures the probability of observing the given data under the model parameters being estimated. For instance, supposed there are three samples (binary), then the likelihood of $y=(1,0,1)$ corresponds to the probability predicted by our model that the first and third patients have got some disease, while the second does not. Now, the probability for each $y_ {i}=1$ is $\pi_ {i}$, which is by definition $\text{sigm}(t_ {i})\equiv1/(1-e^{ -t_ {i} })$, where $t=\beta_ {0}+X_ {(i)}\cdot \beta$, and $\beta$’s are the parameters. The probability for $y_ {i}=0$ is hence $1- 1/(1-e^{ -ti })$, which is $e^{ -t_ {i} }/(1-e^{ -t_ {i} })$. Then, the likelihood for observing $y=(1,0,1)$ is simply the product of each probability,</p> \[\text{lik}(1,0,1) = \frac{1}{1-e^{ -t_ {1} }} \frac{e^{ -t_ {2} }}{1-e^{ -t_ {2} }} \frac{1}{1-e^{ -t_ {3} }},\] <p>which is a function of parameters $\beta$’s and observed gene expression $X_ {i}$.</p> <p>Likelihood function can also be written using the <code class="language-plaintext highlighter-rouge">Bernoulli distribution</code> function, which is a probability distribution function (PDF) that has only two possible outcomes, 1 and 0, with probability $\pi$ for $y=1$ and $1-\pi$ for $y=0$. This distribution is a special case of the binomial distribution where the number of trials $n$ is equal to 1. The <strong>Probability Mass Function (PMF)</strong> of Bernoulli distribution is defined as</p> \[P(y) := \pi^y (1-\pi)^{1-y}, \quad y=0 \text{ or }1,\] <p>where $\pi$ is given <em>a priori</em>. The Expected Value is $\left\langle y \right\rangle=\pi$ and the variance is $\text{Var}(y) = \pi(1-\pi)$, you can verify it easily. This neat expression unites both cases $y=0$ and $y=1$. Applying it to the likelihood function we get</p> \[\text{lik}(1,0,1) = \prod_ {i=1}^{3} \pi_ {i}^{y_ {i}}(1-\pi_ {i})^{1-y_ {i}}, \quad \vec{y}=(1,0,1).\] <p>Generalization to arbitrary $\vec{y}$ is trivial,</p> \[\text{lik}(\vec{y}) := \prod_ {i=1}^{n} \pi_ {i}^{y_ {i}}(1-\pi_ {i})^{1-y_ {i}}, \quad \vec{y}=(y_ {1},\cdots,y_ {n}).\] <p>The above expression can be further simplified using logarithms. Recall that logarithm is a <em>monotonically increasing</em> function, it means that $\log(\text{lik}(\vec{y}))$ is maximized iff (if and only if) $\text{lik}(\vec{y})$ is maximized. The reason why it is a good idea to take the logarithm of the likelihood is the following.</p> <ol> <li> <p><strong>Numerical Stability</strong>: The likelihood function in models like logistic regression involves products of probabilities, which can be very small numbers. When multiplying many such small probabilities, the result can become extremely small, potentially leading to numerical underflow (where values are so small that the computer treats them as zero). The logarithm of these small numbers turns them into more manageable, larger negative numbers, reducing the risk of numerical issues.</p> </li> <li> <p><strong>Simplification of Products into Sums</strong>: The likelihood function involves taking the product of probability values across all data points. In contrast, the log-likelihood converts these products into sums by the property of logarithms $\ln(ab) = \ln(a) + \ln(b)$. Sums are much easier to handle analytically and computationally. This is especially useful when dealing with large datasets.</p> </li> <li> <p><strong>Convexity Properties</strong>: The log-likelihood function often yields a convex optimization problem in cases where the likelihood itself is not convex. Convex problems are generally easier to solve reliably and efficiently. For logistic regression, the log-likelihood function is concave, and finding its maximum is a well-behaved optimization problem with nice theoretical properties regarding convergence and uniqueness of the solution.</p> </li> <li> <p><strong>Derivative Computation</strong>: The derivatives of the log-likelihood (needed for optimization algorithms like gradient ascent or Newton-Raphson) are typically simpler to compute and work with than the derivatives of the likelihood function. This simplicity arises because the derivative of a sum (log-likelihood) is more straightforward than the derivative of a product (likelihood).</p> </li> </ol> <p>Last but not least,</p> <ol> <li><strong>Possibility to introduce the Tsallis statistics</strong>: We have explain how the Tsallis statistics modified traditional exponential and logarithmic functions, <em>in our case the log-likelihood function can be generalized by adopting the Tsallis logarithm, namely $\log_ {q}$.</em> In the next section we will try to explain the advantage of such generalization, which will also serve as justification. But of course, being a phenomenological model, the true justification will be the power of prediction, which can only be tested in real-life practice.</li> </ol> <p>But for now, let’s forget about Tsallis and carry on on the road of conventional logic regression method.</p> <p>Taking the natural log of likelihood function gets us</p> \[\begin{align*} \text{loglik}(\vec{y}) &amp;:=\log(\text{lik}(\vec{y}))= \ln \left\lbrace \prod_ {i=1}^{n} \pi_ {i}^{y_ {i}}(1-\pi_ {i})^{1-y_ {i}} \right\rbrace \\ &amp;=\sum_ {i} \left\lbrace y_ {i} \ln\pi_ {i}+(1-y_ {i}) \ln(1-\pi_ {i}) \right\rbrace. \end{align*}\] <p>In the context of logistic regression, people often use the loss function, defined as the negative of the likelihood function, rather than the likelihood function itself. This is primarily due to convention and practical considerations in optimization processes, since most optimization algorithms and tools are designed to minimize functions rather than maximize them. This is a common convention in mathematical optimization and numerical methods. Since maximizing the likelihood is equivalent to minimizing the negative of the likelihood, formulating the problem as a minimization problem allows us the application of standard, widely available optimization software and algorithms without modification. In many statistical learning methods, the objective function is often interpreted as a “cost” or “loss” that needs to be minimized. When working with a loss function, the goal is to find parameter estimates that result in the smallest possible loss. Defining the loss function as the negative log-likelihood aligns with this interpretation because lower values of the loss function correspond to higher likelihoods of the data under the model.</p> <p>Furthermore, using a loss function that is to be minimized creates a consistent framework across various statistical learning methods, many of which inherently involve minimization (like least squares for linear regression, and more complex regularization methods in machine learning). This consistency is helpful not only from an educational and conceptual standpoint but also from a practical implementation standpoint.</p> <p>A <code class="language-plaintext highlighter-rouge">regularized loss function</code> is the loss function with <code class="language-plaintext highlighter-rouge">penalty terms</code>. Penalty terms in regression methods are essential for managing several challenges that arise in statistical modeling, particularly with high-dimensional data. These challenges include <em>overfitting</em>, <em>multicollinearity</em>, and <em>interpretability</em>.</p> <p>One of the primary reasons for using penalty terms is to prevent overfitting. Overfitting occurs when a model captures not just the true underlying pattern but also the random fluctuations (noise) in the data. This makes the model very accurate on the training data but <em>poorly generalized to new, unseen data</em>. By adding a penalty term, which increases as the model complexity increases (e.g., as coefficients become larger), the optimization process is biased towards simpler models. This helps to ensure that the model generalizes well to new data by focusing on the most significant predictors and shrinking the others.</p> <p>Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated. This can make the model estimation unstable and the estimates of the coefficients unreliable and highly sensitive to small changes in the model or the data. Penalty terms, especially those like Lasso regression (L1 penalty) or Ridge regression (L2 penalty), can reduce the impact of multicollinearity by penalizing the size of the coefficients, thus stabilizing the estimates. In high-dimensional datasets where the number of predictors $p$ exceeds the number of observations $n$ or when there are many irrelevant features, selecting the most relevant features becomes crucial. Lasso regression (L1 penalty) is particularly useful for this purpose because it can shrink some coefficients to exactly zero, effectively performing variable selection. This helps in identifying which predictors are most important for the outcome, simplifying the model and improving interpretability.</p> <p>By shrinking coefficients or reducing the number of variables, penalty terms help in simplifying the model. A simpler model with fewer variables or smaller coefficients is easier to understand and interpret. This is particularly valuable in domains like medical science or policy-making, where understanding the influence of predictors is as important as the prediction itself.</p> <p>However it is also important to realize the disadvantages of introducing penalty terms, that is the bias-variance tradeoff. Adding a penalty term introduces bias into the estimator for the coefficients (the estimates are “shrunk” towards zero or towards each other in the case of Ridge). However, this can lead to a significant reduction in variance (the estimates are less sensitive to fluctuations in the data). This trade-off can lead to better predictive performance on new data, which is the ultimate goal of most predictive modeling tasks.</p> <hr/> <p>In our project, we consider the regularized loss function as following.</p> \[l(\beta_ {0}, \vec{\beta}) := -\sum_ {i}^{n} \left\lbrace y_ {i}\ln(\pi_ {i})+(1-y_ {i})\ln(1-\pi_ {i}) \right\rbrace +h(\vec{\beta}),\] <p>where $h(\vec{\beta})$ is the penalty terms and is independent of $\beta_ {0}$, which is simply a shift. $h(\vec{\beta})$ could be the Lasso (<code class="language-plaintext highlighter-rouge">Least Absolute Shrinkage and Selection Operator</code>) term, or ridge term.</p> <hr/> <p>We now introduce the <strong>Group Lasso regression</strong> method. Group Lasso regression is an extension of Lasso that is particularly useful in the context of biostatistics, especially when dealing with models that incorporate multiple predictors which naturally group into clusters. This method not only encourages sparsity in the coefficients, like traditional Lasso, but also takes into account the structure of the data by promoting or penalizing entire groups of coefficients together. Here’s an introduction to how Group Lasso works and why it’s valuable in biostatistics:</p> <p>In many biostatistical applications, predictors can be inherently grouped based on biological function, measurement type, or other domain-specific knowledge. For example, genes might be grouped into pathways, or clinical measurements might be grouped by the type of instrument used or the biological system they measure.</p> <p>The key idea behind Group Lasso is to <em>perform regularization and variable selection at the group level</em>. The formulation of Group Lasso is similar to that of Lasso, but instead of summing the absolute values of coefficients, it sums the norms of the coefficients for each group. The objective function for Group Lasso can be written as:</p> \[\text{Minimize:} \quad \text{loss function} + h(\vec{\beta}), \quad h(\vec{\beta}):= \lambda \sum_ {g=1}^G w_ g \left\lVert \vec{\beta}_{(g)} \right\rVert_ {2}\] <p>where $\vec{\beta}$ represents the coefficient vector, $\vec{\beta}<em>{(g)}$ is the sub-vector of coefficients corresponding to group $g$. Note the difference between $\beta</em> {g}$ and $\beta_ {(g)}$, the former is a component of $\vec{\beta}$ thus a number, while the latter is a sub-vector of $\vec{\beta}$. $G$ is the total number of groups, $w_g$ are weights that can be used to scale the penalty for different groups, often based on group size or other criteria, $\left\lVert \vec{\beta}_{(g)} \right\rVert$ is typically the Euclidean norm (L2 norm) of the coefficients in group $g$ or, as is shown in ShunJie’s paper, the PCC (Pearson correlation coefficient) distance, or shape-based distance. $\lambda$ is a <strong>global</strong> tuning parameter that controls the overall strength of the penalty.</p> <p>In our current project, we use square root of the degree of freedom (dof) as the weight for different groups. The penalty term is then</p> \[h(\beta) = \lambda \sum_ {g=1}^{G} \sqrt{ d_ {g} } \left\lVert \vec{\beta}_ {g} \right\rVert _ {2}\] <p>where $d_ {g}$ is the number of genes in group $g$, or dimension of $\vec{\beta}_ {g}$, where $\vec{\beta}_ {g}$ is treated as a $d_ {g}$-vector of parameters. Naturally it is a subset of $\beta$.</p> <p>Recall that $\sqrt{ d_ {g} }$ is the expected length of $d_ {g}$ i.i.d. variables, independent and identically distributed variables. The factor $\sqrt{d_ g}$ is to adjust for the size of each group, it scales up the penalty proportional to the group size. This means that larger groups, which naturally have a potentially larger norm due to more coefficients, receive a proportionally larger penalty. By multiplying the norm by $\sqrt{d_ g}$, the intention is to make the penalty fair by ensuring that the regularization is proportional to the number of parameters in the group. Without this scaling, smaller groups could be unfairly penalized in relative terms because their norms are naturally smaller due to fewer components. For example, consider two groups in a regression setting:</p> <ul> <li>Group 1: Contains 1 coefficient, $\mathbf{w}_ 1 = (3)$.</li> <li>Group 2: Contains 4 coefficients, $\mathbf{w}_ 2 = (1, 1, 1, 1)$.</li> </ul> <p>Without any normalization, we have</p> <ul> <li>The norm for Group 1 is $\left\lVert w_ {1} \right\rVert_ 2 = \left\lvert 3 \right\rvert = 3$.</li> <li>The norm for Group 2 is $\left\lVert w_ {2} \right\rVert_ 2 = \sqrt{1^2 + 1^2 + 1^2 + 1^2} = 2$.</li> </ul> <p>Here, the raw norms suggest that Group 1 might be more significant, which could distort the model’s view. Using $d_ g$ for normalization instead, we have:</p> <ul> <li>For Group 1: $1 \times \left\lvert 3 \right\rvert= 3$.</li> <li>For Group 2: $4 \times 2 = 8$.</li> </ul> <p>This overcorrects, making the penalty overly sensitive to the number of components, and could lead to under-penalizing smaller groups. Thus we decide to use $\sqrt{d_ g}$ for normalization:</p> <ul> <li>For Group 1: $\sqrt{1} \times 3 = 3$.</li> <li>For Group 2: $\sqrt{4} \times 2 = 2 \times 2 = 4$.</li> </ul> <p>This balances the penalties more reasonably, reflecting that while Group 2 is larger, its collective contribution should be seen in proportion to the natural growth of norms with group size, not exponentially with each addition.</p> <hr/> <p>We recall that $X_ {(i)}$ is the expression of different genes for one patient $i$, in terms of which is given the probability:</p> \[\pi_ {i} = \frac{1}{1+\exp(-\beta_ {0}-X_ {(i)}\cdot \beta)}=\text{sigm}(\beta_ {0}+X_ {(i)}\cdot \beta),\] <p>where the sigmoid function is give by</p> \[\text{sigm}(t_ {i} ) := \frac{1}{1+e^{ -t_ {i} }}, \quad t_ {i} := \beta_ {0}+X_ {(i)}\cdot \beta.\] <p>The loss function is given by the negative log-likelihood function plus the penalty term, in our case a L2-norm one:</p> \[\begin{align*} l(\beta) &amp;= -\sum_ {i} \left\lbrace y_ {i}\ln\left( \frac{\text{sigm}(t_ {i} )}{\text{sigm}(-t_ {i} )} \right) + \ln\, \text{sigm}(t_ {i} ) \right\rbrace + h(\beta) \\ &amp;= -\sum_ {i}[y_ {i}t_ {i} -\ln(1+e^{ -t_ {i} })] + h(\beta)\\ &amp;=: J(\beta)+h(\beta), \end{align*}\] <p>where in the first line we write $l(\vec{\beta})$ in terms of sigmoid function for future convenience. To be specific, in future work we will consider generalization for sigmoid function with Tsallis q-logarithms, so we might just write the loss functions explicitly in terms of it as well. $h(\beta)$ is the Lasso penalty term given by $\lambda \sum_ {g=1}^{G} \sqrt{ d_ {g} } \left\lVert \vec{\beta}_ {g} \right\rVert _ {2}$. $J(\beta)$ is the minus likelihood function, introduced for future convenience.</p> <p>As usual, the loss function is convex and non-linear. Since we took the absolute value of $\left\lVert \vec{\beta}_ {g} \right\rVert$, it is not smooth at $\vec{\beta}=0$. Define the</p> <h2 id="42-with-tsallis-statistics">4.2 With Tsallis statistics</h2> <h1 id="appendix-useful-mathematical-formulae">Appendix. Useful Mathematical Formulae</h1> <p>The definition of $q$-logarithm and $q$-exponential, $x&gt;0, q \in\mathbb{R}$:</p> \[\begin{align*} \ln_ {q}x &amp;:= \frac{x^{1-q}-1}{1-q}, \\ e^{ x }_ {q} &amp;:= (1+(1-q)x)^{1/(1-q)}, \end{align*}\] <p>in the definition of the exponential it is required that $1+(1-q)x&gt;0$, otherwise it is defined to be zero. It is easily checked that they are indeed inverse to each other.</p> <p>many formula for $q$-logarithms reminds us of that for the regular $q$-logarithms.</p> \[\begin{align*} \ln_ {q}(xy) &amp;= \ln_ {q}(x)+ \ln_ {q}(y) + (1-q)\ln_ {q}(x)\ln_ {q}(x) , \\ \ln_ {q}(1+x) &amp;= \sum_ {1}^{\infty} (-1)^{n-1} \frac{(q)_ {n}}{n!} x^{n}, \\ (q)_ {n} &amp;= \frac{\Gamma(q+k)}{\Gamma(q)} = q(q+1)\cdots(q+k-1), \\ \ln_ {q}\prod_ {k=1}^{n}x_ {k} &amp;= \sum_ {k=1}^{n} (1-q)^{k-1}\sum_ {i_ {k} &gt;\cdots&gt;i_ {1}=1}^{n} \ln_ {q}x_ {i_ {1}}\cdots\ln_ {q}x_ {i_ {k}}. \end{align*}\] <p>We also have</p> \[\begin{align*} \ln_ {q} x &amp;= x^{1-q}\ln_ {2-q}x , \\ q \ln_ {1-q}x^{a} &amp;= a \ln_ {1-a} x^{q} . \end{align*}\] <p>Regarding the $q$-exponentials,</p> \[\begin{align*} \left( e_ {q}^{f(x)} \right) ^{a} &amp;= e^{ af(x) }_ {1-(1-q)/a},\\ \frac{d}{dx} e_ {q}^{f(x)} &amp;= (e_ {q}^{f(x)})^{q} \times f'(x) \end{align*}\] <p>For more details, refer to the textbook by Tsallis himself and <a href="https://doi.org/10.1016/S0378-4371(01)00567-2" title="Persistent link using digital object identifier">https://doi.org/10.1016/S0378-4371(01)00567-2</a>.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="tsallis"/><summary type="html"><![CDATA[1. Introduction 1.1. The Basics of Boltzmann-Gibbs Extensive Entropy 1.2. Generalization to non-Extensive entropy 2. Boltzmann-Gibbs Statistical Mechanics 2.1. Three different forms of BG entropy 2.2. Properties of BG entropy 2.3. Constraints and Entropy Optimization 3. Nonextensive Statistical Mechanics 3.1. Mean Value in Tsallis Statistics 4. Tsallis in Logistic Regression Methods 4.1 Traditional logistic regression method With Tsallis statistics Appendix. Useful Mathematical Formulae]]></summary></entry><entry><title type="html">Note on Classical Lumps and Their Quantum Descendants</title><link href="https://baiyangzhang.github.io/blog/2024/Classical-Lumps-and-Their-Quantum-Descendants/" rel="alternate" type="text/html" title="Note on Classical Lumps and Their Quantum Descendants"/><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Classical-Lumps-and-Their-Quantum-Descendants</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Classical-Lumps-and-Their-Quantum-Descendants/"><![CDATA[<ul> <li><a href="#1-classical-lumps">1. Classical lumps</a> <ul> <li><a href="#11-solitonic-waves">1.1. Solitonic waves</a> <ul> <li><a href="#111-some-time-independent-example-in-one-space-dimension">1.1.1. Some time-independent example in one space dimension</a></li> </ul> </li> <li><a href="#12-with-gauge-field">1.2. With gauge field</a></li> </ul> </li> <li><a href="#2-quantum-lumps">2. Quantum lumps</a> <ul> <li><a href="#21-power-expansion-of-the-time-independent-lumps">2.1. Power expansion of the time-independent lumps</a></li> <li><a href="#22-coherent-state-variational-method-for-time-independent-lumps">2.2. Coherent-state variational method for time-independent lumps</a></li> </ul> </li> </ul> <h1 id="1-classical-lumps">1. Classical lumps</h1> <h2 id="11-solitonic-waves">1.1. Solitonic waves</h2> <p>The waves in real world usually dissipate over time. As the energy propagates to infinite far in the form of waves, the energy density goes to zero uniformly in spacetime. The same is true for the solutions of wave equations, except for some singular solutions at least. We give a more rigorous definition for dissipation: a solution of the classical equations of motion is dissipative if</p> \[\lim_{ t \to \infty } \text{max } \mathcal{E}(\mathbf{x},t)=0,\] <p>where $\mathcal{E}(\mathbf{x},t)$ is the energy density.</p> <p>The interesting thing is that there exists wave functions that allows for non-singular, non-dissipative solutions. In the most striking and simplest case, the solution is time-independent. Somehow, lumps of energy hold themselves together by their own self-interaction.</p> <hr/> <h3 id="111-some-time-independent-example-in-one-space-dimension">1.1.1. Some time-independent example in one space dimension</h3> <p>First some conventions. The metric is taken to be $g = \text{diag}(1,-1,-1,\cdots)$, $i,j, \dots$ denote space dimensions, $a,b, \dots$ denotes internal symmetry.</p> <p>Let us consider the simplest relativistic example: theories of a single scalar field in one space and one time dimension, with non-derivative interactions. The dynamics of such a theory are describe by the Lagrangian density</p> \[\mathcal{L} = \frac{1}{2} \partial _ {\mu}\phi \partial ^{\mu}\phi - U(\phi).\] <p>The energy is given by a Legendre transformation $H = L - p\dot{q}$, we get</p> \[H = \int d x \, \left[ \frac{1}{2}(\partial _ {0}\phi)^{2}+\frac{1}{2} (\partial _ {1}\phi)^{2}+U(\phi) \right].\] <p>We sometimes write the total energy as the sum of kinetic and potential energy,</p> \[E = T+V\] <p>where $T$ is the term quadratic in <em>time derivatives</em>, and $V$ is the term involving no time derivatives.</p> <hr/> <p>There are two models we will consider here. First let’s take a look at the phi-fourth theory. There are two degenerate vacua, corresponding to the two directions of the boundary of the space. The potential energy is</p> \[U = \frac{\lambda}{2} (\phi^{2}-a^{2})^{2}.\] <p>The vacuum configuration is clearly given by $\phi = \pm a$ a constant in spacetime. In a different form $a$ is written as $a = \mu^{2} / \lambda$.</p> <p>Next let’s consider the sine-Gordon potential,</p> \[U = \frac{\alpha}{\beta^{2}} (1-\cos \beta \phi).\] <p>The ground states have infinite degeneracy.</p> <p>We want to find the time-independent solutions of the equations of motion. We need to find the stationary point of the energy function, which is very similar to finding the stationary point of the action, with one difference: the sign of the potential term. Mathematically, looking for the time-independent solution is the same as looking for the motion of a particle of unit mass in a potential equal to <strong>minus</strong> $U(x)$.</p> <p>To emphasize, <strong>we can turn a static problem of finding the time-independent solution that minimizes the Hamiltonian, to a dynamical problem of finding the motion of a particle (of unit mass) in a reversed potential.</strong> The latter allows us to make use of our intuition and imagination, and helps to visualize the solution.</p> <p>Every motion of the particle in the potential corresponds to a time-independent solution of the field equations, but not all of them are of finite energy. For the energy integral to converge, the energy density must go to zero at space infinity. In terms of the particle problem, this boundary conditions fixes the possible initial and final positions.</p> <p>In the example of the phi-fourth theory, the potential takes the form of double-well. The minus version of the potential is of the form double hill. It would be helpful to draw some picture here but I am too lazy, interested readers should just read Coleman’s book. The lump solution corresponds to the motion that the particle starts off from one of the hills and rolls over the valley and reaches the other hill and stays there.</p> <p>By convention, we call the solution for which $\phi$ is monotone increasing <code class="language-plaintext highlighter-rouge">lumps</code> and those for which it is monotone decreasing <code class="language-plaintext highlighter-rouge">antilumps</code>.</p> <p>To find the exact expression of the solution, let’s adopt the dynamic picture of a moving particle. The potential is now $-U(x)$ where $x$ is identified with $\phi$ in the static problem. The total energy is conserved to zero during the whole motion, thus</p> \[\frac{1}{2} \left( \frac{dx}{dt} \right)^{2} - U(x) = 0.\] <p>We can translate this back into field language by substitution $x\to \phi$ and the solution can be read off instantly in the integral form,</p> \[\phi(x) = \pm\int d x \,[2U(\phi(x))]^{1/2}\] <p>or</p> \[x = \pm\int_ {\phi_ {0}}^{\phi} d\phi \, [2U(\phi')]^{- 1/2}.\] <p>Note that the solutions enjoys the translation symmetry, meaning the center of the lump can be translated to anywhere.</p> <p>These equation also enables us to simplify the expression for the energy of a lump, thanks to the fact that a lump satisfies the equation of motion.</p> <p>For phi-fourth theory, we find for the form of the lump</p> \[\phi = a \tanh(\mu x).\] <p>The energy of the lump is given by</p> \[E = \frac{4\mu^{3}}{3\lambda}.\] <p>The lumps of $\phi^{4}$ theory is frequently called <code class="language-plaintext highlighter-rouge">kinks</code> in the literature.</p> <p>For the sine-Gordon theory, we find</p> \[\phi = \frac{4}{\beta} \tan^{-1} \exp(\alpha^{1/2}x)\] <p>where which branch of the inverse tangent we choose determines which two zeros we move between. The energy of the lump is</p> \[E = \frac{8\alpha^{1/2}}{\beta^{2}}.\] <p>The lumps of sine-Gordon theory are frequently called <code class="language-plaintext highlighter-rouge">solitons</code> in the literature. In both cases the antilumps are obtained simply by multiplying by minus one.</p> <p>A few words on the stability of the kink solution. We know that the equation of motion in the kink background has solutions called normal modes, each with a eigen value corresponding to the energy. The kink is stable under fluctuation if all the eigen values are non-negative, otherwise the perturbation in the direction of the negative-eigenvalue normal mode would decrease the energy indefinitely, hence making the kink unstable. Hence, if we can prove that there exists no normal modes with negative eigen values, we can prove that the kink is stable. To do this, we need two ingredients: zero mode and the node theorem. Zero mode is a result of translation symmetry of the model, or rather the breaking of it. Zero mode is a constant solution with no nodes, and the node theorem says that for 1-dimensional Schrodinger equation, the lowest wave function has zero node, the first excitation wave function has one nodes, etc. Then the zero mode is the lowest energy state, with energy zero! Hence there exists no negative eigen value.</p> <p>A kink in $\phi^{4}$ theory can be very well regarded as a almost-localized classical particle, with one major difference: the condition for patching adjacent kink solutions together must be …kink-antikink-kink…, otherwise it would not be a solution to the equation of motion.</p> <h2 id="12-with-gauge-field">1.2. With gauge field</h2> <p>Let $\phi$ be a vector of scalar field with gauge group $G$ and gauge field $A$. The covariant derivative and field strength are defined in the usual way, you can pick your favorite convention. Let $\phi=\phi_ {0}$, $A =0$ be the ground state. Due to the gauge invariance (or more precisely, gauge redundancy), there might exist a subset $H$ of $G$ such that it leaves $\phi=\phi_ {0},A=0$ unchanged. It is usually written as $H&lt;G$, then for any $h\in H$ we have $h\phi_ {0}=\phi_ {0}$, $H$ is the little group of $\phi_ {0}$, $\phi_ {0}$ is the fixed point of $H$. If we study the fluctuation near $\phi_ {0}$, we find that the gauge fields associated with the generators of $H$ remains massless. The vacuum manifold would be the coset space $G/H$, for reasons that I have no time to explain here. This is under the assumption that the vacuum manifold is connected and on which the gauge group acts transitively. This excludes both the case of accidental degeneracy and existence of ordinary (non-gauge) symmetry. Accidental degeneracy refers to a situation where two or more energy levels have the same energy not due to an obvious symmetry of the system, but rather due to a more subtle or hidden symmetry, or as a result of a specific mathematical coincidence, such as the accidental degeneracy in the hydrogen atom. This differs from the usual degeneracy associated with the symmetries of the system’s Hamiltonian, such as those arising from rotational or translational invariance.</p> <p>We can wrap the vacuum manifold, namely the coset space, around the space boundary, this gives a homotopy group. This homotopy group classifies topologically different field configurations the theory admits.</p> <p>The rest of Coleman’s lecture goes to a involved discussion of homotopy groups and short exact sequence, we will not note it here. Instead, we will directly dive into the quantum lumps. But before going there, I can’t resist to say that Sidney Coleman’s illustration of the map</p> \[\pi_ {2}(G) \to \pi_ {1}(H)\] <p>is brilliant! I strongly recommend section 3.7 to everyone. I’ve learnt about homotopy groups from other places, mostly from a pure mathematical perspective, and I’ve learnt how to use the short exact sequence to evaluate the homotopy group, but Coleman’s introduction is more intuitive and straightforward. It serves as a pretty good supplement to short exact sequence argument. Coleman’s section 3.7 can be regarded as a special case of Theorem. 22.27 in Frankel’s textbook.</p> <hr/> <h1 id="2-quantum-lumps">2. Quantum lumps</h1> <h2 id="21-power-expansion-of-the-time-independent-lumps">2.1. Power expansion of the time-independent lumps</h2> <p>The quantization of kink is a fascinating topic. We start from the classical solution and consider the small perturbations about it, order by order.</p> <p>Consider the sine-Gordon Lagrangian:</p> \[\boxed{ \mathcal{L} = \frac{1}{2} (\partial \phi)^{2} + \frac{\alpha}{\beta^{2}} (\cos \beta \phi-1), }\] <p>where under the $\cos$ function, $\beta$ cancels the dimension of $\phi$ and renders $\beta \phi$ dimensionless.</p> <p>We can rescale field such that the parameter $\beta$ factors out of the Lagrangian. Since $\beta \phi$ appears together under $\cos$ function, let’s define</p> \[\widetilde{\phi} := \phi \beta\] <p>then the Lagrangian in term it reads</p> \[\mathcal{L} = \frac{1}{\beta^{2}} \left[ \frac{1}{2}(\partial \widetilde{\phi})^{2}+\alpha(\cos \widetilde{\phi}-1) \right].\] <p>If you write down the equation of motion corresponding to the above Lagrangian, you’ll find that $\beta$ does not enter the equation <em>at all</em>! Since classically all the info is given by the equation of motion, it is safe to say that $\beta$ drops out from the classical theory. Of course when you are calculating Hamiltonian, which is just the Legendre transform of the Lagrangian, $\beta$ appears again, but the dependence is trivial. It is similar to study the motion of a point object in gravitational force only, $\beta$ would be similar to the mass of the object, the mass does not change the motion, only trivially changes the gravitational potential.</p> <p>However when we quantize it, things will be different. Follow the path of path integral, recall that the stuff that decides all the physics is the partition function</p> \[Z = \int D\phi \, \exp \left\lbrace \frac{i}{\hbar} \int \mathcal{L(\phi)} \, \right\rbrace = \int D\phi \, \exp \left\lbrace \frac{i}{\boxed{ \hbar \beta^{2}}} \int \mathcal{L(\widetilde{\phi})} \, \right\rbrace,\] <p>note that $\beta^{2}$ always appears together with $\hbar$, and $\hbar\to 0$ corresponds to the classical limit, so $\beta\to 0$ also corresponds to the classical limit! I will not talk too much about it here for this matter is addressed in more details in my other notes. we just mention that the same argument goes for the phi-4th theory with the replacement $\beta^{2}\to \lambda$. A quote from Coleman:</p> <blockquote> <p>These manipulations are trivial, but they teach us something important: if there are particles in the quantum theory that correspond to classical lumps, they are most likely to resemble their classical ancestors for weakly coupled theories. (Conversely, there is no more reason to trust classical analysis for strongly coupled theories than there is to trust the Born approximation.) This suggests that the most direct way to construct quantum lumps is by an expansion in powers of the coupling constant. The leading term in such an expansion should give the classical results, appropriately reinterpreted in quantum language, and the higher terms should give quantum corrections</p> </blockquote> <p>The canonical momentum (density) associated to $\widetilde{\phi}$ is</p> \[\widetilde{\pi} := \frac{\partial \mathcal{L}}{\partial (\partial_ {0}\widetilde{\phi})} = \frac{\partial_ {0}\widetilde{\phi}}{\beta^{2}},\] <p>and the Hamiltonian is</p> \[\begin{align*} \beta^{2}\mathcal{H}(\tilde{\phi}) &amp;= \widetilde{\pi} \, \partial_ {0}\widetilde{\phi}-\mathcal{L}(\widetilde{\phi}) = \int dx \, \left( \frac{\beta^{4}\widetilde{\pi}^{2}}{2} + V[\widetilde{\phi}] \right) \\ &amp;\equiv \int dx \, \frac{1}{2}(\partial_ {0}\widetilde{\phi})^{2}+\frac{1}{2}(\partial _ {i} \widetilde{\phi})^{2} + V(\widetilde{\phi}) \end{align*}\] <p>Let $\widetilde{f}(x-b)$ be a kink solution, and $\widetilde{E}_ {0}$ be the rescaled classical (leading-order) kink energy,</p> \[\widetilde{E}_ {0} := \beta^{2} V(\widetilde{f}(x)).\] <p>Note that all the quantities with a tilde on it are rescaled quantities, constructed so that they are independent of $\beta$.</p> <p>The peculiar thing with $\beta^{2}\mathcal{H}$ is that, in terms of $\widetilde{\pi}$, the small quantity $\beta$ multiplies the kinetic term, namely $\beta^{4}\widetilde{\pi}^{2}$, while leaving the potential untouched. This is opposite to the Lagrangian with a small coupling! Sidney Coleman compared it with the case in diatomic molecules, where the kinetic term is also perturbatively small, reduced by a factor of $\frac{1}{M}$, where $M$ is the nuclear reduced mass. The same perturbation method is adopted to calculate the correction to energy eigen value and eigen states. The final result is divergent, maybe not surprisingly, since we have not introduced any renormalization techniques yet.</p> <hr/> <h2 id="22-coherent-state-variational-method-for-time-independent-lumps">2.2. Coherent-state variational method for time-independent lumps</h2> <p>Another non-relativistic quantum mechanical method we can use to study the quantum lumps is the variational method, with some trial states. The <code class="language-plaintext highlighter-rouge">Rayleigh-Ritz</code> method is based on the variational principle, which states that for a given Hamiltonian $H$ and a trial wave function $\psi$, the expectation value of the Hamiltonian, $\langle \psi \rvert H \lvert \psi \rangle$, provides an <strong>upper bound</strong> to the ground state energy of the system. The goal is to <em>choose a trial wave function that minimizes this expectation value</em>, which then approximates the ground state energy as closely as possible. To do that, we first choose a set of basis functions $\lbrace \phi_ 1, \phi_ 2, \dots, \phi_ n \rbrace$ that are believed to closely resemble the true wave functions of the system. These can be functions that satisfy boundary conditions or other physical considerations of the problem. Then we construct a trial wave function $\psi$ as a linear combination of these basis functions:</p> \[\psi = \sum_ {i=1}^n c_ i \phi_ i\] <p>where $c_ i$ are coefficients to be determined. We can calculate the matrix elements of the Hamiltonian in the basis of the chosen functions. Each element $H_ {ij}$ of the matrix is given by:</p> \[H_ {ij} = \langle \phi_ i \rvert H \lvert \phi_ j \rangle\] <p>The variational principle leads to the matrix equation</p> \[H\mathbf{c} = E\mathbf{c}\] <p>where $H,E$ are now regarded as matrices, $\mathbf{c}$ is the vector of coefficients $c_i$, and $E$ is the energy eigenvalue. Solving this eigenvalue problem gives us the approximate energy levels and the corresponding coefficients for the wave functions.</p> <p>As a quick example, consider a particle in a one-dimensional box of length $L$, with infinitely high walls at $x = 0$ and $x = L$. The Hamiltonian for this system is</p> \[H = -\frac{\hbar^2}{2m} \frac{d^2}{dx^2}\] <p>with boundary conditions $\psi(0) = \psi(L) = 0$.</p> <p>We choose sine functions that satisfy the boundary conditions as basis functions:</p> \[\phi_ n(x) = \sqrt{\frac{2}{L}} \sin\left(\frac{n\pi x}{L}\right), \quad n = 1, 2, 3, \dots\] <p>We could start with a simple trial function using the first two basis functions</p> \[\psi(x) = c_ 1 \phi_ 1(x) + c_ 2 \phi_2(x)\] <p>to calculate elements $H_{ij}$</p> \[H_ {ij} = \frac{\hbar^2}{2m} \left(\frac{n\pi}{L}\right)^2 \delta_ {ij}\] <p>For this simplified example, since the Hamiltonian matrix is diagonal (due to orthogonality and the properties of sine functions), the energy eigenvalues are directly given by:</p> \[E_ n = \frac{\hbar^2}{2m} \left(\frac{n\pi}{L}\right)^2\] <hr/> <p>Coleman talked about one advantage of 1+1 dimensional scalar theory: it is <code class="language-plaintext highlighter-rouge">locally Fock</code>, meaning that when put in a finite box, the exact energy eigenstates are in ordinary Fock space, and this is connected to the mild UV divergence in 2D. How come? If this were true then this indeed facilitates the variational method, since all the trial states are also in Fock space.</p> <p>Another advantage is that in 2D we can write the Hamiltonian in terms of finite parameters (renormalized parameters) alone, in closed form. This avoids us writing the final results in bare parameters.</p> <p>Consider the Hamiltonian (with canonical fields $\phi$, nor rescaled field $\widetilde{\phi}$)</p> \[\mathcal{H} = \frac{1}{2} \pi^{2} + \frac{1}{2}(\partial_ {x}\phi)^{2} + U(\phi),\] <p>There is only one type of divergence: the diagrams with one loop and one internal line (propagator) only. Thus we need only one condition to cancel it: the normal ordering of the Hamiltonian.</p> <p>We will go on with our calculation of variational method in the Schrodinger picture, where the wave functions are time dependent while the operator are not. In Schrodinger picture the Hamiltonian is not given in $\phi$ and $\dot{\phi}$, but $\phi$ and $\pi$, there is no such thing as $\dot{\phi}$.</p> <p>We are all familiar with the time ordering in interaction picture, however here we are talking about normal ordering in Schrodinger picture. The generalization is trivial: we just forget about the time parameter $t$ in interaction picture. We <strong>define</strong> the creation and annihilation part of $\phi$ and $\pi$ as if they are free operators, since the interaction evolves the states not the operators:</p> \[\phi^{\pm }(x) := \frac{1}{2} \left[ \phi(x) \mp \frac{i}{\sqrt{ -\nabla^{2}+m^{2} }}\pi(x) \right]\] <p>and</p> \[\pi^{\pm }(x) := \mp i\sqrt{ -\nabla^{2}+m^{2} }\phi^{\pm }(x),\] <p>where $\phi^{+}$ is the creation part and $\phi^{-}$ the annihilation part, the same for $\pi$. We then define the normal-ordered version of any function of these operators as the function rearranged with all the creation operators on the left and all the annihilation operators on the right. Here $m$ is to be regarded as a free parameter, we should choose the most convenient value of it, usually the pole mass. Different choice of $m$ represents different choice of renormalization condition.</p> <p>We can use Wick’s theorem to perform the normal ordering. In Coleman’s textbook the normal ordering depends on $m$, but in our work it also depends on which sector we are working in.</p> <p>Take the simplest case for an instructive example. Consider the perversely defined Hamiltonian</p> \[\mathcal{H} = \mathcal{N}_ {m}\left\lbrace \mathcal{H}_ {0} + \frac{1}{2} M^{2}\phi^{2} \right\rbrace ,\] <p>perversely in the sense that the mass parameter of the field is clearly $M$ but we are normal-ordering with respect to $m$, and $m\neq M$. This is a free field model so exactly solvable, and serves as a good test ground for our method.</p> <p>Yet we introduce a third mass-parameter $\mu$, which is used to define the vacuum. Assume that the vacuum is such that annihilated by the field with mass $\mu$,</p> \[\phi^{-}(x,\mu) \left\lvert{0,\mu}\right\rangle = \pi^{-}(x,\mu)\left\lvert{0,\mu}\right\rangle =0.\] <p>Then in order to find the Hamiltonian density, we need to normal-order the Hamiltonian in terms of $\mu$, and a change from $\mathcal{N}_ {m}$ to $\mathcal{N}_ {\mu}$ can be again achieved by Wick’s theorem. We’ll skip the details and just present the final result. Eventually the Hamiltonian will look like</p> \[\mathcal{H} = \mathcal{N}_ {\mu}\left\lbrace \cdots \right\rbrace + f(\mu,M,m)\] <p>where $f(\mu,M,m)$ is a scalar function of $\mu,M$ and $m$, which is the non-zero Hamiltonian density.</p> <p>This completes the first step of the <code class="language-plaintext highlighter-rouge">Rayleigh-Ritz</code> method, the computation of the expectation value of $H$ in the trial state $\left\lvert{0,\mu}\right\rangle$. The next step is to differentiate this with respect to the variational parameter $\mu$, find the minimum. I’ll again skip the details, just mention that it indeed gives the correct answer, saying that the best trial state is $\left\lvert{0,M}\right\rangle$.</p> <hr/> <p>If we adopt exactly the same method to sine-Gordon model, something strange happens: The energy vev of the trial state is not bounded from below for $\beta^{2}&gt;8\pi$. The variational method always gives a rigorous upper bound on the ground-state energy; in this case the upper bound is minus infinity. Thus we conclude that <em>the energy of the sine-Gordon theory is unbounded below if</em> $\beta^{2}&gt;8\pi$.</p> <hr/> <p>We have been working with the vacuum states as the trial state, of course it is not gonna be enough, especially when classic kink background is involved. We need to consider trial states where the vev of $\phi$ is not zero. They are called <code class="language-plaintext highlighter-rouge">coherent states</code>.</p> <p>What are coherent states? Consider the familiar harmonic oscillator,</p> \[H = \frac{1}{2}(p^{2}+q^{2})\] <p>where $q$ is the general coordinate and we have set $\omega,m,\hbar$ to be identity. The ladder operators (raising and lower operators) are constructed as</p> \[\begin{align*} a &amp;= \frac{1}{\sqrt{ 2 }}(q+ip), \\ a^{\dagger} &amp;= \frac{1}{\sqrt{ 2 }}(q-ip). \end{align*}\] <p>A coherent state is labeled by a complex number $z$ and constructed from the ground state as</p> \[\left\lvert{z}\right\rangle = D(z)\left\lvert{0}\right\rangle = e^{ - z^{\ast }z/2 } e^{ za^{^{\dagger}} }\left\lvert{0}\right\rangle = e^{ - z^{\ast }z/2 } \sum_ {n\geq 0} \frac{z^{n}}{\sqrt{ n! }}\left\lvert{n}\right\rangle .\] <p>It is most of all the (normalized) eigen vector of the lower operator,</p> \[a\left\lvert{z}\right\rangle = z\left\lvert{z}\right\rangle ,\quad \left\langle{z}\right\rvert a^{\dagger}=\left\langle{z}\right\rvert z^{\ast }.\] <p>Due to this property, coherent states are extremely convenient to deal with normal-ordered functions, since normal ordering ensures that $a^{\dagger}$ always acts on $\left\langle{z}\right\rvert$ from the right and $a$ acts on $\left\lvert{z}\right\rangle$ from left. Let $f(p,q)$ be a normal ordered function of $p$ and $q$, we have</p> \[\left\langle : f(p,q) : \right\rangle = f(\left\langle p \right\rangle,\left\langle q \right\rangle),\] <p>where $\left\langle p \right\rangle$ is $\left\langle{z}\right\rvert \;p \left\lvert{z}\right\rangle$, the same for $q$.</p> <p>Now we can generalize it to the scalar quantum field theory. In scalar quantum field theory, fields themselves are treated as quantum mechanical operators. In Schrodinger picture one can consider a scalar field $\phi(x)$ is similar to $x$ in the quantum mechanical example above. A coherent state $\lvert\alpha \rangle$ in the field theory context is characterized by:</p> \[a(f) \lvert \alpha \rangle = \alpha(f) \lvert\alpha \rangle\] <p>for all test functions $f$, where $a(f)$ is the annihilation operator <em>smeared with</em> $f$, and $\alpha(f)$ is a function defining the coherent state. The smeared annihilation operator is given by:</p> \[a(f) = \int dx \, f(x) a(x)\] <p>where $a(x)$ is the annihilation operator at position $x$, and it relates to the field operators as:</p> \[a(x) = \sqrt{\frac{1}{2}} \phi(x) + \frac{i}{\sqrt{2}} \pi(x).\] <p>Here, $\omega$ is a characteristic frequency, which can be thought of as a parameter from the dispersion relation of the field.</p> <p>Just like in quantum mechanics, the coherent states in QFT are superpositions of the eigenstates of the field Hamiltonian, which represent different quantum states of the field. Coherent states in QFT are used to describe states of the field that behave in a manner closest to classical fields. This is because they minimize the uncertainty relations between field operators and their conjugate momenta, akin to the minimum uncertainty states of quantum harmonic oscillators.</p> <p>Next we can use the coherent states as trial states. I’ll not go into details here, interested readers can refer to Coleman’s original writings.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="Duality"/><category term="QFT"/><category term="Notes"/><summary type="html"><![CDATA[1. Classical lumps 1.1. Solitonic waves 1.1.1. Some time-independent example in one space dimension 1.2. With gauge field 2. Quantum lumps 2.1. Power expansion of the time-independent lumps 2.2. Coherent-state variational method for time-independent lumps]]></summary></entry><entry><title type="html">Note on The Moral Foundations of Politics</title><link href="https://baiyangzhang.github.io/blog/2024/Note-on-The-Moral-Foundations-of-Politics/" rel="alternate" type="text/html" title="Note on The Moral Foundations of Politics"/><published>2024-05-05T00:00:00+00:00</published><updated>2024-05-05T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Note-on-The-Moral-Foundations-of-Politics</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Note-on-The-Moral-Foundations-of-Politics/"><![CDATA[<h1 id="enlightenment-politics">Enlightenment Politics</h1> <blockquote> <p>f there is a single overarching idea shared in common by adherents to different strands of Enlightenment thinking, it is faith in the power of human reason to understand the true nature of our circumstances and ourselves. Human improvement is measured by the yardstick of <strong>individual rights</strong> that embody, and protect, <strong>human freedom</strong>.</p> </blockquote> <blockquote> <p>Descartes announced that he was in search of propositions that are impossible to doubt. His famous example, known as the <code class="language-plaintext highlighter-rouge">cogito</code>, was ‘‘I think, therefore I am.’’</p> </blockquote> <blockquote> <p>Immanuel Kant defined in <code class="language-plaintext highlighter-rouge">The Critique of Pure Reason</code> (1781), of placing knowledge ‘‘on the secure path of a science.’’</p> </blockquote> <blockquote> <p>These developments in philosophy reflected and reinforced the emergence of modern scientific consciousness.</p> </blockquote> <p>Such ideas, as necessary conditions for the development of natural science (not merely technology), seems to never had appeared in China. Year 1781 is the year 乾隆四十六年 in China, one of the most closed, ignorant, and autocratic era in history.</p> <blockquote> <p>During the seventeenth and eighteenth centuries, when the hallmark of scientific knowledge was indubitable certainty, ethics, political philosophy, and the human sciences were regarded as superior to the natural sciences. This view seems strange from the vantage point of the twenty-first century, when fields like physics, chemistry, astronomy, geology, and biology have all advanced with astonishing speed to discoveries that would have been unimaginable in the eighteenth century.</p> </blockquote> <h2 id="the-workmanship-ideal-of-knowledge">The Workmanship Ideal of Knowledge</h2> <blockquote> <p>The first distinctive feature of the early Enlightenment concerns the range of <code class="language-plaintext highlighter-rouge">a priori knowledge</code>, the kind of knowledge that either follows from definitions or is otherwise deduced from covering principals. This is the kind of knowledge Descartes had in mind when he formulated his cogito and that Kant located in the realm of ‘‘analytic judgments.’</p> </blockquote> <p><strong>Epistemology</strong> is a branch of philosophy that studies the nature, origin, and limits of human knowledge. The term comes from the Greek words “episteme,” meaning knowledge or understanding, and “logos,” meaning study or discourse. Epistemology addresses questions such as:</p> <ul> <li>What is knowledge?</li> <li>How is knowledge acquired?</li> <li>What do people know?</li> <li>How do we know what we know?</li> <li>What are the limits of human knowledge?</li> <li>What makes beliefs justified or rational?</li> </ul> <p>In exploring these questions, epistemology deals with the definition of knowledge and its scope and limits. It often involves debating between different theories of knowledge, such as empiricism (the idea that knowledge comes primarily from sensory experience), rationalism (the idea that reason is the main source of knowledge), and constructivism (the idea that knowledge is constructed by individuals through their interactions with the world).</p> <p>Immanuel Kant distinguished between two types of judgments: <code class="language-plaintext highlighter-rouge">analytic</code> and <code class="language-plaintext highlighter-rouge">synthetic</code>. These distinctions are central to his philosophy, especially in his work “Critique of Pure Reason.”</p> <ol> <li> <p><strong>Analytic Judgments</strong>: An analytic judgment is one where the predicate (the part of the sentence that says something about the subject) is contained within the subject itself. The truth of an analytic judgment is derived from the meanings of the words involved and logical reasoning. They are tautological in nature and do not add any new information about the world. For example, the statement “All bachelors are unmarried” is analytic because the predicate “unmarried” is part of the definition of the subject “bachelor.”</p> </li> <li> <p><strong>Synthetic Judgments</strong>: A synthetic judgment, on the other hand, is one where the predicate adds something to the subject that is not contained within it. The truth of a synthetic judgment is determined through how our concepts relate to the world and cannot be known just by understanding the meanings of the words. They require empirical investigation or intuition. For instance, “The cat is on the mat” is a synthetic judgment because the concept of “the cat” does not inherently include the concept of “being on the mat.”</p> </li> </ol> <p>Kant’s distinction between analytic and synthetic judgments is fundamental to his epistemology, particularly in addressing the question of how human beings can have knowledge about the world. He further introduced the concept of “synthetic a priori” judgments, which are synthetic judgments that are known independently of experience (a priori), like mathematical truths.</p> <hr/> <p>The <code class="language-plaintext highlighter-rouge">creationist</code> or <code class="language-plaintext highlighter-rouge">workmanship</code> theory in political science, often associated with the work of John Locke, is a theory of political obligation. It suggests that political authority and legitimacy derive from the consent of the governed, likening the role of the government or ruler to that of a craftsman or creator who constructs a system with the consent and for the benefit of the people.</p> <p>This theory is rooted in the idea that political and social structures are artificial constructs, made by human beings, unlike natural phenomena. The “creationist” aspect implies that political structures are deliberately created or constructed, rather than organically evolved. The “workmanship” aspect emphasizes the idea that the creators or rulers of these structures have a responsibility to the people they govern, similar to how a craftsman is responsible for the quality and function of their creation.</p> <p>Locke’s theory was revolutionary at its time because it challenged the prevailing notion of the divine right of kings, suggesting instead that political authority is justified only when it serves the interests of the governed and respects their rights. This theory laid the groundwork for modern concepts of democracy, individual rights, and the social contract.</p> <hr/> <p>Thomas Hobbes and John Locke, two prominent philosophers, had distinct views on natural law, reflecting their differing perspectives on human nature and the ideal structure of society.</p> <p>Hobbes, in his work “Leviathan,” presented a rather pessimistic view of human nature. He believed that in the state of nature (a hypothetical condition without government or laws), humans are driven by self-interest and a desire for self-preservation, leading to a “war of all against all” (bellum omnium contra omnes). In this state, life would be “solitary, poor, nasty, brutish, and short.”</p> <p>For Hobbes, natural law is a set of precepts or general rules, <em>discovered by reason</em>, which prohibit anything destructive to one’s own life. <em>It’s based on the right of every individual to preserve their own life</em>, leading to the conclusion that humans should seek peace. This is where his famous concept of the social contract comes into play: individuals surrender some of their freedoms and submit to the authority of a ruler (or a ruling assembly) to ensure their own safety and peace. Thus, Hobbes’s natural law is fundamentally about self-preservation and the avoidance of harm to others as a means of securing one’s own safety.</p> <p>Locke’s view, as articulated in “Two Treatises of Government,” is more optimistic about human nature. He believed that in the state of nature, humans live in a state of equality and freedom, not inherently prone to violence or war. For Locke, the <em>law of nature is a moral guide based on the belief that God has given the world to all people in common</em>. It teaches that, since all are equal and independent, no one ought to harm another in their life, health, liberty, or possessions.</p> <p>Locke’s natural law is grounded in the rights to life, liberty, and property. It includes the idea that people have the obligation to respect the rights of others. His social contract theory suggests that people form governments to protect these natural rights. If a government fails to do so, citizens have the right to overthrow it. This view laid the groundwork for modern democracy and significantly influenced the development of political philosophy in the Western world.</p> <p>So, Hobbes saw natural law as a means of avoiding the brutal state of nature through self-preservation and peace, whereas Locke viewed natural law as a moral guide ensuring equality and the inherent rights of life, liberty, and property.</p> <hr/> <blockquote> <p>A basic issue for Locke and many of his contemporaries was the ontological status of natural law and in particular its relation to God’s will.</p> </blockquote> <p>In this sentence, “ontological status” refers to the fundamental nature or essence of natural law, especially in relation to its existence and its relationship to God’s will. Ontology, in philosophy, is the study of being or existence, and it deals with questions concerning what entities exist or can be said to exist, and how such entities can be grouped, related within a hierarchy, and subdivided according to similarities and differences.</p> <p>So, when discussing the “ontological status of natural law” in the context of John Locke and his contemporaries, the focus is on understanding the very essence of natural law: whether it exists as an objective reality independent of human beings, how it relates to or derives from God’s will, and what its fundamental characteristics are. This was a central topic in the philosophical and theological debates of that era, particularly in the context of determining the basis and legitimacy of moral and legal principles. Locke and many others were engaged in trying to understand whether natural laws were inherent aspects of the universe, ordained by God, or whether they were constructs of human reason and society.</p> <p>“Will-centered” refers to the philosophical position known as voluntarism. This is a theory that emphasizes the role of the will, either divine or human, in various philosophical contexts. In the context of Locke’s moral and political writings, being “will-centered” or a voluntarist means that Locke ultimately leaned towards the view that natural law and moral principles are determined by the will, particularly the will of God, rather than being inherent or objective truths that exist independently of any will.</p> <p>In Locke’s time, the debate about the nature of natural law often centered around whether natural laws were intrinsic to the universe (a position known as intellectualism or rationalism) or whether they were decrees of God’s will (voluntarism). A will-centered or voluntarist approach suggests that moral and legal norms derive their authority from an act of will, particularly the divine will, rather than from reason alone or from the inherent nature of reality. In this view, what is right or wrong, just or unjust, is so because God wills it to be that way, and human beings understand and follow these laws through revelation, religious teachings, or other means of discerning God’s will.</p> <blockquote> <p>Locke distinguished “ectype”’ from “archetype” ideas: ectypes are general ideas of substances, and archetypes are ideas constructed by man.</p> </blockquote> <p>John Locke’s distinction between “ectype” and “archetype” ideas is a crucial aspect of his epistemological theory, which he discusses in his work “An Essay Concerning Human Understanding.” This distinction is part of his broader inquiry into the nature of human knowledge and understanding.</p> <p>In Locke’s philosophy, archetypes are the original models or patterns from which copies are made. They are the fundamental, primary ideas that exist in the mind of God or, in a more secular interpretation, the perfect, abstract forms of things. When Locke refers to archetypes as ideas constructed by man, he means that these are the ideal standards or criteria we hold in our minds for categorizing and understanding the world. They represent our understanding of what the essential characteristics of a particular thing are.</p> <p>For instance, the archetype of a tree would be the idealized concept or mental representation of what a tree is supposed to be. This archetype is not derived from any particular tree but is a kind of composite or abstracted idea of “treeness” that we use to recognize and categorize individual trees.</p> <p>Ectypes, on the other hand, are derivative or secondary ideas. They are the imperfect copies or generalizations that we derive from our experience with individual instances in the world. Ectype ideas are more about the general ideas of substances we form based on our sensory experiences and observations. When we see many individual trees, for example, we form a general idea of what a tree is - this is an ectype. It’s a more practical, experiential idea based on the aggregation of real-world instances.</p> <p>In summary, Locke’s distinction between archetype and ectype ideas can be understood as a differentiation between the idealized, abstract concepts we hold in our minds as standards (archetypes) and the more practical, general ideas we form based on our sensory experience of the world (ectypes). Archetypes are about the essence or ideal form of things, while ectypes are about the general, often imperfect, concepts we derive from actual experiences.</p> <h2 id="the-preoccupation-with-certainty">The Preoccupation with Certainty</h2> <blockquote> <p>The post-Humean Enlightenment tradition has been marked by a fallibilist view of knowledge. All knowledge claims are fallible on this account, and science advances not by making knowledge more certain but by producing more knowledge. Recognizing the corrigibility of all knowledge claims and the possibility that one might always be wrong exemplifies the modern scientific attitude. As Karl Popper (1902-1994) noted, the most that we can say, when hypotheses survive empirical tests, is that they have not been falsified so that we can accept them provisionally.</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">Value judgments</code> are statements or opinions that express an evaluation, typically of something’s worth, beauty, goodness, or morality. Examples include statements like “Lying is wrong,” or “This painting is beautiful.” A.J. Ayer was a key figure in the logical positivist movement, which held that for a statement to be meaningful, it must be either empirically verifiable (i.e., testable by observation or experiment) or analytically true (true by definition, like mathematical or logical statements). In logical positivism, a <code class="language-plaintext highlighter-rouge">proposition</code> is a statement that can be either true or false. It’s a claim about the world that can, <em>at least in principle</em>, be tested and verified or falsified.</p> <p>The Logical Positivist movement, also known as Logical Empiricism, was a philosophical movement that emerged in the early 20th century. It primarily revolved around a group of philosophers associated with the Vienna Circle (<code class="language-plaintext highlighter-rouge">Moritz Schlick</code>, <code class="language-plaintext highlighter-rouge">Hans Hahn</code>, ), along with others like A.J. Ayer in Britain. This movement sought to apply the rigor of scientific methodology to philosophy, with a significant focus on the analysis of language and the verification of statements.</p> <p>Key Features of Logical Positivism include</p> <ol> <li> <p><strong>Verification Principle</strong>: The central tenet of Logical Positivism is the verification principle. This principle asserts that a statement is only meaningful if it can be empirically verified or is analytically true (true by virtue of its meaning, like “All bachelors are unmarried”). The idea was to eliminate metaphysical and abstract discussions that couldn’t be supported by empirical evidence or logical reasoning.</p> </li> <li> <p><strong>Empiricism and Science</strong>: Logical Positivists emphasized the importance of empirical evidence and scientific methods in acquiring knowledge. They viewed science as the model for all true knowledge.</p> </li> <li> <p><strong>Rejection of Metaphysics</strong>: They were critical of metaphysics and other traditional philosophical endeavors, which they saw as meaningless since such statements couldn’t be empirically verified. They believed that many philosophical problems arose from misunderstandings of language and could be resolved by clarifying the language used.</p> </li> <li> <p><strong>Language and Meaning</strong>: A significant focus was placed on the analysis of language, particularly the language of science. They aimed to clarify how language is used in scientific theories and to distinguish between meaningful and meaningless statements.</p> </li> <li> <p><strong>Influence of Wittgenstein</strong>: Although not officially part of the Vienna Circle, Ludwig Wittgenstein’s early work, especially his “Tractatus Logico-Philosophicus,” significantly influenced Logical Positivism. Wittgenstein argued that <em>much of philosophy consists of nonsensical propositions and that the role of philosophy should be to clarify thought and language</em>.</p> </li> <li> <p><strong>Ethical and Aesthetic Statements</strong>: Logical Positivists generally considered ethical and aesthetic statements to be expressions of emotions or subjective preferences, rather than statements that could be true or false.</p> </li> </ol> <p>The “positivism” component is linked to the movement’s commitment to a scientific and empirical approach to knowledge. Positivism, as a philosophical stance, argues that knowledge should be based on positive, observable facts and their logical and mathematical treatment. It rejects introspection and intuition as sources of knowledge and instead emphasizes empirical evidence obtained through observation and experimentation. Logical Positivists extended this approach by asserting that statements must be empirically verifiable (or analytically true) to be meaningful.</p> <hr/> <p>Somewhat to my surprise, Karl Popper is not a member of the Vienna circle even though they shared many intellectual engagements. Furthermore, Karl Popper is even critically oppositional. The Vienna Circle advocated for the verification principle, which held that a statement is meaningful only if it can be empirically <em>verified</em>. Popper challenged this view, proposing <em>falsificationism</em> instead. According to Popper, scientific theories cannot be conclusively verified but can be falsified. He argued that a theory is scientific if it is testable and can potentially be refuted by evidence. This approach places a greater emphasis on the role of empirical refutation rather than verification.</p> <p>Also, Popper was critical of what he called <code class="language-plaintext highlighter-rouge">historicism</code> – the belief that <em>history unfolds according to deterministic laws or principles</em>. He argued that such theories, which <em>were often used to justify authoritarian regimes</em>, are fundamentally flawed. He believed that historicism led to totalitarianism because it promoted the idea that certain individuals or groups had access to inevitable truths about societal development, thus justifying their absolute rule. Popper advocated for what he termed an <code class="language-plaintext highlighter-rouge">open society</code>. An open society, in his view, is characterized by a democratic government, individual freedoms, and a critical attitude towards tradition and authority. It allows for change and improvement through rational and critical discourse, as opposed to the unquestioning acceptance of dogmatic principles.</p> <p>Just as Popper applied the <em>principle of falsifiability</em> to scientific theories, he suggested that political policies should also be subjected to critical scrutiny and should be alterable in the face of new evidence or arguments. He was wary of any political theory or system that claimed to have absolute or final answers.</p> <hr/> <p>According to Ayer, the expression of a value judgment is not a proposition since it can not be judged by right and wrong, the question of truth or falsehood does not here arise.</p> <p>Regarding ethics, Ayer points out that many theorists in ethics tend to treat statements about the causes and characteristics of our ethical feelings as if these statements were definitions of ethical concepts. For example, a theory might claim that an action is good if it promotes happiness. Here, the cause of the ethical feeling (happiness) is used to define the ethical concept (good). Ayer argues that ethical concepts are <em>pseudo-concepts</em>, since ethical concepts, in his view, is neither empirically verifiable or analytically correct.</p> <p>Ayer’s stance is closely associated with <code class="language-plaintext highlighter-rouge">emotivism</code>, a meta-ethical view that suggests <em>ethical statements do not assert propositions but express emotional attitudes</em>. According to emotivism, saying “Stealing is wrong” is akin to expressing one’s disapproval of stealing, rather than making an objective claim about the nature of stealing.</p> <h2 id="the-centrality-of-individual-rights">The Centrality of Individual Rights</h2> <blockquote> <p>In addition to faith in science, the Enlightenment’s central focus on individual rights differentiates its political philosophy from the ancient and medieval commitments to order and hierarchy. This focus brings the freedom of the individual to the center of arguments about politics. This move was signaled in the natural law tradition by a shift in emphasis from the logic of law to the idea of natural right.</p> </blockquote> <p>Hobbes contended that it was customary to conflate “Jus and Lex, law and right”. Yet he made the distinction that right, consisted in liberty to do, or to forbeare, whereas law, determines and binds to one of them. Similarly by Locke.</p> <p>John Locke’s oppinion on natural law is as the following. In his work <em>Essays on the Law of Nature</em>, Locke argues a moral law inherent in the world and discoverable through reason.</p> <p>Key points of Locke’s argument include:</p> <ol> <li> <p><strong>Natural Law and Reason:</strong> Locke posits that natural law is an aspect of the natural world, similar to physical laws. According to him, this <em>moral law can be discovered through the use of reason, without the need for divine revelation</em>.</p> </li> <li> <p><strong>Moral Obligations:</strong> He argues that <em>natural law imposes moral obligations on individuals</em>. These moral principles are universal and apply to all people, regardless of their culture or society.</p> </li> <li> <p><strong>Rights and Duties:</strong> Locke’s view of natural law is closely tied to his ideas about individual rights and duties. He believes that natural law forms the basis for understanding human rights, especially the right to life, liberty, and property.</p> </li> <li> <p><strong>Foundation for Political Theory:</strong> These essays lay the groundwork for Locke’s later political theories, particularly those presented in his famous works, “Two Treatises of Government.” He uses the concept of natural law to argue for the rights of individuals and the limitations of governmental power.</p> </li> <li> <p><strong>Human Equality:</strong> Locke emphasizes the inherent equality of all human beings, derived from their natural state. This idea is a critical aspect of his argument against absolute monarchy and for the formation of governments based on the consent of the governed.</p> </li> <li> <p><strong>Religious Tolerance:</strong> Although not as explicitly developed in these essays as in his later works, Locke’s concept of natural law also leads to his advocacy for religious tolerance, seeing religious belief as a matter of individual conscience.</p> </li> </ol> <p>In summary, Locke’s “Essays on the Law of Nature” propose that there is a moral law inherent in the natural world, understandable through human reason, and that this law underpins human rights and forms the basis for just and ethical governance.</p> <hr/> <p>John Locke’s <code class="language-plaintext highlighter-rouge">voluntarist theology</code> reflects his views on the nature of God and the relationship between divine will and moral law. The emphasis is on <em>the will will (voluntas in Latin, hence the name) of God of God as the primary or sole source of moral law</em>. Locke’s voluntarism posits that <em>moral laws are decrees of God’s will</em>. In this view, what is morally right or wrong is so because God wills it, and not necessarily because it aligns with any intrinsic moral truths or rational principles independent of God’s will. Locke emphasizes the <em>absolute freedom</em> and <em>omnipotence</em> of God. He argues that God’s will is not bound by any external standards or principles. Therefore, moral laws are a product of God’s free choice.</p> <p>While Locke is a proponent of reason and believes that human beings can discover moral truths through rational inquiry, he also upholds the importance of divine revelation. In his voluntarist theology, revelation plays a crucial role in imparting knowledge of God’s will, which might not be entirely accessible through reason alone. Locke’s voluntarism is tied to his rejection of innate ideas, a concept he famously critiques in his “Essay Concerning Human Understanding.” He argues against the notion that <em>moral principles are innately known</em>, instead positing that our understanding of moral laws comes from experience, reason, and revelation. Locke’s voluntarist approach suggests that moral obligations are ultimately grounded in obedience to God’s will. This perspective can lead to a form of ethical subjectivism, where moral truths depend on the decrees of a divine authority.</p> <hr/> <blockquote> <p>In Locke’s formulation, natural law dictates that man is subject to divine imperatives to live in certain ways, but, within the limits set by the law of nature, men can act in a godlike fashion. Man as maker has a maker’s knowledge of his intentional actions, and a natural right to dominion over man’s products. … Provided we do not violate natural law, we stand in the same relation to the objects we create as God stands to us; we own them just as he owns us.</p> </blockquote> <h2 id="tensions-between-science-and-individual-rights">Tensions Between Science and Individual Rights</h2> <p>The two enlightenment values, the preoccupation of science and the commitment to individual rights, seem to be in contradiction with each other. Science is deterministic, concerned with discovering the laws that govern the universe, with human being included. This has potential for conflict with an ethic that emphasizes individual freedom, for now the freedom has to be subjugated to the laws (of nature, of God).</p> <p>In Locke’s theory, the freedom to comprehend natural law by one’s own lights supplied the basis of Locke’s right to resist, which could be invoked against the sovereign. No one is in a higher position to monopolize the right to interpret the scripture.</p> <blockquote> <p>We will see this tension surface repeatedly in the utilitarian, Marxist, and social contract traditions, without ever being fully resolved.</p> </blockquote> <h1 id="classical-utilitarianism">Classical Utilitarianism</h1> <p>Jeremy Bentham famously wrote that</p> <blockquote> <p>Nature has placed mankind under the governance of two sovereign masters, <em>pain</em> and <em>pleasure</em>. It is for them alone to point out what we ought to do, as well as to determine what we shall do. On the one hand the standard of right and wrong, on the other the chain of causes and effects, are fastened to their throne. They govern us in all we do, in all we say, in all we think: every effort we can make to throw off our subjection, will serve but to demonstrate and confirm it. In words a man may pretend to abjure their empire: but in reality he will remain subject to it all the while. The principle of utility recognizes this subjection, and assumes it for the foundation of that system, the object of which is to rear the fabric of felicity by the hands of reason and law. Systems which attempt to question it, deal in sounds instead of senses, in caprice instead of reason, in darkness instead of light.</p> </blockquote> <p>The <code class="language-plaintext highlighter-rouge">principle of utility</code>, as Bentham explains, ‘‘approves or disapproves of every action whatsoever, according to the tendency which it appears to have to augment or diminish the happiness of the party whose interest is in question: or, what is the same thing in other words, to promote or to oppose that happiness.’’</p>]]></content><author><name>Baiyang Zhang</name></author><category term="politics"/><summary type="html"><![CDATA[Enlightenment Politics]]></summary></entry><entry><title type="html">Perturbative Spontaneous Symmetry Breaking</title><link href="https://baiyangzhang.github.io/blog/2024/Perturbative-Spontaneous-Symmetry-Breaking/" rel="alternate" type="text/html" title="Perturbative Spontaneous Symmetry Breaking"/><published>2024-04-08T00:00:00+00:00</published><updated>2024-04-08T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Perturbative-Spontaneous-Symmetry-Breaking</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Perturbative-Spontaneous-Symmetry-Breaking/"><![CDATA[<ul> <li><a href="#1-spontaneous-symmetry-breaking">1. <em>Spontaneous</em> symmetry breaking</a></li> <li><a href="#2-degenerate-vacua-good-and-bad">2. Degenerate Vacua, Good and Bad</a></li> <li><a href="#3-effective-action">3. Effective Action</a> <ul> <li><a href="#31-calculating-the-1-loop-effective-potential">3.1. Calculating the 1-Loop Effective Potential</a></li> </ul> </li> <li><a href="#4-the-physical-meaning-of-the-effective-potential">4. The Physical Meaning of the Effective Potential</a></li> </ul> <p>This note is based on Chapter 44 of <em>Lectures of Sidney Coleman on Quantum Field Theory</em>.</p> <h1 id="1-spontaneous-symmetry-breaking">1. <em>Spontaneous</em> symmetry breaking</h1> <p>This section can be neglected.</p> <p>The word <em>spontaneous</em> used to baffle me. It doesn’t seem so “spontaneous” in the context of quantum field theory, such as $\phi^{4}$ model. At least not spontaneous enough in comparison with that in the Ising model. In Ising model, starting from high temperature, above Curie temperature to be specific, the ferromagnetic system is in a rotational symmetric phase, all the little spins point to random directions. As the temperature drops, at the Curie temperature the system undergoes a second order phase transition, the system picks a direction and all of a sudden, most of the spins are aligned in that directions,as a result the rotational symmetry is broken, without any external manipulation, hence the word spontaneous. To be more specific, take 2D (spatial) Ising model for example, which is famously solved by Lars Onsager. We don’t choose 1D Ising model because in 1D, spontaneous symmetry breaking doesn’t occur due to the lack of a phase transition at finite temperatures, there is a critical dimension under which the free energy is dominated by entropy, so the order phase is not preferred, according to the so-called energy-entropy argument.</p> <hr/> <p>For a 2D lattice with spins (which can take values +1 or -1, representing up or down magnetic moments) located on each site, the Hamiltonian is given by:</p> \[H = -J \sum_ {\langle i, j \rangle} s_ i s_ j - h \sum_ i s_ i\] <p>where $J$ is the exchange interaction energy between neighboring spins. It determines the strength of the interaction and can be positive (favoring alignment) or negative (favoring anti-alignment). The sum $\sum_ {\langle i, j \rangle}$ runs over all nearest-neighbor pairs of lattice sites, ensuring that each pair is counted once. $s_ i$ and $s_ j$ are the spin values at sites $i$ and $j$, respectively. $h$ is the external magnetic field, but for the spontaneous phase transition discussion, we consider $h = 0$.</p> <p>The free energy (Helmholtz free energy, to be specific) $F$ of the system combines the <em>internal energy</em> and the <em>entropy</em>. Classically speaking, the free energy describes the work that can be extracted from a system at a constant temperature, but here the physical meaning is that the equilibrium state is such that minimizes the free energy. The free energy in the context of the Ising model can be expressed as:</p> \[F = -k_ B T \ln(Z)\] <p>where $k_ B$ is the Boltzmann constant, $T$ is the temperature, and $Z$ is the partition function of the system, given by the sum over all possible configurations:</p> \[Z = \sum_ {\left\lbrace s \right\rbrace } e^{-\beta H(\left\lbrace s \right\rbrace )}\] <p>where $\beta = \frac{1}{k_ B T}$ and $H(\left\lbrace s \right\rbrace )$ is the Hamiltonian for a particular configuration $\left\lbrace s \right\rbrace$ of spins.</p> <p>The true ground state must minimize the free energy, the free energy is energy minus temperature times entropy, hence the competition between energy and entropy underlies the phase transition:</p> <ul> <li>At low temperatures, the system tends to minimize its energy, leading to an ordered phase where spins align (ferromagnetism).</li> <li>At high temperatures, the entropy dominates, favoring a disordered state where spins are randomly oriented.</li> </ul> <p>The balance between these two tendencies determines the critical temperature $T_ c$ at which the phase transition occurs.</p> <p>The critical dimension $d_ c$ of a system is the minimum dimension in which a phase transition can occur at a nonzero temperature. For the Ising model, it is known that $d_ c = 1$ for a lower critical dimension, meaning that there is no phase transition at any nonzero temperature in 1D. In 2D, however, the model does exhibit a phase transition, as shown by Onsager’s solution.</p> <p>Onsager’s exact solution for the 2D Ising model on a square lattice without an external field showed that the critical temperature $T_ c$ is given by:</p> \[\sinh\left(\frac{2J}{k_ B T_ c}\right) = 1\] <p>This solution demonstrated that a spontaneous magnetization (order) emerges below $T_ c$ and disappears above $T_ c$, marking a second-order phase transition characterized by a continuous change in the order parameter (magnetization).</p> <p>The calculation of the critical properties, such as the critical exponents that describe how physical quantities diverge near $T_ c$, is more intricate and relies on sophisticated mathematical techniques, including renormalization group analyses, which go beyond the scope of this note.</p> <hr/> <p>In the contest of quantum field theory, spontaneous symmetry breaking occurs when the most stable state (or states) of a system, namely its ground state or vacuum state, does not share the symmetry of the system’s Lagrangian (or Hamiltonian). This can happen even though the governing equations or Lagrangian of the system remain symmetric. The system “chooses” a state that breaks some of the symmetry of the Lagrangian.</p> <p>The Higgs mechanism is a well-known example of spontaneous symmetry breaking in QFT. The potential of the Higgs field is symmetric, resembling a “Mexican hat” or “sombrero,” but the lowest energy state (the ground state) is not at the symmetric center (where the field value is zero) but rather at some nonzero value along the “brim” of the hat. This asymmetric ground state breaks the symmetry of the potential.</p> <p>The “spontaneous” aspect is that there’s no external force or parameter explicitly breaking the symmetry; it’s the intrinsic dynamics of the field settling into a minimum energy state that breaks the symmetry. Spontaneous in the context of QFT is not in the dynamical sense, as it is in Ising model.</p> <h1 id="2-degenerate-vacua-good-and-bad">2. Degenerate Vacua, Good and Bad</h1> <p>In a simple quantum system, you might expect a unique ground state. However, in systems where spontaneous symmetry breaking occurs, there can be multiple degenerate ground states, all having the same energy but different physical configurations.</p> <p>All the vacua together form the manifold of vacuum of the Lagrangian, where the basis are just different vacua states, continuous or not. The thing is, there is more than one set of basis for the vacuum states. Let $\left\lvert{0,\alpha}\right\rangle$ be degenerate vacuum states label by some $\alpha$, then any linear combination, properly normalized, is another vacuum state, call it $\left\lvert{0,\beta}\right\rangle$, then we can use the <em>Gram-Schmidt procedure</em> to construct another set of orthonormal basis. Among the infinite sets of basis, there do exists good sets and bad ones. But before defining what is a good vacuum, we must first define what is a quasi-local operator.</p> <p>Recall that operators correspond to observable quantities or actions that can be performed on the field, and “quasilocal” operators are those whose effects are confined to a limited region of space. Take the scalar field theory for example, let $\phi(x)$ be field operator, they are local since it is defined on a point. A <em>quasilocal operator</em> $A$ is something can be constructed from $\phi$ via</p> \[A = \int d^{d}x \, f(x_ {1},\cdots,x_ {n}) \phi(x_ {1})\cdots\phi(x_ {n})\] <p>where $f(x_ {1},\cdots,x_ {n})$ is a function with finite support (support is the closure of the points where $f$ is nonzero).</p> <p>Now coming back to vacua. There is a basis where all the vacua are <em>globally independent</em>, where one vacua can not be transformed into another using some local (or quasi-local, as Coleman called it) operator. Such a basis is called <strong>good</strong> vacuum states. For a set of good vacua, different vacuum states are not just distinct but are fundamentally separate in the sense that you cannot use a simple, localized operation to move from one to another. By construction, the vev of any quasilocal operators between two distinct good vacua is zero,</p> <p><strong>Theroem 1</strong> There exists a basis for the vacuum states $\left\lvert{0,\alpha}\right\rangle$, so called good vacuum states, such that for any quasilocal operator $A$, we have</p> \[\left\langle{0,\alpha}\right\rvert A\left\lvert{0,\alpha'}\right\rangle =0.\] <p>We’ll neglect the proof of the theorem here, just mention that it involves translational invariance of vacuum states, causality, and some cluster decomposition-ish argument.</p> <p>The significance of the theorem is this: it doesn’t matter if you say there’s one vacuum or many; there are always good vacua. It shows that, even if we don’t know anything about spontaneous symmetry breaking, and we’ve chosen a bad set of vacua, by a systematic constructive procedure we can always find a good choice of bases for the vacuum subspace, such that no local operator can connect one vacuum to another.</p> <hr/> <p>In the following we will need to make use of quantum effective action $\Gamma[\varphi]$, for details on this topic see my other note <a href="https://www.mathlimbo.net/blog/2024/Coleman-Weinberg-Potential/">here</a>.</p> <p>As explained by Coleman,</p> <blockquote> <p>…(using perturbation theory), but with the effective action $\Gamma[\overline{\phi}]$ substituted for the classical action $S[\phi]$. Instead of trying to find minima by finding the stationary points of the classical action, I <strong>find ground states by looking at the stationary points of the effective action</strong>; instead of finding effective coupling constants and masses by expanding about the minima of the classical action, I <strong>find 1PI Green’s functions by expanding about the minima of the effective action</strong>. It’s exactly the same game in the quantum and classical theories.</p> </blockquote> <p>Sometimes Coleman talks about classical vev of $\phi$ and quantum vev of $\phi$. By his definition, the classical vev of $\phi$, i.e. $\left\langle \phi \right\rangle$ classical, usually denoted as just $\left\langle \phi \right\rangle$, is the vev of $\phi$ without loop corrections, the quantum one, usually denoted as $\overline{\phi}$, is that with loop corrections. In any cases, $\left\langle \phi \right\rangle$ is supposedly a constant in spacetime. This means that both classical and quantum vacuum preserves the translational symmetry.</p> <p>In solitonic quantum field theory we constantly deal with ground states that are not translations symmetric. Regarding the possibility of SSB of spontaneous translational symmetry breaking, Coleman comment</p> <blockquote> <p>There’s no reason why translation invariance should not be spontaneously broken in a theory that describes the real world. It occurs in statistical mechanics, for example, where the phenomenon is called <code class="language-plaintext highlighter-rouge">crystallization</code>. There, <em>instead of changing the square of the mass to cause the manifest symmetry to break spontaneously, one changes the temperature</em>. Let’s take a typical material such as iron, and imagine an iron universe, spatially infinite. If the temperature is above a certain point, the ground state (in the sense of statistical mechanics) is spatially homogeneous; it’s iron vapor. We lower the temperature below the freezing point of iron, and the ground state becomes an infinite iron crystal, which does not have spatial homogeneity. If we now consider the rotation of a crystal somewhere in the frozen iron, how it rotates depends on its position relative to a central lattice point. That’s an example of spontaneous symmetry breakdown of translational invariance.</p> </blockquote> <p>Note that <strong>spontaneous symmetry breaking does not affect the renormalization</strong>, SSB is essentially a shift of field, has nothing to do with regularization and renormalization. A SSB model has exactly the same counter terms as the original one. Thus we could renormalize first and shift the field later. This is useful in $\phi^{4}$ model with spontaneous symmetry breaking, since the original theory has no $\phi^{3}$ terms but the symmetry broken theory has, so naturally one could ask, do we need a counter term for $\phi^{3}$ interaction, like Mark Srednicki did in his textbook? The answer is no since such a counter term is not needed in the original theory. Then what about the divergence introduced by $\phi^{3}$ theory? Well, if we had done everything correctly, it should be taken care of by itself, by some destined cancellation. Like Coleman said,</p> <blockquote> <p>After we do the shift, of course, a $\phi^{3}$ interaction will appear in the effective action, but we still don’t need a $\phi^{3}$ counterterm, because we’ve already gotten rid of all infinities in computing $\Gamma$ before we’ve made the shift. The shift is a purely algebraic operation without a single integration over internal momenta, and therefore cannot possibly introduce new ultraviolet infinities.</p> </blockquote> <p>Say we are doing renormalization in the original theory. What are the renormalization conditions? We have MS, $\overline{\text{MS}}$, Pauli-Villas, mass-shell, lattice, Momentum Subtraction, etc. It is not a good idea to adopt the mass shell renormalization since in the original theory $m^{2}&lt;0$. Then we could choose a tentative renormalization condition, then adjust it in the kink sector so that the renormalized parameters are physical. The vev of field $\overline{\phi}$ will depend on the renormalization condition, but different condition will describe the same physics.</p> <p>Recall that the effective action $\Gamma[\overline{\phi}]$ is made of 1PI diagrams, where $\overline{\phi}(x)$ itself serves as the external legs, just like the source term $J(x)$ with classical action $S$. We have assumed the $\overline{\phi}$ is a const in spacetime, which translates to zero momentum after we Fourier transform it to the momentum space. So we only need to consider the case where the external legs has zero momenta. Next let’s dive into calculation.</p> <hr/> <h1 id="3-effective-action">3. Effective Action</h1> <p>Let $V(\overline{\phi})$ be the <code class="language-plaintext highlighter-rouge">effective action</code> of $\overline{\phi}$, which if you recall is the quantum vev of $\phi$. For the details of effective action see my other note mentioned at the beginning of this note, here we only present the definition,</p> \[\Gamma[\overline{\phi}] =: -V(\overline{\phi}) \cdot \text{Vol}^{d},\] <p>where $\text{Vol}^{d}$ is the total space of the $d$-dimensional spacetime manifold.</p> <p>The connection between $\Gamma[\overline{\phi}]$ and $S[\overline{\phi}]$ is that, at tree level, that is if you forget about quantum corrections, $\Gamma$ is equal go $S$. When the quantum corrections are included, since $\Gamma$ is exact (incorporates all the quantum effects in path integral) at tree level, we have</p> \[Z[J] = \lim_ { \hbar \to 0 } \int \mathcal{D}\phi \, \exp \left\lbrace \frac{i}{\hbar}\left( \Gamma[\phi]+\int J\phi \right) \right\rbrace = \exp \left\lbrace i\Gamma[\overline{\phi}]+\int dJ\overline{\phi} \, \right\rbrace ,\] <p>it is understood that the last equality is up to multiplicative constants. Also note that in the last expression it is not just any $\phi$, but $\overline{\phi}$ that satisfies certain functional equation, which is exactly the $\overline{\phi}$ we have been using.</p> <p>However we also have</p> \[Z[J] = \int \mathcal{D}\phi \, \exp \left\lbrace \frac{i}{\hbar}\left( S[\phi] +\int J\phi \right) \right\rbrace = \exp \left\lbrace iS[\left\langle \phi \right\rangle ] + \text{loops} \right\rbrace\] <p>where loops are a result of the fluctuations about the classical field configuration $\left\langle \phi \right\rangle$. Thus we have</p> \[\Gamma[\overline{\phi}] = S[\left\langle \phi \right\rangle] + \text{loops}= S[\overline{\phi}] + \text{loops},\] <p>since a swap between $\overline{\phi}$ and $\left\langle \phi \right\rangle$ introduces loop corrections only. But so far let’s stick with $S[\left\langle \phi \right\rangle]$ instead of $S[\overline{\phi}]$, since $\left\langle \phi \right\rangle$ is the quantum vev thus unknown, while $\left\langle \phi \right\rangle$ is the classical vev thus easily known, one just need to solve for the equation of motion.</p> <p>Write the action in terms of the Lagrangian we have</p> \[\Gamma[\overline{\phi}] = \int d^{d}x \, \mathcal{L}(\left\langle \phi \right\rangle ) + \text{loops}.\] <p>We have</p> \[\mathcal{L} = \frac{1}{2} (\partial \phi)^{2} - U(\phi) + \mathcal{L}_ {\text{ct}},\] <p>where $U$ is the classical potential and $\mathcal{L}_ {\text{ct}}$ are the counter terms. Then, since $\overline{\phi}$ is constant we have</p> \[V(\overline{\phi}) = U(\left\langle \phi \right\rangle ) + \text{loops},\] <p>where loop contribution is of form $\hbar(\text{1-loops})+h^{2}(\text{2-loops})+\cdots$. Note that $V(\overline{\phi})$ is not a functional but rather a function of $\overline{\phi}$, it is the negative of the constant density of $\Gamma[\overline{\phi}]$. Since $\Gamma$ generates all the 1PI diagrams, $V(\overline{\phi})$ represents the collection of all the 1PI diagrams with all the external momenta equal to zero and with the $(2\pi)^{d}\delta^{d}(0)$ from overall energy-momentum conservation divided out. That’s just the Fourier space equivalent of the integral $\int d^dx$.</p> <p>As Coleman explained in his lecture notes,</p> <blockquote> <p>The rule for computing $V[\overline{\phi}]$ is very simple. You don’t have to worry about any external momentum. You just have external lines each carrying zero momentum. Sum up all those graphs to one loop or two loops or however many loops you’re going to do.</p> </blockquote> <h2 id="31-calculating-the-1-loop-effective-potential">3.1. Calculating the 1-Loop Effective Potential</h2> <p>With a general renormalizable potential, we start with the Lagrangian that reads</p> \[\mathcal{L} = \frac{1}{2} (\partial \phi)^{2} -U(\phi)+\mathcal{L}_ {\text{ct}}.\] <p>If there is a mass term it goes to $U$. As a result, the propagator is</p> \[\frac{i}{k^{2}+i\epsilon}.\] <p>Recall that calculating $\Gamma[\overline{\phi}]$ means calculating the 1PI diagrams with external legs amputated and replaced by a factor of $\overline{\phi}$. Why are they amputated? We know that when calculating the S-matrix the external legs are also truncated due to the LSZ theorem, but here there is no LSZ theorem so why does it happen? The reason is mostly that 1PI diagrams are used to represent the interaction vertices or the “effective vertices” in the theory, rather than full scattering processes. In the context of effective theory you usually don’t hear things like asymptotic states, scattering matrix, things that you must discuss when talking about S-matrix. The 1PI diagrams contribute to the n-point functions, which are essentially the building blocks of the full scattering amplitudes. These vertex functions describe how particles interact at a point, disregarding the propagation of particles to and from this point. In the renormalization process, 1PI diagrams are essential because they contain the divergences that need to be renormalized. The external propagators do not need to be renormalized in the same way, so they are not included in the 1PI diagrams. The renormalization of the theory focuses on the interactions themselves, which are represented by the 1PI diagrams without the external propagators. For details please refer to note <a href="https://www.mathlimbo.net/blog/2024/Coleman-Weinberg-Potential/">here</a>, at the paragraph before Eq. (15).</p> <p>From the functional Taylor expansion of $i\Gamma[\varphi]$ in momentum space,</p> \[\begin{align*} i\Gamma[\varphi] &amp;= \sum_ {n} \frac{1}{n!} \int \frac{dk_ {1}^{d}}{(2\pi)^{d}} \cdots \frac{dk_ {n}^{d}}{(2\pi)^{d}}\, \tilde{\varphi}(-k_ {1})\cdots \tilde{\varphi}(-k_ {n} )\tilde{\Gamma}^{(n)}(k_ {1},\cdots, k_ {n} )\\ &amp;\;\;\;\; \times (2\pi)^{d}\delta^{d}(k_ {1}+\cdots +k_ {n} ), \end{align*}\] <p>we can expand $\tilde{\Gamma}^{(n)}(k_ {1},\cdots,k_ {n})$ in terms of $k$’s, while keeping the $\tilde{\varphi}$’s untouched, the reason why we don’t expand $\tilde{\varphi}$ is purely technical, because an expansion in $\Gamma^{(n)}$ suffices. If we do that, at the leading order where $k=0$ for all $k$, then we get</p> \[i\Gamma[\varphi] = \sum_ {n} \frac{1}{n!} \int d^{d}x \, \tilde{\Gamma}^{(n)}(0,\cdots ,0) \varphi^{n}(x) + \mathcal{O}(k).\] <p>Note that $\varphi(x)$ are functions of $x$, not its Fourier transformed $\tilde{\varphi}$, which is a result from <em>not</em> expanding $\tilde{\varphi}(k)$ in $k$.</p> <p>By the definition of effective action</p> \[\Gamma[\varphi] = -\int d^{d}x \, V_ {\text{eff}}(\overline{\phi}) + \mathcal{O}(\text{derivatives}),\] <p>we have</p> \[\boxed { V_ {\text{eff}}(\overline{\phi}) = i\sum_ {n} \frac{1}{n!} \int d^{d}x \, \tilde{\Gamma}^{(n)}(0,\cdots ,0) \overline{\phi}^{n}(x) . }\] <p>note we have replaced $\varphi$, a generic classical field to $\overline{\phi}$, the vacuum solution we want to expand about. It doesn’t change the logic or the derivation, just replacing the general case with a special case.</p> <p>In $\phi^{4}$ model, at tree level, we have</p> \[\Gamma^{(2)} (0,0) = -i \mu^{2},\quad \Gamma^{(4)}(0,0,0,0) = -i \lambda,\] <p>where $\mu^{2}$ is the mass of the particle and $\lambda$ the coupling. Of course it depends on how you write the Lagrangian, could differ a factor of $3!$ or something like that.</p> <p>To include the 1-loop corrections, we need to consider 1PI diagrams shown in Fig. (1).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/effectiveAction_1Loop-480.webp 480w,/img/effectiveAction_1Loop-800.webp 800w,/img/effectiveAction_1Loop-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/effectiveAction_1Loop.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig.1. One loop contributions to the effective action $\Gamma[\varphi]$ in the $\phi^4$ model. The first diagram contributes to $\Gamma^{2}$, the second $\Gamma^{4}$, etc. Diagrams with odd number of external legs vanish due to the $\mathbb{Z}_2$ symmetry. Credit: Professor Gustavo Burdman. </div> <p>The $2n$-point amplitude shown in Fig. 1 read</p> \[\Gamma^{(2n)} (0,\cdots,0) = \frac{(2n)!}{2^{n}(2n)} \int \frac{d^{d}k}{(2\pi)^{d}} \, \left( \frac{i}{k^{2}+i\epsilon}(-i\lambda) \right)^{n}.\] <p>Some explanation is in order: for $2n$ external legs, there will be $n$ propagators with the same momentum $k$ (since the external legs all carry zero momentum by construction), and there are $n$ vertices. $2^{n}$ is the symmetry factor for each vertex (for the two external legs), $2n$ in the denominator is the symmetry factor for the entire loop, for $n$ rotations and $n$ reflections. There are $(2n)!$ different ways to assign $x_ {1},\cdots,x_ {2n}$ (in coordinate representation of course) to the $2n$ “bulbs”, hence the factor $(2n)!$. If you are confused about this, just think of what we did with $iW[J]$ when using it to generate connected diagrams, where for a diagram with $n$ external legs, there are $n!$ ways to assign it. For more details refer to Chapter 13 in <em>lectures of Sidney Coleman on quantum field theory</em>, here I just quote a short passage from Coleman that is relevant to our discussion:</p> <blockquote> <p>If we imagine restricting ourselves to the case where the first $\rho$ gives up momentum $k_ {1}$, the second gives a momentum $k_ {2}$, etc., then all of our lines are well-defined, and we have no factor of $1/n!$. On the other hand, when we integrate over all $k$’s in this expression, we overcount each those terms $n!$ times, corresponding to the $n!$ permutations of a given set of $k$’s, and therefore we need a $1/n!$ to cancel it out. I know combinatoric arguments are often not clear the first time you hear them, but after a little thought, they become clear.</p> </blockquote> <p>Coleman also emphasized that here we are <strong>not</strong> normal ordering anything. Normal ordering tend to cause some problems, including 1) it is not compatible with gauge transform, 2) it is not compatible with field shift and 3) it sometimes messes up the symmetry.</p> <p>Also note that even we are talking about 1-loop only, there is arbitrary high power of coupling $\lambda$. This is different from what I am used to in calculating loops, where higher power of coupling usually implies higher number of loops. This is not a problem though.</p> <p>We can go beyond $\phi^{4}$ model to a general polynomial potential $U(\phi)$ (recall that $V$ is preserved for quantum potential), then each vertex on the circle would contribute $-iU’’(\phi)$, where prime means the derivative w.r.t. $\phi$ field. Then for loops with $n$ vertices as shown in Fig.1 we have</p> \[\text{circle with }n \text{ vertices} = \frac{1}{(2n)} \int \frac{d^{d}k}{(2\pi)^{d}} \, \left( \frac{U''(\overline{\phi})}{k^{2}+i\epsilon} \right)^{n}.\] <p>To obtain $\Gamma[\overline{\phi}]$ we just need to sum them up together, we have</p> \[\text{loop correction} = \sum_ {n} \frac{i}{2n} \int \frac{d^{d}k}{(2\pi)^{d}} \, \left( \frac{U''(\overline{\phi})}{k^{2}+i\epsilon} \right)^{n}\] <p>which is the Taylor expansion of $-\ln(1-\bullet)$, if we forget about convergence for now,</p> \[\sum_ {n} \frac{i}{2n} \int \frac{d^{d}k}{(2\pi)^{d}} \, \left( \frac{U''(\overline{\phi})}{k^{2}+i\epsilon} \right)^{n} = - \frac{i}{2} \int \frac{d^{d}k}{(2\pi)^{d}} \, \ln\left( 1-\frac{U''(\overline{\phi})}{k^{2}+i\epsilon} \right),\] <p>to proceed we need to go to Euclidean space by performing Wick rotation,</p> \[\text{loops} = \frac{1}{2} \int \frac{d^{d}k_ {E}}{(2\pi)^{d}} \, \ln\left( 1-\frac{U''(\overline{\phi})}{-k_ {E}^{2}+i\epsilon} \right),\] <p>where $d^{d}k_ {E} = dk_ {E} k^{d-1}_ {E } \Omega_ {d-1}$ for spherically symmetric functions, $\Omega_ {d-1}$ is the area of unit $(d-1)$-sphere. We are only interested in what depends on $\overline{\phi}$, not the constant part (w.r.t $\overline{\phi}$), infinite or not.</p> <p>Cut off $k_ {E}$ at $\Lambda$ and discard some additive infinite stuff which we can absorb into the normalization, we get</p> \[\begin{align*} \text{loops} &amp;= \frac{1}{2} \int^{\Lambda}_ {0} \frac{d^{d}k_ {E}}{(2\pi)^{d}} \, \ln\left( 1 +\frac{U''(\overline{\phi})}{k_ {E}^{2}-i\epsilon} \right) \\ &amp;= \frac{1}{2} \int_ {0}^{\Lambda} \frac{d^{d}k_ {E}}{(2\pi)^{d}} \, \ln\left( \frac{k_ {E}^{2}+U''(\overline{\phi})-i\epsilon}{k_ {E}^{2}-i\epsilon} \right) \\ &amp;= \frac{1}{2} \int_ {0}^{\Lambda} \frac{d^{d}k_ {E}}{(2\pi)^{d}} \, \ln(k_ {E}^{2}+U''(\overline{\phi})-i\epsilon) + \text{Const} \\ &amp;= \frac{1}{2(2\pi)^{d}} \int_ {0}^{\Lambda} d^{d}k_ {E} \, k_ {E}^{d-1} \Omega_ {d-1} \ln(k_ {E}^{2}+U''(\overline{\phi})-i\epsilon) \end{align*}\] <p>where in the last step we have carelessly thrown away the constant term. Since</p> \[\Omega_ {d-1} = \frac{2\pi^{d/2}}{\Gamma\left( \frac{d}{2} \right)}, \quad \Gamma\left( \frac{1}{2} \right) = \sqrt{ \pi } ,\] <p>we have</p> \[\text{loops} = \frac{1}{2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)} \int_ {0}^{\Lambda} d k_ {E} \, k_ {E}^{d-1} \ln(k_ {E}^{2}+U''(\overline{\phi})-i\epsilon).\] <p>For now let’s assume $U’’(\overline{\phi})$ is a positive constant, and neglect $-i\epsilon$, for whenever we need it we can always put it after $U’’$. Define two dimensionless new variables to replace $k_ {E}$ and $U’’$:</p> \[\boxed{ u := \frac{k_ {E}}{\Lambda},\quad t := \frac{U''(\overline{\phi})}{\Lambda^{2}}, }\] <p>note that since $U’’(\overline{\phi})$ is supposed to be a constant, <strong>$t$ goes to zero at the $\Lambda\to \infty$ limit, making it possible for as to expand in powers of it</strong>. We have</p> \[\begin{align*} \text{loops} &amp;= \frac{\Lambda^{d}}{2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)} \int_ {0}^{1} d u \, u^{d-1} [2\ln \Lambda+\ln(u^{2}+t)] \\ &amp;= \frac{\Lambda^{d}}{2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)} \left[ \frac{2}{d} \ln \Lambda+\int_ {0}^{1} du \, u^{d-1} \ln(u^{2}+t) \right]. \end{align*}\] <p>Regarding the integral in the line line, if we perform an integral by part first then use Mathematica code</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">Integrate</span><span class="p">[</span><span class="nv">u</span><span class="o">^</span><span class="p">(</span><span class="nv">d</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nv">u</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">t</span><span class="p">)</span><span class="w"> </span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">u</span><span class="o">,</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="m">1</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nb">Assumptions</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="nv">t</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="nv">t</span><span class="w"> </span><span class="nv">\[Element]</span><span class="w"> </span><span class="nb">Reals</span><span class="o">,</span><span class="w"> </span><span class="nv">d</span><span class="w"> </span><span class="nv">\[Element]</span><span class="w"> </span><span class="nb">Integers</span><span class="o">,</span><span class="w"> </span><span class="nv">d</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="p">}]</span><span class="w">
</span></code></pre></div></div> <p>then we get an expression with Gaussian hypergeometric function\brace</p> \[\text{loops} = \frac{\Lambda^{d}}{d\,2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)} \left\lbrace 2 \ln \Lambda + \left[ \ln(1+t)-2 \times \left( _ {2}F_ {1}\left( 1,\frac{2+d}{2}, \frac{4+d}{2},- \frac{1}{t} \right) \right) \right] \right\rbrace.\] <p>So ugly. So I decided to just put the whole integral into Mathematica, I got instead</p> \[\boxed{ \text{loops} = \frac{\Lambda^{d}}{d\,2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)}\left[ 2\ln \Lambda+\ln(1+t)- \frac{1}{t} \Phi\left( -\frac{1}{t},1,1+d/2\right) \right], }\] <p>where $\Phi(-1,1,1+d / 2)$ is the Lerch transcendent function, defined as:</p> \[\Phi(z, s, a) = \sum_ {n=0}^{\infty} \frac{z^n}{(n+a)^s}\] <p>where $z$ and $a$ are complex numbers, and $s$ is a complex parameter. The function is defined for $\left\lvert z \right\rvert &lt; 1$ or $\left\lvert z \right\rvert = 1$ with $\Re(a) &gt; 0$. Roughly speaking,</p> <ul> <li>$z$ is the value at which the series is evaluated.</li> <li>$s$ is a parameter that controls the power in the denominator.</li> <li>$a$ is a shift parameter in the denominator, which affects the starting point of the summation.</li> </ul> <p>After we fix a dimension $d$, we would have fixed all two parameters of Lerch function, then we can expand $\Phi\left( -\frac{1}{t},1,1+\frac{d}{2} \right)$ at $-\infty$. It certainly feels weird but if you have experience with resurgence theory, it wouldn’t be your first time to expand something at infinity. One way to make you more comfortable with expanding at infinity is to consider the complex plane, compactify all the points at infinity to get a Riemann surface, then infinity becomes just another point on the sphere, and expanding about it is no less natural than expanding about any point.</p> <hr/> <p><strong>Special case at $d=4$:</strong></p> <p>In this case we have</p> \[\begin{align*} u &amp;:= \frac{k_ {E}}{\Lambda},\quad t := \frac{U''(\overline{\phi})}{\Lambda^{2}}, \\ \text{loops} &amp;= \frac{\Lambda^{d}}{d\,2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)}\left[ 2\ln \Lambda+\ln(1+t)- \frac{1}{t} \Phi\left( -\frac{1}{t},1,3\right) \right]. \end{align*}\] <p>Let $x:= 1 / t$ and the last term concerning Lerch function becomes</p> \[- x\, \Phi\left( - x,1,3 \right),\quad x \to \infty.\] <p>I like to drag the minus sign wherever the term goes, harder to make mistakes with a minus sign this way. Using Mathematica command to expand it at infinity</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">Series</span><span class="p">[</span><span class="nb">LerchPhi</span><span class="p">[</span><span class="o">-</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="m">3</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Infinity]</span><span class="o">,</span><span class="w"> </span><span class="m">8</span><span class="p">}]</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nb">Normal</span><span class="w">
</span></code></pre></div></div> <p>we get</p> \[- x\, \Phi\left( - x,1,3 \right)= - \frac{1}{2} + \frac{1}{x} - \frac{\ln(x)}{x^{2}} - \frac{1}{x^{3}}+\mathcal{O}(x^{-4}),\] <p>insert this into the expression of loops we have</p> \[\text{loops} = \frac{\Lambda^{d}}{d\,2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)}\left[ \ln(1+t) + t +t^{2} \ln t \right],\] <p>where $d=4$ and $d 2^{d}\pi^{d/2} \Gamma(d / 2)=64\pi^{2}$. We have discarded surely irrelevant terms such as $\frac{1}{2}$, $2\ln \Lambda$ (they will be absorbed into the renormalization factor), and wrote $x$ in $t$. Next expand</p> \[\ln(1+t) = t - \frac{t^{2}}{2} + \frac{t^{3}}{3} + \mathcal{O}(t^{4})\] <p>we get</p> \[\begin{align*} \text{loops} &amp;= \frac{\Lambda^{4}}{64\pi^{2}}\left[ \ln(1+t) + t +t^{2} \ln t \right] \\ &amp;= \frac{\Lambda^{4}}{64\pi^{2}}\left[ t-\frac{t^{2}}{2} + t +t^{2} \ln t \right] \\ &amp;= \frac{\Lambda^{4}}{64\pi^{2}}\left[ 2t + t^{2}\left( \ln t-\frac{1}{2} \right)\right] \\ &amp;=\frac{1}{64\pi^{2}}\left[ 2\Lambda^{2} U''(\overline{\phi}) + (U''(\overline{\phi}))^{2} \left( \ln \frac{U''}{\Lambda^{2}} -\frac{1}{2} \right)\right]. \end{align*}\] <p>The last result agrees with Coleman’s 1-loop correction, which is shown in Eq. (44.51) in <em>Lectures of Sidney Coleman on Quantum Field Theory</em>, page 976.</p> <p>For a 4D scalar theory, renormalizability strongly constraints the powers you could have on the interactions, it can be no more than $4$, otherwise some terms, such as $(U’’)^{2} \ln \Lambda^{2}$ will generate divergent terms proportional to $(\phi’’^{5})^{2}\sim \phi^{6}$, with no counter terms in the Lagrangian to absorb it, hence non-renormalizable. Coleman remarked in his lecture that</p> <blockquote> <p>Non-renormalizable theories are sick no matter how you look at them; they’re no healthier from this vantage point.</p> </blockquote> <p>However this is the old-fashioned point of view, Wilson will have something more to say on that. But that’s not for this note, here we proceed in the old-fashioned way, to continue to renormalized $\phi^{4}$ theory in 4D.</p> <p>Including the counter terms, the effective potential reads</p> \[V(\overline{\phi}) = U(\overline{\phi})+U_ {ct}(\overline{\phi}) + \frac{1}{64\pi^{2}}(U''(\overline{\phi}))^{2}\ln[U''(\overline{\phi})]+(\Lambda\text{-dependent}),\] <p>where $U_ {ct}$ include all the counter terms and we have written all the divergent terms as $\Lambda$-dependent. After using $U_ {ct}$ to counter the divergence, there could still be some finite part of $U_ {ct}$ left, depending on the renormalization condition. Also, to be mathematically strict, the parameter of $\ln$ function should be dimensionless, while $U’’$ is not. Anyway, this can be easily fixed by introducing another parameter $\mu$ with dimension of mass, and write $\ln(U’’ / \mu^{2})$, this $\mu$ will be fixed once the renormalization condition is fixed.</p> <p>This formula (also with the generalization to $n$ different scalars) was first derived by Coleman and Weinberg: this Coleman and the other Weinberg, Erick Weinberg. Steve Weinberg refers to this work as “that paper with pseudo-Goldstone bosons and a pseudo-Weinberg.”</p> <h1 id="4-the-physical-meaning-of-the-effective-potential">4. The Physical Meaning of the Effective Potential</h1> <p>Roughly speaking, the quantum effective potential $V(\overline{\phi})$ is the quantum generalization of classical potential $U(\phi)$, and $\overline{\phi}$ is the quantum generalization of the $\phi_ {c}$ (or $\left\langle \phi \right\rangle$ as Coleman used), the classical solution to the equation of motion. They agree at tree level and quantum correction kicks in at higher loops, as well the divergences, hence counter terms. Mathematically, $\left\langle \phi \right\rangle$ is the stationary point of the classical action, $\delta S / \delta \phi=0$ at $\left\langle \phi \right\rangle$; $\overline{\phi}$ is the same thing with quantum corrections, meaning it is the expectation value of $\phi$ with quantum corrections, it is the stationary point of the quantum effective action $\Gamma$, $\delta \Gamma / \delta \phi=0$ at $\overline{\phi}$. Physically, $V(\overline{\phi})$ gives the energy density of the vacuum in which expectation value of $\phi$ is $\overline{\phi}$.</p> <p>When $V(\overline{\phi})$ has two local minima, things become interesting. Take the tilted double well for example, there are two local minima but the one with higher energy density is a false vacuum. If we start in the false vacuum, we expect to quantum tunnel (barrier penetration) into the true vacuum eventually. This tunneling phenomenon is non-perturbative.</p> <p>What we are interested in is finding the vacuum state of a QFT model, then it is helpful to look into a similar situation in quantum mechanics. Let’s say we have some Hamiltonian $H$ with interaction, and we want to find the vacuum state $\left\lvert{\psi}\right\rangle$ that minimizes $\left\langle{\psi}\right\rvert H\left\lvert{\psi}\right\rangle$. We further want the the state to be corrected normalized, namely $\left\langle \psi \middle\vert \psi \right\rangle=1$. But that’s not enough, since in the case of QFT we require that the vev of $\phi$ is some certain value, denoted $\overline{\phi}$, here in the quantum mechanical example we also require the vev of some operator $A$ to be $\overline{A}$. To account for these two constraints, we use the Lagrangian multiplier method, defining the Lagrange function to be</p> \[L = \left\langle{\psi}\right\rvert H\left\lvert \psi\right\rangle-E\left\langle \psi \middle\vert \psi \right\rangle -J\left\langle{\psi}\right\rvert A\left\lvert{\psi}\right\rangle ,\] <p>where $E$ and $J$ are Lagrange multipliers, and minimize it. We solve this variational problem with arbitrary $J$, and then eliminate $J$ from the problem to satisfy the constraint condition. We could define a function</p> \[\mathcal{W}(J) = \left\langle{\psi}\right\rvert -H+JA\left\lvert{\psi}\right\rangle ,\] <p>now we assume $\left\lvert{\psi}\right\rangle$ is the ground state with all the constraints. Or, equivalently, $\left\lvert{\psi}\right\rangle$ is the ground state of a modified Hamiltonian $H-JA$. This is similar to the field theoretic $S+\int J \phi$. Now we can replace $J$ by another variable using Legendre transform, define a new variable</p> \[\frac{d W(J)}{dJ} = \left\langle{\psi}\right\rvert A \left\lvert{\psi}\right\rangle = \overline{A}\] <p>and the Legendre transformed function</p> \[\Gamma := \mathcal{W}- J\overline{A},\] <p>what would this $\Gamma$ be? Turns out, it is just the negative energy,</p> \[\Gamma = \left\langle{\psi}\right\rvert -H \left\lvert{\psi}\right\rangle = -E.\] <p>Just like $\Gamma[\varphi]$ in QFT. The above quantum mechanical example is very similar to the field theoretical effective method of effective theory.</p> <p>As remarked by Coleman, the method of effective action may be unreliable under certain circumstances, as he explained in his lecture,</p> <blockquote> <p>…we may run into trouble if level crossing takes place. When the coupling constants are weak, another state that is not the ground state may come up and cross that energy level, and we may find ourselves following the wrong state as we sum up our Feynman graphs. If perturbation theory cannot tell us the true ground state energy, then we won’t get the true ground state energy for the constrained problem, either. On the other hand if perturbation theory serves to give the true ground state energy without constraint, it will also give us the true ground state energy with constraints.</p> </blockquote>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[1. Spontaneous symmetry breaking 2. Degenerate Vacua, Good and Bad 3. Effective Action 3.1. Calculating the 1-Loop Effective Potential 4. The Physical Meaning of the Effective Potential]]></summary></entry><entry><title type="html">Note on Coleman-Weinberg Potential</title><link href="https://baiyangzhang.github.io/blog/2024/Coleman-Weinberg-Potential/" rel="alternate" type="text/html" title="Note on Coleman-Weinberg Potential"/><published>2024-03-31T00:00:00+00:00</published><updated>2024-03-31T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Coleman-Weinberg-Potential</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Coleman-Weinberg-Potential/"><![CDATA[<ul> <li><a href="#1-quantum-effective-action">1. Quantum Effective Action</a> <ul> <li><a href="#11-quantum-action">1.1. Quantum Action</a></li> <li><a href="#12-effective-potential">1.2. Effective Potential</a></li> </ul> </li> <li><a href="#2-coleman-weinberg-potential">2. Coleman-Weinberg Potential</a> <ul> <li><a href="#21-background-fields">2.1. Background Fields</a></li> <li><a href="#22-coleman-weinberg-potential">2.2. Coleman-Weinberg Potential</a></li> </ul> </li> </ul> <h1 id="1-quantum-effective-action">1. Quantum Effective Action</h1> <p>The goal of this chapter is to introduce the quantum action and quantum potential. There are (at least) two kinds of quantum actions, one is the <code class="language-plaintext highlighter-rouge">Wilsonian quantum action</code>, where the higher momentum modes are integrated out to focus on the low-energy, long-distance behavior of a system. It embodies the effective interactions of a quantum system <em>at a certain scale</em>, accounting for the influence of fluctuations at smaller scales. This potential is central to understanding how physical phenomena emerge at different length scales from the underlying quantum fields, particularly in the study of critical phenomena and phase transitions. Take a scalar field theory $\phi$ for example. The “integrating out” procedure is done using the path integral approach, it involves <em>separating the field into high and low energy parts</em>. To be more specific, the scalar field is split into $\phi = \phi_ {\text{low}} + \phi_ {\text{high}}$, where $\phi_ {\text{low}}$ contains modes below a certain energy scale $\Lambda$ and $\phi_ {\text{high}}$ contains modes above $\Lambda$. $\phi_ {\text{high}}$ can be thought of as a thin shell in the momentum space. The path integral over the full field is then re-written as a path integral over these two components of fields. The crucial step is to integrate out the high-energy modes $\phi_ {\text{high}}$. This can be done perturbatively by treating $\phi_ {\text{high}}$ as a different field from $\phi_ {\text{low}}$, using Feynman diagram techniques. The resulting effective action should only depends on the low-energy modes. The Wilsonian effective action captures the dynamics of the field <em>at energies below</em> $\Lambda$. It will typically have a form different from the original action, often with new interactions generated as a result of integrating out the high-energy modes. New scale could even emerge, as in the case of dimensional transmutation, which we will not talk too much about in this note.</p> <p>The other kind of effective action is the so-called <code class="language-plaintext highlighter-rouge">quantum effective actions</code> which is the Legendre transformation of the generating functional of connected diagrams. It is quite a mouthful and we will talk more about it in the next section. When the original action is replaced by the quantum effective action, the tree-level diagrams generated by it will contain <em>all the quantum corrections</em> of the full theory (i.e. the original theory). The quantum effective action has a useful property that it can give us the vacuum expectation value (VEV) of the field operator with quantum correction. As we will see, the <code class="language-plaintext highlighter-rouge">Coleman-Weinberg potential</code> is a special case of the effective action.</p> <p>As M. D. Schwartz put it:</p> <blockquote> <p>Generally speaking, the term effective action, denoted by $\Gamma$, generally refers to a functional of fields (like any action) defined to give the same Green’s functions and S-matrix elements as a given action $S$, which is often called the action for the full theory. We write $\Gamma=\int d^4x \, \mathcal{L}_ {\text{eff}}(x)$, where $\mathcal{L}_ {\text{eff}}$ is called the effective Lagrangian.</p> </blockquote> <hr/> <p>There are in general three ways to calculate the effective action, as listed in the following.</p> <ul> <li><strong>Matching</strong>. We require</li> </ul> \[\int \mathcal{D}\{\text{original dof}\} e^{iS} = \int \mathcal{D}\{\text{effective dof}\} e^{i\Gamma}\] <p>where dof stands for the <code class="language-plaintext highlighter-rouge">degrees of freedom</code>, the LHS is the original theory while the RHS is the effective theory, with different degree of freedom. For example, QCD (at high energy) has quark and gluon as the degrees of freedom, when the energy is lowered the degrees of freedom becomes color singlet particles. $\Gamma$ is the quantum action.</p> <ul> <li> <p><strong>Legendre transformation</strong>, as we will show shortly.</p> </li> <li> <p><strong>Background field method</strong>, that is to separate the field into a static non-propagating background field $\phi_ b$ and a dynamic propagating field $\tilde{\phi}$, the dynamic fields are the fluctuations around the background field. Integrating out the fluctuations (usually done perturbatively) leaves us the effective potential $\Gamma[\phi_ b]$,</p> </li> </ul> \[\int \mathcal{D} \phi e^{i S[\phi_ b + \tilde{\phi}]} = e^{i\Gamma[\phi_ b]}.\] <p>The background field method is also closely related to how we calculate quantum corrections to classical solitonic solutions, such as the quantum correction to kink mass.</p> <p>The first try, before all the three mentioned above, is usually an educated guess. Given that <strong>the effective action must possess the same symmetries as the original action</strong>, one can propose various <em>local terms</em> that fulfill this requirement. The coefficients of these terms are then adjusted based on experimental data. This method relies on symmetry considerations to guide the formulation of acceptable terms in the effective action.</p> <p>For the rest of the note, we will confine our discussion to $\phi^4$ model with real scalar fields.</p> <h2 id="11-quantum-action">1.1. Quantum Action</h2> <p>To keep the notation simple, consider a single real scalar field $\phi$, with possibly mass term and self interaction. The partition function (with source) reads</p> \[Z[J] = \int \mathcal{D} \phi e^{iS[\phi] + i\int \phi J},\] <p>where $S$ is the action, $J$ is the source, $\int \phi J$ is short for $\int_ {M} \phi(x) J(x)$. $\phi$ is integrated out hence $Z$ is a functional of $J$ only.</p> <p>From it we define the generating function $W[J]$ by</p> \[Z[J] = e^{iW[J]} \implies W[J] = -i \ln Z[J],\] <p>$W[J]$ is the summation of <em>connected diagrams</em> with source, connected roughly because if you consider all the diagrams, the disconnected but replica-forming (namely the disconnected diagrams formed by putting two or more replicas of the same diagrams together) diagrams can be arranged into forms of $\bullet^{n}/n!$, where $\bullet$ is (the expression of) some connected diagram. Then we can organized them into an exponential function $e^{ \bullet }$, now $\bullet$ contains information only about connected diagrams. For more details, please refer to Mark Srednicki’s text book on quantum field theory. To repeat, $W[J]$ generates <em>connected diagrams</em> only.</p> <hr/> <p>The expectation value of $\phi$ in the presence of a source $J$ is given by</p> \[\left\langle {\phi} \right\rangle_ J=\frac{1}{Z} \int \mathcal{D} \phi e^{iS[\phi] + i\int \phi J} \phi = \frac{\delta W[J]}{\delta J(x)}\equiv \varphi_ {J},\] <p>Note the difference between $\phi$ and the so-called <code class="language-plaintext highlighter-rouge">varphi</code> $\varphi$, the former is an operator while the latter is a classical field. The subscript $J$ in $\varphi_ {J}$ is to emphasize that the vev of $\phi$ depends on the source $J$. In comparison to classical mechanics of point particles, $W$ is like Lagrangian $L$, $J$ is like $\dot{q}$, and $\delta W / \delta J$ is like $\partial L / \partial \dot{q}$, which introduces a new variable.</p> <p>Since the generating functional $W[J]$ is a functional of $J$, we can perform Legendre transform to define a new functional in terms of $\delta W / \delta J =: \varphi_ {J}$. The result is the quantum action:</p> \[\boxed{ \Gamma[\varphi_ {J}] = W[J] - \int J\varphi_ {J} , }\] <p>which is indeed a functional of $\varphi_ {J}$ and not $J$, since it is independent of variation $\delta J$ of $J$, as the readers can verify. Again we have omitted the measure under the integral sign. Some direct calculation shows that</p> \[\frac{\delta\Gamma[\varphi_ {J}]}{\delta\varphi_ {J}(x)} = -J(x).\] <p>It is not supposed to be obvious, but the effective action $\Gamma$ is the generating functional for 1-particle irreducible (1PI) diagrams! The significance of 1PI diagrams is best explained by Coleman in his lecture note on QFT, which I quote:</p> <blockquote> <p>If we treat the 1PI graphs as giving us effective interaction vertices, then to find the full Green’s functions we only have to sum up tree graphs, never any loops, because all the loops have been stuffed inside the definition of the propagators and the 1PI graphs. This marvelous property of the 1PI graphs is important. Taking the 1PI graph generating functional for a quantum action enables us to turn the combinatorics of building up full Green’s functions from 1PI Green’s functions into an analytic statement, and we end up with the correct expressions for the full Green’s functions. We’re turning a topological statement of one-particle irreducibility into an analytic statement that we will find easy to handle.</p> </blockquote> <p>To see that $\Gamma[\varphi]$ indeed generates the 1PI diagrams, it is easiest (for myself) to inverse the chain of reasoning, first we define an effective action such that its tree level diagrams reproduces the quantum result (which is easier than it looks), then show that such constructed action satisfies the same equation as $\Gamma[\phi_ {J}]$, so they are the same (up to some insignificant constant, such as the normalization constant). We will proceed in this direction.</p> <p>For now, forget about $\varphi_ {J}$. Let’s starting from defining the an effective action $\Gamma[\phi_ {c}]$, which is a functional of of some classical field $\phi_ {c}$. The role of $\phi_ {c}$ in $\Gamma[\phi_ {c}]$ is the same as the role of $\phi$ in the classical action $S[\phi]$, where we usually don’t bother to emphasize that $\phi$ is classical rather than a quantum field, but here we do. $\Gamma[\phi_ {c}]$ is defined such that, if we treat $\Gamma[\phi_ {c}]$ as the classical action $S[\phi]$, substitute $\Gamma$ with $S$ in the path integral, and calculate $Z[J]$ (or equivalently $W[J]$), using <strong>only the tree diagrams</strong>, then we get exact $Z[J]$ with every bit of the quantum correction! At first glance, this might seem almost too convenient, making our calculations significantly simpler, too good to be true. However, there’s no shortcut to the truth; ultimately, we still need to buckle down and work through the loop diagrams. Essentially, the effective action is a clever reorganization of the contributions from these loop diagrams. Even though the effective action doesn’t simplify the calculations per se, it is still quite valuable to us since it provides a new perspective, serving as a powerful tool in quantum field theory, enabling the study of quantum phenomena with a formalism that extends the classical action to include quantum effects.</p> <p>So how should $\Gamma[\phi_ {c}]$ be constructed? For any function $\phi_ {c}$, the effective action $\Gamma[\phi_ {c}]$ has a functional Taylor expansion:</p> \[i\Gamma[\phi_ {c}] = \sum_ {n} \frac{1}{n!} \int d^{d}x_ {1}\cdots d^{d}x_ {n} \, \Gamma^{(n)} (x_ {1},\cdots ,x_ {n}) \phi_ {c}(x_ {1})\cdots \phi_ {c}(x_ {n}).\] <p>For example, if $\Gamma[\phi_ {c}] = \int \, \phi_ {c}^{2}$, then the only non-zero component in the functional Taylor expansion is $\Gamma^{(2)}(x_ {1},x_ {2}) = 2\delta^{d}(x_ {1}-x_ {2})$.</p> <p>When talking about Feynman diagrams, it is more convenient to go to momentum representation, hence we define a modified version of the Fourier transform of $\Gamma^{(n)}$, such that</p> \[\Gamma^{(n)}(x_ {1},\cdots x_ {n}) := \int \frac{dp_ {1}}{(2\pi)^{d}}\cdots \frac{dp_ {n}}{(2\pi)^{d}}\, \tilde{\Gamma}^{(n)} (p_ {1},\cdots ,p_ {n} ) (2\pi)^{d}\delta^{d}(p_ {1}+\cdots +p_ {n} ) .\] <p>This definition contains extra $\delta$-function for future convenience.</p> <p>Recall that the filed $\phi_ {c}(x_ {i})$ themselves in the action eventually becomes <strong>amputated</strong> external legs, amputated in the sense that no propagator is associated to it. All the information is contained in $\tilde{\Gamma}$! We can brutally stuff all the 1PI diagrams, including loop corrections from $S[\phi]$, into $\tilde{\Gamma}$ so that we only need to take into consider the tree diagrams of $\Gamma$. For example, we can draw all the 1PI diagrams with three external legs, calculate them, and define it to be $\tilde{\Gamma}^{(3)}$. If we regard $\Gamma[\phi_ {c}]$ as a machine that takes a function $\phi_ {c}$ as input and spits out a number, then $\tilde{\Gamma}$’s are like its components.</p> <p>To summarize, for $n\geq 2$, the $\tilde{\Gamma}^{(n)}$ are <strong>defined</strong> by the sum of 1PI diagrams with $n$ external legs. As usual the external legs are amputated. The external momenta need not be conserved, that point is taken care of by the $\delta$-function in the definition of $\tilde{\Gamma}$. For $n=2$ the case is slightly more complicated, we need to include a propagator into the definition, but the philosophy is the same.</p> <p>Next we combine the tree-level exactness of $\Gamma[\phi_ {c}]$ with another concept: loop expansion. Loop expansion is equivalent to both semi-classical expansion (expansion in $\hbar$) and perturbative expansion (expansion in coupling $g$), should the right $\hbar$-dependence be made. Sidney Coleman thinks that $\hbar$ expansion is rubbish for two reasons (that I know of), 1) $\hbar$ is dimensional, with dimension of energy multiplies time, therefore is not a good expanding parameter and 2) if we make $\hbar$ dimensionless like we did with natural units, we could always change the units such that $\hbar=1$. In loop expansion, the tree level diagrams dominates the partition function $Z[J]$ when $\hbar$ is small, and becomes exact at $\hbar\to 0$. I am tempted to write</p> \[\text{tree diagrams} = \lim_ { \hbar \to 0 } \int \mathcal{D}\phi_ {c} \, \exp \left\lbrace \frac{i}{\hbar}\Gamma[\phi_ {c}] + \int J\phi_ {c} \right\rbrace\] <p>And this turns out to be correct. I used to think of $\phi_ {c}$ as some pre-determined classical function, which has caused me a lot of confusion. From now on let’s get rid of the subscript $c$ in $\phi_ {c}$, since fields appear under the path integral are always classical field. We will put the subscript back when possible confusion could rise.</p> <p>Thanks to the $\hbar\to 0$ limit, the path-integral can be worked out using the method of stationary phase, up to some normalization constant we have</p> \[\lim_ { \hbar \to 0 } \int \mathcal{D}\phi \, \exp \left\lbrace \frac{i}{\hbar}\Gamma[\phi] +\frac{i}{\hbar} \int J\phi \right\rbrace = \exp \left\lbrace \frac{i}{\hbar}\Gamma[\overline{\phi}]+ \frac{i}{\hbar} \int \, J\overline{\phi} \right\rbrace ,\] <p>where $\overline{\phi}$ is the solution that <strong>extremizes</strong> the exponent on the LHS,</p> \[\frac{\delta \Gamma[\phi]}{\delta \phi}{\Large\mid}_ {\phi=\overline{\phi}} \equiv\frac{\delta \Gamma[\overline{\phi}]}{\delta \overline{\phi}} = -J(x).\] <p>This is exactly the same functional equation we got for $\varphi_ {J}$ before! They might differ by a constant, but that can be absorbed into the normalization factors and cancels out eventually. Now we can comfortably write $\overline{\phi} =\phi_ {J}$ in the quantum action. This equation connects the quantum action we obtained before via a Legendre transform from $iW[J]$ with the generating functional for 1PI diagrams, identifying these two seemingly different quantities. To show the connection ever more clearly, recall that the partition function in terms of $\Gamma$ is</p> \[Z[J] = \exp \left\lbrace \frac{i}{\hbar} \left( \Gamma [\varphi_ {J}]+\int \, J\varphi_ {J} \right) \right\rbrace =\exp \left\lbrace \frac{i}{\hbar}W[J] \right\rbrace\] <p>we have</p> \[W[J] = \Gamma[\phi_ {J}] + \int \, J\phi_ {J}, \quad J \text{ given a priori.}\] <p>Note that we could equally write $W$ as $W+2\pi \mathbb{N}$ but the additive constant can be absorbed into the partition functions as well. This is the Legendre transform we wrote down before!</p> <p>For the sake of completeness we put the pair of Legendre transforms below,</p> \[\begin{align*} W[J] &amp;= \Gamma[\varphi_ {J}] +\int \, J\varphi_ {J} ,\quad -J = \frac{\delta \Gamma[\varphi_ {J}]}{\delta \varphi_ {J}}, \\ \Gamma[\varphi] &amp;= W[J_ {\varphi}] - \int \, J_ {\varphi} \varphi, \quad \varphi=\frac{\delta W[J]}{\delta J}, \end{align*}\] <p>where $\varphi_ {J}$ means that $\varphi$ is determined by $J$, namely $\varphi$ is a (non-local) function of $J$, while $J_ {\varphi}$ means the opposite. Also keep in mind that $\varphi_ {J}$ is the vev of quantum operator $\phi$ in the presence of $J$.</p> <p><strong>Summary.</strong></p> <ul> <li>$\Gamma[\varphi]$ generates 1PI diagrams;</li> <li>$W[J]$ generates connected diagrams;</li> <li>$Z[J]$ generates all kinds of diagrams.</li> </ul> <p><strong>Remark.</strong> The generating functionals such as $W[J]$ and $\Gamma[\varphi]$ are classical functional, dealing with c-numbered functions, no operators and commutation relations involved. In fact, the language of path integral has a close connection with classical, statistical field theory, and many concepts exists in both disciplines, for example, people dealing with statistical field theory also talk about renormalization flow (Wilsonian), and QFT-ists also talk about critical exponents. A great textbook on statistical field theory is that by <a href="https://guava.physics.ucsd.edu/~nigel/"><code class="language-plaintext highlighter-rouge">Nigel Goldenfeld</code></a>.</p> <hr/> <p>Recently I found another approach to effective action, which I will copy here. This new approach gives a different perspective to stuff we talked about before, and it made it manifest that the external legs of $\Gamma[\phi]$ should be amputated, thus I consider it worthy to write it down.</p> <p>Let $G^{(n)}(x_ {1},\cdots,x_ {n})$ be the most general kind of $n$-point function, including disconnected ones. It is generated by the partition function $Z[J]$, which can be written as</p> \[Z[J] = \sum_ {n=0}^{\infty} \frac{i^{n}}{n!} \int d^{d}x_ {1} \cdots d^{d}x_ {n} \, G^{(n)}(x_ {1},\cdots,x_ {n}) J(x_ {1})\cdots J(x_ {n}).\] <p>As you can see, acting $n$-times the functional derivative $\delta / i\delta J$ gets us the $n$-point function.</p> <p>Similarly, the generating functional of connected diagrams $iW[J]$ adopts the functional Taylor expansion</p> \[iW[J] = \sum_ {n=0}^{\infty} \frac{i^{n}}{n!} \int d^{d}x_ {1} \cdots d^{d}x_ {n} \, G_ {c}^{(n)}(x_ {1},\cdots,x_ {n}) J(x_ {1})\cdots J(x_ {n}).\] <p>where $G_ {c}^{(n)}(x_ {1},\cdots)$ is the connected n-point function. Likewise for $\Gamma$ but we have already wrote it down. In the next we will neglect $J$ in $\varphi_ {J}$, it is understood that $\varphi$ is the canonical transformed variable of $J$ and vise versa.</p> <p>Using the functional relation</p> \[\frac{\delta W}{\delta J} = \varphi\] <p>we can do something interesting with the connected 2-point function. Neglect the normalization factor for now, we have</p> \[\begin{align*} iD(x-y) &amp;= \int D\phi \, e^{ i\left( S+\int J\phi \right) } \phi(x)\phi(y)\\ &amp;= \frac{\delta^{2}W}{\delta J(x)\delta J(y)} = \frac{\delta}{\delta J(y)} \frac{\delta W}{\delta J(x)}\\ &amp;= \frac{\delta \varphi(x)}{\delta J(y)} , \end{align*}\] <p>amazingly the functional derivative between $\varphi$ and $J$ is nothing but the quantum, full propagator! On the other hand,</p> \[\frac{\delta J(x)}{\delta J(y)} = \delta^{d}(x-y) = - \frac{\delta}{\delta J(y)} \frac{\delta \Gamma[\varphi]}{\delta \varphi(x)},\] <p>write</p> \[\boxed{ \frac{\delta}{\delta J(y)} = \int d^{d}z \, \frac{\delta \varphi(z)}{\delta J(y)} \frac{\delta}{\delta\varphi(z)} = \int d^{d}z \, iD(z-y) \frac{\delta}{\delta \varphi(z)} }\] <p>where the first equal sign is nothing but the chain rule of functionals, we have</p> \[\begin{align*} \delta^{d}(x-y) &amp;= - \int d^{d}z \, \frac{\delta \varphi(z)}{\delta J(y)} \frac{\delta^{2} \Gamma[\varphi]}{\delta\varphi(z)\delta\varphi(x)} \\ &amp;=-i \int d^{d}z \, D(y-z) \frac{\delta^{2} \Gamma[\varphi]}{\delta\varphi(z)\delta\varphi(x)} . \end{align*}\] <p>Recall that $\delta$-function is the equivalence of identity with functionals, we see that $\delta^{2} \Gamma[\varphi] / \delta\varphi(z)\delta\varphi(x)$ is the inverse of 2-point functions! To be specific</p> \[\boxed{ \left( \frac{\delta^{2}\Gamma[\varphi]}{\delta \varphi(x)\delta \varphi(y)} \right)^{-1} = -i D (y-z)= -\frac{\delta^{2} W[J]}{\delta J(y)\delta J(z)}. }\] <p>Is helps to think of $\delta^{2} / \delta_ {x} \delta_ {y}$ as a matrix $M_ {xy}$, then this inverse relation is for matrices. This is both intuitive and not… intuitive because, recall that with regular Lagrangian $\mathcal{L}$, $\partial^{2} \mathcal{L} / (\partial \phi)^{2}$ is roughly speaking the inverse of the propagator, here the effective action kind of takes the position of $\mathcal{L}$; Counter intuitive since, well, it took me a lot effort to find it intuitive.</p> <p>Now let’s carry on with other n-point functions where $n&gt;2$. But before that we need to solve a math problem: how to take the functional derivative of an inverse matrix.</p> <p>Let $M$ be a matrix function and $M^{-1}$ its inverse. Start with the identity, $I = MM^{-1}$, differentiating this identity yields $0 = dM^{-1}M + M^{-1}dM$, leading to the expression for the differential of the inverse $dM^{-1} = -M^{-1}(dM)M^{-1}$. From this, it follows that the derivative of $M^{-1}$ with respect to some variable $a$ is</p> \[\frac{\partial M^{-1}}{\partial a} = -M^{-1} \frac{\partial M}{\partial a} M^{-1}.\] <p>Now lets consider the 3-point function</p> \[\begin{align*} G_ {c}^{(3)}(x,y,z) &amp;= \frac{\delta^{3}W}{\delta J(x)\delta J(y)\delta J(z)} \\ &amp;= i \int d^{d}w \, D(z,w) \frac{\delta}{\delta\varphi(w)} \frac{\delta^{2}W[J]}{\delta J(y) \delta J(x)} \\ &amp;= - i \int d^{d}w \, D(z,w) \frac{\delta}{\delta\varphi(w)}\left( \frac{\delta^{2}\Gamma[\varphi]}{\delta \varphi(x)\delta \varphi(y)} \right)^{-1}\\ &amp;= -i \int d^{d}w d^{d}w' d^{d}w'' \, D(z-w) \left( \frac{\delta^{2}\Gamma}{\delta\varphi(x)\delta\varphi(w')} \right)^{-1} \\ &amp;\;\;\;\;\;\times \frac{\delta^{3}\Gamma}{\delta\varphi(w)\delta\varphi(w')\delta\varphi(w'')} \left( \frac{\delta \Gamma}{\delta\varphi(w'')\delta\varphi(y)} \right)^{-1} \\ &amp;= - i \int d^{d}w d^{d}w' d^{d}w'' \, D(z-w) D(x-w') D(y-w'')\\ &amp;\;\;\;\;\;\times \frac{\delta^{3}\Gamma[\varphi]}{\delta\varphi(w)\delta\varphi(w')\delta\varphi(w'')} \\ &amp;= -i \int d^{d}w d^{d}w' d^{d}w'' \, D(z-w) D(x-w') D(y-w'') \Gamma^{(3)}(w,w',w''). \end{align*}\] <p>Now, $G_ {c}^{3}(x,y,z)$ is the connected 3-point function defined at $x,y$ and $z$, with its external legs <strong>not amputated</strong>! On the LHS, since all the three external legs are accounted for by the three propagators $D(z-2)$ etc., $\Gamma^{(3)}(w,w’,w’’)$ has its external legs <strong>amputated</strong>! As we dig deeper, you’ll find that this is a general conclusion: the external legs of $\Gamma[\varphi]$ are amputated.</p> <p>We have been sloppy with factor of $i$’s. Taking care of it, the n-point correlation function reads</p> \[\left( i\frac{\delta}{\delta J} \right)^{n} (iW[J]) = \left\langle T \,\phi_ {1}\cdots \phi _ {n} \right\rangle _ {J} =: G^{(n)}_ {\text{c}}(x_ {1},\cdots ,x_ {n}),\] <p>where $c$ is for connected. Since $\Gamma[\varphi]$ generates 1PI diagrams,</p> \[G^{(3)}_ {\text{1PI,am}}(x,y,z) = \frac{\delta^{3} i\Gamma[\varphi]}{\delta\varphi(x) \delta\varphi(y) \delta\varphi(z)}\] <p>where $\text{am}$ for amputated. We have found the relation between connected, not-amputated 3-point functions between 3-point 1PI connections:</p> \[G^{(n)}_ {c}(x,y,z) = \int d^{d}w \, d^{d}w' \, d^{d}w'' \, D(x-w)D(y-w')D(z-w'') G^{(3)}_ {\text{1PI,am}}(w,w',w'').\] <p>Note that $G^{(n)}_ {\text{1PI,am}}$ is nothing but the same $\Gamma^{(n)}$ in the functional Taylor expansion of $\Gamma$ (by construction). The generalization to $n&gt;3$ is straightforward.</p> <p><strong>A Tree-level Example</strong></p> <p>In the classical limit, that is in the limit $\hbar \to 0$, the partition function</p> \[Z = \int \mathcal{D}\phi e^{ \frac{i}{\hbar} \left( S + \int \phi J \right)}\] <p>receives dominant contribution from the stationary configuration, given by</p> \[\frac{\delta}{\delta\phi}\left( S + \int \phi J \right) = 0 \implies \frac{\delta S}{\delta\phi} = - J\] <p>which has a solution $\varphi_ {J}$. This is exactly the euqation satisfied by the quantum action $\Gamma[\phi]$! Anyway, we can carry on to talk about the partition function which is now</p> \[Z = e^{iS[\varphi_ {J}]+i\int \varphi_ {J} J}\] <p>up to a normalization factor. We have</p> \[W[J] = -i \ln Z = S[\varphi_ {J}] + \int \varphi_ {J} J,\] <p>thus</p> \[\Gamma[\varphi] = W - \int \varphi_ {J} J = S[\varphi].\] <p>As expected, at the tree-level, the quantum effective action and the original action are the same.</p> <h2 id="12-effective-potential">1.2. Effective Potential</h2> <p>In the classical dynamics, the vacuum (lowest energy state) configuration of the system is given by the minimum of the potential, which fixes the value of the field in spacetime (usually a constant in spacetime). In the quantum theory, everything receives quantum correction, including the vacuum expectation value (VEV) $\left\langle \phi \right\rangle$ of the field operator $\phi$. In a QFT, the potential term in the Lagrangian or Hamiltonian has minima given by the classical vacuum field configuration, however that’s not the full story, since the field always fluctuates around the vacuum, giving rise to a correction to $\left\langle \phi \right\rangle$. That’s when the effective potential comes to rescue.</p> <p>The effective potential in QFT is a crucial concept, especially when studying systems with spontaneous symmetry breaking, phase transitions, and nonperturbative dynamics. It represents a modification of the classical potential to include quantum corrections, providing a more accurate description of the dynamics of quantum fields. Its application include:</p> <ol> <li> <p><strong>Spontaneous Symmetry Breaking</strong>: The effective potential is instrumental in understanding spontaneous symmetry breaking, a phenomenon where the ground state (vacuum) of a system does not inherit the symmetry of the action. In the context of the Higgs mechanism in the Standard Model of particle physics, the effective potential reveals how the Higgs field acquires a nonzero vacuum expectation value, leading to the generation of masses for the $W$ and $Z$ bosons.</p> </li> <li> <p><strong>Phase Transitions</strong>: In the study of early universe cosmology or condensed matter physics, the effective potential reveals how a system transitions between different phases. For example, it can describe the transition from a symmetric phase to a broken-symmetry phase as the universe cools. The shape of the effective potential changes with temperature, and these changes can indicate phase transitions, such as from a high-temperature symmetric phase to a low-temperature phase where symmetry is broken.</p> </li> <li> <p><strong>Quantum Corrections and Renormalization</strong>: The effective potential incorporates quantum corrections to the classical potential, which are crucial for making precise predictions in QFT. These corrections can significantly alter the behavior of the system, especially at high energies or short distances. The process of renormalization is deeply connected to the effective potential, ensuring that physical quantities remain finite and meaningful.</p> </li> <li> <p><strong>Nonperturbative Effects</strong>: The effective potential can capture nonperturbative effects, which are not accessible through standard perturbative techniques. For instance, in theories with strong coupling or in situations where the perturbative series does not converge (actually it doesn’t converge in any cases), the effective potential can provide insights into the structure and dynamics of the vacuum, solitonic solutions, and other nonperturbative phenomena like instantons and tunneling effects.</p> </li> <li> <p><strong>Dynamical Mass Generation</strong>: In theories where particles are massless at the classical level, the effective potential can show how interactions lead to dynamical mass generation. This is particularly significant in quantum chromodynamics (QCD) and models of dynamical symmetry breaking, where the vacuum structure induced by strong interactions gives rise to constituent masses for particles.</p> </li> <li> <p><strong>Vacuum Stability and Tunneling</strong>: The effective potential allows for the analysis of vacuum stability in various field theories. It can be used to study the probability of tunneling between different vacua, which has implications for the stability of our universe and the decay of false vacuum states.</p> </li> </ol> <p>Overall, the effective potential is a powerful tool in quantum field theory, providing deep insights into the quantum dynamics of fields, the structure of the vacuum, and the various nonperturbative phenomena that arise in complex quantum systems. Next let’s dig into it.</p> <hr/> <p>Recall that the quantum effective action</p> <ul> <li>is a functional of $\varphi_ {J}$ where $\varphi_ {J} = \left\langle {\phi} \right\rangle_ J$, namely $\varphi$ is the expectation value of $\phi$ in the presence of a source term $J$, and</li> <li>satisfies ${\delta \Gamma}/{\delta \varphi} = J$.</li> </ul> <p>Thus <strong>when $J=0$, the solution to ${\delta \Gamma}/{\delta \varphi} = J$ is the vev of $\phi$.</strong></p> <p>Additionally, let’s assume the vacuum exhibits translational symmetry, meaning that the vacuum expectation value (vev) of $\phi$ remains constant across space and time. Under this condition, we only require a single number to describe the field configuration. Consequently, we can introduce a function, $\mathcal{V}_{\text{eff}}$, the minimum of which represents the vev of $\phi$,</p> \[\Gamma[\varphi]|_ {\varphi = \text{const}} = -VT \mathcal{V}_ {\text{eff}}(\varphi)\] <p>where $V$ is the volume of the space and $T$ the extension of time, $\mathcal{V} _ {\text{eff}}(\varphi)$ is the quantum effective potential. Apparently $\mathcal{V} _ {\text{eff}}$ is an intensive, and ${\partial\mathcal{V}}/{\partial\phi} = 0$ reproduces ${\delta \Gamma}/{\delta \varphi} = 0$.</p> <h1 id="2-coleman-weinberg-potential">2. Coleman-Weinberg Potential</h1> <h2 id="21-background-fields">2.1. Background Fields</h2> <p>The method of background field is very useful for calculating beta functions and effective action. For a real scalar field $\phi$, the general idea is as following</p> <ul> <li>separate the field into the static background field $\phi_ b$ and dynamic field $\phi$, $\phi \to \phi_ b + \phi$. By static we mean it is not path-integrated, thus don’t participate in quantum or static activities, such as propagate or fluctuate. This will simplify the calculation a lot (not supposed to be obvious).</li> <li>Define the corresponding action with background field $S_ b[\phi_ b;\phi]$, the resulting generating functional $W_ b[\phi_ b;\phi]$, and the 1PI effective action $\Gamma_ b[\phi_ b;\varphi]$, where $\varphi$ is defined to be the vev of $\phi$ in the presence of the background field $\phi_ b$.</li> <li>$\Gamma_ b[\phi_ b;\varphi]$ has a useful property:</li> </ul> \[\Gamma_ b[\phi_ b=\phi;0] = \Gamma_ b[\phi_ b=0;\varphi] = \Gamma[\varphi],\] <p>due to the fact that $\Gamma[\phi_ {b}+\varphi]$ is a functional of $\phi_ {b}+\varphi$ as a whole, it shows that how to choose the background field is kind of arbitrary, we should choose whatever makes our calculation the simplest. The first term means that the effective action with background field $\phi_ b = \phi$ and zero dynamic field $\varphi = 0$. Since the dynamic field will be set to zero at last, if a diagram has dynamic field external legs, it is also zero. It is similar to the Feynman diagrams with sources, when we set $J=0$ in the end then all the diagrams where all the source bulbs vanish (people who have read Srednicki will know what I am talking about).</p> <p>p.s. In Sidney Coleman’s lectures on QFT, in Eq. (44.31), his $\left\langle \phi \right\rangle$ is our $\phi_ {b}$ and his $\overline{\phi}’$ is our $\varphi$. In our notation, Coleman chose $\phi_ {b}$ to be the vev of $\phi$ with $J=0$ (since he is interested in studying spontaneous symmetry breaking), then he goes on and expand $\varphi$ about $\varphi=0$.</p> <p>The action with a background field is defined as</p> \[S_ b[\phi_ b;\phi] = \int d^4x\mathcal{L}(\phi_ b+\phi),\] <p>the partition function with source is</p> \[\mathcal{Z}[\phi_ b;J] = \int \mathcal{D}\phi \exp\{ i S_ b[\phi_ b;\phi]+i\int J\phi \} = \mathcal{Z}[J] e^{-i\int J\phi_ b},\] <p>note that <strong>the source only couples to the dynamic field</strong>. Since $\phi$ is integrated over, $\mathcal{Z}$ can only be a functional of $\phi_ b$ and $J$. Define the generating functional as</p> \[W_ b[\phi_ b;J] \equiv -i \ln \mathcal{Z}_ b[\phi_ b;J] \implies W_b[\phi_ b;J] = W[J] - \int J\phi.\] <p>Define</p> \[\varphi_ b(x) = \frac{\delta W_ b[J]}{\delta J(x)}\] <p>we have</p> \[\varphi_ b = \varphi - \phi_ b\] <p>which means that in the presence of a background field, $\left\langle \phi \right\rangle$ will be shifted by $\phi_ b$, as expected.</p> <p>Now we need to define the effective action in the presence of a background field, by the means of Legendre transformation again.</p> \[\Gamma_ b [\phi_ b;\varphi_ b] = W_ b - \int \frac{\delta W_ b[J]}{\delta J}J = W_ b - \int \varphi_ b J\] <p>you can check that</p> \[\frac{\delta \Gamma_ b[\phi_ b;\varphi_ b]}{\delta\varphi_ b}= J .\] <p>Replace $W_ b[J]$ with its expression in terms of $W[J]$, we can check that</p> \[\Gamma_ b[\phi_ b;\varphi_ b] = W[J] - \int J (\varphi_ b+\phi_ b) = \Gamma[\phi_ b + \varphi_ b]\] <p>so for example if we want to calculate $\Gamma[\eta(x)]$, we can set $\phi_ b = \eta,\,\varphi = 0$ and use that to simplify calculations.</p> <h2 id="22-coleman-weinberg-potential">2.2. Coleman-Weinberg Potential</h2> <p>Consider the real scalar Lagrangian</p> \[\mathcal{L} = -\frac{1}{2} \phi \partial^2\phi - \frac{1}{2} m^2\phi^2-\frac{1}{4!}\phi^4\] <p>the question is, when $m = 0$, will the quantum effects modify the shape of the potential?</p> <p>Introduce a background field $\phi \to \phi_ b +\phi$, take it into the action and expand, keep in mind that the path integral is over field $\phi$ only, we have</p> \[\begin{align*} e^{i\Gamma[\phi_ b]} &amp;= e^{i\int d^4 x (-\frac{1}{2} \phi_ b \partial^2 \phi_ b-V(\phi_ b))} \\ \\ &amp;\times\int \mathcal{D}\phi e^{i \int d^4 x (-\frac{1}{2} \phi \partial^2 \phi -V(\phi_ b) - \phi V'(\phi_ b)-\frac{1}{2}V''[\phi_ b]\phi^2-\frac{1}{3!}V'''[\phi_ b]\phi^3-\cdots)}. \end{align*}\] <p>In the path integral, one of the terms in the Lagrangian, i.e. $\int \mathcal{D} \phi e^{i\phi V’}$ can be thrown away because we only need to consider 1PI diagrams in the calculation of $\Gamma$, while the Feynman diagram given by $\phi V’(\phi_ b)$ will never contribute to the 1PI diagrams. For the same reason we can also discard $\phi^3$ term in the Lagrangian. At one loop, there will be no contributions from $\phi^4$ and higher terms either. Hence we are left with</p> \[e^{i\Gamma[\phi_ b]} = e^{i\int d^4 x (-\frac{1}{2} \phi_ b \partial^2 \phi_ b-V(\phi_ b))} \int \mathcal{D} e^{i \int d^4 x (-\frac{1}{2} \phi \partial^2 \phi -\frac{1}{2}V''[\phi_ b]\phi^2)}.\] <p>use the master formulae in QFT</p> \[\int\mathcal{D}\phi\exp\left\{ i \int d^4x (\phi M \phi) \right\} = \mathcal{N}\frac{1}{\text{det}^{ {1/2} }{M}}\] <p>we have</p> \[e^{i\Gamma[\phi_ b]} = \mathcal{N}e^{i\int d^4 x (-\frac{1}{2} \phi_ b \partial^2 \phi_ b-V(\phi_ b))} \text{det}^{-1/2}(\partial^2 + V''(\phi_ b))\] <p>In order to calculate the functional determinant, we need to put it in a specific representation, such as the position representation or the momentum representation, to turn $\partial^2 + V’’(\phi_ b)$ into a matrix, then calculate the determinant of that infinite dimensional matrix. Writing</p> \[\Gamma[\phi_ b] = \int d^4 x (-\frac{1}{2} \phi_ b \partial^2 \phi_ b-V(\phi_ b)) + \Delta \Gamma[\phi_ b],\] <p>where</p> \[i\Delta \Gamma[\phi_ b] = -\frac{1}{2}\ln \det (\partial^2 +V''(\phi_ b)) + \text{const}.\] <p>With the help of identity</p> \[\ln \det M = \ln \prod_ i \lambda_ i = \sum_ i \ln \lambda_ i = \text{tr } {\ln M}\] <p>where $\lambda_ i$ are the eigenvalues of $M$, we have</p> \[i\Delta \Gamma[\phi_ b] = -\frac{1}{2} \text{tr } {\ln (\partial^2 +V''(\phi_ b))} + \text{const}.\] <p>Next we assume that $\phi_ b$ is a const in space-time, and define</p> \[m_ {\text{eff}}^2(\phi_ b) \equiv V''(\phi_ b),\] <p>calculate the functional determinant in the representation of $x$, with the help of</p> \[\mathbb{1}= \int \frac{dp^4}{(2\pi)^4} \left\lvert k \right\rangle \left\langle{k}\right\rvert\] <p>we have</p> \[\begin{align*} i\Delta \Gamma[\phi_ b] &amp;= -\frac{1}{2}\int d^4 x \left\langle{x}\right\rvert \ln\left( 1+\frac{V''}{\partial^2} \right) \left\lvert{x}\right\rangle +\text{const}\\ &amp;= -\frac{1}{2}\int d^4 x \int\frac{d^4 k}{(2\pi)^4}\ln\left( 1-\frac{m_ {\text{eff}}^2}{k^2} \right)+\text{const} \end{align*}\] <p>where $\int d^4 x = VT$ is the space-time volume of the system.</p> <p>The integral over momentum is divergent, we will render it finite by a hard cut-off, that is to Wick rotate the system into Euclidean space and insert the momentum cut-off $\Lambda$, yielding</p> \[\begin{align*} \Delta\Gamma[\phi_ b] &amp;= -VT \frac{2\pi^2}{2(2\pi)^4} \int_ 0^\Lambda dk_ E k_ E^3 \ln(1+\frac{m_ {\text{eff}}^2}{k_ E3^2}) +\text{const}\\ &amp;= - \frac{VT}{128\pi^2} \left( 2 m_ {\text{eff}}^2 \Lambda^2 + 2m_ {\text{eff}}^4 \ln\frac{m_ {\text{eff}}^2}{\Lambda^2}+\text{const}\right) \end{align*}\] <p>where $k_ E$ is the Wick-rotated momentum and we have used the relation $\Lambda \gg m_ {\text{eff}}$. The effective potential accordingly is</p> \[V_ {\text{eff}} = V(\phi_ b) + c_ 1 + c_ 2 m_ {\text{eff}}^2 + \frac{1}{64\pi^2} m_ {\text{eff}}^4 \ln\frac{m_ {\text{eff}}^2}{c_ 3}\] <p>where $c_ 1,c_ 2,c_ 3$ are some $\Lambda$-dependent constants. They are independent of $\phi_ b$ thus in general are not of interests to us. For example, $c_ 2 = \Lambda^2 / 64\pi^2$.</p> <p>Next we need to add the counter terms to the potential</p> \[V(\phi) = \frac{1}{2}m_ R^2(1+\delta_ m)\phi^2 +\frac{\lambda_ R}{4!}(1+\delta_ \lambda)\phi^4 +\Lambda_ R (1+\delta_ \Lambda),\] <p>with all the counter terms starting at 1-loop level, that is of $\mathcal{O}(\lambda_ R)$. $m_ {\text{eff}}^2$ is still defined as $V’’$.</p> <p>What about the renormalization conditions?</p> <ul> <li>The question we want to ask is how the quantum corrections change the shape of the potential, when the mass term is zero, thus we require $\lambda_ R^2 = 0$</li> <li>$V(0) = 0 \implies \Lambda_ R = 0$</li> <li>$\lambda_ R = V’’’’(\phi_ R)$ for some arbitrary fixed scale $\phi_ R$</li> </ul> <p>They will fix the counter terms, plugging them in gives</p> \[\boxed{ V_ {\text{eff}}(\phi) = \frac{1}{4!}\phi^4\left\{ \lambda_ R + \frac{3 \lambda_ R^2}{32\pi^2}\left[ \ln \left( \frac{\phi^2}{\phi_ R^2}-\frac{25}{6} \right) \right] \right\} }\] <p>which is known as Coleman-Weinberg potential.</p> <p>Now having the effective potential at hand, we can answer the question: does the quantum correction change the vacuum expectation value of $\phi$? Originally, without the quantum correction, the minimum of the potential is at $\phi = 0$ since there is no quadratic term and only a quartic term. With quantum correction, the vev of $\phi$ is given by the minimum of the effective potential, which is the Coleman-Weinberg potential, which is minimized when</p> \[\lambda_ R \ln \frac{\left\langle \phi^{2} \right\rangle}{\phi_ R^2} = \frac{11}{3} \lambda_ R - \frac{32}{3}\pi^2.\] <p>which gives a nonzero $\left\langle \phi \right\rangle$. It means now we have a double-well potential, instead of the original single-well potential, due to the quantum correction.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="colemanWeinbergPotential"/><category term="effectiveTheory"/><category term="coleman"/><category term="effectivePotential"/><summary type="html"><![CDATA[1. Quantum Effective Action 1.1. Quantum Action 1.2. Effective Potential 2. Coleman-Weinberg Potential 2.1. Background Fields 2.2. Coleman-Weinberg Potential]]></summary></entry><entry><title type="html">Note on the kink mass correction in 3D Part II</title><link href="https://baiyangzhang.github.io/blog/2024/Note-on-Kink-Mass-Correction-in-3D-Part-II/" rel="alternate" type="text/html" title="Note on the kink mass correction in 3D Part II"/><published>2024-03-11T00:00:00+00:00</published><updated>2024-03-11T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Note-on-Kink-Mass-Correction-in-3D-Part-II</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Note-on-Kink-Mass-Correction-in-3D-Part-II/"><![CDATA[<ul> <li><a href="#domain-wall-in-phi-fourth-model">1. Domain wall in phi-fourth model</a></li> <li><a href="#normal-modes">2. Normal Modes</a></li> <li><a href="#fourier-transform">3. Fourier Transform</a></li> <li><a href="#numeric-results">4. Numeric Results</a> <ul> <li><a href="#summation-method">4.1 Summation method</a></li> </ul> </li> </ul> <h1 id="1-domain-wall-in-phi-fourth-model">1. Domain wall in phi-fourth model</h1> <p>Recall that we are working in $D = d+1$ dimensional space-time, the space-dimension is $d$. For now let’s consider $d=2$. In 2-dimensional space, a kink can extend to form a domain wall, for more info about domain walls see my other <a href="https://www.mathlimbo.net/blog/2024/Domain-Wall/">note</a>. The world line of such a domain wall would be world sheet. Now let’s try to calculate the quantum corrections to the tension of such a domain wall.</p> <p>The energy is given by the space integral of the Hamiltonian density,</p> \[H = \int d^{d}x \, \mathcal{H},\quad \mathcal{H} = \frac{1}{\lambda}\left[ \frac{\tilde{\pi}^{2}}{2}+\frac{(\nabla \tilde{\phi})^{2}}{2} + V(\tilde{\phi}) \right]\] <p>where</p> \[\tilde{\phi}:= \sqrt{ \lambda } \phi,\quad \tilde{\pi} := \sqrt{ \lambda } \pi .\] <p>Note that we have written $\phi$ as $\tilde{\phi}$ so that the coupling becomes an overall pre-factor. Keep in mind that, in this formalism, the energy is no longer simply $H=\int \, V$ but $H = \int \, V / \lambda$.</p> <p>Let the spatial coordinates be $x$ and $y$. Consider the potential (expanded in the negative vacuum)</p> \[V(\tilde{\phi}) = \frac{\tilde{\phi}^{2}}{4} (\tilde{\phi}-\sqrt{ 2 }m)^{2}.\] <p>Now consider a flat domain wall in $x=0$ plane. The classical kink solution in the $x$-direction is</p> \[\widetilde{f}(x,y) = \frac{m}{\sqrt{ 2 }} \left( 1+\tanh \frac{mx}{2} \right)\] <p>where again, $\widetilde{f} = \sqrt{ \lambda }f$ by definition.</p> <p>The energy now reads</p> \[H = \int dy \int dx \, \frac{1}{\lambda} \left( \frac{1}{2} (\partial_ {x} \widetilde{f})^{2}+ V(\tilde{f}) \right) =: \int dy \, \rho_ {0}(y),\] <p>where $\rho_ {0}$ is now interpreted as the tension of the domain wall. The calculation is exactly the same as for the kink energy, but to refresh the memory I decided to do it again.</p> <p>Introduce a new variable $t := \frac{mx}{2}$, we have</p> \[\widetilde{f}(t) = \frac{m}{\sqrt{ 2 }} (1+\tanh t),\quad dx = \frac{2}{m} dt,\quad \partial_ {x} = \frac{m}{2} \partial_ {t}.\] <p>The potential contribution to the energy is</p> \[\begin{align*} \int dx \, \frac{1}{\lambda} V(\tilde{f}) &amp;= \frac{m^{3}}{8\lambda}\int dt \, (\tanh ^{2}(t)-1) ^{2}\\ &amp;= \frac{m^{3}}{8\lambda} \int dt \, \frac{1}{\cosh^{4}t} \\ &amp;= \frac{m^{3}}{6\lambda}, \end{align*}\] <p>where we have used</p> \[\int dt \, \frac{1}{\cosh^{4}t} = \frac{4}{3}.\] <p>Similarly, the contribution from the space-derivative term reads</p> \[\begin{align*} \frac{1}{\lambda} \int dx \, \frac{1}{2} (\partial_ {x}\widetilde{f})^{2} &amp;= \frac{m^{3}}{8\lambda} \int dt \, (\partial_ {t}\tanh t)^{2} \\ &amp;= \frac{m^{3}}{8\lambda} \int dt \, \frac{1}{\cosh^{4}t} \\ &amp;= \frac{m^{3}}{6\lambda}. \end{align*}\] <p>Putting Eq. (1) and Eq. (2) together, we have</p> \[\rho_ {0} = \frac{m^{3}}{3\lambda}.\] <hr/> <p>The dimensional analysis gives us</p> \[[\lambda] = 4-D = 3-d,\] <p>hence in $2+1$ dimension the coupling has dimension of mass.</p> <h1 id="2-normal-modes">2. Normal Modes</h1> <p>Since we assumed the domain wall to be flatly lying in the $y$-plane, the normal modes in 2-d space can be <em>factorized</em> into $x$ and $y$ components,</p> \[{\mathfrak g} _ {k_ {x}k_ {y}} (x,y) = {\mathfrak g}_ {k_ {x}}(x) \times e^{ -i y k_ {y}}.\] <p>The normal modes are the solution of the equation of motion in the kink background, or Poschl-Teller potential. These modes include a continuum</p> \[{\mathfrak g} _ {k}(x) = \frac{e^{-ikx}}{\omega_ {k} \sqrt{m^2+4k^2}}\left[2k^2-m^2+(3/2)m^2\text{sech}^2(m x/2)-3im k\tanh(m x/2)\right]\] <p>with eigenvalue $\omega_ {k} = \sqrt{ k^{2}+m^{2} }$, a zero mode</p> \[{\mathfrak g}_ {B} = -\sqrt{ \frac{3m}{8} } \text{sech}^{2}\left( \frac{mx}{2} \right)\] <p>with eigenvalue $0$, and a shape mode</p> \[{\mathfrak g} _ {S} = \frac{\sqrt{ 3m }}{2} \tanh \frac{mx}{2} \text{sech} \frac{mx}{2},\quad \omega_ {S} = \frac{\sqrt{ 3 }}{2}m\] <p>with eigenvalue less then the rest mass of excited particle.</p> <p>Momentum in the $y$-direction also contributes to the total energy, putting them together with the zero modes, shape modes and continuum in the $x$ direction we have</p> \[\boxed { \omega_ {k_ {B}k_ {y}} = \left\lvert k_ {y} \right\rvert ,\quad \omega_ {k_ {S}k_ {y}} = \sqrt{ \frac{3m^{2}}{4}+k_ {y}^{2} },\quad \omega_ {k_ {x} k_ {y}} = \sqrt{ m^{2}+k_ {x}^{2}+k_ {y}^{2} }. }\] <p>Note that</p> <ul> <li>there is a zero mode corresponding to $k_ {B},k_ {y}=0$,</li> <li>the mass gap disappears, due to the mass-gap-less of $y$-momentum.</li> </ul> <p>The formalism we developed in generic dimension $d$ surely also applies to $d=2$. Let $\vec{p}=(p_ {x},p_ {y})$ and $\vec{k}=(k_ {x},k_ {y})$. The Fourier transform is</p> \[\tilde{\mathfrak{g} }_ {k_ {x},k_ {y}} (\vec{p})= \int d^{2}x \, \mathfrak{g} _ {k}(\vec{x}) e^{ -i\vec{k}\cdot \vec{x} } = (2\pi)\delta(k_ {y}+p_ {y}) \times \tilde{\mathfrak{g}} _ k (p_ {x}).\] <p>Again we see the factorization in $x$ and $y$, the $\delta$-function in $y$ direction is due to the plane wave expansion.</p> <p>Note that the infinite volume of a flat $\mathbb{R}$ can be written as Dirac-$\delta$ function $\delta(0)$. To see that, recall the Fourier transform of a function is written as</p> \[\widetilde{f}(\vec{k}) = \int d^{d}x \, e^{ -i\vec{k}\cdot \vec{x} } f(\vec{x})\] <p>which means if we set $f(\vec{x})=1$ then</p> \[\tilde{f}(\vec{k}) = \int d^{d}x \, e^{ -i \vec{k}\cdot\vec{x} } = (2\pi)^{d} \delta^{d}(k),\] <p>If we further set $k=0$ then the integral becomes</p> \[\int d^{d}x \, 1\, = \text{Vol}^{d} = (2\pi)^{d}\delta^{d}(0).\] <p>In the case of 1-dimension, say coordinated by $y$, the total length would be $2\pi \delta(0)$.</p> <p>This identity comes in handy when we take the factorized expression Eq. (3) into the one-loop correction</p> \[\begin{align*} Q_ {1} &amp;= -\frac{1}{4} \sum\!\!\!\!\!\!\!\!\int \frac{\;d^{2}k}{(2\pi)^{2}} \, \int \frac{d^{2}p}{(2\pi)^{2}} \, \left\lvert \tilde{ \mathfrak{g} }_ {k}(\vec{p}) \right\rvert^{2} \frac{(\omega_ {k}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= -\frac{1}{4} \sum\!\!\!\!\!\!\!\!\int \frac{\;d^{2}k}{(2\pi)^{2}} \, \int \frac{d^{2}p}{(2\pi)^{2}} \, \left\lvert (2\pi)\delta(k_ {y}+p_ {y})\tilde{ \mathfrak{g} }_ {k}(p_ {x}) \right\rvert^{2} \frac{(\omega_ {k}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= -\frac{1}{4} \sum\!\!\!\!\!\!\!\!\int \; \frac{dk_ {x}}{2\pi} \int \frac{dk_ {y}}{2\pi} \int \frac{dp_ {x}}{2\pi} (2\pi)\delta(0)\left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \frac{(\omega_ {k_ {x} p_ {y}}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= - \frac{L_ {\text{DM}}}{4}\sum\!\!\!\!\!\!\!\!\int \;\frac{dk_ {x}}{2\pi} \int \frac{dk_ {y}}{2\pi} \int \frac{dp_ {x}}{2\pi} \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \frac{(\omega_ {k_ {x} p_ {y}}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= L_ {\text{DM}} \times (\text{tension correction}). \end{align*}\] <p>In the above equation we have set the dimensionality $d$ to $2$. $\omega_ {k}$ is the short-handed form for $\omega_ {k_ {x}k_ {y}}$, the same for $\omega_ {p}$. Note that <strong>starting from the third line</strong>, we have $\omega_ {k}=\omega_ {k_ {x} p_ {y}}$ due to the Dirac-$\delta$ function. The integral regarding $y$-coordinate gives as a term proportional to $(2\pi)\delta(0)$, which is the total length of the $y$-direction. $L_ {\text{DM}}$ is the length of the domain wall, $L_ {\text{DM}}=\int dy$. It is understood that in the final result $p_ {y}=-k_ {y}$. It agrees with our naive expectation that the correction is proportional to the total length of the domain wall.</p> <p>Eq. (4) can also be written as</p> \[\begin{align*} Q_ {1} &amp;= -\frac{1}{4} \sum\!\!\!\!\!\!\!\!\int \; \frac{dk_ {x}}{2\pi} \int \frac{dp_ {x}}{2\pi} \int \frac{dp_ {y}}{2\pi} (2\pi)\delta(0)\left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \frac{(\omega_ {k}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= - \frac{L_ {\text{DM}}}{4}\sum\!\!\!\!\!\!\!\!\int \;\frac{dk_ {x}}{2\pi} \int \frac{dp^{2}}{(2\pi)^{2}} \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \frac{(\omega_ {k}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= \int dy \, \left( - \frac{1}{4} \right)\sum\!\!\!\!\!\!\!\!\int \;\frac{dk_ {x}}{2\pi} \int \frac{dp^{2}}{(2\pi)^{2}} \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \frac{(\omega_ {k}-\omega_ {p})^{2}}{\omega_ {p}} \\ &amp;= : \int dy \, \rho_ {1}(y). \end{align*}\] <p>Here again $\omega_ {k}$ is understood to be $\omega_ {k_ {x} k_ {y}}$, and we have defined the one-loop correction $\rho_ {1}$ to the tension. Note that <em>one-loop correction comes not from the interaction, as in QFT models in the vacuum sector, but rather comes from the non-trivial soliton background.</em></p> <h1 id="3-fourier-transform">3. Fourier Transform</h1> <p>We still need to evaluate the one-loop correction</p> \[\boxed { \rho_ {1} = \left( - \frac{1}{4} \right)\sum\!\!\!\!\!\!\!\!\int \;\frac{dk_ {x}}{2\pi} \int \frac{dp^{2}}{(2\pi)^{2}} \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \frac{(\omega_ {k}-\omega_ {p})^{2}}{\omega_ {p}}. }\] <p>To do this, first we need to Fourier-Transform various normal modes.</p> <p>To Fourier transform it we use <code class="language-plaintext highlighter-rouge">Mathematica</code>. In Mathematica, the convention used for the Fourier transform is represented by the function <code class="language-plaintext highlighter-rouge">FourierTransform[expr, t, ω]</code>, where <code class="language-plaintext highlighter-rouge">expr</code> is the expression to be transformed, <code class="language-plaintext highlighter-rouge">t</code> is the time variable, and <code class="language-plaintext highlighter-rouge">ω</code> is the frequency variable. This gives the symbolic Fourier transform of the expression. For multidimensional Fourier transforms, you can use <code class="language-plaintext highlighter-rouge">FourierTransform[expr, {t1, t2, …}, {ω1, ω2, …}]</code>, but here we won’t need it.</p> <p>The basis function used by Mathematica’s <code class="language-plaintext highlighter-rouge">FourierTransform</code> is dependent on the <code class="language-plaintext highlighter-rouge">FourierParameters</code> setting. The <code class="language-plaintext highlighter-rouge">FourierParameters</code> option is specified as {a, b}, where a and b are parameters that influence the Fourier transform’s definition. Specifically, they appear in the exponential factor and the normalization constant of the Fourier transform equations. The default setting for <code class="language-plaintext highlighter-rouge">FourierParameters</code> in Mathematica is <code class="language-plaintext highlighter-rouge">{0, 1}</code>, which corresponds to using the basis function $e^{i t \omega}$ for the Fourier transform,</p> \[\mathcal{F}\left\lbrace f(t) \right\rbrace (\omega) = \frac{1}{\sqrt{ 2\pi }} \int_{-\infty}^{\infty} dt \, f(t) e^{ i \omega t }\] <p>where $\mathcal{F}\left\lbrace f \right\rbrace$ is the Fourier transform of function $f(t)$. In general, under <code class="language-plaintext highlighter-rouge">FourierParameters-&gt;{a,b}</code> the Fourier Transform is set to be</p> \[\mathcal{F}\left\lbrace f(t) \right\rbrace (\omega){\Large\mid}_ {\left\lbrace a,b \right\rbrace } = \sqrt{\frac{\left\lvert b \right\rvert }{( 2\pi)^{1-a} }} \int_{-\infty}^{\infty} d t \, f(t) e^{i b \omega t }\] <p>In order to change it to our desired basis, we need to set $a=1,b=-1$, which is the so-called physics convention. As a test let’s try to transform a plane wave $\exp(-ipx)$, with our convention mathematica should return $2\pi \delta(p+k)$. The code reads</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">FourierTransform</span><span class="p">[</span><span class="nb">Exp</span><span class="p">[</span><span class="o">-</span><span class="nb">I</span><span class="w"> </span><span class="nv">p</span><span class="w"> </span><span class="nv">x</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">k</span><span class="o">,</span><span class="w"> </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">
</span></code></pre></div></div> <p>which indeed returns <code class="language-plaintext highlighter-rouge">2 Pi DiracDelta[k + p]</code>.</p> <hr/> <p><strong>The shape mode</strong></p> <p>The shape mode as a function of $x$ reads</p> \[{\mathfrak g} _ {S}(x) = \frac{\sqrt{ 3m }}{2} \tanh \frac{mx}{2} \text{sech} \frac{mx}{2},\] <p>whose Fourier transform can be calculated with mathematica code</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">fkgS</span><span class="p">[</span><span class="nv">x</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">Sqrt</span><span class="p">[</span><span class="m">3</span><span class="w"> </span><span class="nv">m</span><span class="p">]</span><span class="o">/</span><span class="m">2</span><span class="w"> </span><span class="nb">Tanh</span><span class="p">[(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="nb">Sech</span><span class="p">[</span><span class="w"> </span><span class="p">(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">]</span><span class="o">;</span><span class="w">
</span><span class="nb">FourierTransform</span><span class="p">[</span><span class="nv">fkgS</span><span class="p">[</span><span class="nv">x</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">p</span><span class="o">,</span><span class="w"> </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">
</span></code></pre></div></div> <p>The result reads</p> \[\boxed { \tilde{ \mathfrak{g} }_ {S}(p) = -\frac{2i\sqrt{ 3 }\pi p}{m^{3/2}} \mathrm{sech}\left( \frac{p\pi}{m} \right). }\] <hr/> <p><strong>The zero mode</strong></p> <p>From the following Mathematica code</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">fkgB</span><span class="p">[</span><span class="nv">x</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">-</span><span class="nb">Sqrt</span><span class="p">[((</span><span class="m">3</span><span class="w"> </span><span class="nv">m</span><span class="p">)</span><span class="o">/</span><span class="m">8</span><span class="p">)]</span><span class="w"> </span><span class="nb">Sech</span><span class="p">[(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">]</span><span class="o">^</span><span class="m">2</span><span class="o">;</span><span class="w">
</span><span class="nb">FourierTransform</span><span class="p">[</span><span class="nv">fkgB</span><span class="p">[</span><span class="nv">x</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">k</span><span class="o">,</span><span class="w"> </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">
</span></code></pre></div></div> <p>we have</p> \[\boxed { \tilde{ \mathfrak{g} }_ {B}(p) = -\frac{\sqrt{ 6 }\pi p}{m^{3/2}}\text{csch}\left(\frac{\pi p}{m} \right) }\] <hr/> <p><strong>The continuum</strong></p> <p>The Fourier transform is given by the following Mathematica code,</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">fkgk</span><span class="p">[</span><span class="nv">x</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="nb">E</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="nb">I</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nv">\[Omega]k</span><span class="w"> </span><span class="nb">Sqrt</span><span class="p">[</span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="o">+</span><span class="m">4</span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="p">])(</span><span class="m">2</span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="o">-</span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="o">+</span><span class="m">3</span><span class="o">/</span><span class="m">2</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="nb">Sech</span><span class="p">[(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">3</span><span class="nb">I</span><span class="w"> </span><span class="nv">m</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nb">Tanh</span><span class="p">[(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">])</span><span class="o">;</span><span class="w">
</span><span class="nb">FourierTransform</span><span class="p">[</span><span class="nv">fkgk</span><span class="p">[</span><span class="nv">x</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">p</span><span class="o">,</span><span class="w"> </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">
</span></code></pre></div></div> <p>which gives us</p> \[\tilde{ \mathfrak{g} }_ {k}(p) = \frac{6\pi p}{\omega_ {k}\sqrt{ 4k^{2} + m^{2} }} \text{csch}\left(\frac{(k+p)\pi}{m}\right).\] <p>However, if we perform the Fourier transform term-by-term we get</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">In[1]:=</span><span class="w"> </span><span class="nb">FourierTransform</span><span class="p">[</span><span class="nb">E</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="nb">I</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nv">\[Omega]k</span><span class="w"> </span><span class="nb">Sqrt</span><span class="p">[</span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="p">])</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">p</span><span class="o">,</span><span class="w"> 
 </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">

</span><span class="gp">Out[1]=</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="nv">\[Pi]</span><span class="w"> </span><span class="nb">DiracDelta</span><span class="p">[</span><span class="nv">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">p</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="nb">Sqrt</span><span class="p">[</span><span class="m">4</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="nv">\[Omega]k</span><span class="p">)</span><span class="w">

</span><span class="gp">In[2]:=</span><span class="w"> </span><span class="nb">FourierTransform</span><span class="p">[</span><span class="nb">E</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="nb">I</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nv">\[Omega]k</span><span class="w"> </span><span class="nb">Sqrt</span><span class="p">[</span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="p">])</span><span class="w"> </span><span class="p">(</span><span class="m">3</span><span class="o">/</span><span class="m">2</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="nb">Sech</span><span class="p">[(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">]</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">p</span><span class="o">,</span><span class="w"> 
 </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">

</span><span class="gp">Out[2]=</span><span class="w"> </span><span class="p">(</span><span class="m">6</span><span class="w"> </span><span class="p">(</span><span class="nv">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">p</span><span class="p">)</span><span class="w"> </span><span class="nv">\[Pi]</span><span class="w"> </span><span class="nb">Csch</span><span class="p">[((</span><span class="nv">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">p</span><span class="p">)</span><span class="w"> </span><span class="nv">\[Pi]</span><span class="p">)</span><span class="o">/</span><span class="nv">m</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="nb">Sqrt</span><span class="p">[</span><span class="m">4</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="nv">\[Omega]k</span><span class="p">)</span><span class="w">

</span><span class="gp">In[3]:=</span><span class="w"> </span><span class="nb">FourierTransform</span><span class="p">[</span><span class="nb">E</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="nb">I</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nv">\[Omega]k</span><span class="w"> </span><span class="nb">Sqrt</span><span class="p">[</span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="p">])</span><span class="w"> </span><span class="p">(</span><span class="o">-</span><span class="m">3</span><span class="w"> </span><span class="nb">I</span><span class="w"> </span><span class="nv">m</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nb">Tanh</span><span class="p">[(</span><span class="nv">m</span><span class="w"> </span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">])</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">p</span><span class="o">,</span><span class="w"> 
 </span><span class="nb">FourierParameters</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">1</span><span class="p">}]</span><span class="w">

</span><span class="gp">Out[3]=</span><span class="w"> </span><span class="o">-</span><span class="p">((</span><span class="m">6</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nv">\[Pi]</span><span class="w"> </span><span class="nb">Csch</span><span class="p">[((</span><span class="nv">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">p</span><span class="p">)</span><span class="w"> </span><span class="nv">\[Pi]</span><span class="p">)</span><span class="o">/</span><span class="nv">m</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="nb">Sqrt</span><span class="p">[</span><span class="m">4</span><span class="w"> </span><span class="nv">k</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">m</span><span class="o">^</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="nv">\[Omega]k</span><span class="p">))</span><span class="w">
</span></code></pre></div></div> <p>Putting them together we get</p> \[\tilde{ \mathfrak{g} }_ {k}(p) = \frac{6\pi p}{\omega_ {k}\sqrt{ 4k^{2} + m^{2} }} \text{csch}\left(\frac{(k+p)\pi}{m}\right)+\frac{2\pi(2k^{2}-m^{2})}{\omega_ {k}\sqrt{ 4k^{2} + m^{2} }} \delta(p+k).\] <p>I have no idea why this delta function disappeared. Any help here?</p> <hr/> <p>Let’s proceed to calculate $\rho_ {1}$. Since $k_ {x}$ is separated into three parts, i.e. zero mode, shape mode and continuum, we can separate $\rho_ {1}$ into three corresponding parts, writing</p> \[\rho_ {1} = \rho_ {1B} + \rho_ {1S} + \int \frac{dk_ {x}}{2\pi} \, \rho_ {1k_ {x}}\] <p>where</p> \[\begin{align*} \rho_ {1B} &amp;= - \frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, (\tilde{ \mathfrak{g} }_ {B}(p_ {x}))^{2} \int \frac{dp_ {y}}{2\pi}\, \frac{(\omega_ {k_ {B}p_ {y}}-\omega_ {p_ {x}p_ {y}})^{2}}{\omega_ {p_ {x}p_ {y}}}, \\ \rho_ {1S} &amp;= -\frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, (\tilde{ \mathfrak{g} }_ {S}(p_ {x}))^{2} \int \frac{dp_ {y}}{2\pi} \, \frac{(\omega_ {k_ {S}p_ {y}}-\omega_ {p_ {x}p_ {y}})^{2}}{\omega_ {p_ {x}p_ {y}}}, \\ \rho_ {1C} &amp;= \int \, \frac{dk_ {x}}{2\pi} \left( - \frac{1}{4} \right) \int \frac{dp_ {x}}{2\pi}\, \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \int \frac{dp_ {y}}{2\pi} \, \frac{(\omega_ {k_ {x}p_ {y}}-\omega_ {p_ {x}p_ {y}})^{2}}{\omega_ { p_ {x}p_ {y}}} \\ &amp;=: \int \frac{dk_ {x}}{2\pi} \, \rho_ {1k_ {x}}. \end{align*}\] <p>The subscript $C$ in $\rho_ {1C}$ stands for continuum.</p> <p>We have separated the integral over $d^{2}p$ into its two components for a reason, so we can perform the integral over the flat direction $p_ {y}$ analytically first. For the sake of convenience let’s define a general integral of form</p> \[\mathcal{I} (a,b) := \int_ {-\infty}^{\infty} \frac{d p}{2\pi} \, \frac{(\sqrt{ a+p^{2} }-\sqrt{ b+p^{2} })^{2}}{\sqrt{ b+p^{2} }} ,\] <p>With the help of mathematica again we get</p> \[\mathcal{I} (a,b) = \frac{a}{2\pi} \left( \frac{b}{a}-\ln \left\lvert \frac{b}{a} \right\rvert -1 \right).\] <p>This form shows that if $a=b$ then $\mathcal{I}_ {ab}=0$, as it should be by definition. If we allow the parameters of $\ln$ function to be dimensionful, another form is more useful to us,</p> \[\boxed { \mathcal{I} (a,b) = \frac{1}{2\pi} (b-a-a\ln \left\lvert b \right\rvert +a\ln \left\lvert a \right\rvert ). }\] <p>This form made obvious that $\mathcal{I}(a,b)$ behaviors nice at $a=0$.</p> <hr/> <p>To calculate Eq. (6), recall that (since $k_ {y}=-p_ {y}$)</p> \[\boxed { \omega_ {k_ {B}p_ {y}} = \left\lvert p_ {y} \right\rvert ,\quad \omega_ {k_ {S}p_ {y}} = \sqrt{ \frac{3m^{2}}{4}+p_ {y}^{2} },\quad \omega_ {k_ {x} p_ {y}} = \sqrt{ m^{2}+k_ {x}^{2}+p_ {y}^{2} }. }\] <p>We have</p> \[\begin{align*} \rho_ {1B} &amp;= - \frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, (\tilde{ \mathfrak{g} }_ {B}(p_ {x}))^{2} \int \frac{dp_ {y}}{2\pi}\, \frac{(\omega_ {k_ {B}p_ {y}}-\omega_ {p_ {x}p_ {y}})^{2}}{\omega_ {p_ {x}p_ {y}}}, \\ &amp;= - \frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, (\tilde{ \mathfrak{g} }_ {B}(p_ {x}))^{2} \mathcal{I}(0,m^{2}+p^{2}_ {x}) \\ &amp;= - \frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, (\tilde{ \mathfrak{g} }_ {B}(p_ {x}))^{2} \frac{m^{2}+p^{2}_ {x}}{2\pi} \\ &amp;= - \frac{1}{8\pi} \int \frac{dp_ {x}}{2\pi} \, (\tilde{ \mathfrak{g} }_ {B}(p_ {x}))^{2} (m^{2}+p^{2}_ {x})\\ &amp;= -\frac{3 m^2}{20 \pi} \end{align*}\] <p>The last line is obtained by Mathematica code</p> <pre><code class="language-Mathematica">tldgB[p_] := -((Sqrt[6] \[Pi] p) /m^(3/2)) Csch[(p \[Pi])/m];
Integrate[-1/(8 \[Pi]) 1/(2 Pi) tldgB[px]^2 (m^2 + px^2), {px, -\[Infinity], \[Infinity]}, 
 Assumptions -&gt; {m &gt; 0, m \[Element] Reals}]
</code></pre> <hr/> <p>Next let’s move on to Eq. (7). We have</p> \[\begin{align*} \rho_ {1S} &amp;= -\frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, \left\lvert \tilde{ \mathfrak{g} }_ {S}(p_ {x}) \right\rvert ^{2} \int \frac{dp_ {y}}{2\pi} \, \frac{(\omega_ {k_ {S}p_ {y}}-\omega_ {p_ {x}p_ {y}})^{2}}{\omega_ {p_ {x}p_ {y}}} \\ &amp;= -\frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, \left\lvert \tilde{ \mathfrak{g} }_ {S}(p_ {x}) \right\rvert ^{2} \, \mathcal{I}\left( \frac{3}{4}m^{2},m^{2}+p_ {x}^{2} \right) \\ &amp;= -\frac{1}{4} \int \frac{dp_ {x}}{2\pi} \, \left\lvert \tilde{ \mathfrak{g} }_ {S}(p_ {x}) \right\rvert ^{2} \, \mathcal{I}\left( \frac{3}{4}m^{2},m^{2}+p_ {x}^{2} \right) \end{align*}\] <p>where</p> \[\mathcal{I}\left( \frac{3}{4}m^{2},m^{2}+p_ {x}^{2} \right) = \frac{m^{2}}{2\pi} \left[ \frac{1}{4} + \frac{p_ {x}^{2}}{m^{2}} + \frac{3}{4}\ln\left( \frac{3}{4} \right) - \frac{3}{4}\ln\left( 1+\frac{p_ {x}^{2}}{m^{2}} \right) \right].\] <p>We’ll have to turn to numerical calculation now. For the sake of numerical calculation, we better change $p_ {x}$ to something without dimension. Set $t:= p_ {x} / m$, we have</p> \[\begin{align*} dp_ {x} &amp;= m dt, \\ \tilde{ \mathfrak{g} }_ {S}(t) &amp;= - \frac{2i\sqrt{3} \pi t}{\sqrt{ m }} \mathrm{sech}\,(\pi t), \\ \mathcal{I}(t) &amp;= \frac{m^{2}}{2\pi} \left[ \frac{1}{4} + t^{2} + \frac{3}{4}\ln\left( \frac{3}{4} \right) - \frac{3}{4}\ln\left( 1+ t^{2} \right) \right]. \end{align*}\] <p>All of this gives us</p> \[\rho_ {1S} = -\frac{1}{4} \int_ {\infty}^{\infty} \frac{m dt}{2\pi} \, \left\lvert \tilde{ \mathfrak{g} }_ {S}(t) \right\rvert^{2} \, \mathcal{I}(t) = -0.00725 m^{2}.\] <hr/> <p>It is a little harder to do Eq. (8).</p> \[\begin{align*} \rho_ {1C} &amp;= - \frac{1}{4} \int \, \frac{dk_ {x}}{2\pi} \int \frac{dp_ {x}}{2\pi}\, \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \int \frac{dp_ {y}}{2\pi} \, \frac{(\omega_ {k_ {x}p_ {y}}-\omega_ {p_ {x}p_ {y}})^{2}}{\omega_ { p_ {x}p_ {y}}} \\ &amp;= - \frac{1}{4} \int \, \frac{dk_ {x}}{2\pi} \int \frac{dp_ {x}}{2\pi}\, \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \, \mathcal{I}(m^{2}+k_ {x}^{2},m^{2}+p_ {x}^{2}) \end{align*}\] <p>where</p> \[\mathcal{I}(m^{2}+k_ {x}^{2},m^{2}+p_ {x}^{2}) = \frac{1}{2\pi} \left[ p_ {x}^{2}-k_ {x}^{2}+(m^{2}+k_ {x}^{2})\ln\left( \frac{m^{2}+k_ {x}^{2}}{m^{2}+p_ {x}^{2}} \right) \right].\] <p>This leaves</p> \[\rho_ {1k_ {x}} = - \frac{1}{8\pi} \int \frac{dp_ {x}}{2\pi}\, \left\lvert \tilde{\mathfrak{g}}_ {k_ {x}} (p_ {x})\right\rvert ^{2} \, \left[ p_ {x}^{2}-k_ {x}^{2}+(m^{2}+k_ {x}^{2})\ln\left( \frac{m^{2}+k_ {x}^{2}}{m^{2}+p_ {x}^{2}} \right) \right]\] <p>Now we simply need to substitute</p> \[\tilde{ \mathfrak{g} }_ {k_ {x}}(p_ {x}) = \frac{6\pi p_ {x}}{\omega_ {k_ {x}}\sqrt{ 4k_ {x}^{2} + m^{2} }} \text{csch}\left(\frac{(k_ {x}+p_ {x})\pi}{m}\right)+\frac{2\pi(2k_ {x}^{2}-m^{2})}{\omega_ {k_ {x}}\sqrt{ 4k_ {x}^{2} + m^{2} }} \delta(p_ {x}+k_ {x}).\] <p>where $\omega_ {k_ {x}} = \sqrt{ m^{2}+k_ {x}^{2} }$. Note that since $\mathcal{I}(m^{2}+k_ {x}^{2},m^{2}+p_ {x}^{2})$ equals to zero when $p=-k$, hence we can discard the $\delta$ term and focus on the $\text{csch}$ term.</p> <p>Again we have to turn to numerical methods, and again we need to cook up some dimension-less variables from $k_ {x}$ and $p_ {x}$. Define $\kappa:= k_ {x} / m$ and $\rho := p_ {x} / m$, we have</p> \[\frac{\rho_ {1k_ {x}}(\kappa)}{m} = \int d \rho \, (-9 \rho ^2) \frac{ \left(\left(\kappa ^2+1\right) \log \left(\frac{\kappa ^2+1}{\rho ^2+1}\right)-\kappa ^2+\rho ^2\right) }{4 \left(4 \kappa ^4+5 \kappa ^2+1\right)} \, \text{csch}^2(\pi (\kappa +\rho )).\] <p>The figure of the integral is shown in the below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/rhokmarNew-480.webp 480w,/img/rhokmarNew-800.webp 800w,/img/rhokmarNew-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/rhokmarNew.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The contribution $\rho_ {1k_ x}$ to the one loop tension arising from each continuum normal mode $k_ x$. </div> <p>The numerical result shows that the total contribution is</p> \[\rho_ {1C} = -0.0312775 m^{2}\] <p>which is slightly different from that in the draft where $\rho_ {1C}=−0.03156 m^{2}$. We will explain how we get the numerical result and how reliable they are in the following section.</p> <h1 id="4-numeric-results">4. Numeric Results</h1> <p><strong>Result Using Mathematica function <code class="language-plaintext highlighter-rouge">NIntegral</code></strong></p> <p>We can directly perform the double integral over $dk_ {x}dp_ {x}$, based on Eq. (5.18) in the paper. Using the Mathematica code in the following (it is meant to be copied and opened in Mathematica notebook)</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">Module</span><span class="p">[{</span><span class="nv">integrandTem</span><span class="p">}</span><span class="o">,</span><span class="w"> 
 </span><span class="nv">integrandTem</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">_,</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="m">9</span><span class="o">/</span><span class="p">(</span><span class="m">2</span><span class="o">*</span><span class="m">4</span><span class="w"> </span><span class="nb">Pi</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="p">)))</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="p">(</span><span class="nb">Csch</span><span class="p">[</span><span class="nb">Pi</span><span class="w"> </span><span class="p">(</span><span class="nv">\[Kappa]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="p">)])</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="p">(</span><span class="o">-</span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="nb">Log</span><span class="p">[(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="p">)])</span><span class="o">;</span><span class="w"> 
 </span><span class="nb">NIntegrate</span><span class="p">[</span><span class="nv">integrandTem</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="nv">\[Infinity]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Infinity]</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Rho]</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="nv">\[Infinity]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Infinity]</span><span class="p">}]</span><span class="w">
 </span><span class="p">]</span><span class="w">
</span></code></pre></div></div> <p>where $\kappa := k_ {x} / m$ and $\rho := p_ {x} / m$, they are the reduced momenta.</p> <p>We get -0.0290278 without any error or warning message. However since the slow-converging nature and the fact that the 2D integral domain is infinity, this result is not very reliable, especially it differs from results that we got from more reliable method, which we will elaborate in the following.</p> <p>When the parameter <code class="language-plaintext highlighter-rouge">WorkingPrecision</code> and <code class="language-plaintext highlighter-rouge">MaxRecursion</code> is set too high, $50$ for example, the integral result will be unstable and oscillates wildly. It can be seen from running the following code, we will skip the final result rather just mention that it doesn’t work.</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">Module</span><span class="p">[{</span><span class="nv">data</span><span class="p">}</span><span class="o">,</span><span class="w"> 
 </span><span class="nv">rho1kx</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> 
  </span><span class="nb">NIntegrate</span><span class="p">[</span><span class="w">
   </span><span class="nv">integrand</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Rho]</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="nv">\[Infinity]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Infinity]</span><span class="p">}</span><span class="o">,</span><span class="w"> 
   </span><span class="nb">WorkingPrecision</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="m">50</span><span class="o">,</span><span class="w"> </span><span class="nb">MaxRecursion</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="m">50</span><span class="o">,</span><span class="w"> </span><span class="nb">PrecisionGoal</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="m">50</span><span class="o">,</span><span class="w"> 
   </span><span class="nb">AccuracyGoal</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="m">50</span><span class="p">]</span><span class="o">;</span><span class="w">
 </span><span class="nb">Plot</span><span class="p">[</span><span class="nv">rho1kx</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="m">10</span><span class="o">,</span><span class="w"> </span><span class="m">10</span><span class="p">}]]</span><span class="w">
</span></code></pre></div></div> <p>Similarly, <code class="language-plaintext highlighter-rouge">PrecisionGoal-&gt;50</code> does not help, for <em>the result is not even monotonically decreasing at large $k_ {x}$</em>, we list the code and the result:</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">Module</span><span class="p">[{</span><span class="nv">data</span><span class="p">}</span><span class="o">,</span><span class="w"> 
 </span><span class="nv">rho1kx</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> 
  </span><span class="nb">NIntegrate</span><span class="p">[</span><span class="w">
   </span><span class="nv">integrand</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Rho]</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="nv">\[Infinity]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Infinity]</span><span class="p">}</span><span class="o">,</span><span class="w"> 
   </span><span class="nb">WorkingPrecision</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="m">50</span><span class="o">,</span><span class="w"> </span><span class="nb">MaxRecursion</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="m">50</span><span class="o">,</span><span class="w"> </span><span class="nb">PrecisionGoal</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="m">50</span><span class="o">,</span><span class="w"> 
   </span><span class="nb">AccuracyGoal</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="m">50</span><span class="p">]</span><span class="o">;</span><span class="w">
 </span><span class="nv">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">Table</span><span class="p">[{</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="nv">rho1kx</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="p">]}</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="m">100</span><span class="o">,</span><span class="w"> </span><span class="m">10</span><span class="p">}]</span><span class="o">;</span><span class="w">
 </span><span class="nb">TableForm</span><span class="p">[</span><span class="nv">data</span><span class="o">,</span><span class="w"> 
  </span><span class="nb">TableHeadings</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="nb">None</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="s">"\[Kappa]"</span><span class="o">,</span><span class="w"> </span><span class="s">"rho1kx[\[Kappa]]"</span><span class="p">}}]]</span><span class="w">
</span></code></pre></div></div> <table> <thead> <tr> <th>$\kappa:=k_ {x} / m$</th> <th style="text-align: left">$2\pi\rho_ {1k_ {x}}(\kappa)$</th> </tr> </thead> <tbody> <tr> <td>0</td> <td style="text-align: left">-0.01006749649788711098711857084994935329969685541183878214107904582123515308302</td> </tr> <tr> <td>10</td> <td style="text-align: left">-0.0011686020701178210972909521296487066189263742489601351585665823047453837217</td> </tr> <tr> <td>20</td> <td style="text-align: left">-0.00029683035584775581156058547901822136103318225147929961904900419907102724281</td> </tr> <tr> <td>30</td> <td style="text-align: left">-0.00013231530085233583442497003327532617560145592710253697711716088939790650143</td> </tr> <tr> <td>40</td> <td style="text-align: left">-0.00007450450777691673214866577793384482867825300381463112353663045604557837695</td> </tr> <tr> <td>50</td> <td style="text-align: left">-0.00004770576549529244561173447135702738762050345037729455603576967068239692329</td> </tr> <tr> <td>60</td> <td style="text-align: left">-0.00003313763983924425143831035983787787108360534775902718717778116210540211577</td> </tr> <tr> <td>70</td> <td style="text-align: left">-3.05720109112434448297096772257979519066270693516655258261985652468392e-67</td> </tr> <tr> <td>80</td> <td style="text-align: left">-0.00001864475446773486568369540682272388850447396654084892463438555277298711842</td> </tr> <tr> <td>90</td> <td style="text-align: left">-1.4130111258010705126942302422531874366831796760314017313726306134102608247521657e-56</td> </tr> <tr> <td>100</td> <td style="text-align: left">-1.08733498976318462751508987228401098685929997485580324213060115407081675e-83</td> </tr> </tbody> </table> <p>It shows wild oscillation, hence is not trust-worthy.</p> <p>With reasonable options</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">NIntegrate</span><span class="p">[</span><span class="w">
 </span><span class="nv">rho1kx</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="m">2</span><span class="w"> </span><span class="nb">Pi</span><span class="p">)</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="nv">\[Infinity]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Infinity]</span><span class="p">}</span><span class="o">,</span><span class="w">
 </span><span class="nb">WorkingPrecision</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="m">30</span><span class="o">,</span><span class="w"> </span><span class="nb">PrecisionGoal</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="m">6</span><span class="o">,</span><span class="w"> </span><span class="nb">AccuracyGoal</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="m">6</span><span class="w">
 </span><span class="p">]</span><span class="w">
</span></code></pre></div></div> <p>we get the following result:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">NIntegrate</code> failed to converge to prescribed accuracy after 9 recursive bisections in [Kappa] near {[Kappa]} = {169.954045306876363791355871581}. <code class="language-plaintext highlighter-rouge">NIntegrate</code> obtained -0.0313611336264321023970618041371 and 2.68446200766726199070353491885 * 10^-6 for the integral and error estimates”.</p> </blockquote> <p>Which translates to $\boxed {-0.031361 \pm 0.0000027}$. However, the origin of this error estimation is unclear to me, and multiple warnings about converging too slowly make it even less convincing. This motivates us to try another more direct but more controllable method to perform the integral.</p> <hr/> <p><strong>Cutoff on $\rho$-integral</strong></p> <p>Let’s take a closer look at the integral Eq. (9). The integrand contains a factor roughly $\text{csch}^{2}(\kappa+\rho )$, proportional to $e^{-2 (\kappa+\rho) }$, a exponential suppression. When $\kappa$ is fixed, the only integral domain that contributes to the integral is $\rho \in (\kappa-\Lambda,\kappa+\Lambda)$ for some cutoff $\Lambda$. The below numerical results will show that it is sufficient for $\Lambda=4$. We compare the integrals with and without a cutoff, and calculate the numerical difference. The Mathematica code is as follow.</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">(*cut_:4 means the defalut value for cut is 4, which is  used unless \</span><span class="err">
</span><span class="c">an explicit value is provided for cut in the function call. rho1kxCut \</span><span class="err">
</span><span class="c">can be called as, for example, rho1kxCut[0] or rho1kxCut[0,6] *)</span><span class="w">

</span><span class="nv">integrand</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">_,</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">-</span><span class="p">((</span><span class="w">
   </span><span class="m">9</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="nb">Csch</span><span class="p">[</span><span class="nv">\[Pi]</span><span class="w"> </span><span class="p">(</span><span class="nv">\[Kappa]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="p">)]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="p">(</span><span class="o">-</span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">\</span><span class="w">
</span><span class="nv">\[Rho]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="nb">Log</span><span class="p">[(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">^</span><span class="m">2</span><span class="p">)]))</span><span class="o">/</span><span class="p">(</span><span class="w">
   </span><span class="m">4</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">4</span><span class="p">)))</span><span class="o">;</span><span class="w">
</span><span class="nv">rho1kxCut</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">_,</span><span class="w"> </span><span class="nv">cut</span><span class="o">_</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="m">4</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> 
  </span><span class="nb">NIntegrate</span><span class="p">[</span><span class="w">
   </span><span class="nv">integrand</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Rho]</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="nv">\[Kappa]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">cut</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="nv">\[Kappa]</span><span class="w"> </span><span class="o">+</span><span class="w"> 
     </span><span class="nv">cut</span><span class="p">}]</span><span class="o">;</span><span class="w">

</span><span class="bp">Module</span><span class="p">[{</span><span class="nv">data</span><span class="p">}</span><span class="o">,</span><span class="w">
 </span><span class="nv">rho1kxOld</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> 
  </span><span class="nb">NIntegrate</span><span class="p">[</span><span class="w">
   </span><span class="nv">integrand</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Rho]</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="nv">\[Infinity]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Infinity]</span><span class="p">}]</span><span class="o">;</span><span class="w">
 </span><span class="nv">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">Table</span><span class="p">[{</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> 
    </span><span class="nv">rho1kxCut</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">(</span><span class="nv">rho1kxCut</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">rho1kxOld</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="p">])</span><span class="o">/</span><span class="w">
     </span><span class="nv">rho1kxOld</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="p">]}</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="m">70</span><span class="o">,</span><span class="w"> </span><span class="m">5</span><span class="p">}]</span><span class="o">;</span><span class="w">
 </span><span class="nb">Labeled</span><span class="p">[</span><span class="w">
  </span><span class="nb">TableForm</span><span class="p">[</span><span class="nv">data</span><span class="o">,</span><span class="w"> 
   </span><span class="nb">TableHeadings</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="nb">None</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="s">"\[Kappa]"</span><span class="o">,</span><span class="w"> </span><span class="s">"rho1kxCut[\[Kappa]]"</span><span class="o">,</span><span class="w"> 
      </span><span class="s">"Relative Error"</span><span class="p">}}]</span><span class="o">,</span><span class="w"> 
  </span><span class="s">"\[Rho]\[Element](-\[Kappa]-4,-\[Kappa]+4)"</span><span class="o">,</span><span class="w"> </span><span class="nb">Top</span><span class="p">]</span><span class="w">
 </span><span class="p">]</span><span class="w">
</span></code></pre></div></div> <p>The result is shown in the following, where relative error is defined as $(\text{cut}-\text{uncut})/\text{cut}$.</p> <p>$\rho \in(\kappa-4,\kappa+4)$</p> <table> <thead> <tr> <th>$\kappa$</th> <th>rho1kxCut</th> <th>Relative Error</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>-0.0100675</td> <td>-8.70642e-7</td> </tr> <tr> <td>5</td> <td>-0.00439214</td> <td>-3.62796e-9</td> </tr> <tr> <td>10</td> <td>-0.0011686</td> <td>-3.03801e-9</td> </tr> <tr> <td>15</td> <td>-0.000525522</td> <td>-2.66222e-9</td> </tr> <tr> <td>20</td> <td>-0.00029683</td> <td>-2.60459e-9</td> </tr> <tr> <td>25</td> <td>-0.000190336</td> <td>-2.83341e-9</td> </tr> <tr> <td>30</td> <td>-0.000132315</td> <td>-2.21074e-7</td> </tr> <tr> <td>35</td> <td>-0.0000972723</td> <td>-2.50754e-9</td> </tr> <tr> <td>40</td> <td>-0.0000745045</td> <td>6.09896e-6</td> </tr> <tr> <td>45</td> <td>-0.0000588842</td> <td>9.25771e-6</td> </tr> <tr> <td>50</td> <td>-0.0000477058</td> <td>-0.000611088</td> </tr> <tr> <td>55</td> <td>-0.0000394321</td> <td>-3.35955e-6</td> </tr> <tr> <td>60</td> <td>-0.0000331376</td> <td>7.45096e-6</td> </tr> <tr> <td>65</td> <td>-0.0000282381</td> <td>7.13059e-7</td> </tr> <tr> <td>70</td> <td>-0.0000243498</td> <td>0.000272326</td> </tr> </tbody> </table> <p>$\rho \in(-\kappa-5,-\kappa+5)$:</p> <table> <thead> <tr> <th>$\kappa$</th> <th>rho1kxCut</th> <th>Relative Error</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>-0.0100675</td> <td>-4.05348e-9</td> </tr> <tr> <td>5</td> <td>-0.00439214</td> <td>-1.14805e-11</td> </tr> <tr> <td>10</td> <td>-0.0011686</td> <td>-2.57375e-10</td> </tr> <tr> <td>15</td> <td>-0.000525522</td> <td>-1.60657e-11</td> </tr> <tr> <td>20</td> <td>-0.00029683</td> <td>-2.65453e-11</td> </tr> <tr> <td>25</td> <td>-0.000190336</td> <td>-2.58825e-10</td> </tr> <tr> <td>30</td> <td>-0.000132315</td> <td>-2.18558e-7</td> </tr> <tr> <td>35</td> <td>-0.0000972723</td> <td>2.75179e-11</td> </tr> <tr> <td>40</td> <td>-0.0000745045</td> <td>6.10152e-6</td> </tr> <tr> <td>45</td> <td>-0.0000588842</td> <td>9.26034e-6</td> </tr> <tr> <td>50</td> <td>-0.0000477058</td> <td>-0.000611086</td> </tr> <tr> <td>55</td> <td>-0.0000394321</td> <td>-3.35706e-6</td> </tr> <tr> <td>60</td> <td>-0.0000331376</td> <td>7.45363e-6</td> </tr> <tr> <td>65</td> <td>-0.0000282381</td> <td>7.15863e-7</td> </tr> <tr> <td>70</td> <td>-0.0000243498</td> <td>0.000272329</td> </tr> </tbody> </table> <p>For $\rho \in(\kappa-4,\kappa+4)$, the relative error is negligible. <em>Hence we will used the cutoff version.</em></p> <h2 id="41-summation-method">4.1. Summation method</h2> <p>At lower values of $\kappa$ we can afford finer step size, the basic Mathematica code we are gonna use is (ignore it or copy to a note book to read)</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">(*Set up the discretization parameters*)</span><span class="w">
</span><span class="nv">\[CapitalLambda]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="o">;</span><span class="w">
</span><span class="nv">\[Kappa]Min</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="nv">\[CapitalLambda]</span><span class="o">;</span><span class="w">  </span><span class="c">(*LargeValue should be a large \</span><span class="err">
</span><span class="c">number representing the approximation to -Infinity*)</span><span class="w">
</span><span class="nv">\[Kappa]Max</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">\[CapitalLambda]</span><span class="o">;</span><span class="w">   </span><span class="c">(*Similarly for +Infinity*)</span><span class="w">
</span><span class="nv">stepSize</span><span class="w"> </span><span class="o">=</span><span class="w"> 
  </span><span class="m">0.1</span><span class="o">;</span><span class="w">  </span><span class="c">(*DesiredStep is the step size for the discretization,choose \</span><span class="err">
</span><span class="c">based on desired precision*)</span><span class="w">

</span><span class="c">(*Sum up the values of rho1kx over the range using the specified step \</span><span class="err">
</span><span class="c">size*)</span><span class="w">
</span><span class="c">(*The first argument defined the range of Subscript[\[Rho], \</span><span class="err">
</span><span class="c">1Subscript[k, x]], the second argument defines the step size.*)</span><span class="w">

</span><span class="nv">totalSum</span><span class="p">[</span><span class="nv">\[CapitalLambda]</span><span class="o">_,</span><span class="w"> </span><span class="nv">stepSize</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> 
 </span><span class="bp">Module</span><span class="p">[{</span><span class="nv">\[Kappa]Min</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="nv">\[CapitalLambda]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Kappa]Max</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">\</span><span class="w">
</span><span class="nv">\[CapitalLambda]</span><span class="p">}</span><span class="o">,</span><span class="w">
  </span><span class="nb">Sum</span><span class="p">[</span><span class="nv">rho1kxCut</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="p">]</span><span class="o">*</span><span class="w">
    </span><span class="nv">stepSize</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Kappa]Min</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Kappa]Max</span><span class="o">,</span><span class="w"> </span><span class="nv">stepSize</span><span class="p">}]</span><span class="o">/</span><span class="p">(</span><span class="m">2</span><span class="w"> </span><span class="nb">Pi</span><span class="p">)]</span><span class="w">
 
</span></code></pre></div></div> <p>To see if summation method is stable at step size $0.01$ for various $\Lambda$, execute</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">Module</span><span class="p">[{</span><span class="nv">data</span><span class="p">}</span><span class="o">,</span><span class="w">
 </span><span class="nv">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">Table</span><span class="p">[</span><span class="w">
   </span><span class="nv">totalSum</span><span class="p">[</span><span class="nv">\[CapitalLambda]</span><span class="o">,</span><span class="w"> 
    </span><span class="nv">stepSize</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[CapitalLambda]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="m">20</span><span class="o">,</span><span class="w"> </span><span class="m">1</span><span class="p">}}</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">stepSize</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="m">0.01</span><span class="p">}}]</span><span class="o">;</span><span class="w">
 </span><span class="nb">TableForm</span><span class="p">[</span><span class="nv">data</span><span class="o">,</span><span class="w"> 
  </span><span class="nb">TableHeadings</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="nb">Table</span><span class="p">[</span><span class="w">
     </span><span class="s">"\[CapitalLambda]="</span><span class="w"> </span><span class="o">&lt;&gt;</span><span class="w"> 
      </span><span class="nb">ToString</span><span class="p">[</span><span class="nv">\[CapitalLambda]tem</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[CapitalLambda]tem</span><span class="o">,</span><span class="w"> </span><span class="m">20</span><span class="p">}]</span><span class="o">,</span><span class="w"> 
    </span><span class="nb">Table</span><span class="p">[</span><span class="s">"stepSize="</span><span class="w"> </span><span class="o">&lt;&gt;</span><span class="w"> </span><span class="nb">ToString</span><span class="p">[</span><span class="nv">a</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">a</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="m">0.01</span><span class="p">}}]}]</span><span class="w">
 </span><span class="p">]</span><span class="w">
</span></code></pre></div></div> <p>and we get</p> <p>$\text{step size=0.01}$, $\kappa \in(-\Lambda,+\Lambda)$:</p> <table> <thead> <tr> <th>$\Lambda$</th> <th style="text-align: center">$2\pi\rho_ {1k_ {x}}$</th> </tr> </thead> <tbody> <tr> <td>1</td> <td style="text-align: center">-0.046851</td> </tr> <tr> <td>2</td> <td style="text-align: center">-0.0963558</td> </tr> <tr> <td>3</td> <td style="text-align: center">-0.124661</td> </tr> <tr> <td>4</td> <td style="text-align: center">-0.14134</td> </tr> <tr> <td>5</td> <td style="text-align: center">-0.152061</td> </tr> <tr> <td>6</td> <td style="text-align: center">-0.159464</td> </tr> <tr> <td>7</td> <td style="text-align: center">-0.16486</td> </tr> <tr> <td>8</td> <td style="text-align: center">-0.168959</td> </tr> <tr> <td>9</td> <td style="text-align: center">-0.172175</td> </tr> <tr> <td>10</td> <td style="text-align: center">-0.174763</td> </tr> <tr> <td>11</td> <td style="text-align: center">-0.176889</td> </tr> <tr> <td>12</td> <td style="text-align: center">-0.178667</td> </tr> <tr> <td>13</td> <td style="text-align: center">-0.180176</td> </tr> <tr> <td>14</td> <td style="text-align: center">-0.181471</td> </tr> <tr> <td>15</td> <td style="text-align: center">-0.182596</td> </tr> <tr> <td>16</td> <td style="text-align: center">-0.183581</td> </tr> <tr> <td>17</td> <td style="text-align: center">-0.184452</td> </tr> <tr> <td>18</td> <td style="text-align: center">-0.185226</td> </tr> <tr> <td>19</td> <td style="text-align: center">-0.185919</td> </tr> <tr> <td>20</td> <td style="text-align: center">-0.186544</td> </tr> </tbody> </table> <p>Looks stable enough.</p> <p>I decided to increase the upper and lower limit $\Lambda$ until the first four or five digit (after the decimal point) stabilizes, with step size 0.01, and include the $2\pi$ factor:</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">(*cut_:4 means the defalut value for cut is 4, which is  used unless \</span><span class="err">
</span><span class="c">an explicit value is provided for cut in the function call. rho1kxCut \</span><span class="err">
</span><span class="c">can be called as, for example, rho1kxCut[0] or rho1kxCut[0,6] *)</span><span class="w">

</span><span class="nv">integrand</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">_,</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">-</span><span class="p">((</span><span class="w">
   </span><span class="m">9</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="nb">Csch</span><span class="p">[</span><span class="nv">\[Pi]</span><span class="w"> </span><span class="p">(</span><span class="nv">\[Kappa]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="p">)]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="p">(</span><span class="o">-</span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">\</span><span class="w">
</span><span class="nv">\[Rho]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="nb">Log</span><span class="p">[(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="o">^</span><span class="m">2</span><span class="p">)]))</span><span class="o">/</span><span class="p">(</span><span class="w">
   </span><span class="m">4</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="nv">\[Kappa]</span><span class="o">^</span><span class="m">4</span><span class="p">)))</span><span class="o">;</span><span class="w">
</span><span class="nv">rho1kxCut</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">_,</span><span class="w"> </span><span class="nv">cut</span><span class="o">_</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="m">4</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> 
  </span><span class="nb">NIntegrate</span><span class="p">[</span><span class="w">
   </span><span class="nv">integrand</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Rho]</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Rho]</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="nv">\[Kappa]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">cut</span><span class="o">,</span><span class="w"> </span><span class="o">-</span><span class="nv">\[Kappa]</span><span class="w"> </span><span class="o">+</span><span class="w">  </span><span class="nv">cut</span><span class="p">}]</span><span class="o">;</span><span class="w">

</span><span class="c">(*Set up the discretization parameters*)</span><span class="w">
</span><span class="nv">\[CapitalLambda]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="o">;</span><span class="w">
</span><span class="nv">\[Kappa]Min</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="nv">\[CapitalLambda]</span><span class="o">;</span><span class="w">  </span><span class="c">(*LargeValue should be a large \</span><span class="err">
</span><span class="c">number representing the approximation to -Infinity*)</span><span class="w">
</span><span class="nv">\[Kappa]Max</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">\[CapitalLambda]</span><span class="o">;</span><span class="w">   </span><span class="c">(*Similarly for +Infinity*)</span><span class="w">
</span><span class="nv">stepSize</span><span class="w"> </span><span class="o">=</span><span class="w"> 
  </span><span class="m">0.1</span><span class="o">;</span><span class="w">  </span><span class="c">(*DesiredStep is the step size for the discretization,choose \</span><span class="err">
</span><span class="c">based on desired precision*)</span><span class="w">

</span><span class="c">(*Sum up the values of rho1kx over the range using the specified step \</span><span class="err">
</span><span class="c">size*)</span><span class="w">
</span><span class="c">(*The first argument defined the range of Subscript[\[Rho], \</span><span class="err">
</span><span class="c">1Subscript[k, x]], the second argument defines the step size.*)</span><span class="w">

</span><span class="nv">totalSum</span><span class="p">[</span><span class="nv">\[CapitalLambda]</span><span class="o">_,</span><span class="w"> </span><span class="nv">stepSize</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> 
 </span><span class="bp">Module</span><span class="p">[{</span><span class="nv">\[Kappa]Min</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="nv">\[CapitalLambda]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Kappa]Max</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">\</span><span class="w">
</span><span class="nv">\[CapitalLambda]</span><span class="p">}</span><span class="o">,</span><span class="w">
  </span><span class="nb">Sum</span><span class="p">[</span><span class="nv">rho1kxCut</span><span class="p">[</span><span class="nv">\[Kappa]</span><span class="p">]</span><span class="o">*</span><span class="w">
    </span><span class="nv">stepSize</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[Kappa]</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Kappa]Min</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Kappa]Max</span><span class="o">,</span><span class="w"> </span><span class="nv">stepSize</span><span class="p">}]</span><span class="o">/</span><span class="p">(</span><span class="m">2</span><span class="w"> </span><span class="nb">Pi</span><span class="p">)]</span><span class="w">
 
</span><span class="bp">Module</span><span class="p">[{</span><span class="nv">data</span><span class="p">}</span><span class="o">,</span><span class="w">
 </span><span class="nv">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">Table</span><span class="p">[</span><span class="w">
   </span><span class="nv">totalSum</span><span class="p">[</span><span class="nv">\[CapitalLambda]</span><span class="o">,</span><span class="w"> 
    </span><span class="nv">stepSize</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[CapitalLambda]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="m">50</span><span class="o">,</span><span class="w"> </span><span class="m">170</span><span class="o">,</span><span class="w"> </span><span class="m">5</span><span class="p">}}</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">stepSize</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="m">0.01</span><span class="p">}}]</span><span class="o">;</span><span class="w">
 </span><span class="nb">TableForm</span><span class="p">[</span><span class="nv">data</span><span class="o">,</span><span class="w"> 
  </span><span class="nb">TableHeadings</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="nb">Table</span><span class="p">[</span><span class="w">
     </span><span class="s">"\[CapitalLambda]="</span><span class="w"> </span><span class="o">&lt;&gt;</span><span class="w"> 
      </span><span class="nb">ToString</span><span class="p">[</span><span class="nv">\[CapitalLambda]tem</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">\[CapitalLambda]tem</span><span class="o">,</span><span class="w"> </span><span class="m">20</span><span class="p">}]</span><span class="o">,</span><span class="w"> 
    </span><span class="nb">Table</span><span class="p">[</span><span class="s">"stepSize="</span><span class="w"> </span><span class="o">&lt;&gt;</span><span class="w"> </span><span class="nb">ToString</span><span class="p">[</span><span class="nv">a</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">a</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="m">0.01</span><span class="p">}}]}]</span><span class="w">
 </span><span class="p">]</span><span class="w">
</span></code></pre></div></div> <p>We get</p> <table> <thead> <tr> <th>$\kappa \in(-\Lambda,\Lambda)$</th> <th>stepSize=0.01</th> </tr> </thead> <tbody> <tr> <td>Λ=100</td> <td>-0.0312054</td> </tr> <tr> <td>Λ=105</td> <td>-0.0312235</td> </tr> <tr> <td>Λ=110</td> <td>-0.0312399</td> </tr> <tr> <td>Λ=115</td> <td>-0.0312549</td> </tr> <tr> <td>Λ=120</td> <td>-0.0312687</td> </tr> <tr> <td>Λ=125</td> <td>-0.0312813</td> </tr> <tr> <td>Λ=130</td> <td>-0.031293</td> </tr> <tr> <td>Λ=135</td> <td>-0.0313038</td> </tr> <tr> <td>Λ=135</td> <td>-0.0313038</td> </tr> <tr> <td>Λ=140</td> <td>-0.0313139</td> </tr> <tr> <td>Λ=145</td> <td>-0.0313233</td> </tr> <tr> <td>Λ=150</td> <td>-0.031332</td> </tr> <tr> <td>Λ=155</td> <td>-0.0313402</td> </tr> <tr> <td>Λ=160</td> <td>-0.0313478</td> </tr> <tr> <td>Λ=165</td> <td>-0.031355</td> </tr> <tr> <td>Λ=170</td> <td>-0.0313618</td> </tr> <tr> <td>Λ=175</td> <td>-0.0313682</td> </tr> <tr> <td>Λ=180</td> <td>-0.0313742</td> </tr> <tr> <td>Λ=195</td> <td>-0.0313904</td> </tr> <tr> <td>Λ=200</td> <td>-0.0313953</td> </tr> <tr> <td>Λ=210</td> <td>-0.0314044</td> </tr> <tr> <td>Λ=220</td> <td>-0.0314126</td> </tr> <tr> <td>Λ=230</td> <td>-0.0314201</td> </tr> <tr> <td>Λ=240</td> <td>-0.031426995</td> </tr> <tr> <td>Λ=250</td> <td>-0.0314333</td> </tr> <tr> <td>Λ=300</td> <td>-0.0314587</td> </tr> <tr> <td>Λ=400</td> <td>-0.0314903</td> </tr> <tr> <td>Λ=500</td> <td>-0.0315093</td> </tr> <tr> <td>Λ=600</td> <td>-0.031522</td> </tr> <tr> <td>Λ=800</td> <td>-0.0315378</td> </tr> <tr> <td>Λ=1000</td> <td>-0.0315473</td> </tr> <tr> <td>Λ=1200</td> <td>-0.0315537</td> </tr> <tr> <td>Λ=1400</td> <td>-0.0315582</td> </tr> <tr> <td>Λ=1600</td> <td>-0.0315616</td> </tr> </tbody> </table> <p>With step size 0.01, the first four digits after decimal point can no longer be improved by enlarging $\Lambda$.</p> <p>Next, we divide the integral domain into an inner region where we use fine summations (smaller step size) and an outer region where we use coarser summation (large step size), separated by an limit $\Lambda=1600$. We do two things simultaneously, 1) increase the inner region step size from $0.005$ to $0.01$ and 2) increase the outer region step size from $0.05$ to $0.1$ until the first four or five digits after the decimal point stabilizes.</p> <p>$\Lambda=1600$, Upper limit of $\kappa$ is set to be 10 000.</p> <p>In the following table, large step size means 0.01 for inner region and 0.1 for outer region; small step size means 0.005 for inner region and 0.05 for outer region. Hope I had a stronger computer…</p> <table> <thead> <tr> <th> </th> <th>Small Step Size (fine sum)</th> <th>Large Step Size (coarse sum)</th> </tr> </thead> <tbody> <tr> <td>Inner</td> <td>-0.0315615</td> <td>-0.03156158063750391`</td> </tr> <tr> <td>Outer</td> <td>-0.000019948</td> <td>-0.0000199484</td> </tr> <tr> <td>Total</td> <td>-0.03158147</td> <td>-0.03158153</td> </tr> </tbody> </table>]]></content><author><name>Baiyang Zhang</name></author><category term="kink"/><summary type="html"><![CDATA[1. Domain wall in phi-fourth model 2. Normal Modes 3. Fourier Transform 4. Numeric Results 4.1 Summation method]]></summary></entry><entry><title type="html">Note on the kink mass correction in 3D Part I</title><link href="https://baiyangzhang.github.io/blog/2024/Note-on-Kink-Mass-Correction-in-3D-Part-I/" rel="alternate" type="text/html" title="Note on the kink mass correction in 3D Part I"/><published>2024-03-10T00:00:00+00:00</published><updated>2024-03-10T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Note-on-Kink-Mass-Correction-in-3D-Part-I</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Note-on-Kink-Mass-Correction-in-3D-Part-I/"><![CDATA[<ul> <li><a href="#1-introduction">1 Introduction</a> <ul> <li><a href="#11-background">1.1 Background</a></li> <li><a href="#12-digression-on-%5Chbar-expansion">1.2 Digression on $\hbar$-expansion</a></li> </ul> </li> <li><a href="#2-kinks-in-d-dimension">2 Kinks in d-Dimension</a> <ul> <li><a href="#21-normal-modes-and-quantization">2.1 Normal modes and quantization</a></li> <li><a href="#22-renormalization-methods-review">2.2 Renormalization methods review</a></li> <li><a href="#23-lattice-quantization">2.3 Lattice quantization</a> <ul> <li><a href="#231-latticization-of-scalar-field">2.3.1 Latticization of scalar field</a></li> <li><a href="#232-second-quantization-in-the-vacuum-sector">2.3.2 Second quantization in the vacuum sector</a></li> </ul> </li> <li><a href="#24-kink-in-21-dimension">2.4 Kink in (2+1)-dimension</a></li> </ul> </li> </ul> <h1 id="1-introduction">1 Introduction</h1> <h2 id="11-background">1.1 Background</h2> <p>We will establish the model and notation in this section. First, some nomenclatures.</p> <p><code class="language-plaintext highlighter-rouge">Shape modes</code>: In theories with a kink solution, the equation of motion in the background of the kink typically has discrete, bounded solutions. They are called <code class="language-plaintext highlighter-rouge">shape modes</code>. They are localized around the kink. Shape modes represent small fluctuations or deformations of the kink’s shape. Unlike the continuum of delocalized modes that represent free particle states, the shape modes are confined to the vicinity of the kink, with discrete energy levels.</p> <p><code class="language-plaintext highlighter-rouge">Oscillons</code>: They are localized, non-solitonic, and semi-stable field configurations that oscillate in time. They are interesting because they represent a form of partial stable, localized energy concentration that can persist for long times, even though they are not topologically protected like solitons or kinks. Oscillons are found in various nonlinear field theories and have been studied in different contexts, including cosmology and condensed matter physics. Their persistence and behaviors under various conditions are subjects of ongoing research in theoretical physics. However, it is believed that quantum correction will make oscillons rapidly decay into a pair of fundamental particles.</p> <p><code class="language-plaintext highlighter-rouge">The Unruh effect</code>: The Unruh effect is a prediction in quantum field theory, stating that an observer accelerating through a vacuum will perceive a thermal bath of particles, whereas an inertial observer would see none. This effect arises from the realization that <em>the concept of a vacuum state (or empty space) is observer-dependent</em>. For an accelerating observer, what appears as empty space to an inertial observer is seen as a warm gas of particles. This effect, named after physicist William Unruh, highlights the interplay between quantum theory and relativity, especially in contexts like black hole physics.</p> <p>In the frame work of the linearized soliton perturbation theory, we can systematically study the quantum corrections to both static and time-dependent solitonic solutions.</p> <hr/> <p>Firstly, we work in the Schrodinger picture where the operator are time-independent while the wave functions (vectors in Hilbert space) are time dependent. Secondly, we will start with the Hamiltonian formalism where the canonical momentum, $\pi$, is considered as a fundamental ingredient, rather than the time derivative of $\phi$, the field of the Lagrangian.</p> <p>In $2+1$ dimension, the Hamiltonian $H$ is the integral of the Hamiltonian density $\mathcal{H}$ over the 2D manifold of space alone,</p> \[H = \int_ {M} d^{2}x\, \mathcal{H}(\phi,\pi),\quad \mathcal{H}=\frac{1}{2}\pi^{2}(x)+\frac{1}{2} (\partial_ i \phi)^{2} + \frac{1}{\lambda}V(\sqrt{ \lambda }\phi(x))\] <p><strong>Dimensional analysis</strong></p> <p>In Lagrangian formalism, $[S]=[\hbar]$ where $S$ is the action, and $S = \int d^d x \, \mathcal{L}$, where $\mathcal{L}$ is the Lagrangian (density) and $d$ is the space-time dimension. Note that right now we are working with a partially “natural” units, where $c=1$ but $\hbar \neq 1$. Since $c=1$ we still have $[x] = [t]$, namely $L=T$ ($L$ is length and $T$ is time), but no longer do we have $T=E^{-1}$ since this is a result of $[\hbar]=ET=1$ (recall that $\hbar \omega=E$, meaning $[\hbar \omega]=E$, and $\omega$ is the frequency with dimension $1 / T$ ).</p> <p>Let $d$ be the dimension of spacetime, in the partial natural unit we have</p> \[\int d^{d}x \, \mathcal{L}\sim \hbar \implies [\mathcal{L}]=[\hbar] L^{-d}\] <p>Take a specific term, say $(\partial \phi)^{2}$, to continue the analysis,</p> \[[(\partial \phi)^{2}] = L^{-2} [\phi^{2}] = [\mathcal{L}] = [\hbar]L^{-d} \implies [\phi^{2}]=[\hbar] L^{2-d}\] <p>which is</p> \[[\phi] = [\sqrt{ \hbar }] \, L^{1-d / 2}.\] <p>This agrees with the convention that field operator $\phi$ scales as $\sqrt{ \hbar }$. Furthermore, this scaling property does not depend on the spacetime dimension $d$, it holds for any spacetime dimension.</p> <p>I don’t think the $\hbar$-dependence given by Eq. (1) is unique, apparently there exist other possibilities, we can move $\hbar$ around in the Lagrangian, just to make sure that the action altogether is of dimension $\hbar$. However it seems that $\phi\sim \hbar^{1/2}$ is indeed the most convenient option. Another way to see the advantage of this choice is from the action,</p> \[S \sim \hbar \sim \int d^{4}x \, (m^{2} \phi^{2}+(\partial \phi)^{2} +U(\phi)),\] <p>then</p> \[\int d^{4}x \, \left[ m^{2} \xi^{2}+(\partial \xi)^{2} + U(\xi) \right] \sim 1, \quad \xi:= \frac{\phi}{\sqrt{ \hbar } } .\] <p>In this way, we can absorb $\hbar$ into the definition of $\phi$, this is similar to absorbing the coupling $g$ into the field definition in gauge theory. We now <em>define</em> $\xi$ to be <em>independent</em> of $\hbar$.</p> <p>Having fixed the $\hbar$-dependence of $\phi$, we can substitute it to the potential $U$, for example the $\phi^{4}$ model to determine the $\hbar$ dependence of the coupling,</p> \[[S]\sim\int d^{d}x \, \lambda \phi^{4} \sim L^{d} [\lambda][\hbar^{2}] L^{4-2d} = L^{4-d}[\hbar^{2}][\lambda],\] <p>on the other hand we already know that $S\sim \hbar$ so</p> \[L^{4-d}[\hbar^{2}][\lambda] = [\hbar] \implies [\lambda \hbar] = L^{d-4}.\] <p>If we choose $L$ to be the fundamental unit instead of energy $E$, it is clear the $\lambda \hbar$ is independent of $\hbar$. Plus, we see that $d=4$ is special.</p> <p>For the sake of completeness, let’s consider the mass term in the Lagrangian. Similar to what we have for the kinetic term,</p> \[[m^{2} \phi^{2}] = E^{2}\, [\phi^{2}] = [\mathcal{L}] = [\hbar]\,L^{-d} \implies [\phi] = [\sqrt{ \hbar }]L^{-d/2}E^{-1} .\] <p><strong>Dimension and measurement</strong></p> <p>If two things are of the same dimension, for example, say</p> \[[A] \sim [a] = L\] <p>where $\sim$ means having the same dimension. Then we can use one of them as the unit to measure the other, say, use $a$ as the “ruler” to measure $A$, the result $\widetilde{A} :=A / a$ is a dimensionless number.</p> <p>Suppose $[\phi]=[\hbar]^{1/2}$, another way to say the same ting is $\phi \sim \sqrt{ \hbar }$, note that the tilde does not imply any relation between the <em>values</em> of $\phi$ and $\sqrt{ \hbar }$, it only means that they have the same dimension. The point is, since they have the same dimension, we can use one of them to measure the other, for example we can define</p> \[\tilde{\phi} = \phi / \sqrt{ \hbar }\] <p>which is a dimensionless number. Since $\hbar$ scales the “quantumness”, the more classical the world is, the smaller $\hbar$ (and $\sqrt{ \hbar }$), hence the bigger numeric value of $\tilde{\phi}$.</p> <hr/> <p>In the partial natural units, I’d like to think there are two fundamental “rulers” to measure all the quantities, such as mass, coupling, field, etc. One of them is the unit of energy, for example $\text{MeV}$, the other is $\hbar$ whose dimension is $ET$. To measure the length of something, we can use $\frac{\hbar}{\text{MeV} }$ as unit. The advantage of the partial natural unit is that it makes explicit the $\hbar$ factor, revealing the direct relations between quantities with $\hbar$, which is the scale of quantumness, this enables us to discern the importance of various quantities in the classical limit, making the analysis regarding semi-classical more straightforward.</p> <h2 id="12-digression-on-hbar-expansion">1.2 Digression on $\hbar$-expansion</h2> <p>To appreciate the importance of $\hbar$, just recall that in canonical quantization $[x,p]=i\hbar$, $\hbar$ enters explicitly in the commutation relation, providing the fundamental basis of quantum theory. This is also true in the case of quantum field theory. Furthermore, at each order of an expansion in $\hbar$, the physical symmetries (Lorentz invariance, $U(1)$ symmetry, etc.) must be satisfied, otherwise there will be some special value of $\hbar$ only at which the symmetries are preserved, which is just strange.</p> <p>In the unit where $c=1$, the Planck’s constant $\hbar$ has the unit of action, or rather the action has the unit of $[\hbar]=ET$, where $E$ is the energy scale and $T$ the time. It turns out that there are more than one way to assign $\hbar$-dependence for quantities such as the mass $m$, the coupling $g,\lambda$ etc. A criterion for the “right” choice is that, at $\hbar\to 0$ limit, the quantum theory agrees with the classical theory. As an example, Stanley Brodsky and Paul Hoyer in their <a href="https://arxiv.org/pdf/1009.2313.pdf">paper</a> used the quantum mechanical harmonic oscillator as an example in Eq.(1). The gist is that, you can rescale $x$ to $x / \sqrt{ \hbar }$, then the propagator is formally independent of $\hbar$. However, this will change how we view distance, in the $\hbar\to 0$ limit, for a fixed distance $L$, the “length” measure will increase as $1 / \sqrt{ \hbar }$, hence we are going to smaller and smaller area.</p> <p>It is generally understood that each loop contribution to amplitudes is associated with one factor of $\hbar$. However, to fully define the $\hbar\to 0$ limit one need to specify the $\hbar$ dependence of various quantities in the Lagrangian as mentioned before, such as the field operator, the mass, the coupling, etc. This is not as straightforward as one might think, for $\hbar$ not only appears in the action $iS / \hbar$ but also appears in the Lagrangian. In Brodsky’s paper mentioned above, the authors proposed a way to establish the $\hbar$ dependence such that the loop and $\hbar$ expansions are equivalent. We will go to more details in the following.</p> <p><strong>First, regard $\hbar$ as a constant of nature with certain dimension, use $\hbar$ to make terms in the Lagrangian dimensionless.</strong></p> <p>Again, let’s work with the assumption that $c = \epsilon_ {0} = 1$. Require $[S]=\hbar$, and $\alpha_ {s} = g^{2} / 4\pi \hbar$ is dimensionless, the latter implies that $[g]=\sqrt{ \hbar }=\sqrt{ ET }=\sqrt{ EL }$. From the self-energy of gluons $G_ {\mu \nu}G^{\mu \nu}$ where $G = \partial A - \partial A +ig / \hbar [A,A]$ we have</p> \[[A] = \sqrt{ \frac{E}{L} }.\] <p>For the same reasons, in the scalar QED the classical electric charge $e$ and mass $m$ are divided by $\hbar$,</p> \[S_ {\text{sQED} } = \int d^{4}x \, \left\lbrace \left\lvert D\phi \right\rvert ^{2}-\frac{m^{2} }{\hbar^{2} }\left\lvert \phi \right\rvert ^{2} \right\rbrace , \quad D = \partial +i \frac{e}{\hbar }A.\] <p>The boson field dimension</p> \[[\phi]=[A]= \sqrt{ \frac{E}{L} }.\] <p>Fermion fields are more complicated, since they have no classical counterparts, their dimensions are convention-dependent. We will deal with fermions in a different note perhaps.</p> <p><strong>Second step is to specify $\hbar$ dependence of all quantities appearing in the action.</strong></p> <p>The choice made by Brodsky and Hoyer is as following:</p> \[\widetilde{A}:= \frac{A}{\sqrt{ \hbar } },\quad \tilde{\phi}:= \frac{\phi}{\sqrt{ \hbar } }\] <p>where $\widetilde{A},\tilde{\phi}$ are $\hbar$-independent. Similarly, define the following $\hbar$-independent quantities</p> \[\widetilde{g}:= \frac{g}{\hbar},\quad \widetilde{e}:= \frac{e}{\hbar},\quad \widetilde{m}:= \frac{m}{\hbar}.\] <p>Then one can write the Lagrangian in terms of these $\hbar$-independent quantities to check the $\hbar$ dependence explicitly. It turns out that, at least in the simple models discussion in the paper, $\hbar$ always appears in the combination</p> \[\widetilde{g}\sqrt{ \hbar } \quad \text{and}\quad \widetilde{e}\sqrt{ \hbar }\] <p>that is, with the coupling. Hence loop correction of $\mathcal{O}(g^{2},e^{2})$ will be of order $\hbar$.</p> <p>This derivation is equivalent to the standard one of, for example, Mark Srednicki’s textbook, which associates a factor $\hbar$ to each propagator and $h^{-1}$ with each vertex, and assume the parameters appearing in the action to be independent of $\hbar$.</p> <p>Fore more details please refer to Brodsky and Hoyer’s paper mentioned above.</p> <h1 id="2-kinks-in-d-dimension">2 Kinks in d-Dimension</h1> <p>In R. Jackiw’s <a href="https://www.sciencedirect.com/science/article/abs/pii/037015737690048X">1976 paper</a>, he made three assumption:</p> <ol> <li>The energy (mass) is finite;</li> <li>The energy is locally minimum, meaning the soliton is stable;</li> <li>The potential $U$ depends on a coupling constant $\lambda$ according to the scaling law</li> </ol> \[U(\phi;\lambda) = \frac{1}{\lambda}U(\sqrt{ \lambda }\,\phi;1).\] <p>The choice is such that all the $\lambda$ dependence are now moved to the pre-factor $1 / \lambda$. As for $\hbar$-expansion, we take the scheme such that $\hbar$-expansion agrees with $\lambda$-expansion, namely each loop brings in a factor of $\hbar$.</p> <p>Let the Hamiltonian be</p> \[H = \int d^{2}x \, : \mathcal{H} :_ {a},\quad \mathcal{H}(x) = \frac{\pi^{2} }{2} + \frac{(\partial_ {x}\phi(x))^{2} }{2} + \frac{1}{\lambda} V\left(\sqrt{ \lambda}\, \phi(x) \right).\] <p>Now, in order to get the equation of motion, we have two options: 1) Legendre-transform the equation to the Lagrangian formalism and adopt Euler-Lagrange equation, or 2) stick with the Hamiltonian formalism and adopt the Hamiltonian equations of motion (Hamilton equations) instead. Here we will take the second option.</p> <p>Recall that the Hamilton equations in classical field theory reads</p> \[\begin{align*} \frac{\delta \mathcal{H} }{\delta \phi_i} &amp;=-\dot{\pi}(x), \\ \frac{\delta \mathcal{H} }{\delta \pi_i} &amp;= \dot{\phi}_i. \end{align*}\] <p>This is a non-trivial generalization of the familiar Hamiltonian in classical mechanics, non-trivial since the connection between variational derivative and partial derivative is not as simple as one might think, we have</p> \[\frac{\delta \mathcal{H} }{\delta \phi} = \frac{\partial\mathcal{H} }{\partial \phi } - \left( \partial_ x\frac{\partial \mathcal{H} }{\partial(\partial_ {x}\phi)} \right).\] <p>Taking everything into consideration, we obtain the equation of motion by straightforward calculation. But before going there, let’s rewrite the Hamiltonian in a more compact form:</p> \[\boxed{ \lambda \, \mathcal{H} = \frac{1}{2} \tilde{\pi}^{2} + \frac{1}{2} \vec{\nabla}^{2}\,\tilde{\pi} + V(\tilde{\phi}),\quad \tilde{\pi} := \sqrt{ \lambda }\, \pi,\quad \tilde{\phi} := \sqrt{ \lambda }\,\phi. }\] <p>Then we can first obtain the EOM in terms of $\tilde{\phi}$ and $\tilde{\pi}$, the changing to un-tilded version is trivial. Finally we have</p> \[\ddot{\phi} - \vec{\nabla}^{2} \phi(x,t) + \frac{1}{\sqrt{ \lambda } } V^{(1)}(\sqrt{ \lambda }\phi) = 0\] <p>with definition</p> <p>\(V^{(n)} := \frac{ \partial^{n } V(\tilde{\phi})}{ \partial \tilde{\phi}^{n} }.\)</p> <h2 id="21-normal-modes-and-quantization">2.1 Normal modes and quantization</h2> <p>When kink solutions are placed in more than one spatial dimension, they become extended planar structures called “domain walls.”</p> <p>Now, how can we borrow the kink result form 2D directly to 3D? Consider a static kink solution in the $x$ direction</p> \[\phi(x,y) =: f(x,y)=: f_ {1}(x)\times f_ {2}(y) ,\] <p>where we have assumed the possibility of separation of variables. To say the kink is in the $x$ direction is to say the solution satisfies the equation of motion in the $x$ direction,</p> \[\frac{ \partial^{2} f(x,y) }{ \partial x^{2} } = \frac{1}{\lambda} \frac{ \partial V }{ \partial \phi } = \frac{1}{\sqrt{ \lambda } } \frac{ \partial V }{ \partial \tilde{\phi} } = \frac{1}{\sqrt{ \lambda } }V^{(1)}(\tilde{\phi}).\] <hr/> <p>First, consider the case in 2D. Writing the filed as a kink background plus fluctuation,</p> \[\phi(x,t) =: f(x) + {\mathfrak g}(x) e^{ -i\omega t }\] <p>where $f(x)$ is the 1-dimensional kink solution, the equation of motion in terms of ${\mathfrak g}$ reads</p> \[[-\omega^{2}-\partial_ {x}^{2}+V^{(2)}(\sqrt{ \lambda }f(x))]\, {\mathfrak g}(x) = 0.\] <p>As we mentioned before, there are three kinks of solutions: the zero mode, the shape mode and the continuum.</p> <hr/> <p>The equation of motion is the Sturm-Liouville equation. A general Sturm-Liouville problem is typically written in the form:</p> \[\frac{d}{dx}\left[ p(x) \frac{dy}{dx} \right] - q(x)y + \lambda r(x)y = 0\] <p>Here, $y$ is the function of the variable $x$ that we are solving for, and $p(x)$, $q(x)$, and $r(x)$ are known functions that specify the particular Sturm-Liouville problem. The parameter $\lambda$ is often referred to as the eigenvalue.</p> <p>Key characteristics and applications of the Sturm-Liouville equation include:</p> <ol> <li> <p><strong>Eigenvalue Problem</strong>: The Sturm-Liouville equation is an eigenvalue problem. The solutions $y(x)$ are eigenfunctions, and the associated values of $\lambda$ are eigenvalues. These eigenvalues are typically discrete and can be ordered as a sequence $\lambda_1, \lambda_2, \lambda_3, \ldots$, where each $\lambda_n$ corresponds to a particular eigenfunction $y_n(x)$.</p> </li> <li> <p><strong>Orthogonality and Completeness</strong>: The eigenfunctions of a Sturm-Liouville problem are orthogonal with respect to the weight function $r(x)$. This property is crucial in solving partial differential equations, as it allows the expansion of functions in terms of these eigenfunctions (similar to Fourier series).</p> </li> <li> <p><strong>Boundary Conditions</strong>: Sturm-Liouville problems are typically accompanied by boundary conditions that the solutions must satisfy. These conditions are usually specified at the endpoints of the interval in which the equation is defined.</p> </li> <li> <p><strong>Physical Applications</strong>: The Sturm-Liouville problem appears in various areas of physics, such as quantum mechanics (in solving the Schrödinger equation), heat conduction, wave propagation, and vibrations analysis. It is essential in the separation of variables technique for solving partial differential equations.</p> </li> <li> <p><strong>Self-Adjoint Form</strong>: The equation is often referred to as a self-adjoint form, which has important implications in the theory of linear operators and functional analysis.</p> </li> </ol> <p>In our case, the weight function is trivial.</p> <hr/> <p>We will denote the zero mode by ${\mathfrak g}_ {B}$ and the shape mode by ${\mathfrak g}_ {S}$. The $B$ in ${\mathfrak g}_ {B}$ has a historical reason, but in our note it is just part of the name. The normalization conditions are</p> \[\int dx \, {\mathfrak g}_ {S}^{2} = \int dx \, {\mathfrak g}_ {B}^{2} = 1 , \quad \int dx \, {\mathfrak g}_ {B}{\mathfrak g}_ {S} = 0 ,\quad \int dx \, {\mathfrak g}_ {k}(x){\mathfrak g}_ {p}(x) = 2\pi i\delta(p-k).\] <p>The sign of ${\mathfrak g}_ {B}$ is fixed using</p> \[{\mathfrak g}_ {B}(x) = - \frac{f'(x)}{\sqrt{ Q_ {0} } },\] <p>where $f$ is again the kink solution.</p> <p>We choose to expand in the $x$ direction in normal modes (in the kink background), while in the $y$ direction in plane waves. The 2D momentum $\vec{k}=\left\lbrace k_ {x},k_ {y} \right\rbrace$, where $k_ {x}=\left\lbrace B,S,k \right\rbrace$, $B$ for the zeromode (bounded solution), $S$ for the shape mode (also bounded) and $k$ for the continuum. A nice illustration of normal modes in the background of kink is shown in the figure below, which I shamelessly copied from Tanmay Vachaspati’s book, all the credits goes to Vachaspati.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kinkLevel-480.webp 480w,/img/kinkLevel-800.webp 800w,/img/kinkLevel-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kinkLevel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A trivial potential on a periodic space with period $L$ is shown on the left, while the normal modes in the background of a kink solution is shown on the right. What used to be the $n=0$ mode in the trivial potential (on the left) becomes the lowest bound state, the zero mode, in the non-trivial potential. Similarly a linear combination of the $n=\pm 1$ modes in the trivial case may become the second bound state ($n=+1$ in the illustration), and the other states remain unbounded but shift in form. </div> <p>We want to expand the static fluctuation field $\phi(r)$ (defined by $\phi=f_ {\text{kink} }+g$) in terms of normal modes. Since we have defined the indices $k$ in ${\mathfrak g}_ {k}(x)$ to include everything, we can conveniently write the field expansion as</p> \[\begin{align*} \phi(r) &amp;= \sum\!\!\!\!\!\!\!\!\int \frac{ d^{d}k}{(2\pi )^{d} } \, \left( B_ {k} ^{\ddagger} +\frac{B_ {-k} }{2\omega _ {k} } \right){\mathfrak g}_ {k} (r),\\ \pi(r) &amp;= \sum\!\!\!\!\!\!\!\!\int \frac{ d^{d}k}{(2\pi )^{d} } \, \left( B_ {k} ^{\ddagger} -\frac{B_ {-k} }{2} \right){\mathfrak g}_ {k} (r), \end{align*}\] <p>where $r = (x,y)$. We adopt the convention</p> \[B^{\ddagger}_ {k} = \frac{B^{\dagger}_ {k} }{2\omega _ {k} }.\] <p>This helps us to switch between different conventions for the field expansion.</p> <p>We have omitted the vector sign (or bold font) in $r$ since it would not raise any misunderstanding. We assume (quite reasonably) the separation of variables $x$ and $y$ for 2D normal modes ${\mathfrak g}(r)$,</p> \[{\mathfrak g}(r) = {\mathfrak g}_ {x}\times g_ {y},\quad {\mathfrak g}_ {x} = \text{kink normal modes},\, {\mathfrak g}_ {y} = \text{plane waves.}\] <p>The quantization in terms of $\phi$ and $\pi$ reads</p> \[[\phi(r),\pi(r')] = i\delta^{(d)}(r-r').\] <p>This represents the fundamental quantization relation, unaffected by the selection of sectors. We haven’t given a formal definition of sectors, roughly speaking, within each sector, there exists a distinct set of normal modes for expanding both $\phi$ and $\pi$. Each mode must conform to the aforementioned relation, namely the quantization relation given in space-time positions $r$. Ultimately, the difference across different sectors lies in the diverse backgrounds (regarded as classical functions) used for field expansion. However, as we are analyzing the same theory within the same space-time, the theory should be quantized only once, and, all sectors must consistently align with the same quantization process.</p> <h2 id="22-renormalization-methods-review">2.2 Renormalization methods review</h2> <p>Things does get more complicated when renormalization is included. The commonly used renormalization method include (but not limited to):</p> <ol> <li><code class="language-plaintext highlighter-rouge">Dimensional regularization</code>. This is particularly useful in 4D. This technique regularizes integrals by analytically continuing the number of dimensions of space $d$, usually reduce it by an infinitesimal quantity $\epsilon$. It preserves the gauge symmetry and Lorentz invariance.</li> <li><code class="language-plaintext highlighter-rouge">Cutoff regularization</code>. This method introduces a high-energy cutoff in the energy. By doing this we are making the momentum integrals finite by force, but as a price we would introduce a new parameter $\Lambda$ with the dimension of energy. This $\Lambda$ has profound physical consequence, such as in dimensional transmutation and, most importantly, the Wilsonian RG flow.</li> <li><code class="language-plaintext highlighter-rouge">Pauli-Villars Regularization.</code> This approach involves adding hypothetical heavy particles to the theory to cancel out the divergences. The mass of the heavy particles acts effectively as the cutoff in the integrals.</li> <li><code class="language-plaintext highlighter-rouge">Lattice reguglarization</code>. This method discretizes the spacetime into a lattice. Computations are performed on this lattice, the spacing between different sites, or the resolution of the lattice serves as a cutoff.</li> </ol> <p>There also exist tons of other renormalization methods, such as holographic renormalization, Hopf algebra renormalization, Connes-Kreimer renormalization, and so on. They sounds like fun but, unfortunately, they lie beyond the scope of our work and my comprehension.</p> <p>Different renormalization methods may yield different results depending on the specific sector they are applied to. For instance, the momentum cutoff method produces distinct outcomes when used in the trivial vacuum sector compared to the one-kink sector. From my perspective, it seems more appropriate to <em>consider the regularization method as an integral part of the quantization process</em>. Quantization itself is not a natural, free functor (just means a ); whether one opts for geometrical quantization or path integral quantization (among other methods), additional elements are always needed. For example, incorporating a momentum cutoff is often a crucial step.</p> <p>Different sectors also seem to give different perspectives regarding the relations between different regularization methods. For example, in trivial vacuum sector (which we will just call vacuum sector), the momentum cutoff method is closely connected to the lattice renormalization method, by roughly $\Lambda = 2\pi / a$ where $a$ is the lattice spacing. This very relation is harder to see in, for instance, one-kink sector (which we will just call kink sector).</p> <p>A natural question that follows is, if there exists a regularization method that applies to all the sectors (unlike the momentum cutoff renormalization) equally well? If so, which one? Apparently the space-time itself is the common ground of all sectors, since the lattice regularization is dependent on spacetime alone, it is independent of the specific mode-expansion we adopt for fields, so it seems reasonable to use it as the renormalization scheme. Other advantages of lattice renormalization includes, 1) it made obvious the Wilsonian RG flow, which is continuous change of various parameters in the theory depending on a continuous change of the lattice size. 2) The connection between the lattice quantization and momentum-cutoff renormalization is already know in the trivial vacuum sector. We just need to find a way to generalize it to other sectors.</p> <h2 id="23-lattice-quantization">2.3 Lattice quantization</h2> <p>In our work we will start with the lattice quantization. Recall that the canonical quantization relation in a continuous $d$-dimensional spacetime reads</p> \[[\phi(r),\pi(r')] = i \delta^{(d)} (r-r'),\] <p>In lattice quantization, the continuous spacetime of the theory is replaced by a discrete set of points, and the fields are defined only at these points. It is a powerful, comprehensive change of viewpoint, not only of numerical importance, but really alters our view of spacetime. In principal we can translate all concepts we have defined in spacetime continuum into the lattice spacetime, such as the gauge connection, gauge field strength, etc. Sometimes, it is convenient to think the lattices as “sample points” of a spacetime continuum. In this view, between the lattice sites there maybe exists something other than the pure void, but it is meaning less to talk about them anyway. This view is rather useful when discussing the connection between lattice quantization and momentum cutoff quantization.</p> <p>The commutation relation on a lattice should take the form</p> \[[\phi_ {i},\pi_ {j}] =i\delta_ {ij},\] <p>where $i,j$ are the indices of different sites.</p> <p>Recall that in canonical quantization, Eq. (2.1) translate to the momentum space rather trivially, yielding</p> \[[a_ {p},a^{\dagger}_ {p'}] = (2\pi)^{d} \delta^{(d)}(p-p').\] <p>So the question is, how does the lattice commutation relation translates to different sectors? Particularly, in the trivial vacuum sector and the one-kink sector?</p> <h3 id="231-latticization-of-scalar-field">2.3.1 Latticization of scalar field</h3> <p>Write the scalar field $\phi$ in continuum as as function of spacetime position $\phi(x)$, and write the lattice position as a label of the field $\phi_ {a^{\mu} }$, where $a^{\mu}=m^{\mu} a$, $m^{\mu}\in \mathbb{N}^{d}$ is the $d$-dimensional count of the lattice site, $a$ is the lattice distance. The summation on lattices goes to integral in the continuous limit with the following dictionary,</p> \[\begin{align*} \sum_ {m^{d} } &amp;\to \int, \\ a^{d} &amp;\to d^{d}x,\\ f_ {a^{\mu} } &amp;\to f(x) \end{align*}\] <p>where $m^{d}$ indicates the $d$-dimensional lattice. Combined together we have the familiar formula</p> \[\sum_ {m^{d} } a^{d} f_ {a^{\mu} } \to \int d^{d}x \, f(x).\] <p>We can define two types of differences, corresponding to derivatives in the continuous case</p> \[\begin{align*} \partial_ {\mu} f_ {x} &amp;= \frac{1}{a} (f_ {x+\hat{a}^{\mu} }-f_ {x}),\\ \partial'_ {\mu} f_ {x} &amp;= \frac{1}{a} (f_ {x}-f_ {x-\hat{a}^{\mu} }), \end{align*}\] <p>where $\hat{a}^{\mu}$ is a vector in direction $x^{\mu}$ of length $a$, namely $\hat{a}$ moves to the next lattice site in the $x^{\mu}$ direction. These two differences are like differences “from right” and “from left”. For smooth functions of course their continuous limit all give the derivative.</p> <p>The interesting thing is that these two types of differences are kind of dual to each other. Define an inner product</p> \[(f_ {x},g_ {x}) := \sum_ {x} f_ {x} g_ {x},\] <p>similar to the case of differential forms, then</p> \[(f_ {x},\partial g_ {x}) = (-\partial' f_ {x},g_ {x}),\] <p>which corresponds to the integral by part</p> \[\int \, f\partial g = \int \,(-\partial f) g.\] <h3 id="232-second-quantization-in-the-vacuum-sector">2.3.2 Second quantization in the vacuum sector</h3> <p>First let’s review how the second quantization is achieved in the vacuum sector without latticization, namely on a continuum of spacetime of dimension $d+1$. This is what we learnt from textbooks.</p> <p>Note that we will adopt a slight change of variable here. Before we used $\vec{r}$ to denote the spatial vector where $\vec{r}=(x,y, \cdots)$ since in two dimensional space time, it is easier to write $x,y$ than $x_ {1},x_ {2}$. However, since now we are dealing with $d+1$ dimensional spacetime, we will also adopt a different convention, namely $x=(t,\vec{x})$ where $\vec{x}$ is a $d$-dimensional vector with components $x_ {1},\cdots,x_ {d}$. In summary, Latin letters without a vector sign $x$ are covariant $d+1$-vectors, the generalization of $4$-vector to arbitrary dimension.</p> <p>The simplest special relativistic equation of motion a field $\phi$ can satisfy is the massless Klein-Gordon equation,</p> \[\partial^{2}\phi=0,\quad \partial^{2}=\partial_ {\mu}\partial^{\mu}=\partial_ {t}^{2}-\nabla^{2}.\] <p>Decompose $\phi$ into a continuum of momentum mode, each mode can be written as</p> \[a_ {p} (t) e^{ i\vec{p}\cdot \vec{x} }\] <p>where we have assume the separation of variable $t$ and $\vec{x}$, and the time-dependent part of the Klein-Gordon equation gives</p> \[(\partial_ {t}^{2}+\vec{p}\cdot \vec{p})a_ {p} (t) = 0\] <p>with solutions</p> \[a_ {p} (t) =a_ {p} e^{ \pm i\omega t},\quad \omega^{2}= \vec{p}^{2}.\] <p>where $a_ {p}$ now is just some c-number, constant in time. The field can be expanded as a linear combination of all the momentum modes</p> \[\phi(t,\vec{r}) = \int \frac{d^{d}p}{(2\pi)^{d} } \, (a_ {p} e^{ -ipx }+a_ {p} ^{\ast }e^{ipx })\] <p>where the second term in the parenthesis is to make sure that $\phi$ is real. We have assembled $\omega$ and $\vec{p}$ into a Lorentzian vector $p=(\omega,\vec{p})$.</p> <p>Now the second quantization. This is usually first done in the momentum space, since we are treating each mode as a harmonic oscillator. You can already see one aspect of the difference between the previously define lattice quantization and the textbook second quantization. We introduce the <code class="language-plaintext highlighter-rouge">equal-time commutation relation</code></p> \[[a_ {k} ,a_ {p}^{\dagger}] = (2\pi)^{d} \,\delta^{d}(\vec{k}-\vec{p}),\] <p>where we have omitted the vector sign in the superscript $k,p$ to avoid overly cumbersome notation. The factors of $2\pi$ are a convention, stemming from our convention for Fourier transform, for the details see my other blog on conventions.</p> <p>We want the operators $a_ {p}^{\dagger}$ to create particles with momentum $\vec{p}$. Let $\left\lvert{\vec{p} }\right\rangle$ be a physical state with a single particle with momentum $\vec{p}$, <em>define</em></p> \[a_ {p} ^{\dagger}\left\lvert{\Omega}\right\rangle = \frac{1}{\sqrt{ 2\omega _ {p} } }\left\lvert{\vec{p} }\right\rangle ,\] <p>where $\left\lvert{\Omega}\right\rangle$ is the ground state in the vacuum sector. This factor of $1 / \sqrt{ 2\omega _ {p} }$ is just another convention.</p> <p>From the normalization $\left\langle \Omega \middle\vert \Omega \right\rangle=1$ and the commutation of $a,a^{\dagger}$ we get</p> \[\left\langle \vec{p} \middle\vert \vec{k} \right\rangle =2\omega _ {p} (2\pi)^{d}\delta^{d}(\vec{p}-\vec{k}).\] <p>As a result, the identity operator for one particle states is</p> \[\mathbb{1}=\int \frac{d^{d}p}{(2\pi)^{d} } \, \frac{1}{2\omega _ {p} }\left\lvert{\vec{p} }\right\rangle \left\langle{\vec{p} }\right\rvert .\] <p>We define the quantum field as integrals over $a_ {p}$ and $a_ {p}^{\dagger}$,</p> \[\phi_ {0}(\vec{x})= \int \frac{d^{d}p}{(2\pi)^{d} } \, \frac{1}{\sqrt{ 2\omega } } (a_ {p} e^{ i\vec{p} \cdot \vec{x} } + a_ {p} ^{\dagger}e^{ -i\vec{p}\cdot \vec{x} })\] <p>where the subscript $0$ indicates this is a free field. The traditional view is to take it as the definition of the field operator $\phi_ {0}$ constructed from the creation and annihilation operators $a_ {p}$ and $a_ {p}^{\dagger}$ (<em>Schwartz M.D.</em>), but we shall try an opposite viewpoint, as will be shown later.</p> <p>Later we will work with the Schrodinger picture which is less commonly used compared to the Heisenberg or interaction pictures. To finish the review on the second quantization, we just mentioned that in Heisenberg picture, all the time dependence is in operators such as $\phi$ and $a_ {p}$, the field operator reads</p> \[\phi_ {0}(\vec{x},t) = \int \frac{d^{d}p}{(2\pi)^{d} } \, \frac{1}{\sqrt{ 2\omega _ {p} } }(a_ {p} e^{ -ipx } + a_ {p} ^{\dagger}e^{ ipx }),\] <p>which is <em>not</em> Lorentz invariant.</p> <hr/> <p>Instead of the traditional view of harmonic oscillator quantization in the momentum space, let’s take Eq.(2) as the starting point. Adopt a somewhat un-conventional field expansion</p> \[\begin{align*} \phi(\vec{x}) &amp;= \int \frac{d^{d}p}{(2\pi)^{d} } \, e^{ -i\vec{x}\cdot \vec{p} }\phi_ {p},\\ \pi(\vec{x}) &amp;= \int \frac{d^{d}p}{(2\pi)^{d} } \, e^{ -i \vec{x}\cdot \vec{p} }\pi_ {p}. \end{align*}\] <p>The next question is how to inverse it on a lattice…</p> <h2 id="24-kink-in-21-dimension">2.4 Kink in (2+1)-dimension</h2> <p>If we adopt (1) the ultraviolet cutoff $\Lambda$ and (2) the harmonic quantization <em>in the trivial vacuum sector</em> and translate it to the physical spacetime. As a result we get a non-local commutation relation in spacetime,</p> \[[\phi(x),\pi(y)]_ {\Lambda} = i \int_{-\Lambda}^{\Lambda} \frac{dp}{2\pi} \, e^{ -ip(x-y)} = i \frac{\sin(\Lambda x)}{\pi x} .\] <p>This is non-local in the sense that for certain $x\neq y$ the commutator is nonzero. However, this commutation relation does agree with the lattice picture, if we set the lattice spacing to be $\pi / \Lambda$! (check for yourself)</p> <p>But for now let’s forget about the momentum cutoff and work with a more general picture. We can define the normal ordering in the vacuum sector, then propagate it to the kink sector via the displacement operator, which we will talk about shortly.</p> <p>Recall the equation of motion reads</p> \[\partial^{2}\phi-\frac{1}{\lambda} \frac{dV(\sqrt{ \lambda }\phi)}{d\phi}=0,\] <p>separate $\phi(\vec{x},t)$ into the kink background $f(\vec{x})$ and the fluctuation (time-dependent) ${\mathfrak g}$,</p> \[\phi(\vec{x},t) = f(\vec{x})+{\mathfrak g}(\vec{x},t),\] <p>insert it into the equation of motion and use the fact that $f(\vec{x})$ satisfies the time-independent EoM, we get the EoM for the fluctuation field:</p> \[\begin{align*} 0 &amp;= \partial^{2} (f+{\mathfrak g}) -\frac{1}{\lambda} \frac{d V(\sqrt{ \lambda }(f+{\mathfrak g}))}{d\phi} \\ &amp;= \partial^{2} f + \partial^{2}{\mathfrak g}-\frac{1}{\lambda} \frac{d}{d\phi}\left( V( \sqrt{ \lambda } f)+{\mathfrak g} \frac{dV(\sqrt{ \lambda }\phi)}{d\phi} \mid_ {\phi=f} + \mathcal{O}({\mathfrak g^{2} }) \right) \\ &amp;= \partial^{2} f - \frac{1}{\lambda} \frac{dV(\sqrt{\lambda} f)}{df} + \partial^{2}{\mathfrak g}-\frac{ {\mathfrak g} } {\lambda} \frac{d^{2}V(\sqrt{\lambda}f)}{d f^{2} } +\mathcal{O}( {\mathfrak g}^{2} ) \\ &amp;= \partial^{2} {\mathfrak g}+V^{(2)}(\sqrt{ \lambda }f) + \mathcal{O}({\mathfrak g}^{2}), \end{align*}\] <p>where</p> \[V^{(n)} := \frac{d^{n}V(\tilde{\phi})}{d\tilde{\phi}^{n} },\quad \tilde{\phi}:=\sqrt{ \lambda }\phi.\] <hr/> <p>In the below are some results needed for the derivation, not carefully organized in a readable order. I will tidy it up later.</p> <p>The commutation relation reads</p> \[[\phi(\vec{x}_ {1}),\pi(\vec{x}_ {2})] = i \delta^{d}(\vec{x}_ {1}-\vec{x}_ {2}),\] <p>together with the decomposition we have</p> \[\begin{align*} [\phi_ {p_ {1} },\pi_ {p_ {2} }] &amp;= i(2\pi)^{d}\delta^{d}(p_ {1}+p_ {2}),\\ [\phi_ {k_ {1} },\pi_ {k_ {2} }] &amp;= i(2\pi)^{d}\delta^{d}(k_ {1}+k_ {2}), \end{align*}\] <p>note the plus sign instead of minus in the parenthesis.</p> <p>The decomposition into ladder operators reads</p> \[\begin{align*} \phi_ {p} &amp;= A^{\ddagger}_ {p}+\frac{A_ {-p} }{2\omega_ {p} },\quad \pi_ {p}=i\omega_ {p}A^{\ddagger}_ {p}- \frac{iA_ {-p} }{2},\quad A^{\ddagger}_ {p} = \frac{A^{\dagger}_ {p} }{2\omega _ {p} },\\ \phi_ {k} &amp;= B^{\ddagger}_ {k}+\frac{B_ {-k} }{2\omega_ {k} },\quad \pi_ {k}=i\omega_ {k}B^{\ddagger}_ {k}- \frac{iB_ {-k} }{2},\quad B^{\ddagger}_ {k} = \frac{B^{\dagger}_ {k} }{2\omega _ {k} }. \end{align*}\] <p>The commutation relation in terms of those reads</p> \[[B_ {k_ {1} },B^{\ddagger}_ {k_ {2} }] = (2\pi)^{d}\delta^{d}(k_ {1}-k_ {2}).\] <p>Note the minus sign. It is exactly the same as what we’ve got in the trivial vacuum sector where</p> \[[A_ {p_ {1} },A^{\ddagger}_ {p_ {2} }] = (2\pi)^{d}\delta^{d}(p_ {1}-p_ {2}).\] <p>Expand $A,A^{\ddagger}$ in terms of $B,B^{\ddagger}$ we have</p> \[\begin{align*} A^{\ddagger}_ {p} &amp;= \sum\!\!\!\!\!\!\!\!\int \; \frac{d^{d}k}{(2\pi)^{d} } \, \frac{\tilde{ {\mathfrak g} }_ {k}(-\vec{p})}{2\omega_ {p} } \left[ (\omega_ {p} +\omega _ {k} )B_ {k} ^{\ddagger}+(\omega _ {p} -\omega _ {k} )\frac{B_ {-k} }{2\omega_ {k} } \right] ,\\ A_ {-p}&amp;= \sum\!\!\!\!\!\!\!\!\int \; \frac{d^{d}k}{(2\pi)^{d} } \, \tilde{ {\mathfrak g} }_ {k}(-\vec{p}) \left[ (\omega _ {p} -\omega _ {k} ) B_ {k} ^{\ddagger}+(\omega _ {p} +\omega _ {k} )\frac{B_ {-k} }{2\omega _ {k} } \right]. \end{align*}\] <p>The idea is to</p> <ol> <li>write terms in the Hamiltonian in normal order with respect to $A,A^{\ddagger}$,</li> <li>use the above relation to rewrite it in terms of $B,B^{\ddagger}$,</li> <li>use the commutation relation to diagonalize $H$ in terms of $B,B^{\ddagger}$.</li> </ol> <p>The normal ordered leading order Hamiltonian (in kink sector) reads</p> <p>\(H'_ {2} = A+B+C\) where</p> \[\begin{align*} A &amp;= \frac{1}{2} \int d^{d}x \, :\pi^{2}(\vec{x}):_ {a}, \\ B &amp;= \frac{1}{2} \int d^{d}x \, :(\nabla\phi(\vec{x}))^{2}:_ {a}, \\ C &amp;= \frac{1}{2} \int d^{d}x \, V^{(2)} (\sqrt{ \lambda }f) :\phi^{2}(\vec{x}):_ {a}. \end{align*}\] <p>In plane-wave space and $\vec{k}$-space we have</p> \[\begin{align*} A &amp;= \frac{1}{2} \int\frac{d^{d}p}{(2\pi)^{d}} \, :\pi_ {p}\pi_ {-p}:_ {a} , \\ B+C &amp;= \frac{1}{2} \sum\!\!\!\!\!\!\!\!\int \;\frac{d^{d}k}{(2\pi)^{d}} \, \omega^{2}_ {k} \int \frac{d^{d}p_ {1}}{(2\pi)^{d}} \int \frac{d^{d}p_ {2}}{(2\pi)^{d}} \, \tilde{ {\mathfrak g}}_ {-k}(\vec{p}_ {1}) \tilde{ {\mathfrak g}}_ {k}(\vec{p}_ {2}) :\phi_ {p_ {1}}\phi_ {p_ {2}}:_ {a}. \end{align*}\] <hr/> <p>Define the Fourier transformation $\tilde{f}$ of function $f(x)$ to be</p> \[\tilde{f}(\vec{p}) := \int d^{d}x \, f(\vec{x})e^{ -i\vec{p}\cdot \vec{x} }.\] <p>The Fourier transformation of normal modes reads</p> \[\tilde{ {\mathfrak g} }_ {k}(p) = \int d^{d} x \, {\mathfrak g}_ {k}( \vec{x} ) e^{-i\vec{p} \cdot \vec{x} }\] <p>which satisfies relation</p> \[\tilde{ {\mathfrak g} }_ {k}^{\ast}(\vec{p}) = \tilde{ {\mathfrak g} }_ {-k}(-\vec{p}) .\] <p>Sometime this relation can help to make the numerical calculation easier.</p> <p>The normalization relations for ${\mathfrak g}$ reads</p> \[\begin{align*} \int d^{d}x \, {\mathfrak g}^{\ast }_ {k}(\vec{x}){\mathfrak g}_ {k'}(\vec{x}) &amp;=(2\pi)^{d}\delta ^{d}(\vec{k}-\vec{k}'), \\ \int \frac{d^{d}p}{(2\pi)^{d} } \tilde{ {\mathfrak g} }_ {k}(\vec{p}) \tilde{ {\mathfrak g} }_ {k'}(\vec{p}) &amp;= (2\pi)^{d}\delta ^{d}(\vec{k}+\vec{k}'), \\ \int \frac{d^{d}p}{(2\pi)^{d} } \tilde{ {\mathfrak g} }_ {k}(\vec{p}) \tilde{ {\mathfrak g} }^{\ast }_ {k'}(\vec{p}) &amp;= (2\pi)^{d}\delta ^{d}(\vec{k}-\vec{k}') , \end{align*}\] <p>and the completeness condition (in both spacetime and momentum space)</p> \[\begin{align*} \int \frac{d^{d}k}{(2\pi)^{2}} \, {\mathfrak g}_ {k}(\vec{x}) {\mathfrak g}^{\ast }_ {k}(\vec{y}) &amp;=\delta ^{d}(\vec{x}-\vec{y}), \\ \sum\!\!\!\!\!\!\!\!\int \; \frac{d^{d}k}{(2\pi)^{d}} \, \tilde{ {\frak g} }_ {k}(\vec{p}_ {1}) \tilde{ {\frak g} }^{\ast} _ {k}(\vec{p}_ {2}) &amp;= (2\pi)^{d} \delta^{d}(\vec{p}_ {1} - \vec{p}_ {2}). \end{align*}\] <p>Note that in our convention of Fourier transformation, instead of $+i\vec{p}\cdot \vec{x}$ we have minus sign. This is only the spatial part of the Fourier transformation (recall that our spacetime is $d+1$ dimensional).</p> <hr/> <p>We have divided the leading order, kink-sector Hamiltonian into $A+B+C$ three parts, in normal order we have</p> \[\begin{align*} A &amp;=\frac{1}{2} \int \frac{d^{d}p}{(2\pi)^{d} } \, \left( -\omega^{2}_ {p} A^{\ddagger}_ {p} A^{\ddagger}_ {-p}+ \omega _ {p} A^{\ddagger}_ {p} A_ {p} -\frac{1}{4} A_ {-p}A_ {p} \right), \\ B+C &amp;= \frac{1}{2} \sum\!\!\!\!\!\!\!\!\int \;\frac{d^{d}k}{(2\pi)^{d} } \int \frac{d^{d}p}{(2\pi)^{d} } \frac{d^{d}p'}{(2\pi)^{d} } \,\omega_ {k}^{2}\, \tilde{ {\mathfrak g} }_ {-k}(\vec{p} )\tilde{ {\mathfrak g} }_ {k}(\vec{p}') \\ &amp;\;\;\;\; \times \left[ A^{\ddagger}_ {p }A^{\ddagger}_ {p'} + \frac{A^{\ddagger}_ {p }A_ {-p'} }{2\omega_ {p'} } + \frac{A^{\ddagger}_ {p'}A_ {-p } }{2\omega_ {p } } + \frac{A_ {-p } }{2\omega_ {p } } \frac{A_ {-p'} }{2\omega_ {p'} } \right]. \end{align*}\] <p>In the calculation just keep in mind the property of $\tilde{ {\mathfrak g}}$ and invariance under combined exchange $p \leftrightarrow p’$ with $k \to -k$.</p> <p>The following relations are useful in derivation, with terms that do not annihilate the kink ground state $\left\lvert{0}\right\rangle$:</p> \[\begin{align*} A^{\ddagger}_ {p} A^{\ddagger}_ {p'} &amp;\cong \sum\!\!\!\!\!\!\!\!\int \frac{d^{d}k}{(2\pi)^{d} } \frac{d^{d}k'}{(2\pi)^{d} } \frac{\tilde{ {\mathfrak g} }_ {k}(-\vec{p}) \tilde{ {\mathfrak g} }_ {k'}(-\vec{p}')}{2\omega_ {p} 2\omega_ {p'} } \\ &amp;\;\;\;\;\; \times \left( (\omega _ {p} +\omega _ {k} )(\omega_ {p'}+\omega_ {k'})B^{\ddagger}_ {k} B^{\ddagger}_ {k'}+ \frac{1}{2\omega _ {k} }(\omega _ {p} -\omega _ {k} )(\omega_ {p'}+\omega_ { {k'} }) B_ {-k} B^{\ddagger}_ {k'}) \right), \\ A^{\ddagger}_ {p} A_ {-p'} &amp;\cong \sum\!\!\!\!\!\!\!\!\int \frac{d^{d}k}{(2\pi)^{d} } \frac{d^{d}k'}{(2\pi)^{d} } \frac{1}{2\omega_ {p} } \tilde{ {\mathfrak g} }_ {k}(-\vec{p})\tilde{ {\mathfrak g} }_ {k'}(-\vec{p}') \\ &amp;\;\;\;\;\; \times \left( (\omega _ {p} +\omega _ {k} )(\omega_ {p'}-\omega_ {k'})B^{\ddagger}_ {k} B^{\ddagger}_ {k'}+ \frac{1}{2\omega _ {k} } (\omega _ {p} -\omega _ {k} )(\omega_ {p’}-\omega_ { {k'} }) B_ {-k}B^{\ddagger}_ {k'}) \right), \\ A_ {-p} A_ {-p'} &amp;\cong \sum\!\!\!\!\!\!\!\!\int \frac{d^{d}k}{(2\pi)^{d} }\frac{d^{d}k'}{(2\pi)^{d} } \tilde{ {\mathfrak g} }_ {k}(-\vec{p})\tilde{ {\mathfrak g} }_ {k'}(-\vec{p}')\\ &amp;\;\;\;\;\; \times \left( (\omega _ {p} -\omega _ {k} )(\omega_ {p'}-\omega_ {k'})B^{\ddagger}_ {k} B^{\ddagger}_ {k'} + \frac{1}{2\omega _ {k} }(\omega _ {p} +\omega _ {k} )(\omega_ {p'}-\omega_ { {k'} }) B_ {-k}B^{\ddagger}_ {k'}) \right). \\ \end{align*}\] <p>We just need to keeps the terms surviving acting on $\left\lvert{0}\right\rangle$, namely the terms that are proportional to $B^{\ddagger} B^{\ddagger}$ and $B B^{\ddagger}$.</p> <p>The part in $A$ and $B+C$ that are proportional to $B^{\ddagger}B^{\ddagger}$ reads</p> \[\begin{align*} A &amp;\supset \frac{1}{8} \int \frac{d^{d}p}{(2\pi)^{d} } \sum\!\!\!\!\!\!\!\!\int \frac{\;d^{d}k}{(2\pi)^{d} } \frac{d^{d}k'}{(2\pi)^{d} } \tilde{ {\mathfrak g} }_ {k}(-\vec{p})\tilde{ {\mathfrak g} }_ {k'}(\vec{p}) B_ {k}^{^{\ddagger} } B_ {k'}^{\ddagger} \\ &amp;\;\;\;\;\; \times (-4\omega _ {k} \omega_ {k'} + 2\omega _ {p} \omega _ {k} -2\omega _ {p} \omega_ {k'}) \\ &amp;= - \frac{1}{2} \sum\!\!\!\!\!\!\!\!\int \, \frac{d^{d}k}{(2\pi)^{d} } \, \omega^{2}_ {k}B^{\ddagger}_ {k} B^{\ddagger}_ {-k}, \\ B+C &amp;\supset \frac{1}{2} \sum\!\!\!\!\!\!\!\!\int \; \frac{d^{d}k}{(2\pi)^{d} } \frac{d^{d}k_ {1} }{(2\pi)^{d} } \frac{d^{d}k_ {2} }{(2\pi)^{d} } \int \frac{d^{d}p_ {1} }{(2\pi)^{d} } \frac{d^{d}p_ {2} }{(2\pi)^{d} } \\ &amp; \;\;\;\;\; \times \omega^{2}_ {k} \, \tilde{\mathfrak g}_ {k_ {1} }(-\vec{p}) \tilde{\mathfrak g}_ {k_ {2} }(-\vec{p}') \tilde{\mathfrak g}_ {-k}(\vec{p}) \tilde{\mathfrak g}_ {k}(\vec{p}') B^{\ddagger}_ {k_ {1} }B^{\ddagger}_ {k_ {2} } \\ &amp;= \frac{1}{2} \sum\!\!\!\!\!\!\!\!\int \frac{d^{d}k}{(2\pi)^{d} } \,\omega _ {k} ^{2} B^{\ddagger}_ {k}B^{\ddagger}_ {-k} . \end{align*}\] <p>Note that the last two terms in the second line cancel each other since $k,k’$ are dummy indices, we can simply exchange them. We have used the normalization condition for $\tilde{ {\mathfrak g} }$ functions. Apparently, put together, in $A+B+C$ the terms proportions to $B^{\ddagger}B^{\ddagger}$ disappear! We are left with terms proportional to $B B^{\ddagger}$ only, which can be obtained similarly. We will omit the explicit result here since it can be easily found in other papers.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="kink"/><summary type="html"><![CDATA[1 Introduction 1.1 Background 1.2 Digression on $\hbar$-expansion 2 Kinks in d-Dimension 2.1 Normal modes and quantization 2.2 Renormalization methods review 2.3 Lattice quantization 2.3.1 Latticization of scalar field 2.3.2 Second quantization in the vacuum sector 2.4 Kink in (2+1)-dimension]]></summary></entry><entry><title type="html">Domain Wall</title><link href="https://baiyangzhang.github.io/blog/2024/Domain-Wall/" rel="alternate" type="text/html" title="Domain Wall"/><published>2024-03-06T00:00:00+00:00</published><updated>2024-03-06T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Domain-Wall</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Domain-Wall/"><![CDATA[<ul> <li><a href="#1-kink-in-11-dimension">1. Kink in 1+1 dimension</a></li> <li><a href="#2-walls-in-3--1-dimensions">2. Walls in 3 + 1 dimensions</a></li> <li><a href="#3-some-solutions">3. Some solutions</a></li> </ul> <h1 id="1-kink-in-11-dimension">1. Kink in 1+1 dimension</h1> <p>Domain walls in theoretical physics are fascinating phenomena that arise in a variety of contexts, from condensed matter physics to cosmology. They are a type of topological soliton that occurs when a discrete symmetry is spontaneously broken. In simpler terms, domain walls can be thought of as boundaries between different phases or domains where the order parameter (some scalar degree of freedom) differs on either side.</p> <p>When kink solutions are placed in more than one spatial dimension, they become extended planar structures called “domain walls.” For example, in $3+1$ dimension, a flat $\mathcal{Z}_ {2}$ domain wall in $yz$-plane is</p> \[\phi(t,x,y,z) = \eta\, \tanh\left( \sqrt{ \frac{\lambda}{2} } \, \eta x \right).\] <p>The energy density is given by that of $\mathcal{Z}_ {2}$ kink, the our convention it reads</p> \[\mathcal{E} = \frac{\lambda \eta^{4}}{2} \cosh^{-4}\left( \sqrt{ \frac{\lambda}{2} } \, \eta x \right)\] <p>since the Lagrangian is</p> \[\frac{1}{2} (\partial \phi)^{2} - \frac{\lambda}{4} (\phi^{2} - \eta^{2})^{2}\] <p>and the $\mathcal{Z} _2$ kink solution (with center at $x=0$) reads</p> \[\phi_ {k} (x,t) = \eta \tanh\left( \sqrt{ \frac{\lambda}{2} } \eta x \right).\] <p>The new aspects of domain walls are that they can be curved and deformations can propagate along them. Another feature of the planar domain wall is that it is invariant under boosts in the plane parallel to the wall. This is simply because the solution is independent of t, y and z and any transformations of these coordinates do not affect the solution.</p> <hr/> <p>The characteristic length of a kink is roughly speaking inverse to some mass, which turns out to be the mass of the fluctuating field in the kink sector $1 / \eta\sqrt{ \lambda }$. For distance scale much larger than that of a kink, we can regard it as a point particle. Recall that in the context of general relativity, the true trajectory of a particle through spacetime is such that it extremizes the proper time. In curved spacetime the trajectory (geodesic) followed by a free particle <em>maximizes the proper time</em> compared to nearby paths. Particles are “<em>lazy</em>” in the sense that they try to maximize the time spent to get to point B from point A. The action can be written as</p> \[S_ {1+1} = -M \int d\tau ,\] <p>the superscript $1+1$ denotes the dimension.</p> <p>The proper time is also the line element of the world line of the particle, let $X^{\mu}$ be the coordinates of the kink center, we can write</p> \[d\tau^{2} = g_ {\mu \nu} \left( \frac{dX^{\mu}}{dt} \right) \left( \frac{dX^{\nu}}{dt} \right) dt^{2}.\] <hr/> <p>The question is, can we derive the action Eq. (1) from the original action for $\phi$ field, of which the kink is a solution? Such a derivation should lead to Eq. (1) plus corrections that depend on the internal structure of the kink. Then Eq. (1) would be the effective action of kink.</p> <p>The key assumption to derive the effective action is that, the field profile of a kink is well-approximated by that of the <em>static kink solution</em> in the <code class="language-plaintext highlighter-rouge">instantaneous rest frame</code>. Recall that, if the kink were to be given a velocity $v$, it would be Lorentz boosted to a moving frame. In that case, <em>the instantaneous rest frame would refer to the original frame before the Lorentz boost</em>, where the kink solution is static. Lorentz boosting the kink solution involves applying a Lorentz transformation to the coordinates, which would modify the form of kink solution $\phi_ {k}$ to account for the relativistic effects of motion, but in its own rest frame, the kink solution retains the static form given above.Generally speaking, the instantaneous rest frame of a moving object is a reference frame in which the object is at rest at a particular instant in time. This frame is also known as the object’s <code class="language-plaintext highlighter-rouge">comoving frame</code>or <code class="language-plaintext highlighter-rouge">proper frame</code>. In this frame, the laws of physics take their simplest form, just like they would in a frame where the particle was always at rest.</p> <p>Let’s work in the <em>kink frame coordinates</em> which are denoted by $y^{a} = (\tau,\xi)$, $a=0,1$. $\tau$ is also called the kink world-line coordinate. These coordinates are functions of the background coordinates that are denoted by $x^{\mu}=(t,x)$, $\mu=0,1$. The kink world line is the world line of the kink center, denoted by $X^{\mu}=(t,X(t))$. Therefore the tangent vector to the world line is</p> <p>\(T^{\mu} \propto \partial_ {t} X^{\mu} = \left( 1, V \right), \quad V:= \frac{ \partial X }{ \partial t }\) and $T$ is normalized to $T^{2}=1$.</p> <p>The unit vector $\hat{N} = N^{\mu}(\tau)$ orthogonal to $\hat{T} = T^{\mu}$ is found by solving</p> \[g_ {\mu \nu} T^{\mu} N^{\nu} = 0, \quad N^{\mu}N_ {\mu}=N^{2} = -1.\] <p>Note the normalization of $N$ is $-1$ since it is time-like. Since we are working in 2D, there is only one $N^{\mu}$. In the special case of a Minkowski background we choose $g_ {\mu \nu} = \eta_ {\mu \nu} = \text{diag}(1,-1)$. (This is gonna piss off some theorists I know) We find</p> \[T^{\mu} = \gamma(1,V),\quad N^{\mu} = \gamma(V,1),\quad \gamma = \frac{1}{\sqrt{ 1-V^{2} }}.\] <p>The coordinate axis $\tau$ is along $T^{\mu}$ and $\xi$ is along $N^{\mu}$. Therefore, if we arbitrarily choose a time $\tau_ {0}$ when the kink is located at spacetime point $X_ {0}$, then near the kink any other spacetime point coordinate can be written as</p> \[x^{\mu} = X_ {0}^{\mu} + \tau \hat{T}_ {0} + \xi \hat{N}_ {0},\] <p>where $\hat{T}_ {0}$ and $\hat{N}_ {0}$ are the tangent and normal unit vector at time $\tau_ {0}$. However, we can always choose a time $\tau$ at which the coordinate in $\hat{T}$ direction is zero (at least locally), when $x^{\mu}$ is not too far away from kink world line. It is illustrated in the following figure, which I shamelessly copied from Tanmay’s textbook.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/kinkWorldLine-480.webp 480w,/img/kink/kinkWorldLine-800.webp 800w,/img/kink/kinkWorldLine-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/kinkWorldLine.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Thick curve is the kink world line. The kink-frame coordinates $y^a=(\tau,xi)$ are defined in the instantaneous rest frame of the kink and functions of the background coordinates $x^\mu=(t,x)$. </div> <p>Generally speaking, given a curve $C:=p(t)$ parametrized by $t$ in manifold $\mathcal{M}$, the curve is a map from the parameter to the manifold,</p> \[C : \mathbb{R} \to \mathcal{M}.\] <p>Curve $p(t)$ defines a velocity vector $\dot{p}(t)$. Note that in a general manifold (including affine space), points and vectors are distinct concepts, each serving a different role, although they are closely related and often used together in geometric computations and theoretical discussions. The tangent vector really takes a set of parametrized points and turn them into something entirely different: vectors. Furthermore, vectors can be equivalently regarded as differentials operators. The philosophy can be roughly states as: we use functions to measure the space, and we use differential to measure vectors. For example, we use coordinate patches to denote different points on a manifold, but a coordinate patch is nothing but a map from (sub-) manifold to $\mathbb{R}^{n}$, hence is a real-valued function. A vector $V$ with components $V^{0},\cdots,V^{i}$ in basis $\left\lbrace x \right\rbrace$ can be regarded as a differential operator $V = V^{i} \partial_ {i}$.</p> <p>In our case, the curve $X(\tau)$ is parametrized by $\tau$, and the worldline of the kink (center) can be regarded as a <code class="language-plaintext highlighter-rouge">coordinate curve</code>, that is to say the tangent vector given by $\partial_ {\tau}$ is not the basis. Recall that the 2-dimensional coordinate system was defined as $y^{a}=(\tau,\xi)$, we can talk about the metric in terms of $y$’s, denoted as $h_ {ab}$, in contrast to $g_ {\mu \nu}$ in terms of $x$’s.</p> <p>However, the treatment of $(\tau,\xi)$ in Vachaspati’s textbook is a little weird, it seems to regard $X^{\mu}_ {0}$ (see the figure above) as some kink of origin, which is problematic in curved manifold. Maybe I missed something here? Anyways, here I will just treat $(\tau,\xi)$ as a (locally defined) coordinate system. We can also construct basis $\partial_ {\xi}$ such that their Lie bracket is zero,</p> \[[\partial_ {\tau},\partial_ {\xi}]=0\] <p>as is required for coordinate basis, this is locally equivalent to $\partial_ {\tau}$ being parallel to $\partial_ {\xi}$.</p> <p>We can go on and calculate the metric in terms of $y$ coordinates. Let $\left\langle X,Y \right\rangle$ be the inner product of vectors $X$ and $Y$, we have</p> \[h_ {00} := \left\langle \partial_ {\tau}, \partial_ {\tau} \right\rangle = \left\langle \partial_ {\tau}X,\partial_ {\tau}X \right\rangle = \left\langle (\partial_ {\tau}X^{\mu}) \partial_ {\mu},(\partial_ {\tau}X^{\nu}) \partial_ {\nu} \right\rangle = g_ {\mu \nu} (\partial_ {\tau}X^{\mu}) (\partial_ {\tau}X^{\nu}).\] <p>By construction we have</p> \[h_ {01} := \left\langle \partial_ {\tau},\partial_ {\xi} \right\rangle =0\] <p>and</p> \[h_ {11} := \left\langle \partial_ {\xi},\partial_ {\xi} \right\rangle = -1.\] <p>Next we need to write down the action. We assume the only import region is spacetime is that close to the kink world line, far from it the spacetime would be flat and the fluctuation would be a bunch of plane waves, it shouldn’t matter too much to the quantum correction of the domain wall. Thus we will only consider <em>the action given by a “band” around the kink world line.</em></p> <hr/> <p>In the context of a curved spacetime, particularly in a two-dimensional, the action for a scalar field can be described using the general form that incorporates the effects of curvature. The action $S$ for a scalar field $\phi$ in a curved spacetime can be written as:</p> \[S = \int d^2x \sqrt{-g} \left( -\frac{1}{2} g^{\mu\nu} \partial_\mu \phi \partial_\nu \phi - V(\phi) + \frac{1}{2} g R \phi^2 \right)\] <p>Here $d^2x \sqrt{-g}$ represents the <em>invariant differential volume element</em> in two-dimensional spacetime, $g^{\mu\nu}$ is the inverse metric tensor, used to raise indices. $g$ is a dimensionless coupling constant that describes the coupling of the scalar field to the Ricci scalar $R$, a scalar quantity that describes the curvature of spacetime and is obtained by contracting the Ricci tensor $R_{\mu\nu}$. The term $\frac{1}{2} \xi R \phi^2$ is known as the non-minimal coupling term. In two dimensions, the choice of $g$ can be particularly interesting due to the conformal properties of the spacetime. For now we will neglect the non-minimal coupling term.</p> <p>In our coordinates $y=(\tau,\xi)$ and metric is $h_ {ab}$, the determinant of $h_ {ab}$ reads</p> \[h := \det h = - g_ {\mu \nu} \partial_ {\tau}X^{\mu} \partial_ {\tau}X^{\nu}.\] <p>As before we want to study the effects of small fluctuation about the kink background, so we write</p> \[\phi = \phi_ {k} + \psi\] <p>where $\phi_ {k}$ is the classical kink solution, $\psi$ is the deviation from it. Upon quantization we will only quantize $\psi$ not $\phi_ {k}$. The action now follows from Eq. (2),</p> \[\begin{align*} S &amp;= \int d\tau d\xi \, \sqrt{ \left\lvert h \right\rvert } \mathcal{L}(\phi) \\ &amp;= \int d\tau d\xi \, \sqrt{ \left\lvert h \right\rvert } \mathcal{L}(\phi_ {k}+\psi) \\ &amp;= \int d\tau d\xi \, \sqrt{ \left\lvert h \right\rvert } \mathcal{L}(\phi_ {k}) + \int \frac{\delta^{2}S}{\delta \phi \delta \phi} \, \psi^{2} + \mathcal{O}(\psi^{3}) \\ &amp;= -M_ {\text{kink}} \int d\tau \, \sqrt{ \left\lvert h \right\rvert } + \mathcal{O}(\psi^{2}). \end{align*}\] <p>This is just we we have conjectured in Eq.(1), plus some higher order corrections! If the kink is moving relatively slowly, $\left\lvert h \right\rvert$ would be approximately $1$. Note that the negative kink mass comes from the integral</p> \[\int d \xi \, \mathcal{L} = \int d\xi \, (T-V) = -\int d\xi \, V =M_ {\text{kink}},\] <p>the first order of $\psi$ disappears due to the fact that $\phi_ {k}$ is a solution to the equation of motion.</p> <p>Note that the leading term in the effective action $\int d\tau$ is proportional to the world volume. This result can easily be extended to walls (and strings) propagating in higher dimensions. Even if the self-gravity of the domain wall is taken into account, the dominant contribution to the effective action is still the Nambu-Goto action.</p> <h1 id="2-walls-in-3--1-dimensions">2. Walls in 3 + 1 dimensions</h1> <p>In $3+1$ dimension, the position of domain wall, being a co-dimension one object in space, is described by $2+1$ dimensional coordinates. To be more specific, a domain wall in 3-dimensional space is a 2D plane, hence is parametrized by two parameters, call them $\xi^{1}$ and $\xi^{2}$. As a consequence the world-volume of such a domain wall is 2+1 dimensional hence should be parametrized by three parameters. This is reflected in how we describe the location of domain wall. Let the location of the domain wall be</p> \[X^{\mu} = X^{\mu}(y),\quad y = (\tau,\xi^{1},\xi^{2}),\] <p>$\tau$ is again the proper time, $\xi^{1,2}$ are spatial coordinates. $y$ is the coordinates on the domain-wall world-volume, similar to the case in $1+1$ dimension, with metric given by</p> \[h_ {ab} = g_ {\mu \nu}(X) \frac{ \partial X^{\mu} }{ \partial y^{a} } \frac{ \partial X^{\nu} }{ \partial y^{b} } .\] <p>The major difference between the kink in 1 + 1 dimensions and the domain wall is that the wall can be curved, and so the profile $\phi_ {\text{kink}}$ does not solve the equation of motion. The dynamics of a domain wall is much richer, for example, as the wall moves it accelerates and emits radiation.</p> <p>The Nambu-Goto action of domain wall can be derived in the same way as in 1+1 kink,</p> \[S_ {0} = - \sigma \int d^{3} y \, \sqrt{ \left\lvert h \right\rvert }\] <p>where $\sqrt{ \left\lvert h \right\rvert }$ comes from the invariant integral measure $\sqrt{ \left\lvert h \right\rvert } d^{3}y$, and $\sigma$ is the tension of the wall.</p> <p>From the effective action of the domain wall Eq.(3) we can derive the equation of motion for the wall. But before that we need to know how the variation of the determinant of a matrix $h$. There are two ways to achieve that. One can calculate $\delta \det h$ directly. Under the variation $h\to h+\delta h$ where $\delta h$ is assumed to be infinitesimal, we have</p> \[\begin{align*} \det (h+\delta h) &amp;= \det (h(\mathbb{1}+h^{-1} \delta h))\\ &amp;= \det h \times \det(\mathbb{1}+h^{-1} \delta h)\\ &amp;= \det h ( 1 + \mathrm{Tr}\,(h^{-1} \delta h) + \mathcal{O}(\delta h^{2}) )\\ &amp;= \det h + \det h \times \mathrm{Tr}\,(h^{-1} \delta h). \end{align*}\] <p>Hence</p> \[\delta \det h \equiv \det (h+\delta h)-\det(h) = \det h \times \mathrm{Tr}\,(h^{-1} \delta h).\] <p>We just mention on the fly that there is another formula</p> \[\det M = \exp(\mathrm{Tr}\,\ln M).\] <p>We could also start with this formula and arrive at the same result.</p> <p>Instead of giving the full derivation of the equation of notion, we only mention here that the equation is highly non-linear since $h_ {ab}$ itself is defined as a quadratic in derivative of $X^{\mu}$.</p> <h1 id="3-some-solutions">3. Some solutions</h1> <p>The effective action of domain wall, or Nambu-Goto action, is valid when the domain wall is not curved too much, and separate walls (or different parts of the same wall) are not placed too closely. Otherwise the single kink solution would not be a very good approximation. In addition, in the center of mass frame, the velocity of the wall should be relatively small. If these conditions are met, we can use Nambu-Goto effective action to study the properties of walls.</p> <p>Recall that we use $y^{a}, a=0,1,2$ for the parametrization of the world volume of the domain wall. (We will not dwell on the difference between parametrization and coordination here, but we should be aware.) In components $y=(\tau,\xi^{1},\xi^{2})$. The Minkowski metric is defined as $\eta=(1,-1,-1,-1)$.</p> <p>A domain wall flat in $x-y$ direction is described by</p> \[X^{\mu} = (X^{0}=t, X^{1}=\xi^{1}, X^{2} = \xi^{2}, X^{3}=0).\] <p>Next consider a planar domain wall with some “ripples” in $3$-direction, then $X^{3}$ is a function of $y$’s just as $X^{1}$ and $X^{2}$. Write</p> \[X^{\mu} = (X^{0}=t, X^{1}=\xi^{1}, X^{2} = \xi^{2}, X^{3}=X^{3}(y)).\] <p>The metric is given by</p> \[h_ {ab} = \eta_ {\mu \nu} \partial_ {a}X^{\mu} \partial_ {b}X^{\nu} = \eta_ {ab} - \partial_ {a} X^{3} \partial_ {b}X^{3}\] <p>Then we can calculate the inverse of $h_ {ab}$, insert it into the equation of motion, and solve it. Of course it is easier said then done, here we only mention some of the results. One class of solutions found by Friedlander etc. in 1976 corresponds to a pulse of arbitrary shape on a planar domain wall that propagates in the $+X^{1}$ direction <em>at the speed of light</em>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/movingWall-480.webp 480w,/img/kink/movingWall-800.webp 800w,/img/kink/movingWall-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/movingWall.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Sketch of a traveling wave on a planar domain wall. The pulse propagates at the speed of light along the wall. Figure taken from Tanmay Vachaspati, who is a physics professor at Case Western Reserve University. </div> <p>These solutions are known as <code class="language-plaintext highlighter-rouge">travelling waves</code>.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="kink"/><category term="domainWall"/><summary type="html"><![CDATA[1. Kink in 1+1 dimension 2. Walls in 3 + 1 dimensions 3. Some solutions]]></summary></entry><entry><title type="html">Regression Methods in Biostatistics</title><link href="https://baiyangzhang.github.io/blog/2024/Regression-Methods-in-Biostatistics/" rel="alternate" type="text/html" title="Regression Methods in Biostatistics"/><published>2024-03-05T00:00:00+00:00</published><updated>2024-03-05T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Regression-Methods-in-Biostatistics</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Regression-Methods-in-Biostatistics/"><![CDATA[<ul> <li><a href="#1-introduction">1. Introduction</a></li> <li><a href="#2-basic-statistical-methods">2. Basic Statistical Methods</a> <ul> <li><a href="#21-t-test-and-anova-analysis-of-variance">2.1. t-Test and ANOVA (Analysis of Variance)</a> <ul> <li><a href="#211-t-test">2.1.1. t-test</a></li> <li><a href="#212-two-sided-hypothesis-test">2.1.2. Two-sided Hypothesis Test</a></li> <li><a href="#213-f-test">2.1.3. F-test</a></li> <li><a href="#214-robustness">2.1.4. Robustness</a></li> </ul> </li> </ul> </li> <li><a href="#3-correlation">3. Correlation</a></li> <li><a href="#4-linear-regression-method">4. Linear Regression Method</a></li> <li><a href="#5-logistic-regression-method">5. Logistic Regression method</a></li> <li><a href="#6-entropy-in-statistics">6. Entropy in Statistics</a></li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>In life some questions are too important to be left to opinion, superstition, or conjecture. For example, which drug should a doctor prescribe to treat an illness? What factors increase the risk of an individual developing coronary heart attack? To answer these questions (even remotely), we need <em>objective, evidence-based</em> decision making method.</p> <p><strong>Evidence-based practice:</strong> Using sound research findings based on observed or collected data to make decisions.</p> <p>In principle, people collect and process information every day of their lives. Since it’s something we do frequently, you might think we would be really good at it…but we’re not. We are not good at picking out patterns from a sea of noisy data. And, on the flip side, we are too good at picking out non-existent patterns from small numbers of observations.</p> <p>In order to mitigate any of our own personal biases when answering important questions about the way the world works (i.e., to do good science), we must be careful to be rigorous in the way we proceed. The <strong>scientific method</strong> is the process used to answer scientific questions.</p> <ol> <li>Ask the question</li> <li>Construct a hypothesis</li> <li>Test it with a study or an experiment</li> <li>Analyze data and draw conclusions (wish its as simple as that)</li> <li>Communicate results</li> </ol> <p>However, collecting and analyzing data can be complicated. Statistics helps us design studies, test hypotheses, and use data to make scientifically valid conclusions about the world. In general, scientists use the scientific method to make generalizations about classes of people on the basis of their studies. The class of people that they are trying to make generalizations about is called the <strong>population</strong>. Most of the time, it is impractical and expensive to study all individuals in a population, Instead of sampling everyone in the population, or taking a <strong>census</strong>, typically we study only a portion of the population called the <strong>sample</strong>. In order to determine how best to obtain a sample to answer the research questions, we must be cautious about the study design. Then, researchers make <strong>generalizations</strong>, or <strong>inference</strong>, about the entire population based on studying the sample.</p> <p>In this note we consider two broad categories of statistical analysis: <code class="language-plaintext highlighter-rouge">descriptive statistics</code>, which deals with methods for summarizing and/or presenting data and <code class="language-plaintext highlighter-rouge">inferential statistics</code>.</p> <p><strong>Descriptive statistics</strong>: Methods for summarizing and/or presenting data. <strong>Inferential statistics</strong>: Methods for making generalizations about a population using information contained in the sample.</p> <hr/> <p>It is difficult to sort through large streams of data and make any meaningful conclusions. Instead, we can better understand data by condensing it into human readable mediums through the use of data <code class="language-plaintext highlighter-rouge">summaries</code>, often displayed in the forms of tables and figures. However, in doing so, information is often be lost in the process. A good data summary will seek to strike a balance between clarity and completeness of information.</p> <p>There are two broad types of data that we may see in the wild, which we will call <code class="language-plaintext highlighter-rouge">categorical data</code> and <code class="language-plaintext highlighter-rouge">continuous data</code>. As the name suggests, categorical data (sometimes called <code class="language-plaintext highlighter-rouge">qualitative</code> or <code class="language-plaintext highlighter-rouge">discrete</code> data) are <em>data that fall into distinct categories</em>. Categorical data can further be classified into two distinct types:</p> <ul> <li>Nominal data: data that exists without any sort of natural or apparent ordering, e.g., colors (red, green, blue), gender (male, female), and type of motor vehicle (car, truck, SUV).</li> <li>Ordinal data: data that does have a natural ordering, e.g., education (high school, some college, college) and injury severity (low, medium, high).</li> </ul> <p><code class="language-plaintext highlighter-rouge">Continuous data</code> (sometimes called quantitative data), on the other hand, are data that can take on any numeric value on some interval or on a continuum. Examples of continuous data include height, weight, and temperature. Categorical and continuous data are summarized differently, and we’ll explore a number of ways to summarize both types of data.</p> <p>Below are some terminologies.</p> <p><code class="language-plaintext highlighter-rouge">Absolute frequency</code>: The number of observations in a category <code class="language-plaintext highlighter-rouge">Rate/Relative frequency</code>: The number of observations in a category relative to any other quantity <code class="language-plaintext highlighter-rouge">Percent/Proportion</code>: The number of observations per 100 <code class="language-plaintext highlighter-rouge">Bar plot</code>: Visualization of categorical data which uses bars to represent each category, with counts or percents represented by the height of each bar <code class="language-plaintext highlighter-rouge">Stratification</code>: The process of partitioning data into categories prior to summarizing</p> <hr/> <p>The two most common ways to describe the center are with the <code class="language-plaintext highlighter-rouge">mean</code> and the <code class="language-plaintext highlighter-rouge">median</code>. We all know what the mean is. The median is another common <em>measure of the center of a distribution</em>. In particular, for a set of observations, <em>the median is an observed value that is both larger than half of the observations, as well as smaller than half of the observations</em>.</p> <p>Sometimes there lies some data that are extremely different than the rest. Take the salaries of staff of some American university, the highest paid employee is usually the football coach, and much much higher then the rest. This one high salary, which is not representative of most of the salaries collected, is known as an <code class="language-plaintext highlighter-rouge">outlier</code>. In this particular case, the mean is highly sensitive to the presence of outliers while the <em>median is not</em>. Measures that are less sensitive to outliers are called <code class="language-plaintext highlighter-rouge">robust</code> measures. The median is a robust estimator of the center of the data.</p> <p>The shape of a distribution is often characterized by its <code class="language-plaintext highlighter-rouge">modality</code> and its <code class="language-plaintext highlighter-rouge">skew</code>. The modality of a distribution <em>is a statement about its modes, or “peaks.”</em> Distributions with a single peak are called <code class="language-plaintext highlighter-rouge">unimodal</code>, whereas distributions with two peaks are call <code class="language-plaintext highlighter-rouge">bimodal</code>. <code class="language-plaintext highlighter-rouge">Multimodal</code> distributions are those with three or more peaks. The <code class="language-plaintext highlighter-rouge">skew</code> on the other hand describes <em>how our data relates to those peaks</em>. Distributions in which the data is dispersed evenly on either side of a peak are called <code class="language-plaintext highlighter-rouge">symmetric distributions</code>; otherwise, the distribution is considered skewed. The direction of the skew is towards the side in which the tail is longest.</p> <hr/> <p>In addition to measuring the center of the distribution, we are also interested in the spread or dispersion of the data. Two distributions could have the same mean or median without necessarily having the same shape. Perhaps the most intuitive methods of describing the dispersion of our data are those associated with <code class="language-plaintext highlighter-rouge">percentile-based</code> summaries. Formally, the $p$-th percentile is some value $V_ {p}$ such that</p> <ol> <li>$p\%$ of observations are $\leq V_ {p}$;</li> <li>$1-p\%$ of observations are $\gg V_ {p}$.</li> </ol> <p>The <code class="language-plaintext highlighter-rouge">quartile</code> is made of</p> \[\begin{align*} Q_ {1} &amp;= 25\text{th} \text{ percentile} = 1\text{st} \text{ or lower quartile}\\ Q_ {2} &amp;= 50\text{th} \text{ percentile} = 2\text{nd} \text{ quartile or median}\\ Q_ {3} &amp;= 75\text{th} \text{ percentile} = 3\text{rd} \text{ or upper quartile}\\ \end{align*}\] <p>A commonly used percentile-based measure of spread combining these measures is the <strong>interquartile range (IQR)</strong>, defined as</p> \[\text{IQR} := Q_ {3} - Q_ {1}.\] <p>The IQR is not impacted by the presence of outliers, so it is considered a robust measure of the spread of the data. So, like the median, it enjoys the quality of being a robust measure of the data.</p> <p>Percentiles are also used to create another common visual representation of continuous data: the <code class="language-plaintext highlighter-rouge">boxplot</code>, also known as a <code class="language-plaintext highlighter-rouge">box-and-whisker plot</code>. A boxplot consist of the following elements:</p> <ul> <li>A box, indicating the Interquartile Range (IQR), bounded by the values $Q_ {1}$ and $Q_ {3}$;</li> <li>The median, or $Q_ {2}$, represented by the line drawn within the box;</li> <li>The “whiskers,” extending out of the box, which can be defined in a number of ways. Commonly, the whiskers are 1.5 times the length of the IQR from either $Q_ {1}$ or $Q_ {3}$;</li> <li>Outliers, presented as small circles or dots, and are values in the data that are not present within the bounds set by either the box or whiskers.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/boxPlot-480.webp 480w,/img/boxPlot-800.webp 800w,/img/boxPlot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/boxPlot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Just like histograms, boxplots can also illustrate the skew of a data. In a histogram, the skewed was named after the location of the tail and in a boxplot, this corresponds to the side with a longer whisker. Here we can see histograms and boxplots for various distributions of data. </div> <hr/> <p>The variance and the standard deviation are numerical summaries which quantify how spread out the distribution is around its mean.</p> <p>We have two kinds of variances: <code class="language-plaintext highlighter-rouge">sample variance</code> and <code class="language-plaintext highlighter-rouge">population variance</code>. The difference between sample variance and population variance lies in how they are calculated and what they represent in the context of statistical analysis.</p> <p><strong>Population Variance</strong>:</p> <p>Population variance measures how much the members of a population differ from the population mean. It is denoted by $\sigma^2$. If you have a population with $N$ members and population values $x_ 1, x_ 2, …, x_ N$, the population variance $\sigma^2$ is calculated as:</p> \[\sigma^2 = \frac{1}{N} \sum_ {i=1}^{N} (x_ i - \mu)^2\] <p>where $\mu$ is the population mean. Note that $\mu$ is not the mean of some measured data, it is supposed to be given by some theoretical model. Population variance is used when you have access to all the data points in the population.</p> <p><strong>Sample Variance</strong></p> <p>Sample variance measures how much the members of a sample (a subset of the population) differ from the sample mean. It is an estimator of the population variance. Sample variance is denoted by $s^2$. If you have a sample of size $n$ with values $x_ 1, x_ 2, …, x_ n$, the sample variance $s^2$ is calculated as:</p> \[s^2 = \frac{1}{n-1} \sum_ {i=1}^{n} (x_ i - \overline{x})^2\] <p>where $\overline{x}$ is the sample mean.</p> <p>Key Differences:</p> <ol> <li><strong>Purpose:</strong> Population variance describes the variability within an entire population, while sample variance estimates the population variance from a subset of the population.</li> <li><strong>Formula:</strong> The population variance formula divides by $n$ (the total number of population members), whereas the sample variance formula divides by $n-1$ (one less than the sample size).</li> <li><strong>Bias Correction:</strong> The use of $n-1$ in the sample variance formula, known as Bessel’s correction, corrects for the bias in the estimation of population variance from a sample.</li> </ol> <p>When we calculate the variance of a sample, we typically use the sample mean $\overline{x}$ as an estimate of the true population mean. However, using the sample mean introduces a bias because it is based on the same data points that we are using to calculate the variance. This means the sum of the squared deviations $(x_ i - \overline{x})^2$ tends to be smaller than it would be if we used the true population mean, leading to an underestimate of the true population variance.</p> <p>To correct for this bias, we use $n-1$ in the denominator instead of $n$. This adjustment is known as Bessel’s correction. The rationale behind it is that when estimating variance from a sample, we lose one degree of freedom because we have estimated the mean from the same data set. Using $n-1$ effectively compensates for this loss, making the sample variance an unbiased estimator of the true population variance.</p> <p>In summary, the factor $\frac{1}{n-1}$ is used instead of $\frac{1}{n}$ to make the sample variance an unbiased estimator of the population variance, accounting for the fact that the sample mean is used in the variance calculation.</p> <p>The standard deviation, denoted as $s$, is a function of variance. Recall that the mean is not a robust outlier and is highly sensitive to skew or the presence of outliers. Consequently, the variance and the standard deviation are also very sensitive.</p> <hr/> <p>The regression method is a statistical technique used to model and analyze the relationships between a dependent variable and one or more independent variables. The primary goal of regression is to predict the value of the dependent (or <code class="language-plaintext highlighter-rouge">response</code>) variable based on the values of the independent (or <code class="language-plaintext highlighter-rouge">predictor</code>) variables. It is widely used in various fields such as economics, finance, biological sciences, and social sciences for forecasting, estimating, and determining causal relationships.</p> <p>There are several types of regression methods, each suited to different types of data and relationships:</p> <ol> <li> <p><strong>Linear Regression</strong>: The simplest form of regression, linear regression uses a linear approach to model the relationship between the dependent variable and one or more independent variables. The model assumes that the relationship can be described by a straight line in the form $y = \beta_ 0 + \beta_ 1x_ 1 + \epsilon$, where $y$ is the dependent variable, $x_ 1$ is the independent variable, $\beta_ 0$ is the y-intercept, $\beta_ 1$ is the slope of the line, and $\epsilon$ represents the error term.</p> </li> <li> <p><strong>Multiple Linear Regression</strong>: An extension of simple linear regression, this method involves two or more independent variables. The model is expressed as $y = \beta_ 0 + \beta_ 1x_ 1 + \beta_ 2x_ 2 + … + \beta_ nx_ n + \epsilon$, where $x_ 1, x_ 2, …, x_ n$ are the independent variables.</p> </li> <li> <p><strong>Logistic Regression</strong>: Used when the dependent variable is categorical, typically binary. Logistic regression models the probability that the dependent variable belongs to a particular category, using a logistic function.</p> </li> <li> <p><strong>Polynomial Regression</strong>: A form of regression where the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y.</p> </li> <li> <p><strong>Ridge and Lasso Regression</strong>: These are types of linear regression that include regularization. Regularization adds a penalty on the size of coefficients to prevent overfitting. Ridge regression adds a squared magnitude of the coefficient as a penalty term to the loss function, while Lasso regression adds the absolute value of the magnitude of the coefficient as a penalty term.</p> </li> <li> <p><strong>Non-linear Regression</strong>: Used when the data cannot be modeled using linear methods due to a non-linear relationship between the dependent and independent variables. Non-linear regression can fit complex curves to data.</p> </li> </ol> <p>Regression analysis involves selecting the appropriate model for the data, estimating the model parameters (usually through methods like least squares), evaluating the model’s adequacy (checking for goodness-of-fit, residuals, etc.), and interpreting the results to make inferences or predictions.</p> <p>In practice, the choice of regression method depends on the nature of the dependent variable, the shape of the relationship, and the distribution of the residuals, among other factors. Proper model selection, diagnostic testing, and validation are crucial steps in ensuring that the regression model provides reliable and accurate predictions or insights.</p> <h1 id="2-basic-statistical-methods">2. Basic Statistical Methods</h1> <h2 id="21-t-test-and-anova-analysis-of-variance">2.1. t-Test and ANOVA (Analysis of Variance)</h2> <p>My time is really limited here so I’ll direct jump to a short review of some mostly commonly used statistical methods.</p> <p>The basic $t$-test is used to compare two independent samples. The t-statistic on which the test is based is the difference between the two sample averages, divided by the standard error of that difference. The t-test is designed to work in small samples, whereas Z-tests are not.</p> <h3 id="211-t-test">2.1.1. t-test</h3> <p>Below is the gist of the derivation of the t-distribution. Assume we have a population that follows a normal distribution with mean $\mu$ and standard deviation $\sigma$. We take a random sample of size $n$ from this population, and we calculate the sample mean $\bar{x}$. The sample mean $\bar{x}$ is also normally distributed (due to the Central Limit Theorem) with mean $\mu$ and standard deviation $\sigma / \sqrt{n}$. We standardize $\bar{x}$ to transform it into a standard normal variable $Z$:</p> \[Z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}}\] <p>In practice, $\sigma$ (the population standard deviation) is often unknown and must be estimated from the sample. We use the sample standard deviation $s$ as an estimate for $\sigma$, where $s$ is calculated from the sample data. We replace $\sigma$ with $s$ in the standardization formula, but this introduces additional variability because $s$ is an estimate:</p> \[T = \frac{\bar{x} - \mu}{s / \sqrt{n}}\] <p>This ratio $T$ does <strong>not</strong> follow a standard normal distribution anymore due to the uncertainty introduced by using $s$ instead of $\sigma$. Instead, the variable $T$ follows a distribution that is similar to the normal distribution but with heavier tails. This is the t-distribution. <em>The exact shape of the t-distribution depends on the sample size $n$ through the degrees of freedom, which is $n - 1$ in this context</em>. The degrees of freedom account for the additional uncertainty introduced by estimating the population standard deviation.</p> <p>The t-distribution is formally defined through its probability density function (pdf), which is more complex than that of the normal distribution and involves the Gamma function. The pdf of the t-distribution for a given $t$ value and $v$ degrees of freedom (where $v = n - 1$) is given by:</p> \[f(t; v) = \frac{\Gamma((v+1)/2)}{\sqrt{v\pi}\Gamma(v/2)} \left(1 + \frac{t^2}{v} \right)^{-(v+1)/2}\] <p>where $\Gamma$ is the Gamma function, a generalization of the factorial function to complex numbers.</p> <p>In short, remember the following:</p> <ul> <li>The t-distribution accounts for the additional uncertainty in estimating the population mean when the population standard deviation is unknown and the sample size is small.</li> <li>As the sample size increases, the t-distribution approaches the standard normal distribution because the estimate $s$ becomes more accurate, reducing the extra uncertainty.</li> </ul> <hr/> <p>In the context of the t-test and statistical hypothesis testing, “significance” refers to the degree to which the test results allow us to reject the null hypothesis. The null hypothesis typically proposes that there is no effect or no difference between groups or conditions. When we say a result is “statistically significant,” it means that the observed data are unlikely to have occurred under the null hypothesis, suggesting that there is a real effect or difference.</p> <p>The significance level, denoted as $\alpha$, is a threshold set by the researcher before conducting the test, which defines the probability of rejecting the null hypothesis when it is actually true (a type I error). Common values for $\alpha$ are 0.05, 0.01, and 0.10, with 0.05 being the most widely used. Setting $\alpha$ at 0.05 means that there is a 5% risk of concluding that a difference exists when there is no actual difference.</p> <p>The p-value is a key metric derived from the t-test that indicates the probability of observing the test results, or more extreme results, if the null hypothesis were true. A p-value that is less than or equal to the significance level ($p \leq \alpha$) indicates that the observed data are unlikely under the null hypothesis, leading to the rejection of the null hypothesis. In simpler terms, a low p-value (typically ≤ 0.05) suggests that the evidence against the null hypothesis is strong enough to consider the results statistically significant.</p> <p><strong>Interpretation of Significance</strong></p> <ul> <li> <p><strong>Statistically Significant</strong>: If the test result is statistically significant, it suggests that the evidence is strong enough to reject the null hypothesis. This typically means there is a meaningful difference between the groups being compared, which is not likely to have occurred by chance.</p> </li> <li> <p><strong>Not Statistically Significant</strong>: If the result is not statistically significant, it suggests that the evidence is not strong enough to reject the null hypothesis. This could mean that there is no meaningful difference between the groups, or that the study did not have enough power (e.g., sample size was too small) to detect a difference if one exists.</p> </li> </ul> <p>It’s important to note that <em>statistical significance does not necessarily imply practical or clinical significance</em>. A result can be statistically significant but still be of little practical value if the observed effect or difference is too small to be of interest or use in a practical context.</p> <hr/> <h3 id="212-two-sided-hypothesis-test">2.1.2. Two-sided Hypothesis Test</h3> <p>In biostatistics, the two-sided t-test (also known as the two-tailed t-test) is commonly used to determine whether there is a significant difference between the means of two groups, <em>without specifying the direction of the difference</em>. This type of test is employed when the research question is concerned with whether there is any difference at all, rather than predicting which group will have a higher or lower mean.</p> <p>Biostatistics often involves comparing biological measurements or outcomes across different groups. For instance, one might compare the efficacy of two different medications, the impact of a treatment versus a placebo, or physiological measurements (like blood pressure) between two groups with different dietary habits. In these cases, researchers might not have a strong hypothesis about which group will have higher or lower means, or they may wish to test for the possibility of a difference in either direction. The two-sided t-test is ideal for these scenarios because it allows for the detection of significant differences regardless of their direction.</p> <p>The formula for the t-statistic in a two-sided t-test is similar to that of a one-sided t-test, but the <em>interpretation of the p-value and the critical value from the t-distribution is different</em>.</p> <p>For an independent two-sample t-test, the formula for the t-statistic remains:</p> \[t = \frac{\bar{x}_ 1 - \bar{x}_ 2}{\sqrt{s^2 \left(\frac{1}{n_ 1} + \frac{1}{n_ 2}\right)}}\] <p>However, in a two-sided t-test, you’re interested in differences in both directions, so you consider both tails of the distribution when determining the critical t-value or when interpreting the p-value.</p> <p>The hypotheses for a two-sided t-test are formulated as follows:</p> <ul> <li>Null Hypothesis ($H_ 0$): There is no difference between the group means ($\mu_ 1 = \mu_ 2$).</li> <li>Alternative Hypothesis ($H_ a$): There is a difference between the group means ($\mu_ 1 \neq \mu_ 2$).</li> </ul> <p>In a two-sided t-test, the p-value represents the probability of observing a test statistic as extreme as, or more extreme than, the one observed, in either direction, assuming the null hypothesis is true. If this p-value is less than or equal to the chosen significance level ($\alpha$), the null hypothesis is rejected, indicating a statistically significant difference between the two group means.</p> <p>A significant result in a two-sided t-test suggests that there is enough evidence to conclude that a difference exists between the two group means, but it does not indicate which group has the higher mean. This approach is particularly useful in biostatistics, where establishing the existence of a difference can be crucial for further research, clinical decisions, or policy-making, even before the direction of the difference is fully understood.</p> <h3 id="213-f-test">2.1.3. F-test</h3> <p>Suppose that we need to compare sample averages across the arms of a clinical trial with multiple treatments, or more generally across more than two independent samples. For this purpose, one-way analysis of variance (ANOVA) and the F-test take the place of the t-test. F-test technique extends the t-test, which compares only two means, by allowing comparisons among multiple groups simultaneously, thus providing a holistic view of the data.</p> <p>In the context of $F$-test, the <em>null hypothesis is that the mean values of the outcomes from all the populations sampled from are the same</em>, against the alternative that the means differ in at least two of the samples.</p> <p>The F-test is based on the F-distribution and uses an F-statistic to test the null hypothesis. <em>The test essentially compares the variance between the groups to the variance within the groups</em>; a higher ratio suggests that the group means differ significantly.</p> <p>Key Concepts of One-Way ANOVA are</p> <ul> <li><code class="language-plaintext highlighter-rouge">Between-Group Variability</code>: Differences among the group means, which reflect the effect of the independent variable.</li> <li><code class="language-plaintext highlighter-rouge">Within-Group Variability</code>: Variations within each group, attributed to random error or individual differences not due to the treatment effect.</li> <li><code class="language-plaintext highlighter-rouge">F-Statistic</code>: A ratio of the between-group variability to the within-group variability. A larger F-statistic indicates a greater likelihood that significant differences exist among the group means.</li> </ul> <p>F-test assumes that:</p> <ol> <li><strong>Independence of Observations:</strong> The data in different groups should be independent of each other.</li> <li><strong>Normality:</strong> The distribution of the residuals (errors) should be approximately normally distributed for each group.</li> <li><strong>Homogeneity of Variances:</strong> The variances among the groups should be approximately equal.</li> </ol> <p>Next we will give the gist of the derivation of F-distribution, follow by an example of application.</p> <p>Roughly speaking, <em>the F-distribution arises when dividing one $\chi^{2}$ (chi-square) distributed variable by another,</em> each divided by their respective degrees of freedom. Here’s a step-by-step explanation:</p> <p>Consider two independent chi-square distributed variables, $X$ and $Y$, with degrees of freedom $d_ 1$ and $d_ 2$, respectively. These chi-square variables can be thought of as the sum of squares of $d_ 1$ and $d_ 2$ independent standard normal variables.</p> <p>The probability density functions (pdf) for $X$ and $Y$ are given by:</p> \[f_ X(x) = \frac{1}{2^{d_ 1/2}\Gamma(d_ 1/2)} x^{d_ 1/2 - 1} e^{-x/2}, \quad x &gt; 0\] \[f_ Y(y) = \frac{1}{2^{d_ 2/2}\Gamma(d_ 2/2)} y^{d_ 2/2 - 1} e^{-y/2}, \quad y &gt; 0\] <p>where $\Gamma$ denotes the Gamma function.</p> <p>The F-statistic is constructed by dividing $X/d_ 1$ by $Y/d_ 2$, each chi-square variable divided by its degrees of freedom, which normalizes them:</p> \[F = \frac{X/d_ 1}{Y/d_ 2}\] <p>To derive the pdf of the F-distribution, we need to find the distribution of the variable $F$. This involves some complex integration because we have to consider the joint distribution of $X$ and $Y$, and then transform it to the distribution of $F$. The transformation involves the Jacobian of the transformation from $(X, Y)$ to $(F, Y)$, and then integrating out $Y$ to get the marginal distribution of $F$. After performing the necessary mathematical manipulations, the pdf of the F-distribution for a given $f$ value, with degrees of freedom $d_ 1$ and $d_ 2$, is given by:</p> \[f(f; d_ 1, d_ 2) = \frac{\Gamma((d_ 1+d_ 2)/2)}{\Gamma(d_ 1/2)\Gamma(d_ 2/2)} \left(\frac{d_ 1}{d_ 2}\right)^{d_ 1/2} f^{d_ 1/2 - 1} \left(1 + \frac{d_ 1}{d_ 2}f\right)^{-(d_ 1+d_ 2)/2}, \quad f &gt; 0\] <p>This distribution is used to test hypotheses about the equality of variances of two normally distributed populations, among other applications.</p> <p>Next, an example:</p> <p>Imagine a researcher wants to investigate the effect of different teaching methods on student performance. The researcher divides a group of 90 students into three equal groups, each subjected to a different teaching method: Method A (traditional), Method B (interactive), and Method C (technology-assisted). After a semester, the researcher measures the performance of each student on a standardized test.</p> <p>The research question is: “Do the three teaching methods result in different student performance levels?”</p> <p>To address this question using one-way ANOVA, the researcher would:</p> <ol> <li><strong>Calculate the group means:</strong> Find the average performance score for students in each teaching method group.</li> <li><strong>Compute the ANOVA statistics:</strong> Determine the between-group and within-group variances and calculate the F-statistic.</li> <li><strong>Compare the F-statistic to a critical value from the F-distribution:</strong> The critical value depends on the level of significance (usually set at 0.05) and the degrees of freedom for the numerator (between-groups, $k - 1$, where $k$ is the number of groups) and the denominator (within-groups, $N - k$, where $N$ is the total number of observations).</li> </ol> <p>If the computed F-statistic is greater than the critical value, the null hypothesis is rejected, suggesting that there is a significant difference in student performance across at least two of the teaching methods. The researcher might then conduct post-hoc tests to identify specifically which groups differ from each other.</p> <hr/> <h3 id="214-robustness">2.1.4. Robustness</h3> <p>We have assumed normal distribution for the distribution of random variables. However, both the t-test and the F-test are pretty robust to violations of the normality assumption, especially in large samples, similar to the central limit theorem. <em>By robust we mean that the type-I error rate, which is the mistake of rejecting the null hypothesis when it actually holds, is not seriously affected.</em> They are, however, primarily sensitive to outliers, which will mess up the variation.</p> <p>Specifically for the independent two-sample t-test, there’s an important assumption known as the <strong>equal variance assumption</strong> or <strong>homoscedasticity</strong>. This assumption states that the variance within each of the groups being compared should be approximately equal. The t-test is less robust to violations to this assumption, which can seriously affect the type-I error rate (and not always in conservative direction). In contrast, the overall F-test in ANOVA loses efficiency, but the error rate of type-I is use seriously increases. If the assumption of equal variances is violated, adjustments to the t-test can be made to account for the difference in variances. One common approach is to use Welch’s t-test, which does not assume equal population variances. Welch’s t-test adjusts the degrees of freedom of the t-test based on the sample sizes and variances of the two groups, making it more reliable when the variances are unequal.</p> <h1 id="3-correlation">3. Correlation</h1> <p>Pearson correlation coefficient, often symbolized as $r$, is a measure of the linear correlation between two variables $X$ and $Y$. In biostatistics, it’s widely used to quantify the degree to which two biological or health-related variables are linearly related. The value of $r$ ranges from -1 to +1, where:</p> <ul> <li>$r = 1$ indicates a perfect positive linear relationship,</li> <li>$r = -1$ indicates a perfect negative linear relationship,</li> <li>$r = 0$ suggests no linear relationship.</li> </ul> <p>In biostatistics, Pearson correlation is used to explore relationships between various biological, clinical, or health-related variables. Some examples include:</p> <ol> <li> <p><strong>Gene Expression Studies</strong>: Researchers might use Pearson correlation to assess the relationship between the expression levels of two genes across various conditions or tissue types, helping to identify potentially co-regulated genes or gene pairs with opposing expression patterns.</p> </li> <li> <p><strong>Nutritional Epidemiology</strong>: It can be used to explore the relationship between dietary intake (like calorie intake) and health outcomes such as blood pressure or cholesterol levels. A positive correlation might suggest that higher intake is associated with higher blood pressure, while a negative correlation could indicate the opposite.</p> </li> <li> <p><strong>Clinical Trials</strong>: In trials, Pearson correlation might be applied to examine the relationship between the dose of a drug and its effect on a biomarker. A positive correlation would suggest that as the dose increases, the biomarker levels also increase, indicating a possible dose-response relationship.</p> </li> </ol> <p>The Pearson correlation coefficient is calculated as:</p> \[r = \frac{\left\langle (x-\overline{x})(y-\overline{y}) \right\rangle }{\sqrt{ \left\langle (x-\overline{x})^{2} \right\rangle \left\langle (y-\overline{y})^{2} \right\rangle }}\] <p>Where $\left\langle \bullet \right\rangle$ is the <em>sample mean</em> of $\bullet$, not the population mean.</p> <p>While the Pearson correlation coefficient is a powerful tool, it has limitations. It only measures <em>linear relationships</em>, so it may not capture more complex patterns. Additionally, <em>it is sensitive to outliers</em>, which can disproportionately affect the correlation coefficient. Finally, <em>a significant Pearson correlation does not imply causation; it only indicates that a linear relationship exists</em>.</p> <hr/> <p>Like the $t$-test and linear regression, the correlation coefficients are sensitive to outliers. In this case, a <em>robust</em> alternative is the Spearman correlation coefficient, which is equivalent to the Pearson coefficient applied to the <code class="language-plaintext highlighter-rouge">ranks</code> of $x$ and $y$. <em>By rank we mean position in the ordered sequence of the values of a variable</em>. For example, if $x$ takes on values $1.2,5.1,4.3,16.0$, then we first order them from small to large, then the so-called rank is the position; the rank of $1.2$ is 1, the rank of 4.3 is 2, the rank of the outlier 16.0 is 4. In the given order the outliers are by construction either on the smallest end or the largest end. Ranks are used in a range of nonparametric methods, in no small part because of their robustness when the data include outliers. Their disadvantage is that any information contained in the measured values of the outcome beyond the ranks is lost.</p> <p>To be more specific, here’s a step-by-step explanation of how ranking is done, along with an example:</p> <ol> <li> <p><strong>Sort the data</strong>: Arrange the data in ascending or descending order.</p> </li> <li> <p><strong>Assign ranks</strong>: Assign ranks to each observation based on its position in the sorted data. The smallest observation gets a rank of 1, the second smallest gets a rank of 2, and so on.</p> </li> <li> <p><strong>Handle tied ranks</strong>: If there are tied values (i.e., two or more observations with the same value), assign them the average of the ranks they would have received. For example, if two observations tie for the second smallest value, each would receive a rank of 2.5.</p> </li> </ol> <p>Let’s illustrate this with an example. Consider the following dataset: 10, 15, 8, 20, 25, 15, 30</p> <ol> <li> <p>Sort the data: 8, 10, 15, 15, 20, 25, 30</p> </li> <li> <p>Assign ranks:</p> <ul> <li>8 gets a rank of 1</li> <li>10 gets a rank of 2</li> <li>15 gets a rank of 3.5 (average of ranks 3 and 4)</li> <li>15 gets a rank of 3.5 (average of ranks 3 and 4)</li> <li>20 gets a rank of 5</li> <li>25 gets a rank of 6</li> <li>30 gets a rank of 7</li> </ul> </li> </ol> <hr/> <p>Kendall’s tau (often denoted as $\tau$) is a measure of association or correlation between two ranked variables. It’s a <code class="language-plaintext highlighter-rouge">non-parametric statistic</code>, meaning <em>it doesn’t assume any specific distribution for the variables involved</em>. Kendall’s tau is particularly useful when dealing with ranked or ordinal data, where the exact numerical values of the data points might not be as important as their relative ordering.</p> <p>The formula for Kendall’s tau for two variables $X$ and $Y$ with $n$ observations each is given by:</p> \[\tau = \frac{\text{Number of concordant pairs} - \text{Number of discordant pairs}}{\text{Number of possible pairs}}\] <p>Where:</p> <ul> <li>A pair of observations $X_ i, Y_ i$ and $X_ j, Y_ j$ is considered concordant if the ranks agree, i.e., if $(X_ i - X_ j)(Y_ i - Y_ j) &gt; 0$.</li> <li>A pair is discordant if the ranks disagree, i.e., if $(X_ i - X_ j)(Y_ i - Y_ j) &lt; 0$.</li> <li>The number of possible pairs is the total number of pairs of observations, which is $\frac{n(n-1)}{2}$ for n observations.</li> </ul> <p>Let’s go through an example to illustrate Kendall’s tau:</p> <p>Suppose we have the following ranked data for two variables X and Y:</p> <p>\(X: 5, 3, 1, 4, 2\) \(Y: 2, 4, 5, 1, 3\)</p> <p>Step 1: Calculate the number of concordant and discordant pairs.</p> <ul> <li>Concordant pairs: Count the pairs where the ranks agree.</li> <li>Discordant pairs: Count the pairs where the ranks disagree.</li> </ul> \[\text{Concordant pairs} = 4\] <p>pairs (5, 4), (5, 3), (4, 2), (3, 2)</p> \[\text{Discordant pairs} = 6\] <p>pairs (5, 2), (5, 1), (5, 3), (4, 1), (4, 3), (3, 1)</p> <p>Step 2: Calculate Kendall’s tau.</p> \[\tau = \frac{\text{Concordant pairs} - \text{Discordant pairs}}{\text{Number of possible pairs}}\] \[\tau = \frac{4 - 6}{\frac{ {5(5-1)}}{2} } = \frac{-2}{10} = -0.2\] <p>So, Kendall’s tau for the given data is -0.2.</p> <p>Interpretation: Since Kendall’s tau is negative, it suggests a slight negative association between variables X and Y. This means that as the rank of X increases, the rank of Y tends to decrease slightly, and vice versa.</p> <p>Kendall’s tau is widely used in various fields, especially when dealing with ranked or ordinal data, as it provides a robust measure of association that is not sensitive to the specific values of the ranks.</p> <h1 id="4-linear-regression-method">4. Linear Regression Method</h1> <p>Linear regression methods in biostatistics are used to describe the relationship between one or more independent (predictor or explanatory) variables and a continuous dependent (outcome) variable. These methods are fundamental in biostatistical analysis for understanding associations, predicting outcomes, and identifying potential causal relationships in health sciences. The primary methods include:</p> <ol> <li>Simple Linear Regression: <ul> <li><strong>Description</strong>: Examines the relationship between a single independent variable (X) and a dependent variable (Y).</li> <li><strong>Model</strong>: The relationship is modeled as $Y = \beta_ 0 + \beta_ 1X + \epsilon$, where $\beta_ 0$ is the y-intercept, $\beta_ 1$ is the slope of the line (indicating the change in Y for a one-unit change in X), and $\epsilon$ represents the error term.</li> <li><strong>Use Cases</strong>: Used when you want to see how changes in one predictor variable influence changes in the outcome. For example, studying the effect of drug dosage on blood pressure levels.</li> </ul> </li> <li>Multiple Linear Regression (MLR): <ul> <li><strong>Description</strong>: Extends simple linear regression to include multiple independent variables.</li> <li><strong>Model</strong>: $Y = \beta_ 0 + \beta_ 1X_ 1 + \beta_ 2X_ 2 + … + \beta_ kX_ k + \epsilon$, where $\beta_ 0$ is the intercept, $\beta_ i$ are the coefficients for each predictor $X_ i$, and $\epsilon$ is the error term.</li> <li><strong>Use Cases</strong>: Useful when investigating the impact of several factors on an outcome. For instance, assessing how patient age, weight, and smoking status together influence the risk of developing cardiovascular diseases.</li> </ul> </li> <li>Polynomial Regression: <ul> <li><strong>Description</strong>: A form of regression analysis where the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial.</li> <li><strong>Model</strong>: $Y = \beta_ 0 + \beta_ 1X + \beta_ 2X^2 + … + \beta_ nX^n + \epsilon$.</li> <li><strong>Use Cases</strong>: Employed when the relationship between variables is not linear, allowing for a better fit to data that display curvature. For example, modeling the growth rate of bacteria at different temperatures might require a polynomial fit.</li> </ul> </li> <li>Ridge Regression (L2 Regularization): <ul> <li><strong>Description</strong>: Addresses multicollinearity (high correlation among independent variables) in MLR by adding a penalty term equal to the square of the magnitude of the coefficients.</li> <li><strong>Model</strong>: The cost function is $\text{Cost} = \left\lVert Y - X\beta \right\rVert ^2 + \lambda \left\lVert \beta \right\rVert ^2$, where $\lambda$ is the penalty term.</li> <li><strong>Use Cases</strong>: Useful in situations with many predictors, some of which might be correlated. It helps in reducing overfitting by shrinking the coefficients.</li> </ul> </li> <li>Lasso Regression (L1 Regularization): <ul> <li><strong>Description</strong>: Similar to ridge regression but uses an absolute value penalty for the size of coefficients, which can lead to some coefficients being exactly zero.</li> <li><strong>Model</strong>: The cost function is $\text{Cost} = \left\lVert Y - X\beta \right\rVert ^2 + \lambda \left\lVert \beta \right\rVert$.</li> <li><strong>Use Cases</strong>: Used for variable selection and regularization to improve prediction accuracy and interpretability of the statistical model by excluding irrelevant variables.</li> </ul> </li> <li>Elastic Net Regression: <ul> <li><strong>Description</strong>: Combines penalties of ridge regression and lasso regression.</li> <li><strong>Model</strong>: The cost function includes both L1 and L2 penalties, $\text{Cost} = \lVert Y - X\beta\rVert ^2 + \lambda_ 1 \lvert\beta\rvert + \lambda_ 2 \lVert\beta\rVert^2$.</li> <li><strong>Use Cases</strong>: Effective when there are multiple correlated variables, providing a balance between ridge and lasso regression by including both sets of penalties.</li> </ul> </li> </ol> <p>Some comments. Ridge Regression is called L2 regularization because of the nature of the penalty applied to the coefficients in the regression model. In this context, “L2” refers to the L2 norm of the coefficient vector, which is used as the penalty term. The L2 norm is essentially the square root of the sum of the squared vector values, but in the context of ridge regression, the penalty term involves the square of the L2 norm (i.e., the sum of the squared values of the coefficients, not taking the square root).</p> <p>More mathematically, for a regression coefficient vector $\beta = [\beta_ 1, \beta_ 2, …, \beta_ n]$, the L2 norm is defined as:</p> \[\lVert\beta\rVert_ 2 = \sqrt{\beta_ 1^2 + \beta_ 2^2 + ... + \beta_ n^2}\] <p>In ridge regression, the penalty term added to the <code class="language-plaintext highlighter-rouge">cost function</code> (which is minimized during the training of the model) is the square of this L2 norm (hence the term “L2 regularization”), but it’s often just presented without the square root to begin with in the context of ridge regression.</p> <p>The rationale behind using L2 regularization (ridge regression) is to prevent overfitting by shrinking the coefficients of less important features towards zero (though not exactly zero, which is a characteristic of Lasso regression, or L1 regularization). This is particularly useful when dealing with multicollinearity or when the number of predictor variables is large relative to the number of observations. <em>The L2 regularization term penalizes large coefficients, thus enforcing a constraint on the size of coefficients, which can lead to more robust and better-generalized models.</em></p> <p>Overfitting occurs when a statistical model or machine learning algorithm captures the noise of the data rather than the underlying pattern. It happens when the model is too complex relative to the amount and noisiness of the input data. The overfitted model has high variance and low bias, making excellent predictions on the training data but performing poorly on new, unseen data because it has essentially memorized the training dataset rather than learning the general underlying patterns.</p> <hr/> <p>The name “Lasso regression” comes from the term “Least Absolute Shrinkage and Selection Operator.” It was introduced by Robert Tibshirani in 1996 as a new regression method that not only has the capability to shrink the coefficients toward zero, like Ridge regression, but also to set some coefficients exactly to zero. This latter property makes Lasso regression particularly useful for feature selection in addition to regularization.</p> <p>The term “Lasso” itself is a metaphor, likening the method to a cowboy’s lasso used to catch and select specific components (in this case, variables or features in a model). The lasso wraps around the most important features while discarding the less important ones, making it a valuable tool for models with a large number of features, many of which might be irrelevant or redundant.</p> <h1 id="5-logistic-regression-method">5. Logistic Regression method</h1> <p>Logistic regression in biostatistics is a statistical analysis method used to model the relationship between one or multiple independent variables and a dependent variable that is binary (i.e., it takes on two possible outcomes, often coded as 0 and 1). It’s particularly useful in the field of biostatistics for analyzing and predicting the probability of a binary outcome based on one or more risk factors or predictor variables.</p> <p>Logistic regression is a statistical method used for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). It’s used extensively in fields like medicine, biology, and social sciences, among others, for tasks like disease prediction, customer churn prediction, and spam detection.</p> <p>The logistic function, also called the sigmoid function, is an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.</p> <h1 id="6-entropy-in-statistics">6. Entropy in Statistics</h1> <p>In statistics and machine learning, entropy is a measure of uncertainty, randomness, or unpredictability in a set of outcomes. The concept of entropy, borrowed from thermodynamics and information theory, is particularly useful in model fitting and various statistical analyses for quantifying the amount of information, selecting models, and even in decision tree algorithms. Here’s how entropy is applied in these contexts:</p> <p><strong>Information Gain in Decision Trees:</strong></p> <p>In the context of decision trees, particularly in classification problems, entropy is used to measure the impurity or disorder in a set of examples. Information gain, which is based on the decrease in entropy, is then used to decide which feature to split on at each step in the tree.</p> <ul> <li><em>Entropy before Split</em>: Measures the degree of uncertainty or impurity in the dataset before it is divided.</li> <li><em>Entropy after Split</em>: Measures the weighted sum of the entropy of each subset created after splitting the dataset based on a specific feature.</li> <li><em>Information Gain</em>: The difference between the entropy before the split and the weighted entropy after the split. A feature with the highest information gain is chosen for the split because it best reduces uncertainty.</li> </ul> <p><strong>Model Selection and Regularization:</strong></p> <p>Entropy can also be used as a criterion for model selection and regularization, particularly in methods like Maximum Entropy (MaxEnt) modeling, which is used in various fields including natural language processing (NLP) and ecology.</p> <ul> <li><em>Maximum Entropy Modeling</em>: In MaxEnt, the principle of maximum entropy is used to select the probability distribution that best represents the current state of knowledge (the one with the maximum entropy), subject to the given constraints (e.g., the known moments or expectations of certain features). This approach ensures that no additional assumptions are made beyond what is justified by the data, leading to a model that is maximally non-committal with regard to missing information.</li> </ul> <p><strong>Feature Selection and Dimensionality Reduction:</strong></p> <p>Entropy can be used to evaluate the importance or relevance of features in a dataset. Features that do not contribute significantly to reducing uncertainty (or increasing the information gain) can be considered for removal, which is an essential aspect of feature selection and dimensionality reduction.</p> <ul> <li><em>Mutual Information</em>: A related concept, mutual information, measures the amount of information that one variable provides about another. In feature selection, mutual information can be used to quantify the relevance of each feature with respect to the target variable, where features with higher mutual information are preferred.</li> </ul> <p><strong>Quantifying Prediction Uncertainty:</strong></p> <p>In probabilistic modeling and Bayesian statistics, entropy can be used to quantify the uncertainty associated with predictions. Models that produce probability distributions as predictions can have their predictive entropy calculated to assess how uncertain the model is about its predictions.</p> <ul> <li><em>Predictive Entropy</em>: High entropy in the predicted probability distributions indicates high uncertainty in the predictions, which can be crucial for understanding the confidence of the model in its predictions and for decision-making processes where uncertainty needs to be minimized.</li> </ul> <p>Example in Decision Tree:</p> <p>Consider a dataset with two features (X1: Color, X2: Size) and a binary target variable (Y: Defective or Not Defective). The entropy of the target variable represents the uncertainty in the defective status. If a split based on the “Color” feature results in subsets with lower entropy (more purity in terms of the target variable), the information gain from this split is high, making “Color” a good candidate for splitting. The decision tree algorithm will use this entropy-based criterion to construct a tree that aims to reduce the uncertainty (entropy) in the target variable with each split.</p> <p>In summary, entropy serves as a foundational concept in model fitting and statistics, enabling more informed decisions about model structure, feature importance, and the uncertainty in predictions, thereby improving model interpretability and effectiveness.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="biostatistics"/><summary type="html"><![CDATA[1. Introduction 2. Basic Statistical Methods 2.1. t-Test and ANOVA (Analysis of Variance) 2.1.1. t-test 2.1.2. Two-sided Hypothesis Test 2.1.3. F-test 2.1.4. Robustness 3. Correlation 4. Linear Regression Method 5. Logistic Regression method 6. Entropy in Statistics]]></summary></entry><entry><title type="html">Renormalization method in PDF</title><link href="https://baiyangzhang.github.io/blog/2024/Renormalization-method-in-PDF/" rel="alternate" type="text/html" title="Renormalization method in PDF"/><published>2024-03-02T00:00:00+00:00</published><updated>2024-03-02T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Renormalization-method-in-PDF</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Renormalization-method-in-PDF/"><![CDATA[<h1 id="abstract">Abstract</h1> <p>The application of renormalization group method in solving the differential equation in the momentum space, and the error estimation of the solution with finite sized lattice size.</p> <h1 id="introduction">Introduction</h1> <p>The question we want to answer here is very simple: If we solve the PDF in the momentum space with a given cutoff $\Lambda$, how does the solution change with respect to $\Lambda$?</p> <h1 id="first-order-homogeneous-linear-differential-equation">First Order Homogeneous Linear Differential Equation</h1> <p>We want a well behaved function $\mathbb{R} \to \mathbb{R}$ so we begin with Gaussian function $g(t) = e^{-\lambda t^2}$. The equation it solves is</p> \[\dot{g}(t) + 2 \lambda t g(t) = 0,\] <p>which is a homogeneous first order ODE.</p> <p>Imagine that the function is defined on a grid with lattice spacing $a$, then the equation takes the discrete form</p> \[\frac{g(t_ {i+1})-g(t_ i)}{a} + 2 \lambda t g(t_ i) = 0,\quad \forall i \in \text{ lattice},\] <p>the solution will naturally depend on the lattice size, in the $a\to 0$ limit we will recover the continuous solution. We want to know what explicitly the dependence on $a$ looks like, and how to perform the error estimate with small but finite $a$.</p> <p>As the first attempt, in our simplified example, we go to the frequency space by substitute</p> \[g(t) = \frac{1}{\sqrt{2\pi}}\int_ {-\infty}^{\infty} d\omega \tilde{g}(\omega) e^{-i\omega t}.\] <p>The Fourier transformed equation reads</p> \[\int_ {-\infty}^{\infty} \frac{d\omega}{\sqrt{2\pi}} e^{-i\omega t} \left\lbrace -i \omega \tilde{g}(\omega) - 2i\lambda \frac{d}{d\omega} \tilde{g}(\omega) \right\rbrace.\] <p>A lower bound in the lattice size $a$ corresponds to an upper bound $\Lambda$ in the momentum (exchangeable with frequency in our discussion) space, $a \sim 1/\Lambda$, thus we have</p> \[\begin{align} &amp;\int_ {-\Lambda}^{\Lambda} \frac{d\omega}{\sqrt{2\pi}} e^{-i\omega t} \left\lbrace -i \omega \tilde{g}(\omega) - 2i\lambda \frac{d}{d\omega} \tilde{g}(\omega) \right\rbrace = 0, \\ \implies &amp; \omega \tilde{g}(\omega) + 2\lambda \frac{d}{d\omega} \tilde{g}(\omega) = 0\, \forall \, |\omega| &lt; \Lambda, \quad 0 \text{ otherwise} \end{align}\] <p>The equation for $\tilde{g}(\omega)$ takes the same form (up to some multiplicative constants) as that for $g(t)$, reflecting the fact that the Fourier transformed Gaussian function takes the same form as the original function.</p> <p>Here comes the key point: what happens if we decrease the momentum cutoff by infinitesimal, $\Lambda \to b\Lambda$, $b = 1-\epsilon$. With new cutoff $b\Lambda$ the equation above still holds trivially, since different modes are entirely independent of each other, we get</p> \[\omega \tilde{g}(\omega) + 2\lambda \frac{d}{d\omega} \tilde{g}(\omega) = 0\, \forall \, |\omega| &lt; (1-\epsilon)\Lambda, \quad 0 \text{ otherwise}.\] <p>In other words, for the modes that survive the renormalization flow, we have the trivial RG equation</p> \[\frac{d}{d\Lambda}\tilde{g}(\omega) = 0, \quad |\omega| &lt; b\Lambda.\] <p>Switch back to the physics space from frequency space, the error estimate is most easily done by subtracting $g^{(\Lambda)}(t)$ from $g^{(\infty)}(t)$, where</p> \[g^{(\Lambda)}(t) \equiv \frac{1}{\sqrt{2\pi}}\int_ {-\Lambda}^{\Lambda} d\omega \tilde{g}(\omega) e^{-i\omega t}\] <p>where the Fourier transform of Gaussian function is</p> \[\mathcal{F}\left\lbrace e^{-\lambda x^2}\right\rbrace(\omega) = \frac{1}{\sqrt{2 \lambda }}e^{-\frac{\omega ^2}{4 \lambda }}\] <p>Define</p> \[\boxed{\Delta g(t) \equiv g^{(\infty)}(t) - g^{(\Lambda)}(t)},\] <p>we have</p> \[\begin{align} \notag \Delta g(t) &amp;= \left( \int_ {-\infty}^{-\Lambda} + \int_ {\Lambda}^\infty \right) \left( \frac{d\omega}{\sqrt{2\pi}} \frac{1}{\sqrt{2\lambda}} e^{-\omega^2/4\lambda} e^{-i\omega t} \right) \\\notag &amp;=\frac{1}{\sqrt{\pi\lambda}} \int_ {\Lambda}^\infty d\omega e^{-\omega^2/4\lambda} \cos{\omega t} \\\notag &amp;= \frac{i}{2} e^{-\lambda t^2} \left(-2 i +\text{erfi}\left(\frac{-2 \lambda t+i \Lambda }{2 \sqrt{\lambda }}\right)+\text{erfi}\left(\frac{2 \lambda t+i \Lambda }{2 \sqrt{\lambda }}\right)\right)\\ &amp;= e^{-\lambda t^2} - e^{-\lambda t^2}\text{Re}\,\text{erf}\left( \frac{\Lambda + 2i\lambda t}{2\sqrt{\lambda}} \right) \end{align}\] <p>where we have simplified notations, Re erf is the real part of the error function, and $\text{erfi}(z)$ is the so-called imaginary error function defined by $\text{erfi}(z) = -i \,\text{erf}(iz)$.</p> <p>Well, the last expression is not super helpful, we can do better by looking at $\Delta g^2$ as an estimate of the overall error. Square the second line in the previous equation we get</p> \[\begin{align*} \notag \Delta g(t)^2 &amp;=\frac{1}{4\pi\lambda} \int_ {\left\lvert \omega_ 1 \right\rvert &gt;\Lambda} d\omega_ 1 \int_ {\left\lvert \omega_ 2 \right\rvert &gt;\Lambda} d\omega_ 2 \, e^{-\omega_ 1^2/4\lambda - \omega_ 1^2/4\lambda} \cos(\omega_ 1 t) \cos(\omega_ 2 t) \\\notag &amp;=\frac{1}{4\pi\lambda} \int_ {\left\lvert \omega_ {1,2} \right\rvert &gt;\Lambda} d^2\omega e^{-\omega^2/4\lambda} \cos(\omega_ 1 t) \cos(\omega_ 2 t) \\ &amp; &lt; \frac{1}{4\pi\lambda} \int_ {\left\lvert \omega_ {1,2} \right\rvert &gt;\Lambda} d^2\omega e^{-\omega^2/4\lambda} \end{align*}\] <p>where $\omega^2 = \omega_ 1^2 + \omega_ 2^2$. The integral region is shown in the Figure below, the four corners where $\omega_ {1,2}&gt;\Lambda$ corresponds to the integral region. We can extend the region first to $(\mathbb{R}^2 - \text{square})$, then to $(\mathbb{R}^2 - \text{disk})$, each step will increase the error, thus we will get an upper bound. The reason for changing the square to circle is that so we can use the rotation symmetry. Continue with the integral,</p> \[\begin{align} \notag |\Delta g(t)|^2 &amp;&lt; \frac{1}{4\pi\lambda}\int_ {\omega^2&gt;\Lambda^2} d^2\omega e^{-\omega^2/4\lambda}\\\notag &amp;= \frac{1}{4\pi\lambda} \int_ {\omega^2&gt;\Lambda^2} d^2\omega e^{-\omega^2/4\lambda} \\\notag &amp;= \frac{1}{4\pi\lambda}2\pi \int_ {\Lambda}^\infty \int d\omega \, \omega e^{-\omega^2/4\lambda} \end{align}\] <p>where we have used $d^2 \omega = d\omega \omega d\theta$,</p> \[\begin{align} |\Delta g(t)|^2 &amp;&lt; \frac{1}{4\lambda} \int_ {\Lambda}^\infty \int d\omega^2 \, \omega e^{-\omega^2/4\lambda} \\ &amp;= e^{-\Lambda^2/4\lambda}. \end{align}\] <p>The conclusion is that at each $t$, the error $[g^{(\infty)}-g^{(\infty)}]^2 &lt; e^{-\Lambda^2/4\lambda}$.</p> <p><img src="/img/region.png" alt="region"/></p> <p>Out of a more differential point of view, let us consider the change of the function as we vary the cutoff $\Lambda$. We can calculate $\boxed{\frac{d}{d\Lambda} g^{(\Lambda)}(t)}$. Note that sometimes we will neglect the independent variable $t$ to save some space.</p> <p>If we increase $\Lambda$ infinitesimally, we have</p> \[\begin{align} \notag g^{(\Lambda+\epsilon)}(t) &amp; = g^{(\Lambda)} + \frac{1}{\sqrt{\pi\lambda}} \int_ \Lambda^{\Lambda+\epsilon} d\omega e^{-\omega^2/4\lambda} \cos(\omega t)\\ &amp;= g^{(\Lambda)} + \frac{\epsilon}{\sqrt{\pi\lambda}} e^{-\Lambda^2/4\lambda} \cos(\Lambda t), \end{align}\] <p>Thus</p> \[\boxed{ \frac{d}{d\Lambda} g^{(\Lambda)}(t) = \frac{e^{-\Lambda^2 / 4\lambda}}{\sqrt{\pi\lambda}} \cos(\Lambda t) = \text{Re}\,\frac{e^{-\Lambda^2 / 4\lambda}}{\sqrt{\pi\lambda}} e^{i\Lambda t} = \text{Re}\,\frac{e^{-\lambda t^2}}{\sqrt{\pi\lambda}} e^{-\frac{1}{4\lambda} (\Lambda-i2\lambda t)^2}. }\] <p>It can be solved to give,</p> \[g^{(\Lambda)}(t) = C_ 1 - e^{-\lambda t^2} \text{Re} \, \text{erf}\left(i\sqrt{\lambda} t - \frac{\Lambda}{2\sqrt{\lambda}}\right),\] <p>where $C_ 1$ is the constant of integration. To eliminate $C_ 1$, recall that $f^{\infty}(t) = e^{-\lambda t^2}$, thus we have $C_ 1 = 0$. By the end of the day we have</p> \[\boxed{ g^{(\Lambda)}(t) = - e^{-\lambda t^2} \text{Re} \, \text{erf}\left(i\sqrt{\lambda} t - \frac{\Lambda}{2\sqrt{\lambda}}\right) },\] <p>which agrees with the previous equations. The value of $\Delta g(t)$ can also be estimated with the help of Hans Heinrich Burmann’s theorem,</p> \[\text{erf}{x} = \frac{2}{\sqrt{\pi}}\text{sgn}\cdot \sqrt{1-e^{-x^2}} \left( \frac{\sqrt{\pi}}{2}+ \sum_ {k\in \mathbb{Z}^+} c_ k e^{-k x^2} \right), \, c_ 1 = \frac{31}{200},\, c_ 2 = -\frac{341}{8000},\, \cdots.\] <p>In summary, We have obtained the lattice spacing dependence, or equivalently the momentum cutoff dependence, both the error estimate and the RG flow are discussed.</p> <hr/> <h1 id="in-homogeneous-linear-differential-equation">In-homogeneous Linear Differential Equation</h1> <hr/> <h1 id="kink-equation">kink Equation</h1> <hr/> <h1 id="conventions">Conventions</h1> <p>The conventions are chose so be the same as that used by Mathematica.</p> <p>Given a function $f(t):\mathbb{R} \to \mathbb{R}$, the Fourier transform in the symmetrical form is</p> \[\begin{align} \tilde{f} (\omega) &amp;= \frac{1}{\sqrt{2\pi}}\int_ {-\infty}^\infty f(t) e^{i\omega t} dt, \\ f(t) &amp;= \frac{1}{\sqrt{2\pi}}\int_ {-\infty}^\infty \tilde{f}(\omega) e^{-i\omega t} dt \end{align}\] <p>where $\tilde{f}(\omega) \equiv \mathcal{F} \left\lbrace f \right\rbrace(\omega)$.</p> <p>Note the factor of $\frac{1}{\sqrt{2\pi}}$ and the signs in the exponent.</p> <hr/> <h1 id="appendix-a-error-function-in-the-complex-plane">Appendix A. Error function in the complex plane</h1> <p>The error function in the complex plane is defined to be</p> \[\operatorname* {erf}{z} = \frac{2}{\sqrt{\pi}} \int_ {\Gamma} d\zeta \, e^{-\zeta^2}\] <p>where $\Gamma$ is any path going from $0$ to $z$. The real and imaginary part of an error function can be estimated by Abramowitz and Stegun.</p> <p>The error function $\text{erf}(z),\, z \in \mathbb{C}$ satisfy symmetry relations</p> \[\begin{align} \text{erf}(z) &amp;= - \text{erf}(-z), \\ \text{erf}(\overline{z}) &amp;= \overline{\text{erf}(z)}. \end{align}\] <p>A possibly useful series expression for numerical calculation is</p> \[\begin{multline} \operatorname* {erf}(x+i y) = \operatorname* {erf}{x} + \frac{e^{-x^2}}{2 \pi x} [(1-\cos{2 x y})+i \sin{2 x y}]\\ + \frac{2}{\pi} e^{-x^2} \sum_ {k=1}^{\infty} \frac{e^{-k^2/4}}{k^2+4 x^2}[f_ k(x,y)+i g_ k(x,y)] + \epsilon(x,y) \end{multline}\] <p>where</p> \[\begin{align*} f_ k(x,y) &amp;= 2 x [1-\cos(2 x y) \cosh(k y)] + k\sin(2 x y) \sinh(k y), \\ g_ k(x,y) &amp;= 2 x \sin(2 x y) \cosh(k y) + k\cos(2 x y) \sinh(k y). \end{align*}\]]]></content><author><name>Baiyang Zhang</name></author><category term="Math"/><category term="Renormalization"/><summary type="html"><![CDATA[Abstract]]></summary></entry></feed>