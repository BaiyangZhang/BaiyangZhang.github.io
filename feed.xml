<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://baiyangzhang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://baiyangzhang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-26T06:50:20+00:00</updated><id>https://baiyangzhang.github.io/feed.xml</id><title type="html">Baiyang Zhang</title><subtitle>A place dedicated to sharing insights and reflections on mathematics, physics, and social sciences. </subtitle><entry><title type="html">Constructing a Finite Tension Domain Wall in 4D Part III</title><link href="https://baiyangzhang.github.io/blog/2024/Constructing-a-Finite-Tension-Domain-Wall-in-4D-Part-III/" rel="alternate" type="text/html" title="Constructing a Finite Tension Domain Wall in 4D Part III"/><published>2024-12-25T00:00:00+00:00</published><updated>2024-12-25T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Constructing-a-Finite-Tension-Domain-Wall-in-4D-Part-III</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Constructing-a-Finite-Tension-Domain-Wall-in-4D-Part-III/"><![CDATA[<h1 id="divergence-cancellation-in-kink-sector">Divergence cancellation in kink sector</h1> <p>We want to calculate the continuum contribution to the tension, which is Eq. (4.42) in the long version of the note:</p> \[\begin{align*} \rho_ {1}^{A} =&amp; -\int \frac{dk_ {1}}{2\pi} \frac{dp_ {1}}{2\pi} \, \frac{9\pi^{2}p_ {1}^{2}}{(m^{2}+k_ {1}^{2})(m^{2}+4k_ {1}^{2})}\text{csch}^{2}\left( \frac{\pi(p_ {1}-k_ {1})}{m} \right) \\ &amp;\times \int \frac{dk_ {2}dk_ {3}}{(2\pi)^{2}} \, \frac{(\omega _ {k} -\omega_ {p_ {1}k_ {2}k_ {3}})^{2}}{\omega_ {p_ {1}k_ {2}k_ {3}}}, \end{align*}\] <p>where in the last line $\omega _ {k} = \omega_ {k_ {1}k_ {2}k_ {3}}=\sqrt{k_ {1}^{2}+k_ {2}^{2}+k_ {3}^{2}+m^{2}}$, and $\omega_ {p_ {1}k_ {2}k_ {3}}=\sqrt{p_ {1}^{2}+k_ {2}^{2}+k_ {3}^{2}+m^{2}}$.</p> <p>We want to integrate the second line first, call it $I$:</p> \[I = \int \frac{dk_ {2}dk_ {3}}{(2\pi)^{2}} \, \frac{(\omega _ {k} -\omega_ {p_ {1}k_ {2}k_ {3}})^{2}}{\omega_ {p_ {1}k_ {2}k_ {3}}},\] <p>Using the formula</p> \[\int_ {-\infty}^{\infty} dk \, \frac{(\sqrt{a+k^{2}}-\sqrt{b+k^{2}})^{2}}{\sqrt{b+k^{2}}} = -a+b+a\ln\left( \frac{a}{b} \right),\] <p>we can integrate $k_ {3}$ out first,</p> \[I = \int_ {-\infty}^{\infty} \frac{dk_ {2}}{(2\pi)^{2}} \, [(-a+a\ln a)-(-b+a\ln b)].\] <p>where</p> \[a = k_ {1}^{2}+k_ {2}^{2}+m^{2}, \quad b = p_ {1}^{2}+k_ {2}^{2}+m^{2}.\] <p>Define</p> \[a := \alpha+k_ {2}^{2}, \quad b = \beta+k_ {2}^{2},\] <p>and in order to study the behavior at infinite momenta we insert a cutoff $\Lambda$, define:</p> \[\begin{align*} f_ {\Lambda}(k_ {1}):=&amp; \frac{1}{2\pi^{2}}\int_ {0}^{\infty} dk_ {2} \, [-\alpha-k_ {2}^{2}+(\alpha+k_ {2}^{2}\ln(\alpha+k_ {2}^{2}))], \\ f_ {\Lambda}(p_ {1}):=&amp; \frac{1}{2\pi^{2}}\int_ {0}^{\infty} dk_ {2} \, [-\beta-k_ {2}^{2}+(\alpha+k_ {2}^{2}\ln(\beta+k_ {2}^{2}))]. \end{align*}\] <p>so that</p> \[I = f_ {\Lambda}(k_ {1}) - f_ {\Lambda}(p_ {1}).\] <p>We can do the integrals analytically now. Note that when simplifying the integrated result, we need to expand $\ln(\alpha+\Lambda^{2})$ to $\ln \Lambda^{2}+\alpha / \Lambda^{2}-\alpha^{2} / 2\Lambda^{4}$. In the end we have</p> \[\begin{align*} 2\pi^{2}f_ {\Lambda}(k_ {1}) =&amp; \frac{2}{3} \pi \alpha ^{3/2}+\alpha \Lambda \log \left(\Lambda ^2\right)-2 \alpha \Lambda -\frac{5 \Lambda ^3}{9}+\frac{1}{3} \Lambda ^3 \log \left(\Lambda ^2\right), \\ 2\pi^{2}f_ {\Lambda}(p_ {1}) =&amp; \pi \alpha \sqrt{\beta }+\alpha \Lambda \log \left(\Lambda ^2\right)-2 \alpha \Lambda -\frac{5 \Lambda ^3}{9} -\frac{1}{3} \pi \beta ^{3/2}+\frac{1}{3} \Lambda ^3 \log \left(\Lambda ^2\right). \end{align*}\] <p>miraculously the divergence, namely the terms containing $\Lambda$, all cancels out. Thus</p> \[I = f_ {\Lambda}(k_ {1}) - f_ {\Lambda}(p_ {1}) = \frac{\alpha ^{3/2}}{3 \pi }-\frac{\alpha \sqrt{\beta }}{2 \pi }+\frac{\beta ^{3/2}}{6 \pi }.\] <p>The original integral now reads</p> \[\begin{align*} \rho_ {1}^{A} =&amp; -\int \frac{dk_ {1}}{2\pi} \frac{dp_ {1}}{2\pi} \, \frac{9\pi^{2}p_ {1}^{2}}{(m^{2}+k_ {1}^{2})(m^{2}+4k_ {1}^{2})}\text{csch}^{2}\left( \frac{\pi(p_ {1}-k_ {1})}{m} \right) \\ &amp;\times \left( \frac{(k_ {1}^{2}+m^{2})^{3/2}}{3\pi}- \frac{(k_ {1}^{2}+m^{2})\sqrt{p_ {1}^{2}+m^{2}}}{2\pi}+ \frac{(p_ {1}^{2}+m^{2})^{3/2}}{6\pi} \right) \\ =&amp; - \frac{3\pi}{2} \int \frac{dk_ {1}}{(2\pi)} \frac{dp_ {1}}{(2\pi)} \, \frac{p_ {1}^{2}\sqrt{k_ {1}^{2}+m^{2}}}{(m^{2}+4k_ {1}^{2})}\text{csch}^{2}\left( \frac{\pi}{m}(p_ {1}-k_ {1}) \right)\\ &amp;\times \left( 2-3\sqrt{\frac{p_ {1}^{2}+m^{2}}{k_ {1}+m^{2}}}+\left( \frac{p_ {1}^{2}+m^{2}}{k_ {1}^{2}+m^{2}} \right)^{3/2} \right) \end{align*}\] <h1 id="renormalization-with-quantum-action">Renormalization with Quantum Action</h1> <p>The three point function is given by</p> \[\begin{align*} \left\langle \Omega \right\rvert \mathcal{T} \phi(\vec{x}_ {1},E_ {1})\phi(\vec{x}_ {2},E_ {2})\phi(0) \left\lvert \Omega \right\rangle =&amp; -i \int \frac{dE_ {1}dE_ {2}}{(2\pi)^{2}} \frac{d^{d}p_ {1}d^{d}p_ {2}}{(2\pi)^{2d}}\, \frac{e^{ -i(E_ {1}t_ {1}+E_ {2}t_ {2}+\vec{x}_ {1}\cdot \vec{p}_ {1}+\vec{x}_ {2}\cdot \vec{p}_ {2}) }}{(E_ {1}^{2}-\omega_ {1}^{2}+i\epsilon)(E_ {2}^{2}-\omega_ {2}^{2}+i\epsilon)} \\ &amp;\times \frac{\Gamma(\vec{p}_ {1},E_ {1},\vec{p}_ {2},E_ {2})}{(E_ {1}+E_ {2})^{2}-\omega_ {1+2}^{2}+i\epsilon} \end{align*}\] <p>where $\omega_ {1}:=\omega_ {\vec{p}_ {1}}$, $\mathcal{T}$ stands for the time-ordered product. Does integrating over $E_ {1}$ gives us the time ordered product, similar to how we obtained the Feynman propagator? Or is it just the basis of Fourier transform? To see how it works, let’s assume that $t_ {1}&gt;t_ {2}&gt;0$. We can perform the integral over $E_ {1}$ using the contour method, and get</p> \[\left\langle \Omega \right\rvert \mathcal{T} \phi(\vec{x}_ {1},E_ {1})\phi(\vec{x}_ {2},E_ {2})\phi(0) \left\lvert \Omega \right\rangle = \text{I}+\text{II},\] <p>where</p> \[\begin{align*} \text{I} =&amp; - \int \frac{d^{d}p_ {1} d^{d}p_ {2}}{(2\pi)^{2d}} \, e^{ -i(\vec{p}_ {1}\cdot \vec{x}_ {1}+\vec{p}_ {2}\cdot \vec{x}_ {2}+\omega_ {1}t_ {1}) } \int \frac{dE_ {2}}{(2\pi)} \frac{e^{ -iE_ {2}t_ {2} }}{2\omega_ {1}} \\ &amp;\times \frac{\Gamma(E_ {1}=\omega_ {1})}{(E_ {2}+\omega_ {2}-i\epsilon)(E_ {2}-\omega_ {2}+i\epsilon)(E_ {2}+\omega_ {1}+\omega_ {1+2}-i\epsilon)(E_ {2}+\omega_ {1}-\omega_ {1+2})+i\epsilon}, \\ \text{II} =&amp; - \int \frac{d^{d}p_ {1} d^{d}p_ {2}}{(2\pi)^{2d}} \, e^{ -i(\vec{p}_ {1}\cdot \vec{x}_ {1}+\vec{p}_ {2}\cdot \vec{x}_ {2}+\omega_ {1+2}t_ {1}) } \int \frac{dE_ {2}}{2\pi} \frac{e^{ -i E_ {2}(t_ {2}-t_ {1}) }}{(E_ {2}+\omega_ {2})(E_ {2}-\omega_ {2})} \\ &amp;\times \frac{\Gamma(E_ {1}=\omega_ {1+2}-E_ {2})}{2(\omega_ {1+2}+E_ {2}-i\epsilon)(E_ {2}-\omega_ {1+2}+\omega_ {1}-i\epsilon)(E_ {2}-\omega_ {1+2}-\omega_ {1}+i\epsilon)}, \end{align*}\] <p>whenever needed, we can replace $\omega_ {1,2}$ with $\omega_ {1,2}-i\epsilon$. This is consistent with Mark Sredinickie’s convention that $m^{2}$ are to be replaced by $m^{2}-i\epsilon$ when necessary.</p> <p>Integrate $\text{I}$ with respect to $E_ {2}$ follow the lower semi-circle, we have</p> \[\begin{align*} \text{I} =&amp; i \int \frac{d^{d}p_ {1}d^{d}p_ {2}}{(2\pi)^{2d}} \, \frac{e^{ -i(\vec{p}_ {1}\cdot \vec{x}_ {1}+\vec{p}_ {2}\cdot \vec{x}_ {2}+\omega_ {1}t_ {1}) }}{2\omega_ {1}} \\ &amp; \times \left\lbrace \frac{e^{ -it_ {2}\omega_ {2} }\Gamma(E_ {1}=\omega_ {1},E_ {2}=\omega_ {2})}{2\omega_ {2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(\omega_ {1}+\omega_ {2}-\omega_ {1+2})} \right. \\ &amp;+\left. \frac{e^{ -it_ {2}(\omega_ {1+2}-\omega_ {1})}\Gamma(E_ {1}=\omega_ {1},E_ {2}=\omega_ {1+2}-\omega_ {1})}{2\omega_ {1+2}(\omega_ {1+2}-\omega_ {1}+\omega_ {2})(\omega_ {1+2}-\omega_ {1}-\omega_ {2})} \right\rbrace \end{align*}\] <p>and</p> \[\begin{align*} \text{II} =&amp; -i \int \frac{d^{d}p_ {1}d^{d}p_ {2}}{(2\pi)^{2d}} \, e^{ -i(\vec{p}_ {1}\cdot \vec{x}1+\vec{p}_ {2}\cdot \vec{x}_ {2}+\omega_ {1+2}t_ {1}) } \\ &amp; \times \left\lbrace \frac{e^{ -i(t_ {1}-t_ {2})\omega_ {2} } \Gamma(E_ {1}=\omega_ {1+2}-\omega_ {2},E_ {2}=-\omega_ {2})}{4\omega_ {2}(\omega_ {1+2}-\omega_ {2})(-\omega_ {1+2}-\omega_ {2}+\omega_ {1})(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \right. \\ &amp;+ \frac{e^{ -i(t_ {1}-t_ {2})\omega_ {1+2} } \Gamma(E_ {1}=2\omega_ {1+2},E_ {2}=-\omega_ {1+2})}{2(\omega_ {1+2}-\omega_ {2})(\omega_ {1+2}+\omega_ {2})(2\omega_ {1+2}-\omega_ {1})(2\omega_ {1+2}+\omega_ {1})}\\ &amp;- \left. \frac{e^{ -i(t_ {1}-t_ {2})(\omega_ {1}-\omega_ {1+2}) }\Gamma(E_ {1}=\omega_ {1},E_ {2}=\omega_ {1+2}-\omega_ {1})}{4\omega_ {1}(\omega_ {1+2}-\omega_ {1}+\omega_ {2})(\omega_ {1+2}-\omega_ {1}-\omega_ {2})(2\omega_ {1+2}-\omega_ {1})} \right\rbrace . \end{align*}\]]]></content><author><name>Baiyang Zhang</name></author><category term="kink"/><summary type="html"><![CDATA[Divergence cancellation in kink sector]]></summary></entry><entry><title type="html">Note on Coleman-Weinberg Potential</title><link href="https://baiyangzhang.github.io/blog/2024/Coleman-Weinberg-Potential/" rel="alternate" type="text/html" title="Note on Coleman-Weinberg Potential"/><published>2024-12-23T00:00:00+00:00</published><updated>2024-12-23T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Coleman-Weinberg-Potential</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Coleman-Weinberg-Potential/"><![CDATA[<ul> <li><a href="#1-quantum-effective-action">1. Quantum Effective Action</a> <ul> <li><a href="#11-quantum-action">1.1. Quantum Action</a></li> <li><a href="#12-effective-potential">1.2. Effective Potential</a></li> </ul> </li> <li><a href="#2-coleman-weinberg-potential">2. Coleman-Weinberg Potential</a> <ul> <li><a href="#21-background-fields">2.1. Background Fields</a></li> <li><a href="#22-coleman-weinberg-potential">2.2. Coleman-Weinberg Potential</a></li> </ul> </li> </ul> <h1 id="1-quantum-effective-action">1. Quantum Effective Action</h1> <p>The goal of this chapter is to introduce the quantum action and quantum potential. There are (at least) two kinds of quantum actions, one is the <code class="language-plaintext highlighter-rouge">Wilsonian quantum action</code>, where the higher momentum modes are integrated out to focus on the low-energy, long-distance behavior of a system. It embodies the effective interactions of a quantum system <em>at a certain scale</em>, accounting for the influence of fluctuations at smaller scales. This potential is central to understanding how physical phenomena emerge at different length scales from the underlying quantum fields, particularly in the study of critical phenomena and phase transitions. Take a scalar field theory $\phi$ for example. The “integrating out” procedure is done using the path integral approach, it involves <em>separating the field into high and low energy parts</em>. To be more specific, the scalar field is split into $\phi = \phi_ {\text{low}} + \phi_ {\text{high}}$, where $\phi_ {\text{low}}$ contains modes below a certain energy scale $\Lambda$ and $\phi_ {\text{high}}$ contains modes above $\Lambda$. $\phi_ {\text{high}}$ can be thought of as a thin shell in the momentum space. The path integral over the full field is then re-written as a path integral over these two components of fields. The crucial step is to integrate out the high-energy modes $\phi_ {\text{high}}$. This can be done perturbatively by treating $\phi_ {\text{high}}$ as a different field from $\phi_ {\text{low}}$, using Feynman diagram techniques. The resulting effective action should only depends on the low-energy modes. The Wilsonian effective action captures the dynamics of the field <em>at energies below</em> $\Lambda$. It will typically have a form different from the original action, often with new interactions generated as a result of integrating out the high-energy modes. New scale could even emerge, as in the case of dimensional transmutation, which we will not talk too much about in this note.</p> <p>The other kind of effective action is the so-called <code class="language-plaintext highlighter-rouge">quantum effective actions</code> which is the Legendre transformation of the generating functional of connected diagrams. It is quite a mouthful and we will talk more about it in the next section. When the original action is replaced by the quantum effective action, the tree-level diagrams generated by it will contain <em>all the quantum corrections</em> of the full theory (i.e. the original theory). The quantum effective action has a useful property that it can give us the vacuum expectation value (VEV) of the field operator with quantum correction. As we will see, the <code class="language-plaintext highlighter-rouge">Coleman-Weinberg potential</code> is a special case of the effective action.</p> <p>As M. D. Schwartz put it:</p> <blockquote> <p>Generally speaking, the term effective action, denoted by $\Gamma$, generally refers to a functional of fields (like any action) defined to give the same Green’s functions and S-matrix elements as a given action $S$, which is often called the action for the full theory. We write $\Gamma=\int d^4x \, \mathcal{L}_ {\text{eff}}(x)$, where $\mathcal{L}_ {\text{eff}}$ is called the effective Lagrangian.</p> </blockquote> <hr/> <p>There are in general three ways to calculate the effective action, as listed in the following.</p> <ul> <li><strong>Matching</strong>. We require</li> </ul> \[\int \mathcal{D}\{\text{original dof}\} e^{iS} = \int \mathcal{D}\{\text{effective dof}\} e^{i\Gamma}\] <p>where dof stands for the <code class="language-plaintext highlighter-rouge">degrees of freedom</code>, the LHS is the original theory while the RHS is the effective theory, with different degree of freedom. For example, QCD (at high energy) has quark and gluon as the degrees of freedom, when the energy is lowered the degrees of freedom becomes color singlet particles. $\Gamma$ is the quantum action.</p> <ul> <li> <p><strong>Legendre transformation</strong>, as we will show shortly.</p> </li> <li> <p><strong>Background field method</strong>, that is to separate the field into a static non-propagating background field $\phi_ b$ and a dynamic propagating field $\tilde{\phi}$, the dynamic fields are the fluctuations around the background field. Integrating out the fluctuations (usually done perturbatively) leaves us the effective potential $\Gamma[\phi_ b]$,</p> </li> </ul> \[\int \mathcal{D} \phi e^{i S[\phi_ b + \tilde{\phi}]} = e^{i\Gamma[\phi_ b]}.\] <p>The background field method is also closely related to how we calculate quantum corrections to classical solitonic solutions, such as the quantum correction to kink mass.</p> <p>The first try, before all the three mentioned above, is usually an educated guess. Given that <strong>the effective action must possess the same symmetries as the original action</strong>, one can propose various <em>local terms</em> that fulfill this requirement. The coefficients of these terms are then adjusted based on experimental data. This method relies on symmetry considerations to guide the formulation of acceptable terms in the effective action.</p> <p>For the rest of the note, we will confine our discussion to $\phi^4$ model with real scalar fields.</p> <h2 id="11-quantum-action">1.1. Quantum Action</h2> <p>To keep the notation simple, consider a single real scalar field $\phi$, with possibly mass term and self interaction. The partition function (with source) reads</p> \[Z[J] = \int \mathcal{D} \phi e^{iS[\phi] + i\int \phi J},\] <p>where $S$ is the action, $J$ is the source, $\int \phi J$ is short for $\int_ {M} \phi(x) J(x)$. $\phi$ is integrated out hence $Z$ is a functional of $J$ only.</p> <p>From it we define the generating function $W[J]$ by</p> \[Z[J] = e^{iW[J]} \implies W[J] = -i \ln Z[J],\] <p>$W[J]$ is the summation of <em>connected diagrams</em> with source, connected roughly because if you consider all the diagrams, the disconnected but replica-forming (namely the disconnected diagrams formed by putting two or more replicas of the same diagrams together) diagrams can be arranged into forms of $\bullet^{n}/n!$, where $\bullet$ is (the expression of) some connected diagram. Then we can organized them into an exponential function $e^{ \bullet }$, now $\bullet$ contains information only about connected diagrams. For more details, please refer to Mark Srednicki’s text book on quantum field theory. To repeat, $W[J]$ generates <em>connected diagrams</em> only.</p> <hr/> <p>The expectation value of $\phi$ in the presence of a source $J$ is given by</p> \[\left\langle {\phi} \right\rangle_ J=\frac{1}{Z} \int \mathcal{D} \phi e^{iS[\phi] + i\int \phi J} \phi = \frac{\delta W[J]}{\delta J(x)}\equiv \varphi_ {J},\] <p>Note the difference between $\phi$ and the so-called <code class="language-plaintext highlighter-rouge">varphi</code> $\varphi$, the former is an operator while the latter is a classical field. The subscript $J$ in $\varphi_ {J}$ is to emphasize that the vev of $\phi$ depends on the source $J$. In comparison to classical mechanics of point particles, $W$ is like Lagrangian $L$, $J$ is like $\dot{q}$, and $\delta W / \delta J$ is like $\partial L / \partial \dot{q}$, which introduces a new variable.</p> <p>Since the generating functional $W[J]$ is a functional of $J$, we can perform Legendre transform to define a new functional in terms of $\delta W / \delta J =: \varphi_ {J}$. The result is the quantum action:</p> \[\boxed{ \Gamma[\varphi_ {J}] = W[J] - \int J\varphi_ {J} , }\] <p>which is indeed a functional of $\varphi_ {J}$ and not $J$, since it is independent of variation $\delta J$ of $J$, as the readers can verify. Again we have omitted the measure under the integral sign. Some direct calculation shows that</p> \[\frac{\delta\Gamma[\varphi_ {J}]}{\delta\varphi_ {J}(x)} = -J(x).\] <p>It is not supposed to be obvious, but the effective action $\Gamma$ is the generating functional for 1-particle irreducible (1PI) diagrams! The significance of 1PI diagrams is best explained by Coleman in his lecture note on QFT, which I quote:</p> <blockquote> <p>If we treat the 1PI graphs as giving us effective interaction vertices, then to find the full Green’s functions we only have to sum up tree graphs, never any loops, because all the loops have been stuffed inside the definition of the propagators and the 1PI graphs. This marvelous property of the 1PI graphs is important. Taking the 1PI graph generating functional for a quantum action enables us to turn the combinatorics of building up full Green’s functions from 1PI Green’s functions into an analytic statement, and we end up with the correct expressions for the full Green’s functions. We’re turning a topological statement of one-particle irreducibility into an analytic statement that we will find easy to handle.</p> </blockquote> <p>To see that $\Gamma[\varphi]$ indeed generates the 1PI diagrams, it is easiest (for myself) to inverse the chain of reasoning, first we define an effective action such that its tree level diagrams reproduces the quantum result (which is easier than it looks), then show that such constructed action satisfies the same equation as $\Gamma[\phi_ {J}]$, so they are the same (up to some insignificant constant, such as the normalization constant). We will proceed in this direction.</p> <p>For now, forget about $\varphi_ {J}$. Let’s starting from defining the an effective action $\Gamma[\phi_ {c}]$, which is a functional of of some classical field $\phi_ {c}$. The role of $\phi_ {c}$ in $\Gamma[\phi_ {c}]$ is the same as the role of $\phi$ in the classical action $S[\phi]$, where we usually don’t bother to emphasize that $\phi$ is classical rather than a quantum field, but here we do. $\Gamma[\phi_ {c}]$ is defined such that, if we treat $\Gamma[\phi_ {c}]$ as the classical action $S[\phi]$, substitute $\Gamma$ with $S$ in the path integral, and calculate $Z[J]$ (or equivalently $W[J]$), using <strong>only the tree diagrams</strong>, then we get exact $Z[J]$ with every bit of the quantum correction! At first glance, this might seem almost too convenient, making our calculations significantly simpler, too good to be true. However, there’s no shortcut to the truth; ultimately, we still need to buckle down and work through the loop diagrams. Essentially, the effective action is a clever reorganization of the contributions from these loop diagrams. Even though the effective action doesn’t simplify the calculations per se, it is still quite valuable to us since it provides a new perspective, serving as a powerful tool in quantum field theory, enabling the study of quantum phenomena with a formalism that extends the classical action to include quantum effects.</p> <p>So how should $\Gamma[\phi_ {c}]$ be constructed? For any function $\phi_ {c}$, the effective action $\Gamma[\phi_ {c}]$ has a functional Taylor expansion:</p> \[i\Gamma[\phi_ {c}] = \sum_ {n} \frac{1}{n!} \int d^{d}x_ {1}\cdots d^{d}x_ {n} \, \Gamma^{(n)} (x_ {1},\cdots ,x_ {n}) \phi_ {c}(x_ {1})\cdots \phi_ {c}(x_ {n}).\] <p>For example, if $\Gamma[\phi_ {c}] = \int \, \phi_ {c}^{2}$, then the only non-zero component in the functional Taylor expansion is $\Gamma^{(2)}(x_ {1},x_ {2}) = 2\delta^{d}(x_ {1}-x_ {2})$.</p> <p>When talking about Feynman diagrams, it is more convenient to go to momentum representation, hence we define a modified version of the Fourier transform of $\Gamma^{(n)}$, such that</p> \[\Gamma^{(n)}(x_ {1},\cdots x_ {n}) := \int \frac{dp_ {1}}{(2\pi)^{d}}\cdots \frac{dp_ {n}}{(2\pi)^{d}}\, \tilde{\Gamma}^{(n)} (p_ {1},\cdots ,p_ {n} ) (2\pi)^{d}\delta^{d}(p_ {1}+\cdots +p_ {n} ) .\] <p>This definition contains extra $\delta$-function for future convenience.</p> <p>Recall that the filed $\phi_ {c}(x_ {i})$ themselves in the action eventually becomes <strong>amputated</strong> external legs, amputated in the sense that no propagator is associated to it. All the information is contained in $\tilde{\Gamma}$! We can brutally stuff all the 1PI diagrams, including loop corrections from $S[\phi]$, into $\tilde{\Gamma}$ so that we only need to take into consider the tree diagrams of $\Gamma$. For example, we can draw all the 1PI diagrams with three external legs, calculate them, and define it to be $\tilde{\Gamma}^{(3)}$. If we regard $\Gamma[\phi_ {c}]$ as a machine that takes a function $\phi_ {c}$ as input and spits out a number, then $\tilde{\Gamma}$’s are like its components.</p> <p>To summarize, for $n\geq 2$, the $\tilde{\Gamma}^{(n)}$ are <strong>defined</strong> by the sum of 1PI diagrams with $n$ external legs. As usual the external legs are amputated. The external momenta need not be conserved, that point is taken care of by the $\delta$-function in the definition of $\tilde{\Gamma}$. For $n=2$ the case is slightly more complicated, we need to include a propagator into the definition, but the philosophy is the same.</p> <p>Next we combine the tree-level exactness of $\Gamma[\phi_ {c}]$ with another concept: loop expansion. Loop expansion is equivalent to both semi-classical expansion (expansion in $\hbar$) and perturbative expansion (expansion in coupling $g$), should the right $\hbar$-dependence be made. Sidney Coleman thinks that $\hbar$ expansion is rubbish for two reasons (that I know of), 1) $\hbar$ is dimensional, with dimension of energy multiplies time, therefore is not a good expanding parameter and 2) if we make $\hbar$ dimensionless like we did with natural units, we could always change the units such that $\hbar=1$. In loop expansion, the tree level diagrams dominates the partition function $Z[J]$ when $\hbar$ is small, and becomes exact at $\hbar\to 0$. I am tempted to write</p> \[\text{tree diagrams} = \lim_ { \hbar \to 0 } \int \mathcal{D}\phi_ {c} \, \exp \left\lbrace \frac{i}{\hbar}\Gamma[\phi_ {c}] + \int J\phi_ {c} \right\rbrace\] <p>And this turns out to be correct. I used to think of $\phi_ {c}$ as some pre-determined classical function, which has caused me a lot of confusion. From now on let’s get rid of the subscript $c$ in $\phi_ {c}$, since fields appear under the path integral are always classical field. We will put the subscript back when possible confusion could rise.</p> <p>Thanks to the $\hbar\to 0$ limit, the path-integral can be worked out using the method of stationary phase, up to some normalization constant we have</p> \[\lim_ { \hbar \to 0 } \int \mathcal{D}\phi \, \exp \left\lbrace \frac{i}{\hbar}\Gamma[\phi] +\frac{i}{\hbar} \int J\phi \right\rbrace = \exp \left\lbrace \frac{i}{\hbar}\Gamma[\overline{\phi}]+ \frac{i}{\hbar} \int \, J\overline{\phi} \right\rbrace ,\] <p>where $\overline{\phi}$ is the solution that <strong>extremizes</strong> the exponent on the LHS,</p> \[\frac{\delta \Gamma[\phi]}{\delta \phi}{\Large\mid}_ {\phi=\overline{\phi}} \equiv\frac{\delta \Gamma[\overline{\phi}]}{\delta \overline{\phi}} = -J(x).\] <p>This is exactly the same functional equation we got for $\varphi_ {J}$ before! They might differ by a constant, but that can be absorbed into the normalization factors and cancels out eventually. Now we can comfortably write $\overline{\phi} =\phi_ {J}$ in the quantum action. This equation connects the quantum action we obtained before via a Legendre transform from $iW[J]$ with the generating functional for 1PI diagrams, identifying these two seemingly different quantities. To show the connection ever more clearly, recall that the partition function in terms of $\Gamma$ is</p> \[Z[J] = \exp \left\lbrace \frac{i}{\hbar} \left( \Gamma [\varphi_ {J}]+\int \, J\varphi_ {J} \right) \right\rbrace =\exp \left\lbrace \frac{i}{\hbar}W[J] \right\rbrace\] <p>we have</p> \[W[J] = \Gamma[\phi_ {J}] + \int \, J\phi_ {J}, \quad J \text{ given a priori.}\] <p>Note that we could equally write $W$ as $W+2\pi \mathbb{N}$ but the additive constant can be absorbed into the partition functions as well. This is the Legendre transform we wrote down before!</p> <p>For the sake of completeness we put the pair of Legendre transforms below,</p> \[\begin{align*} W[J] &amp;= \Gamma[\varphi_ {J}] +\int \, J\varphi_ {J} ,\quad -J = \frac{\delta \Gamma[\varphi_ {J}]}{\delta \varphi_ {J}}, \\ \Gamma[\varphi] &amp;= W[J_ {\varphi}] - \int \, J_ {\varphi} \varphi, \quad \varphi=\frac{\delta W[J]}{\delta J}, \end{align*}\] <p>where $\varphi_ {J}$ means that $\varphi$ is determined by $J$, namely $\varphi$ is a (non-local) function of $J$, while $J_ {\varphi}$ means the opposite. Also keep in mind that $\varphi_ {J}$ is the vev of quantum operator $\phi$ in the presence of $J$.</p> <p><strong>Summary.</strong></p> <ul> <li>$\Gamma[\varphi]$ generates 1PI diagrams;</li> <li>$W[J]$ generates connected diagrams;</li> <li>$Z[J]$ generates all kinds of diagrams.</li> </ul> <p><strong>Remark.</strong> The generating functionals such as $W[J]$ and $\Gamma[\varphi]$ are classical functional, dealing with c-numbered functions, no operators and commutation relations involved. In fact, the language of path integral has a close connection with classical, statistical field theory, and many concepts exists in both disciplines, for example, people dealing with statistical field theory also talk about renormalization flow (Wilsonian), and QFT-ists also talk about critical exponents. A great textbook on statistical field theory is that by <a href="https://guava.physics.ucsd.edu/~nigel/"><code class="language-plaintext highlighter-rouge">Nigel Goldenfeld</code></a>.</p> <hr/> <p>Recently I found another approach to effective action, which I will copy here. This new approach gives a different perspective to stuff we talked about before, and it made it manifest that the external legs of $\Gamma[\phi]$ should be amputated, thus I consider it worthy to write it down.</p> <p>Let $G^{(n)}(x_ {1},\cdots,x_ {n})$ be the most general kind of $n$-point function, including disconnected ones. It is generated by the partition function $Z[J]$, which can be written as</p> \[Z[J] = \sum_ {n=0}^{\infty} \frac{i^{n}}{n!} \int d^{d}x_ {1} \cdots d^{d}x_ {n} \, G^{(n)}(x_ {1},\cdots,x_ {n}) J(x_ {1})\cdots J(x_ {n}).\] <p>As you can see, acting $n$-times the functional derivative $\delta / i\delta J$ gets us the $n$-point function.</p> <p>Similarly, the generating functional of connected diagrams $iW[J]$ adopts the functional Taylor expansion</p> \[iW[J] = \sum_ {n=0}^{\infty} \frac{i^{n}}{n!} \int d^{d}x_ {1} \cdots d^{d}x_ {n} \, G_ {c}^{(n)}(x_ {1},\cdots,x_ {n}) J(x_ {1})\cdots J(x_ {n}).\] <p>where $G_ {c}^{(n)}(x_ {1},\cdots)$ is the connected n-point function. Likewise for $\Gamma$ but we have already wrote it down. In the next we will neglect $J$ in $\varphi_ {J}$, it is understood that $\varphi$ is the canonical transformed variable of $J$ and vise versa.</p> <p>Using the functional relation</p> \[\frac{\delta W}{\delta J} = \varphi\] <p>we can do something interesting with the connected 2-point function. Neglect the normalization factor for now, we have</p> \[\begin{align*} iD(x-y) &amp;= \int D\phi \, e^{ i\left( S+\int J\phi \right) } \phi(x)\phi(y)\\ &amp;= \frac{\delta^{2}W}{\delta J(x)\delta J(y)} = \frac{\delta}{\delta J(y)} \frac{\delta W}{\delta J(x)}\\ &amp;= \frac{\delta \varphi(x)}{\delta J(y)} , \end{align*}\] <p>amazingly the functional derivative between $\varphi$ and $J$ is nothing but the quantum, full propagator! On the other hand,</p> \[\frac{\delta J(x)}{\delta J(y)} = \delta^{d}(x-y) = - \frac{\delta}{\delta J(y)} \frac{\delta \Gamma[\varphi]}{\delta \varphi(x)},\] <p>write</p> \[\boxed{ \frac{\delta}{\delta J(y)} = \int d^{d}z \, \frac{\delta \varphi(z)}{\delta J(y)} \frac{\delta}{\delta\varphi(z)} = \int d^{d}z \, iD(z-y) \frac{\delta}{\delta \varphi(z)} }\] <p>where the first equal sign is nothing but the chain rule of functionals, we have</p> \[\begin{align*} \delta^{d}(x-y) &amp;= - \int d^{d}z \, \frac{\delta \varphi(z)}{\delta J(y)} \frac{\delta^{2} \Gamma[\varphi]}{\delta\varphi(z)\delta\varphi(x)} \\ &amp;=-i \int d^{d}z \, D(y-z) \frac{\delta^{2} \Gamma[\varphi]}{\delta\varphi(z)\delta\varphi(x)} . \end{align*}\] <p>Recall that $\delta$-function is the equivalence of identity with functionals, we see that $\delta^{2} \Gamma[\varphi] / \delta\varphi(z)\delta\varphi(x)$ is the inverse of 2-point functions! To be specific</p> \[\boxed{ \left( \frac{\delta^{2}\Gamma[\varphi]}{\delta \varphi(x)\delta \varphi(y)} \right)^{-1} = -i D (y-z)= -\frac{\delta^{2} W[J]}{\delta J(y)\delta J(z)}. }\] <p>Is helps to think of $\delta^{2} / \delta_ {x} \delta_ {y}$ as a matrix $M_ {xy}$, then this inverse relation is for matrices. This is both intuitive and not… intuitive because, recall that with regular Lagrangian $\mathcal{L}$, $\partial^{2} \mathcal{L} / (\partial \phi)^{2}$ is roughly speaking the inverse of the propagator, here the effective action kind of takes the position of $\mathcal{L}$; Counter intuitive since, well, it took me a lot effort to find it intuitive.</p> <p>Now let’s carry on with other n-point functions where $n&gt;2$. But before that we need to solve a math problem: how to take the functional derivative of an inverse matrix.</p> <p>Let $M$ be a matrix function and $M^{-1}$ its inverse. Start with the identity, $I = MM^{-1}$, differentiating this identity yields $0 = dM^{-1}M + M^{-1}dM$, leading to the expression for the differential of the inverse $dM^{-1} = -M^{-1}(dM)M^{-1}$. From this, it follows that the derivative of $M^{-1}$ with respect to some variable $a$ is</p> \[\frac{\partial M^{-1}}{\partial a} = -M^{-1} \frac{\partial M}{\partial a} M^{-1}.\] <p>Now lets consider the 3-point function</p> \[\begin{align*} G_ {c}^{(3)}(x,y,z) &amp;= \frac{\delta^{3}W}{\delta J(x)\delta J(y)\delta J(z)} \\ &amp;= i \int d^{d}w \, D(z,w) \frac{\delta}{\delta\varphi(w)} \frac{\delta^{2}W[J]}{\delta J(y) \delta J(x)} \\ &amp;= - i \int d^{d}w \, D(z,w) \frac{\delta}{\delta\varphi(w)}\left( \frac{\delta^{2}\Gamma[\varphi]}{\delta \varphi(x)\delta \varphi(y)} \right)^{-1}\\ &amp;= -i \int d^{d}w d^{d}w' d^{d}w'' \, D(z-w) \left( \frac{\delta^{2}\Gamma}{\delta\varphi(x)\delta\varphi(w')} \right)^{-1} \\ &amp;\;\;\;\;\;\times \frac{\delta^{3}\Gamma}{\delta\varphi(w)\delta\varphi(w')\delta\varphi(w'')} \left( \frac{\delta \Gamma}{\delta\varphi(w'')\delta\varphi(y)} \right)^{-1} \\ &amp;= - i \int d^{d}w d^{d}w' d^{d}w'' \, D(z-w) D(x-w') D(y-w'')\\ &amp;\;\;\;\;\;\times \frac{\delta^{3}\Gamma[\varphi]}{\delta\varphi(w)\delta\varphi(w')\delta\varphi(w'')} \\ &amp;= -i \int d^{d}w d^{d}w' d^{d}w'' \, D(z-w) D(x-w') D(y-w'') \Gamma^{(3)}(w,w',w''). \end{align*}\] <p>Now, $G_ {c}^{3}(x,y,z)$ is the connected 3-point function defined at $x,y$ and $z$, with its external legs <strong>not amputated</strong>! On the LHS, since all the three external legs are accounted for by the three propagators $D(z-2)$ etc., $\Gamma^{(3)}(w,w’,w’’)$ has its external legs <strong>amputated</strong>! As we dig deeper, you’ll find that this is a general conclusion: the external legs of $\Gamma[\varphi]$ are amputated.</p> <p>We have been sloppy with factor of $i$’s. Taking care of it, the n-point correlation function reads</p> \[\left( i\frac{\delta}{\delta J} \right)^{n} (iW[J]) = \left\langle T \,\phi_ {1}\cdots \phi _ {n} \right\rangle _ {J} =: G^{(n)}_ {\text{c}}(x_ {1},\cdots ,x_ {n}),\] <p>where $c$ is for connected. Since $\Gamma[\varphi]$ generates 1PI diagrams,</p> \[G^{(3)}_ {\text{1PI,am}}(x,y,z) = \frac{\delta^{3} i\Gamma[\varphi]}{\delta\varphi(x) \delta\varphi(y) \delta\varphi(z)}\] <p>where $\text{am}$ for amputated. We have found the relation between connected, not-amputated 3-point functions between 3-point 1PI connections:</p> \[G^{(n)}_ {c}(x,y,z) = \int d^{d}w \, d^{d}w' \, d^{d}w'' \, D(x-w)D(y-w')D(z-w'') G^{(3)}_ {\text{1PI,am}}(w,w',w'').\] <p>Note that $G^{(n)}_ {\text{1PI,am}}$ is nothing but the same $\Gamma^{(n)}$ in the functional Taylor expansion of $\Gamma$ (by construction). The generalization to $n&gt;3$ is straightforward.</p> <p><strong>A Tree-level Example</strong></p> <p>In the classical limit, that is in the limit $\hbar \to 0$, the partition function</p> \[Z = \int \mathcal{D}\phi e^{ \frac{i}{\hbar} \left( S + \int \phi J \right)}\] <p>receives dominant contribution from the stationary configuration, given by</p> \[\frac{\delta}{\delta\phi}\left( S + \int \phi J \right) = 0 \implies \frac{\delta S}{\delta\phi} = - J\] <p>which has a solution $\varphi_ {J}$. This is exactly the euqation satisfied by the quantum action $\Gamma[\phi]$! Anyway, we can carry on to talk about the partition function which is now</p> \[Z = e^{iS[\varphi_ {J}]+i\int \varphi_ {J} J}\] <p>up to a normalization factor. We have</p> \[W[J] = -i \ln Z = S[\varphi_ {J}] + \int \varphi_ {J} J,\] <p>thus</p> \[\Gamma[\varphi] = W - \int \varphi_ {J} J = S[\varphi].\] <p>As expected, at the tree-level, the quantum effective action and the original action are the same.</p> <h2 id="12-effective-potential">1.2. Effective Potential</h2> <p>In the classical dynamics, the vacuum (lowest energy state) configuration of the system is given by the minimum of the potential, which fixes the value of the field in spacetime (usually a constant in spacetime). In the quantum theory, everything receives quantum correction, including the vacuum expectation value (VEV) $\left\langle \phi \right\rangle$ of the field operator $\phi$. In a QFT, the potential term in the Lagrangian or Hamiltonian has minima given by the classical vacuum field configuration, however that’s not the full story, since the field always fluctuates around the vacuum, giving rise to a correction to $\left\langle \phi \right\rangle$. That’s when the effective potential comes to rescue.</p> <p>The effective potential in QFT is a crucial concept, especially when studying systems with spontaneous symmetry breaking, phase transitions, and nonperturbative dynamics. It represents a modification of the classical potential to include quantum corrections, providing a more accurate description of the dynamics of quantum fields. Its application include:</p> <ol> <li> <p><strong>Spontaneous Symmetry Breaking</strong>: The effective potential is instrumental in understanding spontaneous symmetry breaking, a phenomenon where the ground state (vacuum) of a system does not inherit the symmetry of the action. In the context of the Higgs mechanism in the Standard Model of particle physics, the effective potential reveals how the Higgs field acquires a nonzero vacuum expectation value, leading to the generation of masses for the $W$ and $Z$ bosons.</p> </li> <li> <p><strong>Phase Transitions</strong>: In the study of early universe cosmology or condensed matter physics, the effective potential reveals how a system transitions between different phases. For example, it can describe the transition from a symmetric phase to a broken-symmetry phase as the universe cools. The shape of the effective potential changes with temperature, and these changes can indicate phase transitions, such as from a high-temperature symmetric phase to a low-temperature phase where symmetry is broken.</p> </li> <li> <p><strong>Quantum Corrections and Renormalization</strong>: The effective potential incorporates quantum corrections to the classical potential, which are crucial for making precise predictions in QFT. These corrections can significantly alter the behavior of the system, especially at high energies or short distances. The process of renormalization is deeply connected to the effective potential, ensuring that physical quantities remain finite and meaningful.</p> </li> <li> <p><strong>Nonperturbative Effects</strong>: The effective potential can capture nonperturbative effects, which are not accessible through standard perturbative techniques. For instance, in theories with strong coupling or in situations where the perturbative series does not converge (actually it doesn’t converge in any cases), the effective potential can provide insights into the structure and dynamics of the vacuum, solitonic solutions, and other nonperturbative phenomena like instantons and tunneling effects.</p> </li> <li> <p><strong>Dynamical Mass Generation</strong>: In theories where particles are massless at the classical level, the effective potential can show how interactions lead to dynamical mass generation. This is particularly significant in quantum chromodynamics (QCD) and models of dynamical symmetry breaking, where the vacuum structure induced by strong interactions gives rise to constituent masses for particles.</p> </li> <li> <p><strong>Vacuum Stability and Tunneling</strong>: The effective potential allows for the analysis of vacuum stability in various field theories. It can be used to study the probability of tunneling between different vacua, which has implications for the stability of our universe and the decay of false vacuum states.</p> </li> </ol> <p>Overall, the effective potential is a powerful tool in quantum field theory, providing deep insights into the quantum dynamics of fields, the structure of the vacuum, and the various nonperturbative phenomena that arise in complex quantum systems. Next let’s dig into it.</p> <hr/> <p>Recall that the quantum effective action</p> <ul> <li>is a functional of $\varphi_ {J}$ where $\varphi_ {J} = \left\langle {\phi} \right\rangle_ J$, namely $\varphi$ is the expectation value of $\phi$ in the presence of a source term $J$, and</li> <li>satisfies ${\delta \Gamma}/{\delta \varphi} = J$.</li> </ul> <p>Thus <strong>when $J=0$, the solution to ${\delta \Gamma}/{\delta \varphi} = J$ is the vev of $\phi$.</strong></p> <p>Additionally, let’s assume the vacuum exhibits translational symmetry, meaning that the vacuum expectation value (vev) of $\phi$ remains constant across space and time. Under this condition, we only require a single number to describe the field configuration. Consequently, we can introduce a function, $\mathcal{V}_{\text{eff}}$, the minimum of which represents the vev of $\phi$,</p> \[\Gamma[\varphi]|_ {\varphi = \text{const}} = -VT \mathcal{V}_ {\text{eff}}(\varphi)\] <p>where $V$ is the volume of the space and $T$ the extension of time, $\mathcal{V} _ {\text{eff}}(\varphi)$ is the quantum effective potential. Apparently $\mathcal{V} _ {\text{eff}}$ is an intensive, and ${\partial\mathcal{V}}/{\partial\phi} = 0$ reproduces ${\delta \Gamma}/{\delta \varphi} = 0$.</p> <h1 id="2-coleman-weinberg-potential">2. Coleman-Weinberg Potential</h1> <h2 id="21-background-fields">2.1. Background Fields</h2> <p>The method of background field is very useful for calculating beta functions and effective action. For a real scalar field $\phi$, the general idea is as following</p> <ul> <li>separate the field into the static background field $\phi_ b$ and dynamic field $\phi$, $\phi \to \phi_ b + \phi$. By static we mean it is not path-integrated, thus don’t participate in quantum or static activities, such as propagate or fluctuate. This will simplify the calculation a lot (not supposed to be obvious).</li> <li>Define the corresponding action with background field $S_ b[\phi_ b;\phi]$, the resulting generating functional $W_ b[\phi_ b;\phi]$, and the 1PI effective action $\Gamma_ b[\phi_ b;\varphi]$, where $\varphi$ is defined to be the vev of $\phi$ in the presence of the background field $\phi_ b$.</li> <li>$\Gamma_ b[\phi_ b;\varphi]$ has a useful property:</li> </ul> \[\Gamma_ b[\phi_ b=\phi;0] = \Gamma_ b[\phi_ b=0;\varphi] = \Gamma[\varphi],\] <p>due to the fact that $\Gamma[\phi_ {b}+\varphi]$ is a functional of $\phi_ {b}+\varphi$ as a whole, it shows that how to choose the background field is kind of arbitrary, we should choose whatever makes our calculation the simplest. The first term means that the effective action with background field $\phi_ b = \phi$ and zero dynamic field $\varphi = 0$. Since the dynamic field will be set to zero at last, if a diagram has dynamic field external legs, it is also zero. It is similar to the Feynman diagrams with sources, when we set $J=0$ in the end then all the diagrams where all the source bulbs vanish (people who have read Srednicki will know what I am talking about).</p> <p>p.s. In Sidney Coleman’s lectures on QFT, in Eq. (44.31), his $\left\langle \phi \right\rangle$ is our $\phi_ {b}$ and his $\overline{\phi}’$ is our $\varphi$. In our notation, Coleman chose $\phi_ {b}$ to be the vev of $\phi$ with $J=0$ (since he is interested in studying spontaneous symmetry breaking), then he goes on and expand $\varphi$ about $\varphi=0$.</p> <p>The action with a background field is defined as</p> \[S_ b[\phi_ b;\phi] = \int d^4x\mathcal{L}(\phi_ b+\phi),\] <p>the partition function with source is</p> \[\mathcal{Z}[\phi_ b;J] = \int \mathcal{D}\phi \exp\{ i S_ b[\phi_ b;\phi]+i\int J\phi \} = \mathcal{Z}[J] e^{-i\int J\phi_ b},\] <p>note that <strong>the source only couples to the dynamic field</strong>. Since $\phi$ is integrated over, $\mathcal{Z}$ can only be a functional of $\phi_ b$ and $J$. Define the generating functional as</p> \[W_ b[\phi_ b;J] \equiv -i \ln \mathcal{Z}_ b[\phi_ b;J] \implies W_b[\phi_ b;J] = W[J] - \int J\phi.\] <p>Define</p> \[\varphi_ b(x) = \frac{\delta W_ b[J]}{\delta J(x)}\] <p>we have</p> \[\varphi_ b = \varphi - \phi_ b\] <p>which means that in the presence of a background field, $\left\langle \phi \right\rangle$ will be shifted by $\phi_ b$, as expected.</p> <p>Now we need to define the effective action in the presence of a background field, by the means of Legendre transformation again.</p> \[\Gamma_ b [\phi_ b;\varphi_ b] = W_ b - \int \frac{\delta W_ b[J]}{\delta J}J = W_ b - \int \varphi_ b J\] <p>you can check that</p> \[\frac{\delta \Gamma_ b[\phi_ b;\varphi_ b]}{\delta\varphi_ b}= J .\] <p>Replace $W_ b[J]$ with its expression in terms of $W[J]$, we can check that</p> \[\Gamma_ b[\phi_ b;\varphi_ b] = W[J] - \int J (\varphi_ b+\phi_ b) = \Gamma[\phi_ b + \varphi_ b]\] <p>so for example if we want to calculate $\Gamma[\eta(x)]$, we can set $\phi_ b = \eta,\,\varphi = 0$ and use that to simplify calculations.</p> <h2 id="22-coleman-weinberg-potential">2.2. Coleman-Weinberg Potential</h2> <p>Consider the real scalar Lagrangian</p> \[\mathcal{L} = -\frac{1}{2} \phi \partial^2\phi - \frac{1}{2} m^2\phi^2-\frac{1}{4!}\phi^4\] <p>the question is, when $m = 0$, will the quantum effects modify the shape of the potential?</p> <p>Introduce a background field $\phi \to \phi_ b +\phi$, take it into the action and expand, keep in mind that the path integral is over field $\phi$ only, we have</p> \[\begin{align*} e^{i\Gamma[\phi_ b]} &amp;= e^{i\int d^4 x (-\frac{1}{2} \phi_ b \partial^2 \phi_ b-V(\phi_ b))} \\ \\ &amp;\times\int \mathcal{D}\phi e^{i \int d^4 x (-\frac{1}{2} \phi \partial^2 \phi -V(\phi_ b) - \phi V'(\phi_ b)-\frac{1}{2}V''[\phi_ b]\phi^2-\frac{1}{3!}V'''[\phi_ b]\phi^3-\cdots)}. \end{align*}\] <p>In the path integral, one of the terms in the Lagrangian, i.e. $\int \mathcal{D} \phi e^{i\phi V’}$ can be thrown away because we only need to consider 1PI diagrams in the calculation of $\Gamma$, while the Feynman diagram given by $\phi V’(\phi_ b)$ will never contribute to the 1PI diagrams. For the same reason we can also discard $\phi^3$ term in the Lagrangian. At one loop, there will be no contributions from $\phi^4$ and higher terms either. Hence we are left with</p> \[e^{i\Gamma[\phi_ b]} = e^{i\int d^4 x (-\frac{1}{2} \phi_ b \partial^2 \phi_ b-V(\phi_ b))} \int \mathcal{D} e^{i \int d^4 x (-\frac{1}{2} \phi \partial^2 \phi -\frac{1}{2}V''[\phi_ b]\phi^2)}.\] <p>use the master formulae in QFT</p> \[\int\mathcal{D}\phi\exp\left\{ i \int d^4x (\phi M \phi) \right\} = \mathcal{N}\frac{1}{\text{det}^{ {1/2} }{M}}\] <p>we have</p> \[e^{i\Gamma[\phi_ b]} = \mathcal{N}e^{i\int d^4 x (-\frac{1}{2} \phi_ b \partial^2 \phi_ b-V(\phi_ b))} \text{det}^{-1/2}(\partial^2 + V''(\phi_ b))\] <p>In order to calculate the functional determinant, we need to put it in a specific representation, such as the position representation or the momentum representation, to turn $\partial^2 + V’’(\phi_ b)$ into a matrix, then calculate the determinant of that infinite dimensional matrix. Writing</p> \[\Gamma[\phi_ b] = \int d^4 x (-\frac{1}{2} \phi_ b \partial^2 \phi_ b-V(\phi_ b)) + \Delta \Gamma[\phi_ b],\] <p>where</p> \[i\Delta \Gamma[\phi_ b] = -\frac{1}{2}\ln \det (\partial^2 +V''(\phi_ b)) + \text{const}.\] <p>With the help of identity</p> \[\ln \det M = \ln \prod_ i \lambda_ i = \sum_ i \ln \lambda_ i = \text{tr } {\ln M}\] <p>where $\lambda_ i$ are the eigenvalues of $M$, we have</p> \[i\Delta \Gamma[\phi_ b] = -\frac{1}{2} \text{tr } {\ln (\partial^2 +V''(\phi_ b))} + \text{const}.\] <p>Next we assume that $\phi_ b$ is a const in space-time, and define</p> \[m_ {\text{eff}}^2(\phi_ b) \equiv V''(\phi_ b),\] <p>calculate the functional determinant in the representation of $x$, with the help of</p> \[\mathbb{1}= \int \frac{dp^4}{(2\pi)^4} \left\lvert k \right\rangle \left\langle{k}\right\rvert\] <p>we have</p> \[\begin{align*} i\Delta \Gamma[\phi_ b] &amp;= -\frac{1}{2}\int d^4 x \left\langle{x}\right\rvert \ln\left( 1+\frac{V''}{\partial^2} \right) \left\lvert{x}\right\rangle +\text{const}\\ &amp;= -\frac{1}{2}\int d^4 x \int\frac{d^4 k}{(2\pi)^4}\ln\left( 1-\frac{m_ {\text{eff}}^2}{k^2} \right)+\text{const} \end{align*}\] <p>where $\int d^4 x = VT$ is the space-time volume of the system.</p> <p>The integral over momentum is divergent, we will render it finite by a hard cut-off, that is to Wick rotate the system into Euclidean space and insert the momentum cut-off $\Lambda$, yielding</p> \[\begin{align*} \Delta\Gamma[\phi_ b] &amp;= -VT \frac{2\pi^2}{2(2\pi)^4} \int_ 0^\Lambda dk_ E k_ E^3 \ln(1+\frac{m_ {\text{eff}}^2}{k_ E3^2}) +\text{const}\\ &amp;= - \frac{VT}{128\pi^2} \left( 2 m_ {\text{eff}}^2 \Lambda^2 + 2m_ {\text{eff}}^4 \ln\frac{m_ {\text{eff}}^2}{\Lambda^2}+\text{const}\right) \end{align*}\] <p>where $k_ E$ is the Wick-rotated momentum and we have used the relation $\Lambda \gg m_ {\text{eff}}$. The effective potential accordingly is</p> \[V_ {\text{eff}} = V(\phi_ b) + c_ 1 + c_ 2 m_ {\text{eff}}^2 + \frac{1}{64\pi^2} m_ {\text{eff}}^4 \ln\frac{m_ {\text{eff}}^2}{c_ 3}\] <p>where $c_ 1,c_ 2,c_ 3$ are some $\Lambda$-dependent constants. They are independent of $\phi_ b$ thus in general are not of interests to us. For example, $c_ 2 = \Lambda^2 / 64\pi^2$.</p> <p>Next we need to add the counter terms to the potential</p> \[V(\phi) = \frac{1}{2}m_ R^2(1+\delta_ m)\phi^2 +\frac{\lambda_ R}{4!}(1+\delta_ \lambda)\phi^4 +\Lambda_ R (1+\delta_ \Lambda),\] <p>with all the counter terms starting at 1-loop level, that is of $\mathcal{O}(\lambda_ R)$. $m_ {\text{eff}}^2$ is still defined as $V’’$.</p> <p>What about the renormalization conditions?</p> <ul> <li>The question we want to ask is how the quantum corrections change the shape of the potential, when the mass term is zero, thus we require $\lambda_ R^2 = 0$</li> <li>$V(0) = 0 \implies \Lambda_ R = 0$</li> <li>$\lambda_ R = V’’’’(\phi_ R)$ for some arbitrary fixed scale $\phi_ R$</li> </ul> <p>They will fix the counter terms, plugging them in gives</p> \[\boxed{ V_ {\text{eff}}(\phi) = \frac{1}{4!}\phi^4\left\{ \lambda_ R + \frac{3 \lambda_ R^2}{32\pi^2}\left[ \ln \left( \frac{\phi^2}{\phi_ R^2}-\frac{25}{6} \right) \right] \right\} }\] <p>which is known as Coleman-Weinberg potential.</p> <p>Now having the effective potential at hand, we can answer the question: does the quantum correction change the vacuum expectation value of $\phi$? Originally, without the quantum correction, the minimum of the potential is at $\phi = 0$ since there is no quadratic term and only a quartic term. With quantum correction, the vev of $\phi$ is given by the minimum of the effective potential, which is the Coleman-Weinberg potential, which is minimized when</p> \[\lambda_ R \ln \frac{\left\langle \phi^{2} \right\rangle}{\phi_ R^2} = \frac{11}{3} \lambda_ R - \frac{32}{3}\pi^2.\] <p>which gives a nonzero $\left\langle \phi \right\rangle$. It means now we have a double-well potential, instead of the original single-well potential, due to the quantum correction.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="colemanWeinbergPotential"/><category term="effectiveTheory"/><category term="coleman"/><category term="effectivePotential"/><summary type="html"><![CDATA[1. Quantum Effective Action 1.1. Quantum Action 1.2. Effective Potential 2. Coleman-Weinberg Potential 2.1. Background Fields 2.2. Coleman-Weinberg Potential]]></summary></entry><entry><title type="html">Some Thoughts on Entropy</title><link href="https://baiyangzhang.github.io/blog/2024/Thoughts-on-Entropy/" rel="alternate" type="text/html" title="Some Thoughts on Entropy"/><published>2024-11-05T00:00:00+00:00</published><updated>2024-11-05T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Thoughts-on-Entropy</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Thoughts-on-Entropy/"><![CDATA[<h1 id="boltzmann-gibbs-bg-entropy">Boltzmann-Gibbs (BG) Entropy</h1> <p>There are so many different interpretations of entropy and somehow there are all equivalent to each other. Conceptually there are different definitions for entropy in physics and information theory, but the mathematical expression are the same, it must mean that there is a deep connection underneath.</p> <p>Chronologically, the first definition for entropy is macroscopic, given by Rudolf Clausius in 1865, when Marx founded the first International. The increase of entropy is defined to be proportional to the heat $Q$ that enters a system at temperature $T$,</p> \[dS := \frac{Q}{T}.\] <p>This definition tells us nothing about what entropy actually is. It is sometimes convenient to visualize entropy as some kind of “fluid” that can flow from one object to another. When heat (a form of energy) flows around between objects, it carries certain amount of entropy with it, $Q / T$. Since heat only flows from high temperature to low temperature, and the amount of heat is of course conserved during the flow, consequently entropy can only increase, it is only “half conserved”, unlike energy.</p> <p>For an isolated system, we have the microscopic definition for entropy given by Boltzmann,</p> \[S = k_ {B} \ln \Omega,\] <p>where $\Omega$ is the multiplicity, namely the number of microscopic states corresponding to certain macroscopic states, and $k_ {B}$ is the Boltzmann constant which we will set to be equal to $1$.</p> <p>There is a pretty good explanation about what are microstates and what are macrostates given by D. Shroeder in his textbook <em>The introduction to thermal physics</em>. For example, consider ten identical but distinguishable coins, the probability space is $(\Omega, \sigma, P)$ where $\Omega$ is the sample space, $\sigma$ is the sigma-algebra and $P$ the probability measure. Let’s toss the ten coins and see what is the final result. Each coins has two possible outcomes, head or tail. A macro state is described by some general properties, for example there are total 2 heads out of ten coins; a micro state requires we to fix each coin, for example the ten coins are <em>htthtttttt</em> where h stands for head and t for tail. A macro state in general comprises of several micro states, for example a state where “there are two heads” include $10\times 9 / 2 =45$ micro states. Another way to see it is that <strong>microstates are points in the sample space $\Omega$, while macro states are sets in the sigma-algebra.</strong></p> <p>The definition for entropy $k \ln \Omega$ only applies to when each microstate has equal probability, it can be generalized to the case where different microstates have different probabilities, which is more suitable to real life and thus often seen in the context of information theory:</p> \[S = - \sum_ {i} p_ {i} \ln(p_ {i}),\] <p>where $p_ {i}$ is the probability for a microstate $\left\lvert i \right\rangle$. To see how to do it, consider an unfair coin with probability $p$ to give head and $1-p$ to give tail. Let’s toss it $M$ times, or equivalently toss $M$ identical coins. We will calculate the total entropy for $M$ coins first. Then let’s assume that entropy is additive (or extensive), that is, the total entropy of $M$ coins is given by $M$ times the entropy of a single coin. Here lies another fundamental assumption: entropy of independent systems are additive.</p> <p>Since the probability to give a head is $p$, among $M$ total incidents there are $pM$ heads. Let’s denote it by $n$, which is assumed to be a big number. Now the question is: how many micro states there are that gives the macro state, namely with $N$ heads in total? The number $\Omega$ of such microstates is given by</p> \[\Omega(n\text{ heads}) = \frac{M!}{n!(M-n)!},\] <p>the total entropy is</p> \[S(n):=S(n\text{ heads}) = \ln \Omega(n \text{ heads}) = \ln\left( \frac{M!}{n!(M-n)!} \right).\] <p>Here we can adopt the Stirling’s approximation,</p> \[n! \approx \sqrt{2\pi n} \left( \frac{n}{e} \right)^{n} \approx \left( \frac{n}{e} \right)^{n}\] <p>This approximation is quite accurate, for $n=3$ the relative error for $\sqrt{2\pi n}(n / e)^{n}$ is $2.7\%$, and $0.3\%$ for $n=10$. Write it as</p> \[\ln n! \approx \frac{1}{2}\ln(n)+n\ln n-n,\] <p>where we have thrown away an additive constant $\frac{1}{2} \ln(1\pi)=0.9189\cdots$. Hell, $\ln n$ is also a small quantity compare to $n$ and $\ln n$ so let’s throw it away as well. Take it to the entropy expression and simplify, we have</p> \[S(n) = M\ln M - n\ln n-(M-n)\ln(M-n) = -(M-n)\ln\left( \frac{M-n}{M} \right)-n\ln\left( \frac{n}{M-n} \right)\] <p>rewrite in terms of probabilities for head and tail, $p_ {h}= n / M$ and $p_ {t}=1-p_ {h}$, we have</p> \[\frac{S}{M} = -p_ {h}\ln(p_ {h})-p_ {t}\ln p_ {t}=-\sum p\ln p.\] <p>Here $\frac{S}{M}$ is simply the entropy of a single coin. This result can be readily generalized to the most general case, for a single system with $n$ possible outcomes whose probabilities are $p_ {n}$, the entropy is</p> \[S = - \sum_ {i=1}^{n} p_ {i}\ln p_ {i}.\] <p>Note that we have dealt with two kinds of entropies:</p> <ol> <li>Entropy associated with a single random variable, like the final state of a coin;</li> <li>Entropy associated with a distribution on multiple identical random variables, such as the final states of many coins.</li> </ol> <p>They are connected by the assumption that entropy is <strong>extensive</strong>. As we will see that this assumption will be violated by Tsallis entropy.</p> <h1 id="information-entropy">Information Entropy</h1> <hr/> <p>In information theory, the expression for entropy adopts an entirely different interpretation, however the mathematical expression is exactly the same, which is amazing! In information theory, people write the entropy as</p> \[S = \sum p_ {i} \ln\left( \frac{1}{p_ {i}} \right)\] <p>and make sense of $\ln(1 / p)$ directly. As far as I know, there are commonly two interpretations:</p> <ul> <li>$\ln(1 / p_ {i})$ measures the “surprise” of a outcome;</li> <li>Shannon’s interpretation [Shannon:1948], $\ln(1 / p_ {i})$ measures the “information content” of a outcome;</li> </ul> <p>The entropy is then the average value of so-called “surprise”, or information content. It is a matter of taste of which you adopt.</p> <p>In his work, Shannon was interested in finding a measure that could quantitatively capture the information content of a message source. He proposed several properties that this measure (which we now call entropy) should satisfy to be a useful and consistent measure of information. These properties included:</p> <ol> <li><strong>Additivity</strong>: The entropy of two independent sources should be the sum of their individual entropies.</li> <li><strong>Continuity</strong>: The measure should change continuously as the message probabilities change.</li> <li><strong>Symmetry</strong>: The measure should not depend on the order of the messages.</li> <li><strong>Maximum</strong>: The measure should be maximal for a uniform distribution, where all messages are equally likely.</li> </ol> <p>Shannon’s Uniqueness Theorem essentially states that, given these properties (along with a few others), the entropy of a discrete random variable is unique and is given by the now-familiar formula:</p> \[S = -\sum_{i} p(x_i) \ln p(x_i)\] <p>The theorem’s significance lies in its establishment of entropy as a <strong>unique measure</strong> that satisfies these intuitive and necessary properties for quantifying information. It solidified the concept of entropy as the foundational metric in information theory, leading to profound implications for communication, coding theory, and even other disciplines like statistics and thermodynamics.</p> <p>Different sets of postulates have been given, which characterize the Shannon entropy. The simplest such set of postulates is given by D. K. Fadeev and A. Feinstein in the late 1950s. Fadeev’s postulates are as follows: Let $(p_ {1},\cdots,p_ {n})$ be a discrete probability distribution,</p> <ol> <li>$S(p_ {1},\cdots,p_ {n})$ is invariant under a permutation of its variables;</li> <li>$S(p,1-p)$ is a continuous function of $p$ for $0\leq p \leq 1$;</li> <li>$S\left( \frac{1}{2},\frac{1}{2} \right)=1$;</li> <li>$S(t p_ {1},(1-t)p_ {1},p_ {2},\cdots,p_ {n})=S(p_ {1},p_ {2},\cdots,p_ {n})+p_ {1}H(t,1-t)$ for $0\leq t \leq_ {1}$.</li> </ol> <p>The proof can be found in Rényi’s original paper on Renyi entropy. Condition (4) is stronger than the additivity, which states that for two probability distributions $\mathcal{P}$ and $\mathcal{Q}$, we have</p> \[S(\mathcal{P}\times \mathcal{Q}) = S(\mathcal{P}) + S(\mathcal{Q}).\] <p>As a matter of fact, if we relax condition (4) to the above expression, there will be a family of quantities satisfying them, for instance, all the quantities characterized by $\alpha&gt;0$ and $\alpha \neq1$:</p> \[H_ {\alpha}(p_ {1},\cdots,p_ {n} ) := \frac{1}{1-\alpha}\ln\left( \sum_ {k=1}^{\infty} p_ {k} ^{\alpha} \right),\] <p>which reduces to Shannon’s entropy at $\alpha=1$.</p> <hr/> <p>Kullback and Leibler (1951) developed the principle of minimum cross entropy (POMCE) and in the late 1950s Jaynes (1957a,b) developed the principle of maximum entropy (POME).</p> <p>The connection between likelihood function and cross entropy.</p> <p>Note that the cross entropy is not a metric since it is not symmetric under the exchange between $p$ and $\hat{p}$.</p>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[Boltzmann-Gibbs (BG) Entropy]]></summary></entry><entry><title type="html">Tsallis Statistics in Logistic Regression</title><link href="https://baiyangzhang.github.io/blog/2024/Tsallis-Statistics-in-Logistic-Regression/" rel="alternate" type="text/html" title="Tsallis Statistics in Logistic Regression"/><published>2024-10-16T00:00:00+00:00</published><updated>2024-10-16T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Tsallis-Statistics-in-Logistic-Regression</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Tsallis-Statistics-in-Logistic-Regression/"><![CDATA[<ul> <li><a href="#1-introduction">1. Introduction</a> <ul> <li><a href="#11-the-basics-of-boltzmann-gibbs-extensive-entropy">1.1. The Basics of Boltzmann-Gibbs Extensive Entropy</a></li> <li><a href="#12-generalization-to-non-extensive-entropy">1.2. Generalization to non-Extensive entropy</a></li> </ul> </li> <li><a href="#2-boltzmann-gibbs-statistical-mechanics">2. Boltzmann-Gibbs Statistical Mechanics</a> <ul> <li><a href="#21-three-different-forms-of-bg-entropy">2.1. Three different forms of BG entropy</a></li> <li><a href="#22-properties-of-bg-entropy">2.2. Properties of BG entropy</a></li> <li><a href="#23-constraints-and-entropy-optimization">2.3. Constraints and Entropy Optimization</a></li> </ul> </li> <li><a href="#3-nonextensive-statistical-mechanics">3. Nonextensive Statistical Mechanics</a> <ul> <li><a href="#31-mean-value-in-tsallis-statistics">3.1. Mean Value in Tsallis Statistics</a></li> </ul> </li> <li><a href="#4-tsallis-in-logistic-regression-methods">4. Tsallis in Logistic Regression Methods</a> <ul> <li><a href="#41-traditional-logistic-regression-method">4.1 Traditional logistic regression method</a></li> <li><a href="#42-with-tsallis-statistics">4.2 With Tsallis statistics</a></li> </ul> </li> <li><a href="#appendix-useful-mathematical-formulae">Appendix. Useful Mathematical Formulae</a></li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p><code class="language-plaintext highlighter-rouge">Tsallis statistics</code> is a generalization of traditional statistical mechanics, devised by <em>Constantino Tsallis</em>, to better characterize complex systems. It involves a collection of mathematical functions and associated probability distributions that can be derived by optimizing the <code class="language-plaintext highlighter-rouge">Tsallis entropic form</code>, a generalization of familiar Boltzmann entropy. A key aspect of Tsallis statistics is the introduction of a real parameter $q$, which adjusts the distributions to exhibit properties intermediate between Gaussian and Levy distributions, reflecting the degree of nonextensivity of the system.</p> <p>Tsallis distributions include various families like the $q$-Gaussian, $q$-exponential, and $q$-Weibull distributions. These distributions are notable for their heavy tails and have been applied across diverse fields such as statistical mechanics, geology, astronomy, economics, and machine learning, among others.</p> <p>The adaptation of Tsallis statistics to these varied fields underscores its versatility in dealing with systems where traditional Boltzmann-Gibbs statistics might not be adequate, particularly in scenarios involving long-range interactions, memory effects, or multifractal structures. Tsallis statistics is particularly useful in the analysis of non-extensive systems, to <code class="language-plaintext highlighter-rouge">biostatistics</code> could offer a novel perspective on analyzing complex biological data. Tsallis statistics has been successfully applied in various complex physical systems, such as space plasmas, atmospheric dynamics, and seismogenesis, as well as in the analysis of brain and cardiac activity, showing excellent agreement between theoretical predictions and experimental data. This demonstrates the versatility and potential of Tsallis statistics in capturing the dynamics of complex systems, which could be beneficial in biostatistical applications.</p> <p>Given the interdisciplinary nature of biostatistics, which often deals with complex, high-dimensional data, the non-extensive framework of Tsallis statistics might offer new methodologies for data analysis. For instance, it could be useful in understanding the dynamics of ecosystems, population genetics, or the spread of diseases, where traditional models might not fully capture the underlying processes due to their complexity and the presence of long-range interactions.</p> <p>To explore this possibility further, one could start by investigating specific biostatistical problems where the assumptions of traditional statistical mechanics are not met, and then applying Tsallis statistics to see if it offers better predictive power or insights. It would also be beneficial to collaborate with experts in biostatistics to identify the most pressing challenges where Tsallis statistics could be applied.</p> <p>While the application of Tsallis statistics to biostatistics is an intriguing prospect, it is an emerging area that would require substantial interdisciplinary research to fully understand its potential and limitations. Below is a list of potential applications, which is to be taken with a grain of salt.</p> <ol> <li> <p><strong>Epidemiological Modeling</strong>: Tsallis statistics could be used to model the spread of diseases, especially in cases where traditional models fail to capture the long-range correlations between individuals in a population.</p> </li> <li> <p><strong>Genetic Data Analysis</strong>: Analysis of genetic sequences and variations, where non-extensive entropy might better capture the complexity and long-range dependencies within genetic information.</p> </li> <li> <p><strong>Protein Folding Dynamics</strong>: Investigating the non-linear dynamics of protein folding, where Tsallis statistics may offer insights into the anomalous diffusion processes involved.</p> </li> <li> <p><strong>Neural Network Analysis</strong>: Modeling the complex interactions within neural networks, particularly in understanding the non-linear dynamics of brain activities and signal transmissions.</p> </li> <li> <p><strong>Ecological Systems</strong>: Applying Tsallis statistics to model the complexity of ecological systems, where interactions can span vast spatial and temporal scales.</p> </li> <li> <p><strong>Cancer Growth Modeling</strong>: Understanding the anomalous growth patterns of tumors, where traditional models might not accurately capture the underlying dynamics.</p> </li> <li> <p><strong>Drug Response Modeling</strong>: Analyzing the variability in drug responses among populations, which may exhibit non-standard distribution patterns that Tsallis statistics could elucidate.</p> </li> <li> <p><strong>Heart Rate Variability Analysis</strong>: Investigating the complex, non-linear dynamics of heart rate variability, potentially uncovering new insights into cardiovascular health.</p> </li> <li> <p><strong>Analysis of Medical Imaging Data</strong>: Enhancing the interpretation of complex patterns in medical imaging, such as MRI or CT scans, through non-extensive statistical models.</p> </li> <li> <p><strong>Public Health Data Analysis</strong>: Applying Tsallis statistics to public health data, potentially uncovering new patterns or correlations in large-scale health trends.</p> </li> </ol> <hr/> <h2 id="11-the-basics-of-boltzmann-gibbs-extensive-entropy">1.1. The Basics of Boltzmann-Gibbs Extensive Entropy</h2> <p><em>The whole theory of Tsallis statistics is based on a single concept: the modified Boltzmann-Gibbs (B-G) entropy</em> $S_ {q}$. $q$ is some index show how much $S_ {q}$ differs from the $BG$ entropy, if $q=1$ then there is no difference.</p> <p>To explain why the Boltzmann-Gibbs entropy is said to be additive, let’s first clarify what we mean by entropy in this context. In statistical mechanics, the Boltzmann-Gibbs entropy is a measure of the number of microstates that correspond to a given macrostate, providing a quantification of the system’s disorder or randomness.</p> <p>The formula for Boltzmann-Gibbs entropy, $S$, for a system in a particular <strong>macrostate</strong> is given by:</p> \[S = -k_B \sum_i p_i \ln p_i\] <p>where $p_i$ is the probability of the system being in the $i$-th microstate, and $k_B$ is the Boltzmann constant.</p> <p>Entropy is said to be additive when, for two independent systems $A$ and $B$, the total entropy $S_{AB}$ of the combined system is the sum of their individual entropies:</p> \[S_{AB} = S_A + S_B\] <p>This additivity property stems from the assumption of statistical independence of the two systems, which implies that the probability of the combined system $AB$ being in a particular microstate is the product of the probabilities of $A$ and $B$ being in their respective microstates. If $A$ is in a microstate with probability $p_i$ and $B$ is in a microstate with probability $q_j$, then the probability of the combined system being in the microstate characterized by both $p_i$ and $q_j$ is $p_i \cdot q_j$.</p> <p>As an example, consider two independent systems $A$ and $B$, each with two possible microstates. For system $A$, let the probabilities of the microstates be $p_1$ and $p_2$, and for system $B$, let them be $q_1$ and $q_2$. The entropies of systems $A$ and $B$ are:</p> <p>\(S_A = -k_B (p_1 \ln p_1 + p_2 \ln p_2)\) \(S_B = -k_B (q_1 \ln q_1 + q_2 \ln q_2)\)</p> <p>For the combined system $AB$, there are four possible microstates, with probabilities $p_1q_1, p_1q_2, p_2q_1,$ and $p_2q_2$. The entropy of the combined system is:</p> \[S _{AB} = -k_B [(p_1q_1) \ln (p_1q_1) + (p_1q_2) \ln (p_1q_2) + (p_2q_1) \ln (p_2q_1) + (p_2q_2) \ln (p_2q_2)]\] <p>With some algebra, you can show that:</p> \[S_{AB} = S_A + S_B .\] <p>This demonstrates the additivity of entropy for independent systems. <em>The crucial point here is the assumption of independence</em>, which allows the probabilities of the combined system to be expressed as products of the individual systems’ probabilities, leading directly to the additivity of entropy.</p> <p>Tsallis in his book compared his generalization of B-G statistics to $q$-statistics to the generalization of a circle to ellipses in explaining the motion of celestial objects. In both cases a single parameter changes everything. However I would argue that in the case of Kepler and others, more physics was revealed then in Tsallis’ case.</p> <h2 id="12-generalization-to-non-extensive-entropy">1.2. Generalization to non-Extensive entropy</h2> <p>In his book Tsallis listed some reasons for considering non-extensive, non-Boltzmann-Gibbs entropy, which can be roughly translated into:</p> <ol> <li>There is no (mathematical or physical) reason not to.</li> <li>A statistical description of a system should be based on the dynamics of the system, the macroscopic theory should come from a microscopic one. This opens the way, especially for complex systems, for other than Boltzmann statistics.</li> <li>The existence of long-range interactions on the microscopic level.</li> </ol> <h1 id="2-boltzmann-gibbs-statistical-mechanics">2. Boltzmann-Gibbs Statistical Mechanics</h1> <h2 id="21-three-different-forms-of-bg-entropy">2.1. Three different forms of BG entropy</h2> <p>No we need to come back to one of the most important concept in physics, statistics and information theory: <strong>entropy</strong>. It appears in various fields, each with its unique perspective but underlying similarities in concept.</p> <p>Generally, <em>entropy represents a measure of disorder, randomness, or uncertainty</em>. In statistics, entropy is a measure of the <strong>unpredictability</strong> or the <strong>randomness</strong> of a distribution. <em>The higher the entropy, the more unpredictable the outcome</em>. For example, in a perfectly uniform distribution where all outcomes are equally likely, entropy is at its maximum, indicating maximum uncertainty or disorder. In contrast, a distribution where one outcome is certain has zero entropy, representing complete order. This concept is used in various statistical methods and models to quantify uncertainty or variability within a dataset. In information theory, entropy is a fundamental concept introduced by <code class="language-plaintext highlighter-rouge">Claude Shannon</code>. It quantifies the average amount of information produced by a stochastic source of data. The more uncertain or random the source, the higher the entropy. In practical terms, entropy helps in understanding the limits of data compression and the efficiency of communication systems. For instance, a message composed of completely random bits has higher entropy and cannot be compressed beyond a certain limit without losing information. On the other hand, a message with a lot of repetitive or predictable parts has lower entropy and can be compressed more effectively.</p> <p>In each of these fields, entropy helps us understand systems’ behavior in terms of unpredictability, disorder, and efficiency. While the context and applications may vary, the core idea revolves around the concepts of uncertainty and the distribution of states or outcomes.</p> <p>In the previous section we showed the definition of entropy without much justification, because there is none! Not from the first principal at least. The programme to derive the expression of entropy that we are using today is sometimes called the Boltzmann program, since that was what Boltzmann was trying to do, before he strangled himself to death using a curtain or something.</p> <p>However, if the possibilities is a continuous distribution, the BG entropy must be modified accordingly, discrete probability $p_ {i}$ must be replaced by probability density $p(x)$ where $x$ is the variable. As a naively guess, I would say that we can write the entropy as</p> \[-k \sum p _ {i} \ln p _ {i} \to -k \int dx \, p(x) \ln(p(x)),\] <p>where the probability distribution function (or probability density) is normalized,</p> \[\int dx \, p(x) =1.\] <p>However, a difference between discrete and continuous probability lies in its dimension! Normalized discrete probabilities $p_ {i}$ sums to 1, $\sum_ {i} p_ {i}=1$, since $1$ is dimensionless, so is $p_ {i}$. This is not true for continuous probability density $p(x)$, since now the normalization condition tells us that $\int dx \, p(x)$ should be dimensionless, and $dx$ has the dimension of length, so $p(x)$ must have dimension of length inversed! Thus, wo need to introduce another parameter, call it $\sigma$, with dimension of length. Then we can define the BG entropy in the continuous scenario:</p> \[S_ {BG} = -k\int dx \, p(x) \ln(\sigma\, p(x)).\] <p>For the case of equal probabilities, that is, $p= 1 / \Omega$ where $\Omega$ is the total number of allowed microscopic states, we have</p> \[S_ {BG} = k \ln\left( \frac{\Omega}{\sigma} \right).\] <p>We just mention on the fly that, in quantum mechanics, the probabilistic distribution of a mixed state in terms of pure states is described using the density matrix $\rho$, and the BG entropy is generalized to</p> \[S_ {BF} = -k\,\mathrm{Tr}\, (\rho \ln \rho).\] <h2 id="22-properties-of-bg-entropy">2.2. Properties of BG entropy</h2> <p>We will list without proof some of the key properties of BG entropy.</p> <ul> <li><strong>Non-negativity</strong>. $S\geq 0$ always. $S = k\ln \Omega$ might help to convince you of it.</li> <li><strong>BG entropy is maximized at equal probability</strong>. Anything that drives the system away from it will decrease the entropy.</li> <li><strong>Expansibility</strong>. Adding to a system new possible states with zero probability should not modify the entropy.</li> <li><strong>Additivity</strong>. Let $A,B$ be two systems with entropy $S(A)$ and $S(B)$, putting them together will result in a new system $A+B$ with entropy $S(A+B)=S(A)+S(B)$.</li> <li><strong>Concavity</strong>. Given two different probability distributions $\left\lbrace p_ {i} \right\rbrace$ and $\left\lbrace p’_ {i} \right\rbrace$, we can define an intermediate probability distribution</li> </ul> \[\widetilde{p}:= \lambda p + (1-\lambda)p',\quad \lambda \in (0,1).\] <p>Then we have</p> \[S(\widetilde{p})\equiv S(\lambda)&gt; \lambda S(p)+(1-\lambda)S(p').\] <p><strong>Shannon Uniqueness Theorem</strong>.</p> <p>In his work, Shannon was interested in finding a measure that could quantitatively capture the information content of a message source. He proposed several properties that this measure (which we now call entropy) should satisfy to be a useful and consistent measure of information. These properties included:</p> <ol> <li><strong>Additivity</strong>: The entropy of two independent sources should be the sum of their individual entropies.</li> <li><strong>Continuity</strong>: The measure should change continuously as the message probabilities change.</li> <li><strong>Symmetry</strong>: The measure should not depend on the order of the messages.</li> <li><strong>Maximum</strong>: The measure should be maximal for a uniform distribution, where all messages are equally likely.</li> </ol> <p>Shannon’s Uniqueness Theorem essentially states that, given these properties (along with a few others), the entropy of a discrete random variable is unique and is given by the now-familiar formula:</p> \[S = -\sum_{i} p(x_i) \ln p(x_i)\] <p>The theorem’s significance lies in its establishment of entropy as a <strong>unique measure</strong> that satisfies these intuitive and necessary properties for quantifying information. It solidified the concept of entropy as the foundational metric in information theory, leading to profound implications for communication, coding theory, and even other disciplines like statistics and thermodynamics.</p> <h2 id="23-constraints-and-entropy-optimization">2.3. Constraints and Entropy Optimization</h2> <p><strong>Imposing the Mean Value</strong></p> <p>We might know a priori the mean value of a variable $x$, i.e.</p> \[\left\langle x \right\rangle := \int dx \, x \, p(x) = \overline{x} \quad \text{ is known.}\] <p>We can apply the Lagrange multiplier method to find the optimizing distribution with the constraint, together with the normalization condition $\int dx \, p(x)=1$. The Lagrangian functional that we want to extremize reads</p> \[\Phi[p(x)] = S_ {BG} - \alpha \int dx \, p(x) - \beta \int dx \, x \, p(x)\] <p>where we have neglected some constant terms since they don’t appear in the Euler-Lagrange equation, that is to say, they don’t affect the final result; and</p> \[S_ {BG} = -\int dx \, p(x)\ln p(x).\] <p>Using the method of variation, we get</p> \[p(x) = \frac{1}{\overline{x}} e^{ -x / \overline{x} }.\] <p><strong>Imposing the Mean value and the Mean Squared Value</strong></p> <p>Supposed that we not only know the mean value $\overline{x}=\left\langle x \right\rangle$, but also the mean square value $\left\langle x^{2} \right\rangle$:</p> \[\left\langle x^{2} \right\rangle \equiv \int dx \, (x-\left\langle x \right\rangle )^{2} p(x)\] <p>This time the Lagrangian reads</p> \[\Phi[p(x)] = S_ {BG} - \alpha \int dx \, p(x) - \beta_ {1}\int dx \, xp(x) - \beta_ {2} \int dx \, (x-\left\langle x \right\rangle )^{2}p(x).\] <p>Exactly as before, the variational method gives us</p> \[p(x) = \sqrt{ \frac{\beta_ {2}}{\pi} } \exp \left\lbrace -\beta_ {2}(x-\left\langle x \right\rangle )^{2} \right\rbrace\] <p>which is just the Gaussian distribution! This tells us that the Gaussian distribution maximizes the entropy with fixed mean and variance.</p> <h1 id="3-nonextensive-statistical-mechanics">3. Nonextensive Statistical Mechanics</h1> <p>Tsallis is convinced that there exists no logical-deductive procedure for generalizing any physical theory. As a possible motivation to generalize the exponential function $e^{ x }$, he started from the equation that $e^{ x }$ satisfies, which is fairly simple:</p> \[\frac{dy}{dx} = y.\] <p>A possible generalization of the equation is to writhe the RHS as $a+by$, we have</p> \[\frac{dy}{dx} = a+by \implies \frac{dy}{a+by}=dx\implies y=\frac{1}{b}(e^{ b(x+c) }-a),\] <p>which does not look very promising. Then Tsallis considered a non-linear generalization:</p> \[\frac{dy}{dx} = y^{q}, \quad q\in \mathbb{R},\quad y(0)=1.\] <p>The boundary condition $y(0)=1$ is such that is agrees with the usual exponential function $e^{ 0 }=1$. Solving it we get</p> <p>\((1-q) (x+c) = y^{1-q} ,\) the boundary condition translates to</p> \[c = \frac{1}{1-q},\] <p>thus</p> \[\boxed{ y = (1+(1-q)x)^{1/(1-q)} =: e_ {q}^{x}. }\] <p>When $1+(1-q)x&lt;0$, $e_ {q}(x)$ is defined to be zero (<strong>why don’t complexify it</strong>?). Also note that $e^{ x }_ {q}$ goes to $e^{ x }$ at $q\to 1$ since, writing $q = 1-\epsilon$, we have</p> \[\lim_ { q \to 1 } e^{ x }_ {q} = \lim_ { \epsilon \to 0 } (1+\epsilon x)^{1/\epsilon} = e^{ x }.\] <p>From the same equation that we got the definition of $e^{ x }_ {q}$, we also get the inverse function of $x$ in terms of $y$:</p> \[\boxed{ x = \frac{y^{1-q}-1}{1-q} =: \log_ {q} y. }\] <p>They are referred to as $q$-exponential functions and $q$-logarithmic functions respectively.</p> <p>Recall that logarithmic functions turns multiplication into addition, $\log(AB)=\log(A)+\log(B)$, for $q$-logarithmic functions we have something similar,</p> \[\log_ {q}(AB) = \log_ {q}(A) + \log_ {q}(B) + (1-q) \log_ {q}(A)\log_ {q}(A).\] <h2 id="31-mean-value-in-tsallis-statistics">3.1. Mean Value in Tsallis Statistics</h2> <p>There are at least three types of Tsallis statistics, depending on how they take the mean value. Next we will discuss each of them in chronological order.</p> <p>Throughout the note we will assume that probabilities are normalized to one,</p> \[\sum_ {i} p_ {i} = 1.\] <p>Given an observable $\mathcal{O}$, what is the expected value $\left\langle \mathcal{O} \right\rangle$? In regular statistics</p> \[\left\langle \mathcal{O} \right\rangle := \sum_ {i} p _ {i} O_ {i}\] <p>where $\mathcal{O}_ {i}$ is the $i$-th possible value of $\mathcal{O}$ with probability $p_ {i}$. However, this definition yields an ill-defined thermodynamic distribution, <em>some energy states will not be allowed due to mathematical rather than physical reasons, and the distribution is not invariant under an overall shift in energy</em>. Normally only the energy difference matter, the only situation that I know of where the absolute energy matters is from gravity, which is clearly not the case here. Thus it is not a good definition in Tsallis statistics.</p> <hr/> <p>Another way to define the average, known as Tsallis type II, is</p> <p>\(\left\langle \mathcal{O} \right\rangle := \sum_ {i} p_ {i}^{q} \mathcal{O}_ i.\) The problem is similar with type I, the sample space is constraint due to some un-natural reason, which I tend to interpret as the evidence of an ill-defined theory. Some divergence that occurs in type I does not occur here, but it introduces new problems, most of all the expected value of unity $\mathbb{1}$ is not $1$.</p> <p>However there is a remedy. Arguing from the point of view of information theory on incomplete probability distributions, Q. A. Wang suggested modifying the normalization of probability as</p> \[\sum_ {i} p_ {i}^{q} = 1.\] <p>This can be rewritten by defining $P_ {i}:= p_ {i}^{1}$, then</p> \[\left\langle \mathcal{O} \right\rangle := \sum_ {i} P_ {i} \mathcal{O}_ {i} .\] <hr/> <p>Type III assumes that the average is defined as type II but with an normalization factor:</p> \[\left\langle \mathcal{O} \right\rangle := N \sum_ {i} p_ {i}^{q}\mathcal{O}_ {i},\quad N = \sum_ {i}p_ {i}.\] <p>This solves the problem that the expectation value of identity $1$ is not $1$. The probability derived from is also becomes invariant under an overall shift.</p> <h1 id="4-tsallis-in-logistic-regression-methods">4. Tsallis in Logistic Regression Methods</h1> <p>This part will be presented in a much less pedagogical way, we will simply introduce new functions and concepts along the way.</p> <p>Before I could apply Tsallis statistics to medical data, I need first to figure out how the traditional method works.</p> <h2 id="41-traditional-logistic-regression-method">4.1 Traditional logistic regression method</h2> <p>Consider a matrix $X_ {n \times p}$ representing the expression of $p$ genes from $n$ samples, obtained through technologies like microarray or RNA-seq. Here, $p \gg n$, hence the name <em>big $p$, small $n$</em> problem. The response, or dependent variables (observables), is denoted by an $n$-vector $\vec{y} = (y_ 1, \cdots, y_ n)$. Each $y_ i$ is a binary variable, taking values 0 or 1. For example, in a clinical context, think of each sample as a patient; $y_ i = 1$ might indicate that patient $i$ is cured of, or has, a certain disease, while $y_ i = 0$ indicates the opposite. Currently, we are focusing on a mathematical model, so the exact biological or clinical significance of $y = 1$ is not specified. The values of $\vec{y}$ follow a Bernoulli distribution, which will be discussed in more detail shortly.</p> <p>We can organize all the gene expressions from all the samples into a single matrix $X$,</p> \[X = \begin{pmatrix} x_ {11} &amp; \cdots &amp; x_ {1p} \\ x_ {21} &amp; \cdots &amp; x_ {2p} \\ \vdots &amp; \ddots &amp; \vdots \\ x_ {n1} &amp; \cdots &amp; x_ {np} \end{pmatrix}.\] <p>In the above matrix, each row is a vector of different genes from one sample, while each column represents one type of gene from different patients. We use $X_ {i}$ to denote the expression of the $i$-th gene across $n$ samples, which corresponds to the $i$-th column of $X$. Conversely, $(X_ {(j)})^{T}$ represents the genes expression observed in $j$-th sample (or patient $j$), namely the $j$-th row of $X$. The transpose makes $X_ {(j)}$ a column vector, just like $X_ {a}$. Be aware that the distinction in notation is merely the addition of parentheses. In general, $X$ is our data and $\vec{y}$ will be the dependent variable. In practice, it is known which patients are treated and which are not, hence we know the observed value of $\vec{y}$. The problem is to find a way to <strong>predict</strong> $y_ {a}$ from an observed vector of genes expressions $X_ {(a)}$. But first, we need to know the role played by each gene expressed in different patients, that is, we need to know how to interpret $X_ {a}$ for gene $a$. For example, if $X_ {a}$ is a constant vector for all the patients, then we know that gene $a$ most likely has nothing to do with the disease of study.</p> <hr/> <p>Let us now delve into the mathematical foundations of the model.</p> <p>Let $\pi_ {i}$ be the probability of $y_ {i}=1$, which we aim to predict. Following the standard logistic regression method, we define the logit (log here is used as a verb, <strong>log</strong>-it) function of $\pi_ {i}$,</p> \[\text{logit}(\pi_ {i} ) := \ln\left( \frac{\pi_ {i}}{1-\pi_ {i}} \right), \quad \text{logit}: [0,1]\to \mathbb{R}.\] <p>$\pi$ is the probability, and $\pi / (1-\pi)$ is called <code class="language-plaintext highlighter-rouge">odd</code> with range $\mathbb{R}^{+}:=[0,\infty)$. Note that in our convention zero is included in $\mathbb{R}^{+}$.</p> <p>Since we have constructed a continuous function $\text{logit}(\pi)$ from a probability function $\pi$, we can now adopt familiar <em>linear regression method</em> and apply it to the logit function. Parametrize the logit function in a linear form</p> \[\text{logit}(\pi_ {i}) := \beta_ {0}+X_ {(i)}\cdot\beta, \quad \beta=(\beta_ {1},\cdots,\beta_ {p}).\] <p>$\beta$ is the parameter vector to be fixed later. Note the dot product is the product between vectors, $X \cdot \beta:= X^{T} \beta$ .</p> <p>Direct derivation shows that</p> \[\pi_ {i} = \frac{1}{1+\exp(-\beta_ {0}-X_ {(i)}\cdot \beta)}=:\text{sigm}(\beta_ {0}+X_ {(i)}\cdot \beta),\] <p>where $\text{sigm}$ stands for sigmoid function, which literally means a function that has an $S$-shape. In our particular case the sigmoid function is give by</p> \[\text{sigm}(t) := \frac{1}{1+e^{ -t }}.\] <p>Note that $\text{sigm}$ is the inverse of $\text{logit}$,</p> \[\text{logit}(\pi)=x \Longleftrightarrow \pi = \text{logit}^{-1}(x) = \text{sigm}(x).\] <p>I am not sure if there exists other widely-applied definitions for sigmoid function except for that given above. There surely are other functions with an S-shape, such as tanh and arctan, but I am not sure if they can be called sigmoids? Or maybe difference in the predictive power by introducing different choices of sigmoid functions are negligible, hence it only makes sense that we stuck with the simplest option?</p> <p>Now, the question is how can we fix the parameters? The natural answer is: by maximizing the likelihood, or equivalent by minimizing the loss function. The likelihood function, by definition, is the probability (likelihood) for a certain observation $\vec{y}$. This function gives the probability of observing the given data within certain model, with some free parameters. For instance, supposed there are three samples (binary), then the likelihood of $y=(1,0,1)$ corresponds to the probability predicted by our model that the first and third patients have got some disease, while the second does not. Now, the probability for each $y_ {i}=1$ is $\pi_ {i}$, which is by definition $\text{sigm}(t_ {i})\equiv1/(1-e^{ -t_ {i} })$, where $t=\beta_ {0}+X_ {(i)}\cdot \beta$, and $\beta$’s are the parameters. The probability for $y_ {i}=0$ is $1- 1/(1-e^{ -ti })$, which is $e^{ -t_ {i} }/(1-e^{ -t_ {i} })$. Then, the likelihood for observing $y=(1,0,1)$ is simply the product of each probability,</p> \[\text{lik}(1,0,1) = \frac{1}{1-e^{ -t_ {1} }} \frac{e^{ -t_ {2} }}{1-e^{ -t_ {2} }} \frac{1}{1-e^{ -t_ {3} }},\] <p>which is a function of observed gene expression $X_ {(i)}$ with parameters $\beta$’s.</p> <p>Likelihood function can also be written using the <code class="language-plaintext highlighter-rouge">Bernoulli distribution</code> function, which is a probability distribution function (PDF) that has only two possible outcomes, 1 and 0, with probability $\pi$ for $y=1$ and $1-\pi$ for $y=0$. This distribution is a special case of the binomial distribution where the number of trials $n$ is equal to 1. The <strong>Probability Mass Function (PMF)</strong> of Bernoulli distribution is defined as</p> \[P(y) := \pi^y (1-\pi)^{1-y}, \quad y=0 \text{ or }1,\] <p>where $\pi$ is given <em>a priori</em>. The Expected Value is $\left\langle y \right\rangle=\pi$ and the variance is $\text{Var}(y) = \pi(1-\pi)$, you can verify it easily. This neat expression unites both cases $y=0$ and $y=1$. Applying it to the likelihood function we get</p> \[\text{lik}(1,0,1) = \prod_ {i=1}^{3} \pi_ {i}^{y_ {i}}(1-\pi_ {i})^{1-y_ {i}}, \quad \vec{y}=(1,0,1).\] <p>Generalization to arbitrary $\vec{y}$ is trivial,</p> \[\text{lik}(\vec{y}) := \prod_ {i=1}^{n} \pi_ {i}^{y_ {i}}(1-\pi_ {i})^{1-y_ {i}}, \quad \vec{y}=(y_ {1},\cdots,y_ {n}).\] <p>The above expression can be further simplified using logarithms. Recall that logarithm is a <em>monotonically increasing</em> function, it means that $\log(\text{lik}(\vec{y}))$ is maximized iff (if and only if) $\text{lik}(\vec{y})$ is maximized. The reason why it is a good idea to take the logarithm of the likelihood is the following.</p> <ol> <li> <p><strong>Numerical Stability</strong>: The likelihood function in models like logistic regression involves products of probabilities, which can be very small numbers. When multiplying many such small probabilities, the result can become extremely small, potentially leading to numerical underflow (where values are so small that the computer treats them as zero). The logarithm of these small numbers turns them into more manageable, larger negative numbers, reducing the risk of numerical issues.</p> </li> <li> <p><strong>Simplification of Products into Sums</strong>: The likelihood function involves taking the product of probability values across all data points. In contrast, the log-likelihood converts these products into sums by the property of logarithms $\ln(ab) = \ln(a) + \ln(b)$. Sums are much easier to handle analytically and computationally. This is especially useful when dealing with large datasets.</p> </li> <li> <p><strong>Convexity Properties</strong>: The log-likelihood function often yields a convex optimization problem in cases where the likelihood itself is not convex. Convex problems are generally easier to solve reliably and efficiently. For logistic regression, the log-likelihood function is concave, and finding its maximum is a well-behaved optimization problem with nice theoretical properties regarding convergence and uniqueness of the solution.</p> </li> <li> <p><strong>Derivative Computation</strong>: The derivatives of the log-likelihood (needed for optimization algorithms like gradient ascent or Newton-Raphson) are typically simpler to compute and work with than the derivatives of the likelihood function. This simplicity arises because the derivative of a sum (log-likelihood) is more straightforward than the derivative of a product (likelihood).</p> </li> </ol> <p>Last but not least,</p> <ol> <li><strong>Possibility to introduce the Tsallis statistics</strong>: We have explain how the Tsallis statistics modified traditional exponential and logarithmic functions, <em>in our case the log-likelihood function can be generalized by adopting the Tsallis logarithm, namely $\log_ {q}$.</em> In the next section we will try to explain the advantage of such generalization, which will also serve as justification. But of course, being a phenomenological model, the true justification will be the power of prediction, which can only be tested in real-life practice.</li> </ol> <p>But for now, let’s forget about Tsallis and carry on on the road of conventional logic regression method.</p> <p>Taking the natural log of likelihood function gets us</p> \[\begin{align*} \text{loglik}(\vec{y}) &amp;:=\log(\text{lik}(\vec{y}))= \ln \left\lbrace \prod_ {i=1}^{n} \pi_ {i}^{y_ {i}}(1-\pi_ {i})^{1-y_ {i}} \right\rbrace \\ &amp;=\sum_ {i} \left\lbrace y_ {i} \ln\pi_ {i}+(1-y_ {i}) \ln(1-\pi_ {i}) \right\rbrace. \end{align*}\] <p>In the context of logistic regression, people often use the <code class="language-plaintext highlighter-rouge">loss function</code>, <em>defined as the negative of the likelihood function</em>. This is primarily due to convention and practical considerations in optimization processes, since most optimization algorithms and tools are designed to minimize functions rather than maximize them. This is a common convention in mathematical optimization and numerical methods. Since maximizing the likelihood is equivalent to minimizing the negative of the likelihood, formulating the problem as a minimization problem allows us the application of standard, widely available optimization software and algorithms without modification. In many statistical learning methods, the objective function is often interpreted as a “cost” or “loss” that needs to be minimized. When working with a loss function, the goal is to find parameter estimates that result in the smallest possible loss. Defining the loss function as the negative log-likelihood aligns with this interpretation because lower values of the loss function correspond to higher likelihoods of the data under the model.</p> <p>Furthermore, using a loss function that is to be minimized creates a consistent framework across various statistical learning methods, many of which inherently involve minimization (like least squares for linear regression, and more complex regularization methods in machine learning). This consistency is helpful not only from an educational and conceptual standpoint but also from a practical implementation standpoint.</p> <p>A <code class="language-plaintext highlighter-rouge">regularized loss function</code> is the loss function with <code class="language-plaintext highlighter-rouge">penalty terms</code>. Penalty terms in regression methods are essential for managing several challenges that arise in statistical modeling, particularly with high-dimensional data. These challenges include <em>overfitting</em>, <em>multicollinearity</em>, and <em>interpretability</em>.</p> <p>One of the primary reasons for using penalty terms is to prevent overfitting. Overfitting occurs when a model captures not just the true underlying pattern but also the random fluctuations (noise) in the data. This makes the model very accurate on the training data but <em>poorly generalized to new, unseen data</em>. By adding a penalty term, which increases as the model complexity increases (e.g., as coefficients become larger), the optimization process is biased towards simpler models. This helps to ensure that the model generalizes well to new data by focusing on the most significant predictors and shrinking the others.</p> <p>Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated. This can make the model estimation unstable and the estimates of the coefficients unreliable and highly sensitive to small changes in the model or the data. Penalty terms, especially those like Lasso regression (L1 penalty) or Ridge regression (L2 penalty), can reduce the impact of multicollinearity by penalizing the size of the coefficients, thus stabilizing the estimates. In high-dimensional datasets where the number of predictors $p$ exceeds the number of observations $n$ or when there are many irrelevant features, selecting the most relevant features becomes crucial. Lasso regression (L1 penalty) is particularly useful for this purpose because it can shrink some coefficients to exactly zero, effectively performing variable selection. This helps in identifying which predictors are most important for the outcome, simplifying the model and improving interpretability.</p> <p>By shrinking coefficients or reducing the number of variables, penalty terms help in simplifying the model. A simpler model with fewer variables or smaller coefficients is easier to understand and interpret. This is particularly valuable in domains like medical science or policy-making, where understanding the influence of predictors is as important as the prediction itself.</p> <p>However it is also important to realize the disadvantages of introducing penalty terms, that is the bias-variance tradeoff. Adding a penalty term introduces bias into the estimator for the coefficients (the estimates are “shrunk” towards zero or towards each other in the case of Ridge). However, this can lead to a significant reduction in variance (the estimates are less sensitive to fluctuations in the data). This trade-off can lead to better predictive performance on new data, which is the ultimate goal of most predictive modeling tasks.</p> <hr/> <p>In our project, we consider the regularized loss function as following.</p> \[l(\beta_ {0}, \vec{\beta}) := -\sum_ {i}^{n} \left\lbrace y_ {i}\ln(\pi_ {i})+(1-y_ {i})\ln(1-\pi_ {i}) \right\rbrace +h(\vec{\beta}),\] <p>where $h(\vec{\beta})$ is the penalty terms and is independent of $\beta_ {0}$, which is simply a shift. $h(\vec{\beta})$ could be the Lasso (<code class="language-plaintext highlighter-rouge">Least Absolute Shrinkage and Selection Operator</code>) term, or ridge term.</p> <hr/> <p>We now introduce the <strong>Group Lasso regression</strong> method. Group Lasso regression is an extension of Lasso that is particularly useful in the context of biostatistics, especially when dealing with models that incorporate multiple predictors which naturally group into clusters. This method not only encourages sparsity in the coefficients, like traditional Lasso, but also takes into account the structure of the data by promoting or penalizing entire groups of coefficients together. Here’s an introduction to how Group Lasso works and why it’s valuable in biostatistics:</p> <p>In many biostatistical applications, predictors can be inherently grouped based on biological function, measurement type, or other domain-specific knowledge. For example, genes might be grouped into pathways, or clinical measurements might be grouped by the type of instrument used or the biological system they measure.</p> <p>The key idea behind Group Lasso is to <em>perform regularization and variable selection at the group level</em>. The formulation of Group Lasso is similar to that of Lasso, but instead of summing the absolute values of coefficients, it sums the norms of the coefficients for each group. The objective function for Group Lasso can be written as:</p> \[\text{Minimize:} \quad \text{loss function} + h(\vec{\beta}), \quad h(\vec{\beta}):= \lambda \sum_ {g=1}^G w_ g \left\lVert \vec{\beta}_{(g)} \right\rVert_ {2}\] <p>where $\vec{\beta}$ represents the coefficient vector, $\vec{\beta}_ {(g)}$ is the sub-vector of coefficients corresponding to group $g$. Note the difference between $\beta_ {g}$ and $\beta_ {(g)}$, the former is a component of $\vec{\beta}$ thus a number, while the latter is a sub-vector of $\vec{\beta}$. $G$ is the total number of groups, $w_g$ are weights that can be used to scale the penalty for different groups, often based on group size or other criteria, $\left\lVert \vec{\beta}_ {(g)} \right\rVert$ is typically the Euclidean norm (L2 norm) of the coefficients in group $g$ or, as is shown in ShunJie’s paper, the PCC (Pearson correlation coefficient) distance, or shape-based distance. $\lambda$ is a <strong>global</strong> tuning parameter that controls the overall strength of the penalty.</p> <p>In our current project, we use square root of the degree of freedom (dof) as the weight for different groups. The penalty term is then</p> \[h(\beta) = \lambda \sum_ {g=1}^{G} \sqrt{ d_ {g} } \left\lVert \vec{\beta}_ {g} \right\rVert _ {2} =: \lambda \sum_ {g=1}^{G} \sqrt{ d_ {g} }\; r_ {g}\] <p>where $d_ {g}$ is the number of genes in group $g$, or dimension of $\vec{\beta}_ {g}$, $\vec{\beta}_ {g}$ is treated as a $d_ {g}$-vector of parameters, and $r_ {g}$ is the radius of $\vec{\beta}_ {g}$. Naturally it is a subset of $\beta$.</p> <p>Recall that $\sqrt{ d_ {g} }$ is the expected length of $d_ {g}$ i.i.d. variables, independent and identically distributed variables. The factor $\sqrt{d_ g}$ is to adjust for the size of each group, it scales up the penalty proportional to the group size. This means that larger groups, which naturally have a potentially larger norm due to more coefficients, receive a proportionally larger penalty. By multiplying the norm by $\sqrt{d_ g}$, the intention is to make the penalty fair by ensuring that the regularization is proportional to the number of parameters in the group. Without this scaling, smaller groups could be unfairly penalized in relative terms because their norms are naturally smaller due to fewer components. For example, consider two groups in a regression setting:</p> <ul> <li>Group 1: Contains 1 coefficient, $\mathbf{w}_ 1 = (3)$.</li> <li>Group 2: Contains 4 coefficients, $\mathbf{w}_ 2 = (1, 1, 1, 1)$.</li> </ul> <p>Without any normalization, we have</p> <ul> <li>The norm for Group 1 is $\left\lVert w_ {1} \right\rVert_ 2 = \left\lvert 3 \right\rvert = 3$.</li> <li>The norm for Group 2 is $\left\lVert w_ {2} \right\rVert_ 2 = \sqrt{1^2 + 1^2 + 1^2 + 1^2} = 2$.</li> </ul> <p>Here, the raw norms suggest that Group 1 might be more significant, which could distort the model’s view. Using $d_ g$ for normalization instead, we have:</p> <ul> <li>For Group 1: $1 \times \left\lvert 3 \right\rvert= 3$.</li> <li>For Group 2: $4 \times 2 = 8$.</li> </ul> <p>This overcorrects, making the penalty overly sensitive to the number of components, and could lead to under-penalizing smaller groups. Thus we decide to use $\sqrt{d_ g}$ for normalization:</p> <ul> <li>For Group 1: $\sqrt{1} \times 3 = 3$.</li> <li>For Group 2: $\sqrt{4} \times 2 = 2 \times 2 = 4$.</li> </ul> <p>This balances the penalties more reasonably, reflecting that while Group 2 is larger, its collective contribution should be seen in proportion to the natural growth of norms with group size, not exponentially with each addition.</p> <hr/> <p>We recall that $X_ {(i)}$ is the expression of different genes from one patient $i$, in terms of $X_ {(i)}$ is given the probability:</p> \[\pi_ {i} = \frac{1}{1+\exp(-\beta_ {0}-X_ {(i)}\cdot \beta)}=\text{sigm}(\beta_ {0}+X_ {(i)}\cdot \beta)=\text{sigm}(t_ {i} ),\] <p>where the sigmoid function is give by</p> \[\text{sigm}(t_ {i} ) := \frac{1}{1+e^{ -t_ {i} }}, \quad t_ {i} := \beta_ {0}+X_ {(i)}\cdot \beta.\] <p>The loss function is given by the negative log-likelihood function plus the penalty term, including a punish term:</p> \[\begin{align*} l(\beta) &amp;= -\sum_ {i}^{n} \left\lbrace y_ {i}\ln(\pi_ {i})+(1-y_ {i})\ln(1-\pi_ {i}) \right\rbrace +h(\vec{\beta})\\ &amp;= -\sum_ {i}^{n} \left\lbrace y_ {i}\ln(\text{sigm}(t_ {i} ))+(1-y_ {i})\ln(\text{sigm}(-t_ {i} )) \right\rbrace +h(\vec{\beta})\\ &amp;= -\sum_ {i} \left\lbrace y_ {i}\ln\left( \frac{\text{sigm}(t_ {i} )}{\text{sigm}(-t_ {i} )} \right) + \ln\, \text{sigm}(-t_ {i} ) \right\rbrace + h(\beta) \\ &amp;= -\sum_ {i}[y_ {i}t_ {i} -\ln(1+e^{ t_ {i} })] + h(\beta)\\ &amp;=: J(\beta)+h(\beta), \end{align*}\] <p>where in the first line we have exploit the fact that the sigmoid function is symmetric, and write $l(\vec{\beta})$ in terms of sigmoid function for future convenience. To be specific, in future work we will consider generalization for sigmoid function with Tsallis q-logarithms, so we might just write the loss functions explicitly in terms of it as well. $h(\beta)$ is the Lasso penalty term given by $\lambda \sum_ {g=1}^{G} \sqrt{ d_ {g} } \left\lVert \vec{\beta}_ {g} \right\rVert _ {2}$. $J(\beta)$ is the minus likelihood function,</p> \[\boxed{ J(\beta):= -\sum_ {i}[y_ {i}t_ {i} -\ln(1+e^{ t_ {i} })],\quad t_ {i} =\beta_ {0}+\beta \cdot X_ {(i)}. } ,\] <hr/> <p>As usual, the loss function is convex and non-linear. Since we took the absolute value of $\left\lVert \vec{\beta}_ {g} \right\rVert$, it is not smooth at $\vec{\beta}=0$. However it is not a big problem in minimization problem, since it is piece-wise smooth, we simply need to consider the behavior from both sides of the non-smooth point. In the case of a LASSO punish term, that is the origin or each $\vec{\beta}_ {g}$.</p> <p>For Group LASSO regression, the goal is to apply LASSO regularization to groups of coefficients rather than individual coefficients. This means entire groups of variables can be zeroed out together, which encourages group-wise sparsity in the model. The challenge with Group LASSO is the non-differentiability of the regularization term, similar to standard LASSO but applied to norms of groups of coefficients.</p> <p>Several methods can be used to solve the Group LASSO problem efficiently:</p> <ol> <li> <p>Coordinate Descent: This method can still be adapted for Group LASSO by modifying it to work on groups of coefficients rather than individual coefficients. In this context, the update step involves a group soft-thresholding operation, which is a generalization of the soft-thresholding used in standard LASSO.</p> </li> <li> <p><strong>Proximal Gradient Methods</strong>: These are particularly well-suited for Group LASSO due to their ability to handle non-smooth terms efficiently. The proximal operator for the Group LASSO regularization term can be defined explicitly, allowing for straightforward implementation. This method iteratively updates the coefficients by taking a gradient step on the differentiable loss function and then applying the proximal operator to handle the group sparsity.</p> </li> <li> <p><strong>Block Coordinate Descent</strong>: This is a variant of coordinate descent that updates blocks (or groups) of variables at once rather than individual variables. It is particularly useful in Group LASSO, where you want to consider the impact of an entire group of variables together when deciding on updates.</p> </li> <li> <p>Primal-Dual Methods: These methods work by simultaneously updating primal and dual variables to find a solution that satisfies the optimality conditions for both. They can be very efficient for problems like Group LASSO, where the structure of the groups can be exploited to simplify the dual problem.</p> </li> </ol> <p>Among these methods, <strong>Proximal Gradient Methods</strong> and <strong>Block Coordinate Descent</strong> are often the most effective for solving Group LASSO due to their ability to directly handle the group-wise structure and non-smoothness of the regularization term. The choice between these methods may depend on the specific structure of your problem and the size of your data. In the following, we will go through some basic methods leading to the proximal gradient method. For a more detailed introduction see the wonderful online course <a href="https://youtu.be/Clw24Fajnqg?si=9h3i6pSmAqpamLfl">here</a>.</p> <hr/> <p><strong>Gradient Descent (GD) method</strong></p> <p>Gradient Descent is an iterative optimization algorithm used to minimize an objective function, which is often used in various statistical and machine learning methods. In biostatistics, it is commonly used for fitting models such as linear regression, logistic regression, and many others.</p> <p>Key Concepts:</p> <ol> <li> <p>Objective Function: The function you want to minimize, usually representing the error or cost associated with the model. For instance, in linear regression, it is the sum of squared errors.</p> </li> <li> <p>Gradient: The gradient of the objective function at a given point indicates the direction of the steepest ascent. Since we want to minimize the function, we move in the opposite direction of the gradient.</p> </li> <li> <p>Learning Rate: A hyperparameter that controls the size of the steps taken to reach the minimum. If it’s too large, the algorithm might overshoot the minimum; if too small, convergence might be slow.</p> </li> </ol> <p>The basic steps of the gradient descent algorithm are:</p> <ol> <li>Initialize: Start with an initial guess for the parameters.</li> <li>Compute Gradient: Calculate the gradient of the objective function with respect to the parameters.</li> <li>Update Parameters: Adjust the parameters in the opposite direction of the gradient.</li> <li>Repeat: Continue until convergence (i.e., when the changes in the parameters become very small).</li> </ol> <p>Mathematically, the update rule for the parameter $\beta$ is:</p> \[\beta^{(t+1)} = \beta^{(t)} - \alpha \nabla f(\beta^{(t)}),\] <p>where $\alpha$ is the learning rate or step length, and $\nabla f(\beta^{(t)})$ is the gradient of the objective function at $\beta^{(t)}$. The derivation of above iteration formula is not as straightforward as, e.g. Newton method, especially it is not clear where the step length comes from. Given a function $f(\beta)$ that we want to minimize, at $\beta^{(k)}$ we are actually approximating it with a quadratic functon</p> \[g(\beta) = f(\beta^{(k)}) + \nabla f(\beta^{(k)})(\beta-\beta^{(k)}) + \frac{1}{2\alpha}(\beta-\beta^{(k)})^{2},\] <p>note we have introduced the step-size parameter $\alpha$ to control the strength of the quadratic term. To find the minimum of $g(\beta)$ around $\beta^{(k)}$, we ask its derivative to be zero, leading to</p> \[g'(\beta) = \nabla f(\beta^{(k)}) + \frac{1}{\alpha} (\beta-\beta^{(k)}) =0\] <p>whose solution reads</p> \[\Delta \beta := \beta-\beta^{(k)} = -\alpha \nabla f(\beta^{(k)}).\] <p>We use $\Delta \beta$ to update $\beta$, namely $\beta^{(k+1)}:= \beta^{(k)}+\Delta \beta$. That’s where the step size parameter comes from. In practice, we usually choose $\alpha=\nabla^{2}f(\beta^{k})$. To make the iteration procedure convergent, there is usually an upper limit for $\alpha$, which is clearly shown in the example of $f(\beta)=\beta^{2}-3\beta+4$, which I’ll skip here, interested readers can take it as a homework.</p> <p>Let’s go through a simple example of fitting a linear regression model using gradient descent. Suppose we have a dataset with $n$ observations and one predictor. The objective function (mean squared error) is:</p> \[f(\beta_ 0, \beta_ 1) = \frac{1}{2n} \sum_ {i=1}^n (y_ i - (\beta_ 0 + \beta_ 1 x_ i))^2,\] <p>where $(x_ i, y_ i)$ are the data points, and $\beta_ 0$ and $\beta_ 1$ are the parameters to be estimated.</p> <ol> <li>Gradient Calculation: <ul> <li>The gradient with respect to $\beta_ 0$ is:</li> </ul> </li> </ol> \[\frac{\partial f}{\partial \beta_ 0} = -\frac{1}{n} \sum_ {i=1}^n (y_ i - (\beta_ 0 + \beta_ 1 x_ i)).\] <ul> <li>The gradient with respect to $\beta_ 1$ is:</li> </ul> \[\frac{\partial f}{\partial \beta_ 1} = -\frac{1}{n} \sum_ {i=1}^n x_ i (y_ i - (\beta_ 0 + \beta_ 1 x_ i)).\] <ol> <li>Update Rules: <ul> <li>Update $\beta_ 0$:</li> </ul> </li> </ol> \[\beta_ 0^{(t+1)} = \beta_ 0^{(t)} - \alpha \frac{\partial f}{\partial \beta_ 0}.\] <ul> <li>Update $\beta_ 1$:</li> </ul> \[\beta_ 1^{(t+1)} = \beta_ 1^{(t)} - \alpha \frac{\partial f}{\partial \beta_ 1}.\] <hr/> <p><strong>Subgradient Method</strong></p> <p>The subgradient method is an optimization technique used for minimizing non-differentiable convex functions. It is a generalization of the gradient descent method. Instead of using gradients, it uses subgradients, which are generalizations of gradients for non-differentiable functions. A very nice introduction of subgradients and subdifferential can be found <a href="https://web.stanford.edu/class/ee364b/lectures/subgradients_notes.pdf">here</a>, in the following we only outline the key concepts.</p> <p>Key Concepts:</p> <ol> <li>Subgradient: For a convex function $f$ at a point $x$, a vector $g$ is a subgradient if:</li> </ol> \[f(y) \geq f(x) + g^T (y - x) \quad \text{for all } y \in \mathbb{R}^n.\] <ol> <li> <p>Subdifferential: The set of all subgradients at $x$, denoted $\partial f(x)$.</p> </li> <li> <p>Update Rule: The algorithm iteratively updates the solution using</p> </li> </ol> \[x^{(k+1)} = x^{(k)} - \alpha_ k g^{(k)},\] <p>where $g^{(k)} \in \partial f(x^{(k)})$ and $\alpha_ k$ is the step size.</p> <p>Example: Minimizing $f(x) = \left\lvert x \right\rvert$</p> <ol> <li> <p>Function Definition: $f(x) = \left\lvert x \right\rvert$</p> </li> <li>Subgradient Calculation: <ul> <li>For $x &gt; 0$, $\partial f(x) = { 1 }$</li> <li>For $x &lt; 0$, $\partial f(x) = { -1 }$</li> <li>For $x = 0$, $\partial f(x) = { t \mid -1 \leq t \leq 1 }$</li> </ul> </li> <li>Subgradient Method Steps: <ul> <li>Initialize $x^{(0)} = 2$</li> <li>Choose a step size $\alpha = 0.1$</li> <li>Iterate the update rule:</li> </ul> </li> </ol> \[x^{(k+1)} = x^{(k)} - \alpha \cdot g^{(k)}\] <ul> <li> <p>Suppose $g^{(k)} = 1$ for simplicity.</p> </li> <li> <p>First iteration:</p> </li> </ul> \[x^{(1)} = 2 - 0.1 \cdot 1 = 1.9\] <ul> <li>Second iteration:</li> </ul> \[x^{(2)} = 1.9 - 0.1 \cdot 1 = 1.8\] <ul> <li>Continue iterating until $x$ converges to 0.</li> </ul> <p>However, there might be some convergence problem in the last step. Instead of converging to the actual minimum $x=0$, the iteration might oscillate around it. In this case, we will need to look for the point where the subdifferential includes zero. This is the generalization of the familiar method of finding the minimum by looking for the zero-gradient point, only that the relation gradient=0 is replaced by $0 \in$ subdifferential. To be specific, a point $x^\ast$ is a minimizer of a function $f(x)$ (no necessarily convex) iff $f(x)$ is subdifferentiable at $x^\ast$ and</p> \[0 \in \partial f(x^\ast ),\] <p>i.e. $0$ is a subgradient at $x^\ast$. Using this method it is easy to see that $x=0$ is indeed the minimizer of $\left\lvert x \right\rvert$.</p> <hr/> <p><strong>Proximal gradient methods</strong></p> <p>Proximal gradient methods are a generalized form of projection used to solve non-differentiable <a href="https://en.wikipedia.org/wiki/Convex_optimization" title="Convex optimization">convex optimization</a> problems.</p> <p>The function we want to minimize is a composite function</p> \[f(x) = g(x) + h(x),\] <p>where $g(x)$ is supposed to be differentiable while $g(x)$ is not. For example, $h(x)$ could be the LASSO punish term $\left\lVert x \right\rVert_ {2}$. Since $h(x)$ is not differential, we can’t use the gradient descent method directly to $g+h$. However, we can use $g(x)$ to define the gradient method, then somehow include the effect of $h(x)$. The $g$-gradient descent iteration will decrease the value of $g(x)$, then we do something to also decrease the value of $h(x)$, thus the final result will decrease both $g(x)$ and $h(x)$. That’s the general idea of proximal gradient methods, we will explain the word proximal later.</p> <p>Say we start with some intermediate value $x^{(k)}$. First let’s consider $g(x)$ only, and define the quadratic approximation function $\overline{g}^{(l)}(x)$ of $g(x)$ at $x^{(k)}$, that is we use a quadratic function $\overline{g}^{(k)}$ (which is concave) to approximate $g$ around $x^{(k)}$, $\overline{g}$ is constructed such that it is tangent to $g$ at $x^{(k)}$. Specifically,</p> \[\overline{g}^{(k)}(z) = g(x^{(k)}) + (z-x^{(k)}) \nabla g(x^{(k)}) + \frac{1}{2\alpha} (z-x^{(k)})^{2}.\] <p>Gradient descent method using $g(x)$ alone will update it as</p> \[x^{(k+1)}_ {g} := \text{arg min}_ {z}\, \overline{g}^{(k)}(z) = x^{(k)} - \alpha \nabla g(x^{(k)}).\] <p>This is the first step. In the next step, we take into consideration $h(x)$ function and minimize the so-called composite function, which is the sum of the quadratic approximation $\overline{g}$ and the non-differentiable function $h(x)$, the result would be $x^{(k+1)}$, the real $(k+1)$-th iteration:</p> \[x^{(k+1)} := \text{arg min}_ {z}\left\lbrace \overline{g}^{(k)}(z) + h(z) \right\rbrace .\] <p>The logic is</p> \[x^{(k)} \to x^{(k+1)}_ {g} \to x^{(k+1)}.\] <p>I hope it is intuitive. Now let’s try to solve it,</p> \[\begin{align*} x^{(k+1)} &amp;:= \text{arg min}_ {z}\left\lbrace \overline{g}^{(k)}(z) + h(z) \right\rbrace \\ &amp;= \text{arg min}_ {z}\left\lbrace g(x^{(k)}) + (z-x^{(k)}) \nabla g(x^{(k)}) + \frac{1}{2\alpha} (z-x^{(k)})^{2} + h(z) \right\rbrace \\ &amp;= \text{arg min}_ {z}\left\lbrace \frac{1}{2\alpha}[z^{2}-2z(x^{(k)}-\alpha \nabla g)+2 \alpha (g- x^{(k)} \nabla g)] + h(z) \right\rbrace \\ &amp;= \text{arg min}_ {z}\left\lbrace \frac{1}{2\alpha}([z-(x^{(k)}-\alpha\nabla g )]^{2} + C_ {1}) + h(z) \right\rbrace \\ &amp;= \text{arg min}_ {z}\left\lbrace \frac{1}{2\alpha}[z-x^{(k+1)}_ {g}]^{2} + h(z) \right\rbrace, \end{align*}\] <p>where in the fourth line we completed the square and in the last line we got rid of a constant term (independent of $z$) in $\text{arg min}$, since adding to a function a constant term does not change the minimizer of it.</p> <p>Notice that the form of $x^{(k+1)}$ is very similar to the gradient descent method with step size $\alpha$. Regarding $x^{(k+1)}$ we are minimizing some function plus $\frac{1}{2\alpha}(z-x_ {g}^{(k)})^{2}$, while in the gradient descent method where we are minimizing some <strong>linear</strong> function plus $\frac{1}{2\alpha}(z-x^{k})^{2}$. This inspires us to look at $x^{(k+1)}$ with another perspective: we are trying to minimize $h(z)$, in the mean while <strong>trying to stay close to</strong> $x_ {g}^{(k+1)}$. In other words, we are looking for the minimizer of $h(z)$ which is also <strong>proximal</strong> to $x^{(k+1)}_ {g}$. Now it is a good time to introduce <code class="language-plaintext highlighter-rouge">proximal operator</code> $\text{prox}_ {h,\alpha}(x)$, which aims to find the minimizer of function $h$, with step length $\alpha$, proximal to $x$,</p> \[\boxed{ \text{prox}_ {h,\alpha}(x) := \text{arg min}_ {z} \left\lbrace \frac{1}{2\alpha} \left\lVert x-z \right\rVert _ {2}^{2}+h(z) \right\rbrace . }\] <p>The <code class="language-plaintext highlighter-rouge">proximal gradient method</code> works as follows. Given the composite function $f=g+h$ where $g$ is differentiable and $h$ is not. We choose initial value $x^{(0)}$ of $x$, iterate it by repeating</p> \[x^{(k)} := \text{prox}_ {h,\alpha^{(k)}}(x^{(k+1)}_ {g}), \quad x_ {g}^{(k+1)} := x^{(k)} - \alpha^{(k)} \nabla g(x^{(k)}),\] <p>where $\alpha^{(k)}$ is the $k$-th step length.</p> <p>As an example, let’s start with a simple problem that we already know how to solve, apply the proximal gradient method and see how it works. Say we want to minimize a function $f(\beta)$ with only one variable $\beta$,</p> \[f(\beta) = \frac{1}{2} \beta^{2} + \left\lvert \beta-1 \right\rvert .\] <p>The plot is given in the below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/compositeFunction-480.webp 480w,/img/compositeFunction-800.webp 800w,/img/compositeFunction-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/compositeFunction.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The test function we want to minimize with proximal gradient method. </div> <p>The minimum is at $\beta=1$, as $0 \in \partial f(\beta){\Large\mid}_ {\beta=1}$. Now let’s try to use the proximal gradient method to solve it. Let us start with</p> \[\beta^{(0)} = 0,\] <p>Use the gradient method with $h(x)=\frac{1}{2} \beta^{2}$ to update the result only, we get</p> \[\beta_ {h}^{(1)} = \beta^{0}-\alpha \nabla h(0) = \beta^{(0)} = 0,\] <p>this is because $h(\beta)$ has zero derivative at $\beta=0$. Next we use proximal operator to update $\beta_ {h}^{(1)}$,</p> \[\begin{align*} \beta^{(1)} &amp;= \text{prox}_ {\left\lvert \bullet-1 \right\rvert,\alpha }(\beta^{(1)}_ {g}) \\ &amp;=\text{arg min}_ {z} \left\lbrace \frac{1}{2\alpha}(z-\beta^{(1)}_ {g})^{2}+\left\lvert z-1 \right\rvert \right\rbrace \\ &amp;= \text{arg min}_ {z} \left\lbrace \frac{1}{2\alpha}z^{2}+\left\lvert z-1 \right\rvert \right\rbrace. \end{align*}\] <p>Now we can use the sub-gradient method to find the minimum. The subgradient reads</p> \[\partial \left( \frac{1}{2\alpha}z^{2}+\left\lvert z-1 \right\rvert \right)= \begin{cases} \frac{z}{\alpha}+1 &amp; z&gt;1 \\ [ \frac{1}{\alpha}-1, \frac{1}{\alpha}+1] &amp; z=1 \\ \frac{z}{\alpha}-1 &amp; z&lt;1 \end{cases}\] <p>it is clear that for any $\alpha$ satisfying $\frac{1}{\alpha}-1&lt;0$ and $\frac{1}{\alpha}+1&gt;0$, zero subgradient is obtained at $z=1$. That is to say, for any $\alpha&gt;0$m we gave</p> \[\beta^{(1)} =1\] <p>which is precisely the result we are looking for. Along the way we have also found a constraint for the value of step size $\alpha$.</p> <hr/> <p>Another thing which might be useful is the so-called <code class="language-plaintext highlighter-rouge">soft-thresholding</code> operator, defined as:</p> \[S_ {\lambda}(x) = \text{sign}(x) \cdot \max(\left\lvert x \right\rvert - \lambda, 0)\] <p>where $\lambda$ is a non-negative threshold parameter, $\text{sign}(x)$ returns the sign of $x$, and $\max(\lvert x\rvert - \lambda, 0)$ essentially shrinks $x$ towards zero by $\lambda$, setting it to zero if $x$ is within $\lambda$ of zero.</p> <p>To minimize this function using the soft-thresholding operator, we need to follow these steps:</p> \[S_ {\lambda}(\theta) = \text{sign}(\theta) \cdot \max(\left\lvert \theta \right\rvert - \lambda, 0)\] <hr/> <p>For future use, we need to find the Hessian of the loss function $J(\beta)$ from logistic regression, we need to compute the second-order partial derivatives of $J(\beta)$ with respect to the components of the parameter vector $\beta$. Given</p> \[J(\beta) := -\sum_ {i} \left[ y_ i t_ i - \ln(1 + e^{t_ i}) \right], \quad t_ i = \beta_ 0 + \beta \cdot X_ {(i)}.\] <p>where</p> \[t_ i = \beta_ 0 + \sum_ {j=1}^{p} \beta_ j X_ {ij}\] <p>and $\beta = (\beta_ 1, \beta_ 2, \ldots, \beta_ p)$, $X_ {(i)} = (X_ {i1}, X_ {i2}, \ldots, X_ {ip})$.</p> <p>First, we compute the gradient of $J(\beta)$ with respect to $\beta_ 0$ and $\beta_ j$.</p> \[\frac{\partial J(\beta)}{\partial \beta_ 0} = -\sum_ {i} \left[ y_ i - \frac{e^{t_ i}}{1 + e^{t_ i}} \right].\] <p>Let $p_ i = \frac{e^{t_ i}}{1 + e^{t_ i}} = \sigma(t_ i)$, the sigmoid function. Then,</p> \[\frac{\partial J(\beta)}{\partial \beta_ 0} = -\sum_ {i} \left[ y_ i - p_ i \right].\] \[\frac{\partial J(\beta)}{\partial \beta_ j} = -\sum_ {i} \left[ y_ i X_ {ij} - \frac{e^{t_ i}}{1 + e^{t_ i}} X_ {ij} \right].\] <p>Using $p_ i = \sigma(t_ i)$, this becomes:</p> \[\frac{\partial J(\beta)}{\partial \beta_ j} = -\sum_ {i} \left[ y_ i X_ {ij} - p_ i X_ {ij} \right] = -\sum_ {i} X_ {ij} \left[ y_ i - p_ i \right].\] <p>Now we compute the second-order partial derivatives to form the Hessian matrix.</p> <p>Second partial derivative with respect to $\beta_ 0$:</p> \[\frac{\partial^2 J(\beta)}{\partial \beta_ 0^2} = -\sum_ {i} \left[ - \frac{\partial p_ i}{\partial t_ i} \frac{\partial t_ i}{\partial \beta_ 0} \right] = \sum_ {i} p_ i (1 - p_ i).\] <p>Mixed partial derivative with respect to $\beta_ 0$ and $\beta_ j$:</p> \[\frac{\partial^2 J(\beta)}{\partial \beta_ 0 \partial \beta_ j} = -\sum_ {i} \left[ - \frac{\partial p_ i}{\partial t_ i} \frac{\partial t_ i}{\partial \beta_ j} \right] = \sum_ {i} p_ i (1 - p_ i) X_ {ij}.\] <p>Second partial derivative with respect to $\beta_ j$ and $\beta_ k$:</p> \[\begin{align*} \frac{\partial^2 J(\beta)}{\partial \beta_ j \partial \beta_ k} &amp;= -\sum_ {i} \left[ - \frac{\partial p_ i}{\partial t_ i} \frac{\partial t_ i}{\partial \beta_ j} \frac{\partial t_ i}{\partial \beta_ k} \right] = \sum_ {i} p_ i (1 - p_ i) X_ {ij} X_ {ik}\\ &amp;= (X^{T} Q X)_ {jk} \end{align*}\] <p>where $Q_ {mn} = \delta_ {mn}p_ {m}(1-p_ {m})$.</p> <p>The full Hessian matrix $H$ is composed of these second-order partial derivatives. It can be written in block form as follows:</p> \[H = \begin{bmatrix} \frac{\partial^2 J(\beta)}{\partial \beta_ 0^2} &amp; \frac{\partial^2 J(\beta)}{\partial \beta_ 0 \partial \beta_ 1} &amp; \cdots &amp; \frac{\partial^2 J(\beta)}{\partial \beta_ 0 \partial \beta_ p} \\ \frac{\partial^2 J(\beta)}{\partial \beta_ 1 \partial \beta_ 0} &amp; \frac{\partial^2 J(\beta)}{\partial \beta_ 1^2} &amp; \cdots &amp; \frac{\partial^2 J(\beta)}{\partial \beta_ 1 \partial \beta_ p} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial^2 J(\beta)}{\partial \beta_ p \partial \beta_ 0} &amp; \frac{\partial^2 J(\beta)}{\partial \beta_ p \partial \beta_ 1} &amp; \cdots &amp; \frac{\partial^2 J(\beta)}{\partial \beta_ p^2} \end{bmatrix}.\] <p>Using the derived second-order partial derivatives, the Hessian matrix can be written as:</p> \[H = \sum_ {i} p_ i (1 - p_ i) \begin{bmatrix} 1 &amp; X_ {i1} &amp; X_ {i2} &amp; \cdots &amp; X_ {ip} \\ X_ {i1} &amp; X_ {i1}^2 &amp; X_ {i1}X_ {i2} &amp; \cdots &amp; X_ {i1}X_ {ip} \\ X_ {i2} &amp; X_ {i2}X_ {i1} &amp; X_ {i2}^2 &amp; \cdots &amp; X_ {i2}X_ {ip} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ X_ {ip} &amp; X_ {ip}X_ {i1} &amp; X_ {ip}X_ {i2} &amp; \cdots &amp; X_ {ip}^2 \end{bmatrix}.\] <h2 id="42-with-tsallis-statistics">4.2 With Tsallis statistics</h2> <p>There exist tremendous potential in applying Tsallis entropy to biostatistics, mostly for two reasons:</p> <ol> <li>Tsallis statistics modifies logarithmic and exponential functions, and they are ubiquitous in logistic regression methods;</li> <li>Tsallis entropy can seamlessly generalize the familiar Shannon entropy, which is closely connected to the cross entropy loss function, which is exactly the loss functions we used in regression method.</li> </ol> <p>As a starter, in this note we will only focus on the generalization of sigmoid function, keeping other components unchanged. But first, let’s repeat the definition for Tsallis-modified log and exponential functions, so-called $q$-log and $q$-exponentials, for future convenience:</p> \[\begin{align*} \log_ {q} (x) := &amp; \frac{x^{1-q}-1}{1-q}, \\ \exp_ {q}(x) :=&amp; (1+(1-q))^{1/(1-q)}. \end{align*}\] <p>As you can check, at the limit $q\to 1$ they regress to normal log and exp functions.</p> <p><strong>q-sigmoid of the first kind</strong></p> <p>The regular sigmoid function reads</p> \[\sigma(z) = \frac{1}{1+e^{ -z }},\] <p>we generalized it to $q$-sigmoid function:</p> \[\boxed{ \sigma_ {q}(z) := \frac{1}{1+\exp_ {q}(-z)}. }\] <p>To ensure that the range is $[0,1]$ for all $q$, we might need to modified it a little bit, for example multiply by a constant or do some cut-off.</p> <p>The $q$-sigmoid function satisfy the following relation:</p> \[\frac{ \partial \sigma_ {q}(z) }{ \partial z } = \beta(q,z)\sigma_ {q}(z)(1-\sigma_ {q}(z))\] <p>where</p> \[\beta(q,z) = \frac{1}{1-z(1-q)}.\] <p>This should be compared to the case of regular sigmoid function:</p> \[\frac{ \partial \sigma(z) }{ \partial z } = \sigma(z) (1-\sigma(z)).\] <p>The cross entropy loss function now reads</p> \[\boxed{ L= - \sum_ {i=1}^{n}[y_ {i}\log(\sigma _ {q_ {i} }(z_ {i} ))+(1-y_ {i})\log(1-\sigma_ {q_ {i} }(z_ {i} ))]. }\] <p>where $z_ {i}=\theta\cdot X^{(i)}$, $X^{(i)}$ the $i$-th observed data and the q-parameters $q_ {i}$ are different for each sample. Someone write it as $z_ {i}=\theta^{T}X^{(i)}$, but I neglect the transpose symbol since now both $\theta$ and $X$ are understood as vectors. For starters we can set all the $q_ {i}$ as the same constant for all samples, varying about $1$, for example from -0.2 to 1.8 or something. This will greatly reduce the number of free parameters hence prevents over fitting.</p> <p>We will use the gradient method to find the optimal values of parameters that minimized the loss function. Next we work out the derivative of loss function.</p> <p>Some straightforward derivation shows that</p> \[\frac{ \partial L(y,\sigma_ {q}(z)) }{ \partial z } = \beta_ {q}(z)(\sigma_ {q}(z)-y),\] <p>where $\beta_ {q}(z) = \frac{1}{1-z(1-q)}$. The derivative of the loss function with respect to parameters can be readily written as</p> \[\boxed{ \frac{ \partial L(y,\theta) }{ \partial \theta ^{a} } = \sum_ {i} \beta_ {q}(z) (\sigma_ {q}(z_ {i} )-y_ {i})X^{(i)}_ {a}, \quad z_ {i} =\sum_ {a}\theta^{a} \cdot X^{(i)}_ {a} = \theta \cdot X^{(i)}. }\] <p>Note that I have replace $q_ {i}$ with an universal $q$. In contrast to the classical result, there is an extra $\beta$ factor which is nonlinear in both $q$ and $z$.</p> <p>After we plugin this result to gradient method, $\beta(q,z)$ will play a similar rule as $\alpha$, hence in a sense, using $q$-sigmoid naturally introduce the learning rate! We can even set $\alpha=1$ and get</p> \[\hat{\theta}_ {(t)} = \hat{\theta}^{(t-1)} -\beta(q,z)(\sigma_ {q}(z)-y).\] <p>The learning rate $\alpha$ can be chosen according to the Lipschitz constant $L$ as</p> \[\alpha = \frac{1}{L}.\] <hr/> <p>Some preliminary results are Obtained and put in the following figure.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/tsallis/Youden1-480.webp 480w,/img/tsallis/Youden1-800.webp 800w,/img/tsallis/Youden1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/tsallis/Youden1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Youden index of various regression methods. </div> <hr/> <p><strong>q-sigmoid of the second kind</strong></p> <p>Another equivalent expression for the standard sigmoid function reads</p> \[\sigma(z) = \frac{e^{ z }}{e^{ z }+1},\] <p>substitute in the q-exponent function, we have the second kind of q-sigmoid function:</p> \[\overline{\sigma}_ {q}(z) := \frac{\exp_ {q}(z)}{\exp_ {q}+1}.\] <p>This is inequivalent to the q-sigmoid of the first kind since q-exponent satisfy</p> \[\exp_ {q}(z)\exp_ {q}(-z) = \exp_ {q}((1-q)z^{2}) \neq 1.\] <p>The second kind of q-sigmoid can be obtained in another way. The original logistic regression method works like the following: let $p(1\mid z)$ be the measured (real) probability for outcome $1$ and $\hat{p}(1\mid z)$ for the theoretical prediction. The logit function is the bridge between the microscopic information represented by $z$, and the predicted macroscopic measure result $\hat{p}(1\mid z)$, in a linear fashion:</p> \[\text{logit}(\hat{p}) = \log\left( \frac{\hat{p}}{1-\hat{p}} \right)=z.\] <p>We can directly generalize it to $q$-logit function by replacing the log with q-log,</p> \[\text{logit}_ {q}(\hat{p}) := \log_ {q}\left( \frac{\hat{p}}{1-\hat{p}} \right)=z.\] <p>Invert the relation we have</p> \[\hat{p}(1\mid z) := \frac{\exp_ {q}(z)}{\exp_ {q}(z)+1},\] <p>which is the q-sigmoid function of the second kind.</p> <p>We have</p> \[\frac{d}{dz} \overline{\sigma}_ {q}(z) = \overline{\sigma}_ {q} (z)(1-\overline{\sigma}_ {q} (z))\beta _ {q} (-z), \quad \beta _ {q} (-z)=\frac{1}{1+(1-q)z}.\] <p>Note the difference from the q-sigmoid of the first kind, where in the derivative, we have $\beta _ {q}(z)$ instead.</p> <p>The derivative of cross-entropy loss function with respect to the parameters reads</p> \[\boxed{ \frac{ \partial L }{ \partial \theta_ {a} } =\sum_ {i=1}^{n} [\overline{\sigma}_ {q} (z_ {i} )-y_ {i}] \beta _ {q} (-z_ {i} )X^{(i)}_ {a}, \quad z_ {i} =\sum_ {a}\theta_ {a}X^{(i)}_ {a}= \vec{\theta}\cdot \vec{X}^{(i)}. }\] <h1 id="appendix-useful-mathematical-formulae">Appendix. Useful Mathematical Formulae</h1> <p>The definition of $q$-logarithm and $q$-exponential, $x&gt;0, q \in\mathbb{R}$:</p> \[\begin{align*} \ln_ {q}x &amp;:= \frac{x^{1-q}-1}{1-q}, \\ e^{ x }_ {q} &amp;:= (1+(1-q)x)^{1/(1-q)},\quad 0 \text{ if } 1+(1-q)x&lt;0. \end{align*}\] <p>Note that in the definition of the exponential it is required that $1+(1-q)x&gt;0$, otherwise it is defined to be zero. This is to make sure the the value of $e_ {q}$ is positive definite. It is easily checked that $\log <em>q$ and $\exp</em> {q}$ are indeed inverse to each other.</p> <p>many formula for $q$-logarithms reminds us of that for the regular $q$-logarithms.</p> \[\begin{align*} \ln_ {q}(xy) &amp;= \ln_ {q}(x)+ \ln_ {q}(y) + (1-q)\ln_ {q}(x)\ln_ {q}(x) , \\ \ln_ {q}(1+x) &amp;= \sum_ {1}^{\infty} (-1)^{n-1} \frac{(q)_ {n}}{n!} x^{n}, \\ (q)_ {n} &amp;= \frac{\Gamma(q+k)}{\Gamma(q)} = q(q+1)\cdots(q+k-1), \\ \ln_ {q}\prod_ {k=1}^{n}x_ {k} &amp;= \sum_ {k=1}^{n} (1-q)^{k-1}\sum_ {i_ {k} &gt;\cdots&gt;i_ {1}=1}^{n} \ln_ {q}x_ {i_ {1}}\cdots\ln_ {q}x_ {i_ {k}}. \end{align*}\] <p>We also have</p> \[\begin{align*} \ln_ {q} x &amp;= x^{1-q}\ln_ {2-q}x , \\ q \ln_ {1-q}x^{a} &amp;= a \ln_ {1-a} x^{q} . \end{align*}\] <p>Regarding the $q$-exponentials,</p> \[\begin{align*} \left( e_ {q}^{f(x)} \right) ^{a} &amp;= e^{ af(x) }_ {1-(1-q)/a},\\ \frac{d}{dx} e_ {q}^{f(x)} &amp;= (e_ {q}^{f(x)})^{q} \times f'(x) \end{align*}\] <p>For more details, refer to the textbook by Tsallis himself and <a href="https://doi.org/10.1016/S0378-4371(01)00567-2" title="Persistent link using digital object identifier">https://doi.org/10.1016/S0378-4371(01)00567-2</a>.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="tsallis"/><summary type="html"><![CDATA[1. Introduction 1.1. The Basics of Boltzmann-Gibbs Extensive Entropy 1.2. Generalization to non-Extensive entropy 2. Boltzmann-Gibbs Statistical Mechanics 2.1. Three different forms of BG entropy 2.2. Properties of BG entropy 2.3. Constraints and Entropy Optimization 3. Nonextensive Statistical Mechanics 3.1. Mean Value in Tsallis Statistics 4. Tsallis in Logistic Regression Methods 4.1 Traditional logistic regression method 4.2 With Tsallis statistics Appendix. Useful Mathematical Formulae]]></summary></entry><entry><title type="html">Calculating States Using Diagrams</title><link href="https://baiyangzhang.github.io/blog/2024/Calculating-States-Using-Diagrams/" rel="alternate" type="text/html" title="Calculating States Using Diagrams"/><published>2024-10-13T00:00:00+00:00</published><updated>2024-10-13T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Calculating-States-Using-Diagrams</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Calculating-States-Using-Diagrams/"><![CDATA[<h1 id="rules-for-hamiltonians">Rules for Hamiltonians</h1> <h2 id="rules-for-h_-3-">Rules for $H_ {3}^{(-)}$</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/H3Rules-480.webp 480w,/img/kink/H3Rules-800.webp 800w,/img/kink/H3Rules-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/H3Rules.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 1. Rules for various components of $H_3$. I regard them as vertices. The meaning of graphical elements are explain later. </div> <h2 id="rules-for-h_-4-">Rules for $H_ {4}^{(-)}$</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/H4Rules-480.webp 480w,/img/kink/H4Rules-800.webp 800w,/img/kink/H4Rules-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/H4Rules.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 2. Rules for various components of $H_4$. </div> <h2 id="rules-for-h_-5-">Rules for $H_ {5}^{(-)}$</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/H5Rules-480.webp 480w,/img/kink/H5Rules-800.webp 800w,/img/kink/H5Rules-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/H5Rules.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 3. Rules for various components of $H_5$. The coefficients $C_ {5,3}$ and $C_ {5,1}$ are made of counter terms. In specific we have $C_ {5,3}= \frac{m\delta g}{\sqrt{2}} +\frac{g \delta m_ {1}^{2}}{2\sqrt{2}m} - \frac{m^{2}\delta m_ {1}^{4}}{2\sqrt{2}g}$ and $C_ {5,1}= m^{2}\delta v_ {3}-\frac{3gm\mathcal{I}_ {1}}{\sqrt{2}}+\frac{m^{3}\delta g^{2}}{\sqrt{2}g^{3}}-\frac{m \delta g\delta m_ {1}^{2}}{2\sqrt{2}g^{2}} -\frac{\delta m_ {1}^{4}}{8\sqrt{2}gm}$. </div> <h1 id="diagrams-for-states">Diagrams for States</h1> <h2 id="leading-order">Leading Order</h2> <p>At leading order there is one vacuum state correction and two momentum eigenstate corrections, their diagrammatic expressions are given in the figure below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/LOStates-480.webp 480w,/img/kink/LOStates-800.webp 800w,/img/kink/LOStates-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/LOStates.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 4. States at leading order. The first panel gives the leading order correction to the vacuum state, while the second and third panel gives the leading order correction to momentum states. </div> <h2 id="next-leading-order">Next Leading Order</h2> <p>The fundamental diagrams for $\left\lvert \vec{p} \right\rangle_ {2}^{(-)}$ are shown in the below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/p2Fundamental-480.webp 480w,/img/kink/p2Fundamental-800.webp 800w,/img/kink/p2Fundamental-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/p2Fundamental.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 5. Fundamental diagrams of $g^2$ corrections to $\left\lvert \vec{p} \right\rangle_ {2}^{(-)}$ States. By fundamental I mean the diagrams directly built up block-by-block by Hamiltonians and lower order states. Fundamental diagrams are good for study the cancelation between loops and counter terms. After the renormalization procedure is done, based on the number of extarnal legs, the fundamental diagrams can ben summed over to give more concise, summarized diagrams. </div> <p>Let’s number the diagrams left to right, top to bottom. The contributions are</p> \[\begin{align*} (1) :\;&amp; H_ {4}^{(4)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} = \frac{g^{2}}{4} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \left\lvert \vec{p},\vec{p}_ {1,2,3,-1-2-3} \right\rangle_ {0} , \\ (2) :\;&amp; H_ {3}^{(3)} \left\lvert \vec{p} \right\rangle_ {1}^{(2)} = \frac{3g^{2}m^{2}}{4\omega_ {p}}\int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \frac{\left\lvert \vec{p}-\vec{p}_ {3}, \vec{p}_ {1,2,3,-1-2} \right\rangle_ {0}}{\omega _ {p}-\omega_ {3}-\omega_ {p-p_ {3}}} , \\ (3) :\;&amp; H_ {3}^{(3)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)} = -\frac{m^{2}g^{2}}{2} \int \frac{d^{3}p_ {1,2,3,4}}{(2\pi)^{12}} \, \frac{\left\lvert \vec{p},\vec{p}_ {1,2,3,4,-1-2,-3-4} \right\rangle_ {0}}{\omega_ {1}+\omega_ {2}+\omega_ {1+2}}, \\ (4) :\;&amp; H_ {3}^{(1)} \left\lvert \vec{p} \right\rangle_ {1}^{(4)} \supset - \frac{3g^{2}m^{2}}{4} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \frac{3\left\lvert \vec{p},\vec{p}_ {1,2,3,-1-2-3} \right\rangle_ {0}}{\omega_ {2+3}(\omega_ {1}+\omega_ {2+3}+\omega_ {1+2+3})} , \\ (5) :\;&amp; H_ {3}^{(1)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)} = - \frac{9g^{2}m^{2}}{4\omega_ {p}} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p}-\vec{p}_ {1}-\vec{p}_ {2},\vec{p}_ {1,2} \right\rangle_ {0}}{\omega_ {1+2}(-\omega _ {p} +\omega_ {p-p_ {1}-p_ {2}}+\omega_ {1+2})} , \\ (6) :\;&amp; H_ {3}^{(1)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)} \supset - \frac{3g^{2}m^{2}}{4} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \frac{\left\lvert \vec{p}_ {1,2,3,-1-2},\vec{p}-\vec{p}_ {3} \right\rangle_ {0}}{\omega_ {p}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} , \\ (7) :\;&amp; H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)} = -\frac{9g^{2}m^{2}}{8\omega _ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p} \right\rangle_ {0}}{\omega_ {1}\omega_ {p+p_ {1}}(-\omega _ {p} +\omega_ {1}+\omega_ {p+p_ {1}})}, \\ (8) :\;&amp; H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)} \supset - \frac{9m^{2}g^{2}}{4\omega _ {p} } \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p}-\vec{p}_ {1}-\vec{p}_ {2},\vec{p}_ {1,2} \right\rangle_ {0}}{\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \\ (9) :\;&amp; H_ {3}^{(-3)}\left\lvert p \right\rangle_ {1}^{(4)} \supset - \frac{3}{8}g^{2}m^{2}\left\lvert \vec{p} \right\rangle_ {0}\int d^{3}x \, \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{1}{\omega_ {1}\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \\ (10) :\;&amp; - H_ {3}^{(-3)}\left\lvert p \right\rangle_ {1}^{(4)} \supset \frac{9g^{2}m^{2}\left\lvert \vec{p} \right\rangle_ {0}}{8\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{1}{\omega_ {1}\omega_ {p+p_ {1}}(\omega _ {p} +\omega_ {1}+\omega_ {p+p_ {1}})} \\ (11) :\;&amp; H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)} \supset - \frac{9m^{2}g^{2}}{4} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p},\vec{p}_ {1,-1} \right\rangle_ {0}}{\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \\ (12) :\;&amp; H_ {4}^{(0)}\left\lvert p \right\rangle_ {0}^{(1)} \supset \int d^{3}x \, A_ {4}' ,\\ (13) : \;&amp; H_ {4}^{(2)}\left\lvert p \right\rangle_ {0}^{(1)} \supset \frac{g^{2}}{2\omega _ {p} }\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \left\lvert \vec{p}_ {1,2},\vec{p}-\vec{p}_ {1}-\vec{p}_ {2} \right\rangle, \\ (14) : \;&amp; H_ {4}^{(2)}\left\lvert p \right\rangle_ {0}^{(1)} \supset - \frac{\delta m^{2}}{2}\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}_ {1,-1},\vec{p}\right\rangle ,\\ (15) : \;&amp; H_ {4}^{(0)}\left\lvert p \right\rangle_ {0}^{(1)} \supset - \frac{\delta m^{2}}{4\omega p^{2}}\left\lvert \vec{p} \right\rangle. \end{align*}\] <p>But in order to turn them into diagrammatic rules for corresponding Hilbert states. We need to inverse $(\omega _ {p}-H_ {2})$ to get components of $\left\lvert \vec{p} \right\rangle_ {2}$, eliminate the integral measures and deal with the creation operators. Note that <em>the 1-meson states will eventually cancel out</em>. Furthermore, we strip-off the propagators in the diagram since we are regarding them as a vertex as a whole, as shown below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/p2FundamentalDiagrams-480.webp 480w,/img/kink/p2FundamentalDiagrams-800.webp 800w,/img/kink/p2FundamentalDiagrams-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/p2FundamentalDiagrams.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 6. Diagrams for order $\mathcal{O}(g^2)$ diagrams as vertices. </div> <p><strong>As diagrammatic rules we get</strong>:</p> \[\begin{align*} (1) :\;&amp; H_ {4}^{(4)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} : -\frac{g^{2}}{4(\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2+3})} , \\ (2) :\;&amp; H_ {3}^{(3)} \left\lvert \vec{p} \right\rangle_ {1}^{(2)} :\frac{3g^{2}m^{2}}{4\omega_ {p}} \frac{1}{(-\omega _ {p}+\omega_ {3}+\omega_ {p-p_ {3}})(-\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2}+\omega_ {p-p_ {3}})} , \\ (3) :\;&amp; H_ {3}^{(3)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)}:\frac{m^{2}g^{2}}{2(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {4}+\omega_ {1+2}+\omega_ {3+4})}, \\ (4) :\;&amp; H_ {3}^{(1)} \left\lvert \vec{p} \right\rangle_ {1}^{(4)} : \frac{9g^{2}m^{2}}{4\omega_ {2+3}(\omega_ {1}+\omega_ {2+3}+\omega_ {1+2+3})(\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2+3})} , \\ (5) : \;&amp; H_ {3}^{(1)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)} : \frac{9g^{2}m^{2}}{4\omega_ {p}}\frac{1}{\omega_ {1+2}(-\omega _ {p} +\omega_ {p-p_ {1}-p_ {2}}+\omega_ {1+2})(-\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {p-p_ {1}-p_ {2}})} , \\ (6) :\;&amp; H_ {3}^{(1)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)} : \frac{3g^{2}m^{2}}{4\omega_ {p}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(-\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2}+\omega_ {p-p_ {3}})} , \\ (8) :\;&amp; H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)} : \frac{9m^{2}g^{2}}{4\omega _ {p} } \, \frac{1}{\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(-\omega _ {p} +\omega_ {p-p_ {1}-p_ {2}}+\omega_ {1}+\omega_ {2})}, \\ (11) :\;&amp; H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {1}^{(8)} : \frac{9m^{2}g^{2}}{8} \frac{1}{\omega_ {1}\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} ,\\ (13): \;&amp; H_ {4}^{(2)}\left\lvert \vec{p} \right\rangle_ {0}^{(1)} : - \frac{g^{2}}{2\omega _ {p} } \frac{1}{(-\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {p-p_ {1}-p_ {2}})}, \\ (14): \;&amp; H_ {4}^{(2)}\left\lvert \vec{p} \right\rangle_ {0}^{(1)} : \frac{\delta m^{2}}{4\omega_ {1}}. \end{align*}\] <h2 id="3rd-order-states">3rd Order States</h2> <p>Let’s start with momentum states.</p> <h3 id="leftlvert-vecp-rightrangle_-32">$\left\lvert \vec{p} \right\rangle_ {3}^{(2)}:$</h3> <p>$H_ {3}^{(-3)}\left\lvert \vec{p} \right\rangle_ {2}^{(5)}$:</p> <p>It includes four contributions: $H_ {3}^{(-3)}$ acting on panel (1), (2), (4) and (6) from figure 6.</p> <p>The contributions $H_ {3}^{(-3)}\cdot(1)$ are shown in Figure 7 by panels (a1,a2). We have</p> \[\begin{align*} (a1) =&amp; \frac{3mg^{3}}{4\sqrt{2}} \left\lvert \vec{p},0 \right\rangle \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{1}{\omega_ {1}\omega_ {2}\omega_ {1+2}(m+\omega_ {1}+\omega_ {2}+\omega_ {1+2})}, \\ (a2) =&amp; \frac{9mg^{3}}{8\sqrt{2}\omega _ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \frac{1}{\omega_ {2}\omega_ {p+2}(\omega_ {1}+\omega_ {2}+\omega_ {p-1}+\omega_ {p+2})} . \end{align*}\] <p>The components of $H_ {3}^{(-3)}\cdot(2)$ is shown in panel (b1,b2,b3):</p> \[\begin{align*} (b 1) =&amp; - \frac{27m^{3}g^{3}}{8\sqrt{2}\omega _ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {1}\omega_ {2}\omega_ {1-2}} \\ &amp; \times \frac{1}{(-\omega _ {p} +\omega_ {1}+\omega_ {p-p_ {1}})(-\omega _ {p}+2\omega_ {1} +\omega_ {2}+\omega_ {1+2}+\omega_ {p-p_ {1}})}, \\ (b 2) =&amp; - \frac{27m^{3}g^{3}}{16\sqrt{2}\omega _ {p} ^{2}} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {2}\omega_ {p-p_ {2}}} \\ &amp;\times \frac{1}{(-\omega _ {p} +\omega_ {2}+\omega_ {p-p_ {2}})(\omega_ {1}+\omega_ {2}+\omega_ {p-p_ {1}}+\omega_ {p-p_ {2}})} , \\ (b 3) =&amp; - \frac{9m^{3}g^{3}}{16\sqrt{2}\omega _ {p} }\int d^{3}x \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2,3}}{(2\pi)^{6}} \, \frac{1}{\omega_ {2}\omega_ {3}\omega_ {2+3}} \\ &amp;\times \frac{1}{(-\omega _ {p} +\omega_ {1}+\omega_ {p-p_ {1}})(-\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {2+3}+\omega_ {p-p_ {1}})}. \end{align*}\] <p>Next we calculate $H_ {3}^{(-3)}\cdot(4)$, given in panel (c1,c2,c3) in figure 6:</p> \[\begin{align*} (c 1) =&amp; - \frac{81m^{3}g^{3}}{8\sqrt{2}} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3} p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {1}\omega_ {2}\omega^{2}_ {1+2}} \\ &amp;\times \frac{1}{(\omega_ {1}+\omega_ {1+2}+\omega_ {2p_ {1}+2})(\omega_ {1}+\omega_ {1+2}+\omega_ {3}+\omega_ {2p_ {1}+2})},\\ (c 2) =&amp; - \frac{81m^{3} g^{3}}{16\sqrt{2}} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {2}\omega _ {p} \omega^{2}_ {p-2}} \\ &amp;\times \frac{1}{(\omega_ {1}+\omega_ {2-p}+\omega_ {1+2-p})(\omega_ {1}+\omega_ {2}+\omega _ {p} +\omega_ {1+2-p})}, \\ (c 3) =&amp; - \frac{27m^{3}g^{3}}{16\sqrt{2}} \int d^{3}x \, \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {2}\omega_ {3}\omega_ {2+3}} \\ &amp;\times \frac{1}{\omega_ {1+2}(\omega_ {3}+\omega_ {1+2}+\omega_ {1+2+3})(\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2+3})}. \end{align*}\] <p>The contribution of $H_ {3}^{(-3)}\cdot(6)$, given in panel (d1,d2,d3):</p> \[\begin{align*} (d 1) =&amp; - \frac{3m^{3}g^{3}\left\lvert \vec{p},0 \right\rangle}{4\sqrt{2}} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \frac{1}{\omega_ {1}\omega_ {2}\omega_ {1+2}\omega_ {p}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \\ &amp;\times \frac{1}{(-\omega _ {p} +\omega_ {1}+\omega_ {2}+2\omega_ {1+2}+\omega_ {p-p_ {1}-p_ {2}})},\\ (d 2) =&amp; - \frac{9m^{3}g^{3}}{4\sqrt{2}} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \left\lvert \vec{p},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2,3}}{(2\pi)^{6}} \, \frac{1}{\omega_ {2}\omega_ {3}\omega^{2} _ {p} } \\ &amp;\times \frac{1}{ (\omega_ {2}+\omega_ {3}+\omega_ {2+3})(-\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {2+3}+\omega_ {p-p_ {1}})} ,\\ (d 3) =&amp; - \frac{9m^{3}g^{3}}{4\sqrt{2}} \int \frac{d ^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2,3}}{(2\pi)^{6}} \, \frac{1}{\omega_ {2}\omega_ {3}\omega _ {p} ^{2}} \\ &amp; \times \frac{1}{(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(-\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2}+\omega _ {p-p_ {3}} )} . \end{align*}\] <p>The diagrams are shown below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/H3Minus3OnP25-480.webp 480w,/img/kink/H3Minus3OnP25-800.webp 800w,/img/kink/H3Minus3OnP25-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/H3Minus3OnP25.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 7. The diagrams of $H_3^{(-3)}\left\lvert \vec{p} \right\rangle_ {2}^{(5)}$. </div> <hr/> <p>$H_ {4}^{(0)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)}:$</p> <p>The diagrams are shown below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/H40OnP12-480.webp 480w,/img/kink/H40OnP12-800.webp 800w,/img/kink/H40OnP12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/H40OnP12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 8. Diagrams for $H_ {4}^{(0)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)}$. </div> <p>Their contributions correspondingly are</p> \[\begin{align*} (e 1) =&amp; \frac{9mg^{3}}{8\sqrt{2}\omega _ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {2}\omega_ {p-2}(\omega _ {p} -\omega_ {2}-\omega_ {p-p_ {2}})} , \\ (e 2) =&amp; -\frac{\delta m^{2} 3mg}{2\sqrt{2}\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}_ {1}, \vec{p}-\vec{p}_ {1} \right\rangle}{\omega_ {1}(-\omega _ {p} +\omega_ {1}+\omega_ {p-p_ {1}})}, \\ (e 3) =&amp; \frac{3A_ {4}'mg}{2\sqrt{2}\omega _ {p} }\int d^{3}x \, \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p},\vec{p}-\vec{p}_ {1} \right\rangle}{-\omega _ {p} +\omega_ {1}+\omega_ {p-p_ {1}}}. \end{align*}\] <hr/> <p>$H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {2}^{(3)}:$</p> <p>It includes three parts: $H_ {3}^{(-1)}$ acting on panel (5,8,11,13,14) in figure 6. The figure are shown below.</p> <p>First of all, $H_ {3}^{(-1)}$ acting on panel (5). They correspond to panel (f1) and (f2) in figure 9 below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/H3Minus1OnP23-480.webp 480w,/img/kink/H3Minus1OnP23-800.webp 800w,/img/kink/H3Minus1OnP23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/H3Minus1OnP23.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 9. The diagrams for $H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {2}^{(3)}$. </div> <p>Their contributions are</p> \[\begin{align*} (f 1) =&amp; - \frac{27g^{3}m^{3}}{8\sqrt{2}\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {1}\omega_ {2}\omega_ {1-2}} \\ &amp; \times \frac{1}{(-\omega _ {p} +\omega_ {1}+\omega_ {p-p_ {1}})(-\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {1-2})}, \\ (f 2) =&amp; -\frac{27m^{3}g^{3}}{4\sqrt2\omega _ {p}} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {2}\omega_ {1+2}\omega_ {p-p_ {1}-p_ {2}}} \\ &amp;\times \frac{1}{(-\omega _ {p} +\omega_ {p-p_ {1}-p_ {1}}+\omega_ {1+2})(-\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {p-p_ {1}-p_ {2}})}, \end{align*}\] <p>Then we move on to $H_ {3}^{(-1)}$ acting on panel (8), given by panel (g1) and (g2) in figure 9. The contributions are</p> \[\begin{align*} (g 1) =&amp; - \frac{27m^{3}g^{3}}{8\sqrt{2}\omega _ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}}\left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle \, \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {1}\omega_ {2}\omega_ {1-2}} \\ &amp; \times \frac{1}{(\omega_ {1}+\omega_ {2}+\omega_ {1-2})(-\omega _ {p} +\omega_ {p-p_ {1}}+\omega_ {2}+\omega_ {1-2})}, \\ (g 2) =&amp; - \frac{27m^{3}g^{3}}{4\sqrt{2}\omega _ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {2}\omega_ {1+2}\omega_ {p-p_ {1}-p_ {2}}} \\ &amp; \times \frac{1}{(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(-\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {p-p_ {1}-p_ {2}})}. \end{align*}\] <p>Then $H_ {3}^{(-1)}$ acting on panel (11),given by panel (h1,h2):</p> \[\begin{align*} (h 1) =&amp; - \frac{27m^{3}g^{3}}{16\sqrt{2}} \left\lvert \vec{p},0 \right\rangle \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{1}{\omega_ {1}^{3}\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})}, \\ (h 2) =&amp; - \frac{27m^{3}g^{3}}{8\sqrt{2}\omega _ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p},\vec{p}-\vec{p}_ {1} \right\rangle \frac{1}{\omega_ {1}^{2}\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} . \end{align*}\] <p>It looks weird that we have a $\left\lvert \vec{p}=0 \right\rangle$ state… it just pops out from the vacuum. Anyway, let’s move on.</p> <p>$H_ {3}^{(-1)}$ acting on panel (13),given by panel (i1):</p> \[(i 1) = \frac{9mg^{3}}{4\sqrt{2}\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {2}\omega_ {1-2}(-\omega _ {p} +\omega_ {2}+\omega_ {1-2}+\omega_ {p-p_ {1}})} ,\] <p>and $H_ {3}^{(-1)}$ acting on panel (13) represented by panel (j1,j2), whose contributions are</p> \[\begin{align*} (j 1) =&amp; - \frac{3\delta m^{2}m g}{32\sqrt{2}} \left\lvert \vec{p},0 \right\rangle \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{1}{\omega^{3}_ {1}}, \\ (j 2) =&amp; - \frac{3mg\delta m^{2}}{4\sqrt{2}\omega _ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle}{\omega_ {1}^{2}}. \end{align*}\] <hr/> <p>$H_ {4}^{(-2)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)}:$</p> <p>This contribution is relatively simple, including one contribution only: $H_ {4}^{(-2)}$ acting on panel 3 in Figure 4. The diagrams are given in Figure 10, with contributions</p> \[\begin{align*} k 1=&amp; \frac{3mg^{3}\left\lvert \vec{p},0 \right\rangle}{4\sqrt{2}} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{1}{\omega_ {1} \omega_ {2} \omega_ {3}(\omega_ {1} + \omega_ {2} + \omega_ {1+2})} ,\\ k 2=&amp; \frac{9 mg^{3}}{4\sqrt{2}\omega _ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p},\vec{p}-\vec{p}_ {1} \right\rangle \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} ,\\ \ell 1=&amp; - \frac{3mg \delta m^{2} }{4\sqrt{2}} \left\lvert \vec{p},0 \right\rangle \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{1}{\omega_ {1}^{2}(2\omega_ {1}+m)} ,\\ \ell 2=&amp; - \frac{3mg \delta m^{2}}{4\sqrt{2}\omega^{2}_ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle}{\omega_ {1}+\omega _ {p} +\omega_ {p-1}} . \end{align*}\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/H4Minus2OnP14-480.webp 480w,/img/kink/H4Minus2OnP14-800.webp 800w,/img/kink/H4Minus2OnP14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/H4Minus2OnP14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 10 Diagrams for $H_ {4}^{(-2)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)}$. </div> <h3 id="reducible-diagrams">Reducible Diagrams</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/3rdOrderReducible-480.webp 480w,/img/kink/3rdOrderReducible-800.webp 800w,/img/kink/3rdOrderReducible-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/3rdOrderReducible.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The blob stands for all divergent diagrams. These three are the divergent contributions to $\left\lvert \vec{p} \right\rangle_ {3}^{(2)}$, in the sense that they can be amputated by cutting an external leg. </div> <p>Now we can sum up the previous results and organize diagrams according to their topology.</p> <p>For the reducible diagrams we have</p> \[\begin{align*} H_ {4}^{(0)} \left\lvert \vec{p} \right\rangle_ {1}^{(2)} =&amp; - \frac{3m\delta m^{2}g}{2\sqrt{2}\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}_ {1}, \vec{p}-\vec{p}_ {1} \right\rangle_ {0}}{\omega_ {1}(-\omega _ {p} +\omega_ {1}+\omega_ {p+p_ {1}})},\\ H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {1}^{(3)} =&amp; - \frac{27m^{3}g^{3}}{8\sqrt{2}\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi^{3})} \, \frac{\left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle_ {0}}{\omega_ {1}(-\omega _ {p} +\omega_ {1}+\omega_ { p-p_ {1}})} \\ &amp;\times \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {1+2}\omega_ {2}(-\omega _ {p} +\omega_ {1+2}+\omega_ {2}+\omega_ {p-p_ {1}})}, \\ H_ {3}^{(-3)}\left\lvert \vec{p} \right\rangle_ {2}^{(5)} &amp;= - \frac{27m^{3}g^{3}}{8\sqrt{2}\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \frac{\left\lvert \vec{p}_ {1}, \vec{p}-\vec{p}_ {1} \right\rangle}{\omega_ {1}(-\omega _ {p} +\omega_ {1}+\omega_ {p-p_ {1}})} \\ &amp;\times \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {2}\omega_ {1+2}(-\omega _ {p} +2\omega_ {1}+\omega_ {2}+\omega_ {1+2}+\omega_ {p-p_ {1}})} \end{align*}\]]]></content><author><name>Baiyang Zhang</name></author><category term="kink"/><summary type="html"><![CDATA[Rules for Hamiltonians Rules for $H_ {3}^{(-)}$]]></summary></entry><entry><title type="html">Asymptotic Methods Applied on Anharmonic Oscillator</title><link href="https://baiyangzhang.github.io/blog/2024/Asymptotic-Methods-Applied-on-Anharmonic-Oscillator/" rel="alternate" type="text/html" title="Asymptotic Methods Applied on Anharmonic Oscillator"/><published>2024-10-11T00:00:00+00:00</published><updated>2024-10-11T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Asymptotic-Methods-Applied-on-Anharmonic-Oscillator</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Asymptotic-Methods-Applied-on-Anharmonic-Oscillator/"><![CDATA[<h1 id="chapter-1">Chapter 1</h1>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[Chapter 1]]></summary></entry><entry><title type="html">Diagrammatic Rules for Jarah Diagrams</title><link href="https://baiyangzhang.github.io/blog/2024/Rules-for-Jarah-Diagrams/" rel="alternate" type="text/html" title="Diagrammatic Rules for Jarah Diagrams"/><published>2024-09-18T00:00:00+00:00</published><updated>2024-09-18T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Rules-for-Jarah-Diagrams</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Rules-for-Jarah-Diagrams/"><![CDATA[<p>These rules are currently being used by myself and no one else, it is by no means standard or widely accepted. I find it extremely fun to play with those rules and diagrams.</p> <p>The rules are shown in the figures below. The numbers label the momentum, for example $1$ stands for $\vec{p}_ {1}$. We only need to label the independent momenta for two reasons: 1) each vertex conserves the momenta hence we can fix the not-labeled momentum without problem and 2) it is important to balance the momenta and delta functions, as we will explain later. One of the various kinds of divergences, namely a factor of $(2\pi)^{3}\delta^{3}(0)$ or equivalently (not mathematically rigorously though) $\int d^3x$ comes from the fact that the Dirac delta functions over-determine the momenta, it is also included in the diagrammatic method.</p> <p>In the following rules, some factors (such as a factor of 3) is perhaps better treated as symmetry factors, but for practical purpose I treat them as part of the definition of the the vertices.</p> <p>I note that the diagrams I have are used to calculate quantities such as $H_ {3}\left\lvert \vec{p} \right\rangle_ {1},H_ {4}\left\lvert \Omega \right\rangle_ {2}$ only, in order to obtain the final <strong>state</strong> we need to further invert some operators. The inversion is quite straightforward though.</p> <h1 id="rules-for-hamiltonians">Rules for Hamiltonians</h1> <h2 id="rules-for-h_-3-">Rules for $H_ {3}^{(-)}$</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/H3Rules-480.webp 480w,/img/kink/H3Rules-800.webp 800w,/img/kink/H3Rules-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/H3Rules.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig.1. Rules for various components of $H_3$. I regard them as vertices. The meaning of graphical elements are explain later. </div> <h2 id="rules-for-h_-4-">Rules for $H_ {4}^{(-)}$</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/H4Rules-480.webp 480w,/img/kink/H4Rules-800.webp 800w,/img/kink/H4Rules-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/H4Rules.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig.2. Rules for various components of $H_4$. </div> <h1 id="feynman-like-rules">Feynman-like Rules</h1> <p>With above ingredients we may introduce the rules to connect them now.</p> <p>Roughly speaking we find all the possible way to concatenate a Hamiltonian to a state. Each red circle indicates that it must be concatenated to some other line to form a propagator, if not the diagram vanishes. After the concatenation we still need to <em>balance the momenta against the delta functions</em> (I am not sure if balance is the right verb here), as we will explain shortly.</p> <p>For example, say we want to calculate $H_ {4}^{(2)}\left\lvert \Omega\right\rangle_ {0}$, we need to do the following:</p> <ol> <li>Find the diagrams corresponding to $H_ {4}^{(2)}$, there are two of them. Find the diagram corresponding to $\left\lvert \Omega \right\rangle_ {0}$, there is only one.</li> <li>Put the diagrams from $H_ {4}^{(2)}$ on the left and that from $\left\lvert \Omega \right\rangle_ {0}$ on the right, with all the independent momenta labeled in the fashion that is given in the figures below. Find all the possible ways to concatenate them.</li> <li>For each concatenation, write down the symmetry factor first. There is usually more than one way to connect a circle to a leg (in the diagram of $\left\lvert \Omega \right\rangle_ {0}$), the number of doing that is the symmetry factor.</li> <li>If a circle is connected to two independent labeled momenta, we’ll call it “fully labeled.” If it’s connected to only one labeled momentum, we’ll call it “half-labeled.” Arrange the labels so that <strong>as many circles as possible are fully labeled</strong>. Each circle will cancel out a connected momentum, and you can cross them off simultaneously. If a circle isn’t connected to any labeled momentum, chance is they will survive the balancing process and give us a divergent factor. It’s easier than it sounds. Once you’ve crossed them off, replace the circle with an arrow pointing in the direction of increasing coupling. A line with an arrow represents a regular propagator.</li> <li>If there is a circle that no matter how to label and re-label the independent momenta still can’t be eliminated, keep it, a circle connected with two legs is an irregular propagator, it will contribute a factor of $\int d^{3}x$, or $(2\pi)^{3}\delta^{3}(0)$.</li> <li>Use the momentum conservation at each vertex to write down the momentum for each leg, external and internal. They should all be expressed in terms of independent momenta.</li> <li>Read from left to right, don’t forget to write down the symmetry factor. The left-most external legs give us the final state, each of them gives us an momentum eigenstate. <strong>If there is not a single leg then the state is $\left\lvert \Omega \right\rangle_ {0}$</strong>. Whenever you see an independent momentum, write the integral measure $\int d^3p / (2\pi)^{3}$. Whenever you see a propagator, write down the corresponding factor, which is listed in the figure below. Write down the expression for the vertex and the state.</li> <li>Simplify and check.</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/jarahRules-480.webp 480w,/img/kink/jarahRules-800.webp 800w,/img/kink/jarahRules-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/jarahRules.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Rules for two kinds of propagators, regular and circled ones. Each regular propagator contributes a factor of $\frac{1}{2\omega}$, in the mean while a circled propagator contributes an extra divergent factor of $\int d^3 x$, or $(2\pi)^3\delta^3(0)$ as I sometimes like to write. </div> <h1 id="some-examples">Some examples</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/example-480.webp 480w,/img/kink/example-800.webp 800w,/img/kink/example-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Here is an example of how to calculate $H_ 3^{(-3)}\lvert\Omega\rangle_ 1^{(3)}$. Here the first factor $6$ comes from the 6 different ways to connect the circles. The top two circles are fully labeled, the buttom one has no label at all, thus there is no independent momentum to ballance against the circle. It contributes the spatial integral in the final expression. Since all the vertices preserve the momenta, and the top to propogators are labeled with $p_ 1$ and $p_ 2$, the last line is fixed to be $-p_ 1-p_ 2$. It is not common to encounter an irregular propagator. There is no external leg on the left-most of the diagram, hence we assign $\left\lvert \Omega \right\rangle_ {0}$ to it. </div>]]></content><author><name>Baiyang Zhang</name></author><category term="kink"/><summary type="html"><![CDATA[These rules are currently being used by myself and no one else, it is by no means standard or widely accepted. I find it extremely fun to play with those rules and diagrams.]]></summary></entry><entry><title type="html">Quantum Domain Wall in 4D Part II</title><link href="https://baiyangzhang.github.io/blog/2024/Constructing-a-Finite-Tension-Domain-Wall-in-4D-Part-II/" rel="alternate" type="text/html" title="Quantum Domain Wall in 4D Part II"/><published>2024-08-25T00:00:00+00:00</published><updated>2024-08-25T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Constructing-a-Finite-Tension-Domain-Wall-in-4D-Part-II</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Constructing-a-Finite-Tension-Domain-Wall-in-4D-Part-II/"><![CDATA[<h1 id="spontaneous-symmetry-breaking">Spontaneous Symmetry Breaking</h1> <p>Recall that the original, un-shifted Hamiltonian in terms of bare parameter, without normal ordering reads</p> \[\hat{\mathcal{H}}^{0}(\vec{x}) = \frac{1}{2}\pi^{2}(\vec{x})+\frac{1}{2} (\partial_ {i}\phi)^{2} - \frac{m_ {0}^{2}}{4} \phi^{2}(\vec{x}) + \frac{\lambda_ {0}}{4} \phi^{4}(\vec{x}) + A,\] <p>It has two classical minima are obtained at $\pm \frac{m_ {0}}{\sqrt{ 2\lambda_ {0} }}$. As a result we have two degenerate vacua states $\left\lvert{0^{\pm}}\right\rangle$ satisfying</p> \[\left\langle{0^{-}}\right\rvert \phi \left\lvert{0^{-}}\right\rangle =- \frac{m_ {0}}{\sqrt{ 2\lambda_ {0} }},\quad \left\langle{0^{+}}\right\rvert \phi \left\lvert{0^{+}}\right\rangle =+ \frac{m_ {0}}{\sqrt{ 2\lambda_ {0} }}\] <p>Now we want to study the quantum physics around one of the vacua, say $\left\langle \phi \right\rangle = \frac{m_ {0}}{\sqrt{ 2\lambda_ {0} }}$, and we want to apply perturbation methods, since it is usually the only way we know how to proceed. The question is how to do perturbation theory with Hamiltonian formalism? For the validation of perturbation method, the field fluctuation should be small, meaning $\left\langle \phi \right\rangle$ should be small, but it is clearly not the case here, since $m_ {0} / \sqrt{ 2 } g_ {0}$ is not small by any sense.</p> <hr/> <p>Another thing we need to keep in mind is the difference between bare and renormalized parameters, eventually we need to write down the observables in terms of not bare, but renormalized parameters. Following the philosophy of <em>renormalized perturbative method</em>, we should 1) separate the Hamiltonian into renormalized terms and counter terms, then 2) use the counter terms to cancel the divergence from loops, which are now written in terms of renormalized parameters. We wrote the vev of $\phi$ in terms of bare parameters, but that is only for pedagogical reasons, what we should really do it first write the Hamiltonian into the renormalized perturbation theory form,</p> \[\hat{\mathcal{H}} = \frac{1}{2}\pi^{2}(\vec{x})+\frac{1}{2} (\partial_ {i}\phi)^{2} - \frac{m^{2}}{4} \phi^{2}(\vec{x}) + \frac{\lambda}{4} \phi^{4}(\vec{x}) + A + \text{counter terms,}\] <p>then look for the minimum of the double-well potential, now $\left\langle \phi \right\rangle = m / \sqrt{ 2 }g$, with out naught subscript.</p> <hr/> <p>The last but not the least thing we need to worry about is the normal ordering. So far my assumption is that, the Hamiltonian is defined at some intrinsic parameter $m_ {0}$, so the normal ordering should also be defined at $m_ {0}$; however, to make the calculation easier, we shift the normal ordering to another scale: $m$. We have already talked in length about how to perform this shift. The final result should not depend on the shift though.</p> <hr/> <p>In the context of Lagrangian formalism and path integral, the perturbation method is quite straightforward, we can redefine the field operator according to $\phi\to \frac{m }{\sqrt{ 2\lambda }}+\phi’$, so that the classical vacuum is obtained at $\phi’$ equals zero. The small fluctuation about $\frac{m }{\sqrt{ 2\lambda }}$ gives us the quantum effects, we will essentially calculate</p> \[Z[J] = \int D\phi' \, \exp \left\lbrace iS[\phi']+i \int \phi' J \right\rbrace ,\] <p>but only include $\phi’\sim 0$.</p> <p>How can we do the same thing but with Hamiltonian formalism? Anything that can be done with one formalism can equally be done in the other, it is just a matter of convenience.</p> <p>The problem is, in Hamiltonian formalism and working with $\hat{\mathcal{H}}$, perturbation methods are just not applicable, because the expectation value $\left\langle \phi \right\rangle := \left\langle{0^{+}}\right\rvert \phi \left\lvert{0^{+}}\right\rangle$ is by no means a small quantity, in fact $\left\langle \phi \right\rangle$ scales as $1 / \sqrt{ \lambda }$, it actually blows up as $\lambda \to 0$. Then, similar to the Lagrangian case, we need to find some new field operator $\phi’$ such that $\left\langle \phi’ \right\rangle=0$, then we can study the effects of fluctuation of $\left\langle \phi’ \right\rangle$ perturbative in $\left\lvert{0^{+}}\right\rangle$ state. Turns out, $\phi’$ is connected to $\phi$ by a unitary transformation, and in the case at hand the unitary operator turns out to be the displacement operator $\mathcal{D}_ {v }$, where $v $ is some parameter to be determined. Note that we have replaced the arbitrary function $f(\vec{x})$ with a constant function $v $. We will explain it in the following.</p> <p>Recall the defining property of displacement operator, $\mathcal{D}_ {f}^{\dagger}\phi \mathcal{D}_ {f}=\phi+f$. Thus if we let</p> \[\boxed{ v = -\frac{m }{\sqrt{ 2\lambda }} }\] <p>then (at least at leading order)</p> \[\left\langle{0^{+}}\right\rvert \mathcal{D}_ {v }^{\dagger} \phi \mathcal{D}_ {v }\left\lvert{0^{+}}\right\rangle = \left\langle{0^{+}}\right\rvert \phi+v \left\lvert{0^{+}}\right\rangle = \frac{m }{\sqrt{ 2\lambda }}-\frac{m }{\sqrt{ 2\lambda }}=0,\] <p>hence</p> \[\boxed{ 0=\left\langle{0^{+}}\right\rvert \phi'\left\lvert{0^{+}}\right\rangle, \quad \phi':= \mathcal{D}_ {v }^{\dagger}\phi \mathcal{D}_ {v }. }\] <p>It inspires us to define a new Hamiltonian exactly in the same fashion,</p> \[\boxed{ \mathcal{H} := \mathcal{D}_ {v }^{\dagger} \hat{\mathcal{H}} \mathcal{D}_ {v }. }\] <p>We are interested in the spectrum of the Hamiltonian. The good news is that, a unitary transformation preserves the spectrum! Now, in order to study the quantum correction, instead of working with $\hat{H}$, $\phi$ and $\left\lvert{0^{+}}\right\rangle$ where perturbative methods fail, we can work with $H, \phi’$ and $\left\lvert{0^{+}}\right\rangle$ where $\phi’$ can be dealt with perturbatively.</p> <p><strong>Remarks.</strong> Generally speaking, let $\mathcal{O}$ be any operator and $\left\lvert{\psi}\right\rangle$ its eigenstate. If $\mathcal{O}’=\mathcal{D}^{\dagger}\mathcal{O} \mathcal{D}$ is the unitary transformation of $\mathcal{O}$, then its eigenstate is $\mathcal{D}^{\dagger}\left\lvert{\psi}\right\rangle$, not $\left\lvert{\psi}\right\rangle$. It may seem weird to some people to team up $\mathcal{O}’$ and $\left\lvert{\psi}\right\rangle$, not $\mathcal{O}’$ and $\mathcal{D}^{\dagger}\left\lvert{\psi}\right\rangle$. In fact, it is exactly the point of our method! If we pair $\mathcal{O}’$ with $\mathcal{D}^{\dagger}\left\lvert{\psi}\right\rangle$ it would be a trivial transform compare to working with $\mathcal{O}$ and $\left\lvert{\psi}\right\rangle$, we will not get anything new! Paring up $\mathcal{O}’$ and $\left\lvert{\psi}\right\rangle$ makes it possible for perturbation methods. However, since $\left\lvert{\psi}\right\rangle$ is not the eigen state of $\mathcal{O}’$, it indeed raise some problems, the first and foremost is that now $\mathcal{O}’$ is probably not diagonalized in $\left\lvert{\psi}\right\rangle$, we need to find a way to diagonalized it. In our current case, since the displacement operator only shifts the operators by a constant, it actually preserves the diagonalization, meaning if $\hat{\mathcal{H}}$ is diagonalized in $\left\lvert{\psi}\right\rangle$’s then $\mathcal{D}^{\dagger} \hat{\mathcal{H}}\mathcal{D}$ is also diagonalized in $\left\lvert{\psi}\right\rangle$’s. But for a generic $\mathcal{D}_ {f}$ with non-trivial $f$, this will no longer be true. That would be the topic for the other half the the note, after we introduce the kink solution.</p> <p>A comparison between displacement operator method to the case of regular functions might be helpful.</p> <table> <thead> <tr> <th style="text-align: center">Functions</th> <th style="text-align: center">QFT</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">A functions $f(x)$, a neighborhood about a special locus $x_ 0$ where the interesting things happen.</td> <td style="text-align: center">A Hilbert space, a quantum state $\left\lvert{\Psi}\right\rangle$ of interest represented by a functional $\left\langle \Psi(x),- \right\rangle$, and some operator $\mathcal{O}(\phi)$.</td> </tr> <tr> <td style="text-align: center">We want to study the function $f(x )$ about $x_ 0$ perturbatively,</td> <td style="text-align: center">We want to study the $\left\langle{\Psi}\right\rvert\mathcal{O}\left\lvert{\Psi}\right\rangle$ perturbatively,</td> </tr> <tr> <td style="text-align: center">but $x_ 0$ is not a small quantity so Maclaurin expansion (Taylor expansion at the origin) fails.</td> <td style="text-align: center">but $\left\langle{\Psi}\right\rvert\phi \left\lvert{\Psi}\right\rangle=:f(x)$ is too large for $\phi$ to be treated perturbatively.</td> </tr> <tr> <td style="text-align: center"><strong>In a passive perspective, we shift the origin</strong> to $x_ 0$, then small deviation from $x_ 0$ can now be studied using Maclaurin expansion perturbatively.</td> <td style="text-align: center">In a passive perspective, we shift the operators, especially the field operator $\phi$ since it is usually the building block of other operators. The expectation value of the new, shifted operator $\phi’$ should be zero, $\left\langle{\Psi}\right\rvert \phi’\left\lvert{\Psi}\right\rangle =0$. Now we can treat $\phi$ perturbatively.</td> </tr> <tr> <td style="text-align: center">We are using a new, shifted coordinate system $\left\lbrace \overline{x} \right\rbrace$ to study the same old functions $f(x)$.</td> <td style="text-align: center">We are using shifted operators $\mathcal{D}^{\dagger}\mathcal{O}\mathcal{D}$ to study the same old states $\left\lvert{\Psi}\right\rangle$.</td> </tr> </tbody> </table> <p>In summary, now $\mathcal{H}=\mathcal{D}^{\dagger}_ {v}\hat{\mathcal{H}}\mathcal{D}_ {v}$ is a operator-valued function of $\phi’ = \mathcal{D}_ {v}^{\dagger} \phi \mathcal{D}_ {v}$, and $\phi’$ can be dealt with perturbatively. For the value of $v$, we could choose it to be the bare parameter or renormalized parameter, the final result should not depend on the convention we choose, in our note, in the spirit of renormalized perturbation theory, we choose $v$ to be renormalized, the associated displacement operator is $\mathcal{D}_ {v}$. Still, $v$ receives quantum corrections order by order, the full expression for $v$ would be an expansion in $g$:</p> \[v = -\frac{m}{\sqrt{ 2 }g} +\delta v, \quad \delta v = \delta v_ {1}+\delta v_ {2}+\cdots, \quad \delta v_ {i} \sim \mathcal{O}(g^{i}).\] <hr/> <p>In renormalized parameters the Hamiltonian reads (up to $\mathcal{O}(g^{3})$)</p> \[\begin{align*} \hat{H}(\vec{x}) &amp;\supset \int d^{3}x \, : \frac{1}{2}\pi^{2}+\frac{1}{2}(\partial_ {i}\phi)^{2} - \frac{m^{2}}{4}\phi^{2}+\frac{g^{2}}{4}\phi^{4}:_ {m} \\ &amp;\;\;\;\; + \int d^{3}x \, : \frac{\delta m^{2}}{4}\phi^{2}-\frac{1}{2}g \delta g\phi^{4} + \frac{(\delta g)^{2}}{4}\phi^{4} + \frac{3}{2}(g-\delta g)^{2}I\phi^{2}-\frac{3}{4}m^{2}I +A :_ {m}. \end{align*}\] <p>Recall that $I$ is some integral of higher order of the coupling, we defined it in part I of the note. We need to sandwich the Hamiltonian by $\mathcal{D}_ {v}$ to get the shifted Hamiltonian density $\mathcal{H}$. We will expand $\mathcal{H}$ in power of $g=\sqrt{ \lambda }$. With some help from Mathematica, we find the following results.</p> \[\begin{align*} \mathcal{H}_ {0} &amp;= A_ 0-\frac{m^4}{16 g^2}, \\ \mathcal{H}_ {1} &amp;= A_ {1} \equiv 0, \\ \mathcal{H}_ {2} &amp;= \frac{\pi^{2}}{2} + \frac{(\partial \phi)^{2}}{2} + \frac{m^{2}\phi^{2}}{2} + \frac{m^{4}}{8g^{2}}\left( -\frac{\delta g}{g} + \frac{\delta m^{2}_ {1}}{m^{2}} + \frac{\delta m^{4}_ {1}}{8g^{2}} \right) + A_ {2}, \\ \mathcal{H}_ {3} &amp;= - \frac{mg}{\sqrt{ 2 }} \phi^{3} + \phi\left( m^{2}\delta v_ {1}+\frac{m^{3}}{\sqrt{ 2 }g} \left( \frac{\delta g}{g} - \frac{\delta m^{2}_ {1}}{2m^{2}} - \frac{\delta m^{4}_ {1}}{2g^{2}} \right) \right) + A_ {3}, \\ \mathcal{H}_ {4} &amp;= \frac{g^{2}}{4}\phi^{4} + \phi^{2}\left( - \frac{3gm\delta v_ {1}}{\sqrt{ 2 }} - \frac{3m^{2}\delta g}{2g} + \frac{\delta m^{2}_ {1}}{4} + \frac{3m^{2}\delta m_ {1}^{4}}{4g^{2}} \right)+ \phi(m^{2}\delta v_ {2}) \\ &amp;\;\;\;\; + \frac{m^{4}(\delta g)^{2}}{16g^{4}} + \frac{m^{3}\delta v_ {1}}{\sqrt{ 2 }g}\left( \frac{g}{\sqrt{ 2 }m} \delta v_ {1} + \frac{\delta g_ {1}}{g} - \frac{\delta m_ {1}^{2}}{2m^{2}} - \frac{\delta m_ {1}^{4}}{2g^{2}} \right) + A_ 4, \\ \mathcal{H}_ {5} &amp;= \phi^{3}\left( g^{2}\delta v_ {1}+\sqrt{2}m\delta g_ {1}- \frac{m\delta m_ {1}^{4}}{\sqrt{2}g} \right) - \phi^{2}\left( \frac{3mg \delta v_ {2}}{\sqrt{2}} \right) \\ &amp;\;\;\;\; + \phi\left( m^{2}\delta v_ {3} - \frac{3gm\delta v_ {1}^{2}}{\sqrt{2}} - \frac{3m^{2}\delta g_ {1}\delta v_ {1}}{g} - \frac{m^{3}(\delta g)^{2}}{2\sqrt{2}g^{3}} + \frac{1}{2}\delta m_ {1}^{2}\delta v_ {1} - \frac{3mg I_ {1}}{\sqrt{2}} + \frac{3m^{2}\delta m_ {1}^{4}\delta v_ {1}}{2g^{2}} \right) \\ &amp;\;\;\;\; + \frac{m^{3}\delta v_ {2}}{\sqrt{2}g}\left( \frac{\delta g}{g} - \frac{\delta m_ {1}^{2}}{2m^{2}} - \frac{\delta m_ {1}^{4}}{2g^{2}} + \frac{\sqrt{2}g\delta v_ {1}}{m} \right) + A_ {5}. \end{align*}\] <h1 id="hamiltonian-renormalization">Hamiltonian Renormalization</h1> <p>The wavefunctions are expanded as</p> \[\begin{align*} \left\lvert{\Psi}\right\rangle &amp;= \left\lvert{\Psi_ {0}}\right\rangle + \left\lvert{\Psi_ {1}}\right\rangle + \cdots = \sum_ {i=0}^{\infty} \left\lvert{\Psi_ {i}}\right\rangle , \\ \left\lvert{\Psi _ {i} }\right\rangle &amp;\sim g^{i}. \end{align*}\] <p>The Hamiltonian renormalization conditions (HRC) are as follows.</p> <ol> <li>Vacuum condition: $H\left\lvert{\Psi}\right\rangle=0$ at all orders.</li> <li>Theory is defined at mass $m$: $H\left\lvert{\vec{p}}\right\rangle = \omega_ {p,m}\left\lvert{\vec{p}}\right\rangle$. We will just neglect $m$.</li> <li>$_ {0}\left\langle \vec{p}_ {1}\vec{p}_ {2} \middle\vert \vec{p}\right\rangle_ {i\geq2}=0$,</li> <li>No tadpole, $\left\langle{\Omega}\right\rvert\phi \left\lvert{\Omega}\right\rangle=0$.</li> </ol> <p>These are non-perturbative conditions, meaning they apply to all the orders, and the states involved are in general non-perturbative states, they can be expanded order by order. $\left\lvert{\Omega}\right\rangle$ is the full vacuum state with interaction, sometimes called physical vacuum. $\left\lvert{\vec{p}_ {1}\cdots\vec{p}_ {n}}\right\rangle$ is an $n$-meson momenta eigenstate. We define the leading order of the vacuum to be:</p> \[A_ {p} \left\lvert{\Omega_ {0}}\right\rangle =0.\] <p>Similarly $\left\lvert{\vec{p}}\right\rangle$ is the full momentum eigenstate, $\left\lvert{\vec{p}}\right\rangle=\left\lvert{\vec{p}_ {0}}\right\rangle + \left\lvert{\vec{p}_ {1}}\right\rangle+\cdots$. Recall that <strong>in the interaction picture</strong> we have $\left\lvert{\vec{p}}\right\rangle=\frac{a^{\dagger}}{\sqrt{2\omega_ {p,m}}}\left\lvert{0}\right\rangle$, it is because in the interaction picture the operator $\phi$ is exactly solvable, the solution is by construction the same as if interaction does not exist. However we are working in Schrodinger picture, so this simple relation only holds at leading order,</p> \[\left\lvert{\vec{p}}\right\rangle_ {0} = A^{\ddagger}_ {p}\left\lvert{\Omega_ {0}}\right\rangle .\] <p>The non-perturbative definition, $H\left\lvert{\vec{p}}\right\rangle=\vec{p}\left\lvert{\vec{p}}\right\rangle$, holds at each and every order.</p> <p>The orthonormal conditions are:</p> \[\begin{align*} \left\langle \Omega_ {0} \middle\vert \Omega_ {0} \right\rangle &amp;= 1,\\ \left\lvert{\vec{p}_ {1}\cdots \vec{p}_ {n} }\right\rangle_ {0} &amp;= A^{\ddagger}_ {p_ {1}}\cdots A^{\ddagger}_ {p_ {n} }\left\lvert{\Omega_ {0}}\right\rangle . \end{align*}\] <p>In this note I will use $\left\lvert{\Omega_ {i}}\right\rangle$ and $\left\lvert{\Omega}\right\rangle_ {i}$ interchangeably.</p> <p>The first Hamiltonian renormalization condition (HRC) is expanded to</p> \[\boxed{ H\left\lvert{\Omega}\right\rangle = \sum_ {i}^{\infty}H_ {i=0} \sum_ {j=0}^{\infty} \left\lvert{\Omega _ {j} }\right\rangle = \sum_ {i,j=0}^{\infty} H_ {i} \left\lvert{\Omega _ {j} }\right\rangle\sim g^{i+j-2}. }\] <p>The non-perturbed Fock states consists a full basis of Hilbert space, the $n$-meson states is defined to be</p> \[\left\lvert{\vec{p}_ {1}\cdots\vec{p}_ {n}}\right\rangle_ 0 := A_ {p_ {1}}^{\ddagger}\cdots A_ {p_ {n} }^{\ddagger} \left\lvert{\Omega_ {0}}\right\rangle .\] <p>We will use them to expand states of higher order in $g$, for example $\left\lvert{\Omega_ {n}}\right\rangle$ is the $g^{n}$-order vacuum correction, we will expand it as</p> \[\left\lvert{\Omega_ {n}}\right\rangle = \sum_ {i=0}^{\infty} \left\lvert{\Omega_ {n}}\right\rangle^{(i)} , \quad \left\lvert{\Omega}\right\rangle^{(i)} \in i\text{-meson Fock space.}\] <p>We have two things to match: 1)different orders of the coupling and 2)different Fock space.</p> <h2 id="order-g-2-and-g-1">Order $g^{-2}$ and $g^{-1}$</h2> <p>At the lowest order, the Hamiltonian density is just a constant without ladder operators, so $H_ {0}\left\lvert{\Omega_ {0}}\right\rangle=0$ gives as $\mathcal{H}_ {0}=0$, which implies $A_ {0} = m^{4} / 16g^{2}$.</p> <hr/> <p>By construction we have $H_ {1}=0$.</p> <p>Take away:</p> <p>\(\boxed{ H_ {0}=H_ {1}=0. }\)</p> <h2 id="order-g0">Order $g^{0}$</h2> <p>The first HRC (HRC1), namely $H\left\lvert{\Omega}\right\rangle=0$ implies that</p> \[H_ {2}\left\lvert{\Omega_ {0}}\right\rangle = 0 ,\] <p>which in turn implies</p> \[A_ {2} = - \frac{m^{4}}{8g^{2}} \left( -\frac{\delta g}{g} + \frac{\delta m_ {1}^{2}}{m^{2}}+ \frac{\delta m_ {1}^{4}}{\delta g^{2}} \right),\] <p>as a result</p> \[\begin{align*} \mathcal{H}_ {2} &amp;= \frac{\pi^{2}}{2} + \frac{(\partial \phi)^{2}}{2} + \frac{1}{2}m^{2}\phi^{2},\\ H_ {2} &amp;=\int d^{3}x \, : \mathcal{H}_ {2} :_ {m} = \int \frac{d^{3}p}{(2\pi)^{3}} \, \omega_ {p} A^{\ddagger}_ {p} A_ {p} . \end{align*}\] <hr/> <h2 id="order-g1">Order $g^{1}$</h2> <p>Similar to order $g^{0}$, HRC1 implies that</p> \[H_ {2}\left\lvert{\Omega_ {1}}\right\rangle = -H_ {3} \left\lvert{\Omega_ {0}}\right\rangle . \tag{HRC1}\] <p>Now we need to expand $\left\lvert{\Omega_ {1}}\right\rangle$ in $\left\lvert \Omega_ {0} \right\rangle$ and free field Fock states. Such state are created by free creation operators acting on free vacuum state,</p> \[\left\lvert \vec{p} \right\rangle_ {0} = A^{\ddagger}_ {p} \left\lvert \Omega_ {0} \right\rangle\] <p>and</p> \[\left\lvert \vec{p}_ {1}\cdots \vec{p}_ {n} \right\rangle_ {0} = A_ {p_ {1}}^{\ddagger}\cdots A^{\ddagger}_ {p_ {n} }\left\lvert \Omega_ {0} \right\rangle .\] <p>What is the norm of $\left\lvert \vec{p} \right\rangle_ {0}$? We have $_ {0}\left\langle \vec{p} \right\rvert=\left\langle \Omega_ {0} \right\rvert (A_ {p}^{\ddagger})^{\dagger}=\left\langle \Omega_ {0} \right\rvert(A_ {p})/2\omega_ {p}$ and $[A_ {p},A^{\ddagger}q]=(2\pi)^{3} \delta^{3}(\vec{p}-\vec{q})$, thus</p> \[\boxed{ _ {0}\left\langle \vec{p} \middle\vert \vec{q} \right\rangle_ {0} = \frac{(2\pi)^{3}}{2\omega _ {p} } \delta^{(3)}(\vec{p}-\vec{q}). }\] <p>The free Fock states consist a full basis of Hilbert space. We can expand the first order correction to the vacuum states in these basis,</p> \[\left\lvert{\Omega_ {1}}\right\rangle = \left\lvert{\Omega_ {1}^{(0)}}\right\rangle + \left\lvert{\Omega_ {1}^{(1)}}\right\rangle + \cdots\] <p>where $\left\lvert{\Omega_ {1}^{(0)}}\right\rangle$ isthe same as $\left\lvert{\Omega_ {1}}\right\rangle^{(0)}$, the superscript indicates the number of mesons, $\left\lvert \Omega_ {1}^{(n)} \right\rangle$ means the $n$-meson Fock space component of $\left\lvert \Omega_ {1} \right\rangle$ state. Explicitly we have</p> \[\left\lvert{\Omega^{(0)}_ {1}}\right\rangle = c_ {1,0} \left\lvert \Omega_ {0} \right\rangle , \quad c_ {1,0}\in \mathbb{C}.\] <p>I will assume that $c_ {1,0}=0$, which is quite reasonable since the correction should be something new. And</p> \[\left\lvert \Omega_ {1}^{(1)} \right\rangle = \sum c_ {1,i} \left\lvert p_ {i} \right\rangle_ 0 , \quad c_ {1,i}\in \mathbb{C}.\] <p>Generalizing it to multi-meson Fock space, we have</p> \[\left\lvert \Omega_ {1}^{(n)} \right\rangle =\sum_ {I} c_ {1,I} \left\lvert p_ {I} \right\rangle , \quad I = i_ {1}\cdots i_ {n},\; n&gt;1,\] <p>where $I$ is the general index, $\left\lvert p_ {I} \right\rangle=\left\lvert p_ {i_ {1}}p_ {i_ {2}}\cdots p_ {i_ {n}} \right\rangle$.</p> <p>Are states from different Fock spaces orthogonal to each other? It will look something like</p> \[_ {0}\left\langle \vec{p}_ {I} \middle\vert \vec{p}_ {J} \right\rangle_ {0} \propto \left\langle \Omega_ {0} \right\rvert A\cdots A A^{\ddagger}\cdots A^{\ddagger}\left\lvert \Omega \right\rangle .\] <p>According to the equal-time Wick theorem, a string of ladder operators is equal to the normal ordering of all its contraction, which, when sandwiched between vacuum state, can only be non-zero if it is a full contraction, where there are equal number of creation and annihilation operators. It means that these two states must</p> <p>have same number of particles in it, hence belong to the same order of Fock subspace. Thus</p> \[\left\langle \Psi^{(m)} \middle\vert \Psi^{(n)} \right\rangle =0 \text{ if } m \neq n.\] <p>Coming back to $\left\lvert \Omega_ {1} \right\rangle$. Focusing on the $0$-meson sub Fock space, we have</p> \[H_ {2}\left\lvert{\Omega_ {1}}\right\rangle \supset H_ {2}(\cdots)\left\lvert{\Omega_ {0}}\right\rangle =0,\] <p>since $H_ {2}$ is normal ordered. On the right hand side, the zero-meson part of $-H_ {3}\left\lvert \Omega_ {0} \right\rangle$ is what? $H_ {3}$ can be written as</p> \[H_ {3} = \int d^{3}x \, :- \frac{mg}{\sqrt{2}} \phi^{3}+\phi m^{2} \delta v_ {1}' + A_ {3} :_ {m},\] <p>where $\delta v_ {1}’$ is some function of counter terms. Thanks to the normal ordering, in $0$-Fock subspace we have</p> \[0\text{-meson:}\quad H_ {3}\left\lvert \Omega_ {0} \right\rangle =A_ {3}\left\lvert \Omega_ {0}\right\rangle=0 \implies A_ {3}=0 .\] <p>To study what is going on with multi-mesons states, we need to expand the Hamiltonians $H_ {2}$ and $H_ {3}$ in ladder operators. According to the convention we introduced in the first part of the note, we have</p> \[H_ {2} = \int \frac{d^{3}p}{(2\pi)^{3}} \, \omega_ {p} A^{\ddagger}_ {p} A_ {p} .\] <p>Let $(123)$ be the permutation action that maps 1st element to 2nd, 2nd to 3rd, and 3rd to 1st, we have</p> \[\begin{align*} H_ {3} = &amp;-\frac{mg}{\sqrt{2}} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \frac{d^{3}p_ {2}}{(2\pi)^{3}}\, \bigg\{ A_ {p_ {1}}^{\ddagger}A_ {p_ {1}}^{\ddagger}A_ {p_ {3}}^{\ddagger} \\ &amp;+ \frac{1}{2\omega_ {p_ {3}}}A_ {p_ {1}}^{\ddagger}A_ {p_ {2}}^{\ddagger}A_ {-p_ {3}} + (123)+(123)^{2} \\ &amp;+\frac{1}{2\omega_ {p_ {2}}} \frac{1}{2\omega_ {p_ {3}}} A_ {p_ {1}}^{\ddagger}A_ {-p_ {2}} A_ {-p_ {3}} + (123)+(123)^{2} \\ &amp; + \left.\frac{1}{2\omega_ {p_ {1}}2\omega_ {p_ {2}}2\omega_ {p_ {3}}} A_ {-p_ {1}}A_ {-p_ {2}}A_ {-p_ {3}} \right\rbrace \\ &amp;+ m^{2}\delta v_ {1}'\left( A^{\ddagger}_ {p=0}+\frac{A_ {p=0}}{2m} \right), \\ p_ {3} =&amp; -p_ {1}-p_ {2},\\ \delta v_ {1}' =&amp; \delta v_ {1}+\frac{m}{\sqrt{2}g} \left( \frac{\delta g}{g} - \frac{\delta m^{2}_ {1}}{2m^{2}} - \frac{\delta m^{4}_ {1}}{2g^{2}}\right). \end{align*}\] <p>In order to solve the order $g$ HRC1 equation</p> \[H_ {2}\left\lvert{\Omega_ {1}}\right\rangle = -H_ {3} \left\lvert{\Omega_ {0}}\right\rangle\] <p>for $\left\lvert \Omega_ {1} \right\rangle$, we would like to find the inverse of $H_ {2}$. The kernel of $H_ {2}$ is $\left\lvert \Omega_ {0} \right\rangle$, of course inverse of $H_ {2}$ does not exist on $\left\lvert \Omega_ {0} \right\rangle$. But we can always define the inverse on other Fock states. For example, consider a 3-meson state $\left\lvert \vec{p}_ {1}\vec{p}_ {2}\vec{p}_ {3} \right\rangle_ {0}$, since $H_ {2}$ is diagonalized in these basis, the inverse can be trivially written as $\frac{1}{H_ {2}}$ where $H_ {2}$ is to be understood as its eigenvalues,</p> \[\frac{1}{ H_ {2}} \left\lvert \vec{p}_ {1}\vec{p}_ {2}\vec{p}_ {3} \right\rangle_ {0} = \frac{1}{\omega_ {p_ {1}}\omega_ {p_ {2}}\omega_ {p_ {3}}} \left\lvert \vec{p}_ {1}\vec{p}_ {2}\vec{p}_ {3} \right\rangle_ {0}.\] <p>Since we have established that $\left\lvert \Omega_ {1} \right\rangle$ does not contain $\Omega_ {0}$ component, that is $\Omega_ {1}$ does not contain the kernel of $H_ {2}$, we can write formally</p> \[\left\lvert \Omega_ {1} \right\rangle = - \frac{1}{H_ {2}} H_ {3} \left\lvert \Omega_ {0} \right\rangle\] <p>Substitute the expression for $H_ {3}$, we have (after some trivial rearrangement)</p> \[\begin{align*} \left\lvert \Omega_ {1} \right\rangle =&amp; \frac{mg}{\sqrt{2}} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}_ {1}\vec{p}_ {2}\vec{p}_ {3} \right\rangle_ {0} }{(\omega_ {p_ {1}}+\omega_ {p_ {2}}+\omega_ {p_ {3}})} \\ &amp;- m\delta v_ {1}' \left\lvert \vec{p}=0 \right\rangle_ {0} ,\\ \vec{p}_ {3} =&amp; -\vec{p}_ {1}-\vec{p}_ {2}. \end{align*}\] <p>The no-tadpole rule demands that $\left\langle \Omega \right\rvert\phi \left\lvert \Omega \right\rangle=0$, expand $\left\lvert \Omega \right\rangle$ up to $\left\lvert \Omega_ {1} \right\rangle$ we get</p> \[\begin{align*} \left\langle \Omega \right\rvert \phi \left\lvert \Omega \right\rangle &amp;= (\left\langle \Omega_ {0} \right\rvert +\left\langle \Omega_ {1} \right\rvert ) \phi (\left\lvert \Omega_ {0} \right\rangle +\left\lvert \Omega_ {1} \right\rangle ) \\ &amp;= 2 \left\langle \Omega_ {0} \right\rvert \int \frac{d^{3}k}{(2\pi)^{3}} \, e^{ -i\vec{k}\cdot \vec{x} } \frac{A_ {-k}}{2\omega_ {k}} (-m\delta v_ {1}')A^{\ddagger}_ {p=0} \left\lvert \Omega_ {0} \right\rangle \\ &amp;= -\delta v_ {1}'\\ &amp;=0. \end{align*}\] <p>The long-ass expression for $\delta v_ {1}’$ turns out to be zero. This can be translated into a relation for $\delta v_ {1}$, which will help us to simplify the higher order Hamiltonians:</p> \[\boxed{ \delta v_ {1}=- \frac{m}{\sqrt{2}g} \left( \frac{\delta g}{g}- \frac{\delta m_ {1}^{2}}{2m^{2}}- \frac{\delta m_ {1}^{4}}{2g^{2}} \right). }\] <p>Putting everything together, we find</p> \[\boxed{ \mathcal{H}_ {3}(\vec{x}) = -\frac{mg}{\sqrt{2}} \phi^{3}(\vec{x}), }\] <p>everything else has disappeared. The first order correction to vacuum state is</p> \[\boxed{ \left\lvert \Omega_ {1} \right\rangle = \frac{mg}{\sqrt{2}} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}_ {1}\vec{p}_ {2}\vec{p}_ {3} \right\rangle_ {0} }{(\omega_ {p_ {1}}+\omega_ {p_ {2}}+\omega_ {p_ {3}})}, \quad \vec{p}_ {1}+\vec{p}_ {2}+\vec{p}_ {3}=0 }\] <p>We are not quite done yet, we need to find the interaction correction to the momentum eigenstates. At lowest order we have $H_ {0}\left\lvert \vec{p} \right\rangle_ {0}=\omega_ {p}\left\lvert \vec{p} \right\rangle_ {0}$, and after we include all orders we require that $H\left\lvert \vec{p} \right\rangle=\omega_ {p}\left\lvert p \right\rangle$. Our task is to find the difference between $\left\lvert - \right\rangle_ {0}$ and $\left\lvert - \right\rangle$ order by order.</p> <p>In order to make it easier to keep tracks of the number of mesons, let’s introduce the following notation.</p> <ul> <li>For a state, we use a superscript in parenthesis to denote how many meson it contains, $\left\lvert \psi \right\rangle^{(m)}$ indicates $\left\lvert \psi \right\rangle$ contains $m$ mesons, hence belongs to the $m$-meson Fock (sub)space. The superscript is sometimes inside the ket notation, sometimes out (as in here), I will in general not pay too much attention to it as long as it is clear what is meant;</li> <li>For an operator, we use the same notation to denote <strong>how many mesons it creates</strong> when acting on a state, for example $\mathcal{O}^{(m)}$ means that $\mathcal{O}^{(m)}\left\lvert \psi(n) \right\rangle$ as a whole has $m+n$ states.</li> </ul> <p>There is also a nice diagrammatic symbol that Jarah created (Jarah diagram) to indicate different contributions. An explanation of how such diagrams work is given in the figure below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/jarahDiagram-480.webp 480w,/img/kink/jarahDiagram-800.webp 800w,/img/kink/jarahDiagram-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/jarahDiagram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The diagram is separated into two parts, the vertex lies in the middle. First, look at the equation at the bottum, where the left hand side is the higher order correction that we want to calculate, the right hand side is how we calculate it, by acting on a lower-order state by some operator. The operator is represented by the bullet in the middle, that's why $\mathcal{O}$ is placed right under the bullet. The diagram can be separated into two parts, the left panel corresponds to the left hand side of the equation, the right panel shows on what states the operator acts. The arrow-lines are not fermions but mesons, the arrow shows the direction in which the coupling-order increases. The coupling order always increases from right to the left. Note that in vacuum sector, momenta are conserved at each vertex. We will omit the vertical dashed lines that intersect meson lines. </div> <p>In the case of vacuum correction, I copy Jarah’s explanation here shamelessly.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/3_0_Vacuum-480.webp 480w,/img/kink/3_0_Vacuum-800.webp 800w,/img/kink/3_0_Vacuum-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/3_0_Vacuum.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> This graph represents the calculation of $\left\lvert \Omega_ {1} \right\rangle ^{3}$, which is $-H_ {2}^{-1} H_ {3}\left\lvert \Omega_ {0} \right\rangle ^{(0)}$. The vertex represents the operator $-H_ {2}^{-1} H_ {3}$. The order $i$ of $\left\lvert \Omega_ {i} \right\rangle$ increases as one moves to the left. Consider a vertical slice. It intersects some number of lines $n$. This represents the $n$-meson Fock space. To the right of the vertex there are no lines, reflecting the fact that $\left\lvert \Omega_ {0} \right\rangle=\left\lvert \Omega_ {0} \right\rangle^{(0)}$ lives in the zero-meson Fock space. To the left, there are three lines, as we are calculating a contribution $\left\lvert \Omega_ {1} \right\rangle^{(3)}$ to $\left\lvert \Omega_ {1} \right\rangle$ in the three-meson Fock space. </div> <p>As an application of the notation we talked about, consider $H_ {3}$. It has different parts creating different numbers of operators, we write them as</p> \[H_ {3}= H_ {3}^{(-3)} + H_ {3}^{(-1)}+H_ {3}^{(1)}+H_ {3}^{(3)},\] <p>where $H^{(-3)}$ is the part with three annihilation operators hence decrease the number of mesons by 3, etc. All these are summarized in the last chapter.</p> <hr/> <p>Now we can make use of the second HRC that defines the momenta eigenstates. It says</p> \[H _ {\text{full}} \left\lvert \vec{p} \right\rangle = \omega_ {p} \left\lvert \vec{p} \right\rangle ,\] <p>we do the following expansion:</p> \[H_ {\text{full}} = \sum_ {i\geq 2} H_ {i} , \quad \left\lvert \vec{p} \right\rangle = \left\lvert \vec{p} \right\rangle_ {0}+\sum_ {j\geq 1} \left\lvert \vec{p} \right\rangle_ {1}^{(j)}\] <p>since $H_ {2}\left\lvert \vec{p} \right\rangle_ {0}=\omega_ {0}\left\lvert \vec{p} \right\rangle_ {0}$, after some rearrangement we have the master formula</p> \[\boxed{ \sum_ {i\geq 3} H_ {i} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} = \left( \omega_ {p}-\sum_ {i\geq 2}H_ {i} \right)\sum_ {j\geq 1} \left\lvert \vec{p} \right\rangle_ {1}^{(j)}. }\] <p>Now, both parts will have contribution from different Fock space, for example $H_ {4}$ acting on $\left\lvert \vec{p} \right\rangle_ {0}^{(1)}$ may yield something with $2,3,\cdots$ free mesons. Now, besides matching the orders in coupling, we will also match the number of mesons.</p> <p>Let’s see what happens at leading order $\sim g$ of the coupling. At this order, with regards to the Hamiltonian, we only need up to $H_ {3}$, thus the master formula reads</p> \[H_ {3}\left\lvert \vec{p} \right\rangle_ {0}^{(1)} = (\omega _ {p} -H_ {2}) \left\lvert \vec{p} \right\rangle_ {1}.\] <p>There is not $H_ {3}\left\lvert \vec{p} \right\rangle_ {1}$ since $\left\lvert \vec{p} \right\rangle_ {1}\sim g$. Out goal is to express $\left\lvert \vec{p} \right\rangle_ {1}$ in terms of $\left\lvert \vec{p} \right\rangle_ {0}$.</p> <p>The RHS adopts an expansion in Fock space:</p> \[(\omega_ {p}-H_ {2})\sum_ {n\geq 1}\left\lvert \vec{p} \right\rangle_ {1}^{(n)},\quad \text{with n mesons.}\] <p>The same expansion on the left hand side reads</p> \[H_ {3} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} = H_ {3}^{(-3)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} + H_ {3}^{(-1)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} + H_ {3}^{(1)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} + H_ {3}^{(3)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} ,\] <p>Again, there is nothing else since $H_ {4}$ is of order $g^{2}$ already. To see how many particles (we will use particle and meson interchangeably in this note) are there, we just add all the upstairs number together, for example $H^{(3)}\left\lvert \vec{p} \right\rangle^{(1)}$ has $3+1=4$ particles in it.</p> <p>Putting The LHS and RHS together we have</p> \[\sum_ {i}H_ {3}^{(i)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} = (\omega_ {p}-H_ {2})\sum_ {n\geq 1}\left\lvert \vec{p} \right\rangle_ {1}^{(n)}, \quad i\in \left\lbrace -3,-1,1,3 \,\middle\vert\, \right\rbrace .\] <p>Balancing the superscripts, namely require $i+1=n$, we get</p> \[\begin{align*} H_ {3}^{(1)}\left\lvert \vec{p} \right\rangle_ {0}^{(1)} =&amp; (\omega _ {p} -H_ {2}) \left\lvert \vec{p} \right\rangle_ {1}^{(2)}, \\ H_ {3}^{(3)}\left\lvert \vec{p} \right\rangle_ {0}^{(1)} =&amp; (\omega _ {p} -H_ {2}) \left\lvert \vec{p} \right\rangle_ {1}^{(4)}. \end{align*}\] <p>Next we need to substitute the expression for $H_ {3}$ in terms of ladder operator and contract, which can be found in the last section of the note. By the end of the day we get two components of $\left\lvert \vec{p} \right\rangle_ {1}$, namely the 2-free-meson and 4-free-meson components, as shown below:</p> \[\begin{align*} \left\lvert \vec{p} \right\rangle_ {1}^{(2)} &amp;= (\omega _ {p} -H_ {2})^{-1}H_ {3}^{(1)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)}， \\ \left\lvert \vec{p} \right\rangle_ {1}^{(4)} &amp;= (\omega _ {p} -H_ {2})^{-1} H_ {3}^{(3)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)}. \end{align*}\] <p>Substitute the expression for $H_ {3}$ in terms of ladder operators, after some simplification we get</p> \[\left\lvert \vec{p} \right\rangle_ {1}^{(2)} = \frac{3mg}{2\sqrt{2}\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}}\, \frac{\left\lvert \vec{p}_ {1},\vec{p}- \vec{p}_ {1} \right\rangle^{(2)}_ {0}}{\omega_ {p_ {1}}+\omega_ {p-p_ {1}}-\omega_ {p}} .\] <p>Let’s take a closer look at $\left\lvert \vec{p} \right\rangle_ {1}^{(2)}$. Since $\omega(p)$ is not a linear function in $p$, considering $\vec{p}_ {1}+\vec{p}_ {2}=\vec{p}$, we have $\omega(\vec{p}_ {1})+\omega (\vec{p_ {2}}) \neq \omega(\vec{p})$, thus the integrand does not go to zero, hence is free of singularities.</p> <p>Another component is</p> \[\left\lvert \vec{p} \right\rangle_ {1}^{(4)} = \frac{mg}{\sqrt{2}} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p},\vec{p}_ {1,2,-1-2} \right\rangle_ {0}^{(4)}}{\omega_ {p_ {1}}+\omega_ {p_ {2}}+\omega_ {p_ {1}+p_ {2}}}.\] <hr/> <h2 id="order-g2">Order $g^{2}$</h2> <p>The Hamiltonian is</p> \[H_ {4}=\int d^{3}x : \mathcal{H}_ {4}:\] <p>where the Hamiltonian density reads (<strong>we have used the relation for $\delta v_ {1}$</strong>)</p> \[\mathcal{H}_ {4} = \frac{g^{2}}{4}\phi^{4} + \phi^{2}\left( - \frac{\delta m_ {1}^{2}}{2} \right) + \phi(m^{2}\delta v_ {2})+ \frac{m^{4}(\delta g)^{2}}{16g^{4}} - \frac{m^{2}\delta v_ {1}^{2}}{2} + A_ 4,\] <p>Let’s rewrite it as</p> \[\mathcal{H}_ {4} = \frac{g^{2}}{4} \phi^{4} + C_ {2} \phi^{2} + C_ {1} \phi + C_ {0}.\] <p>The vacuum renormalization condition at order $g^{2}$ reads</p> \[H_ {4}\left\lvert \Omega_ {0} \right\rangle+H_ {3}\left\lvert \Omega_ {1} \right\rangle+H_ {2}\left\lvert \Omega_ {2} \right\rangle=0.\] <p>We need to decompose $H_ {4}$ into Fock subspace. We already know how to do that for $\phi^{3}$, we just need to workout $\phi^{4}$. The same calculation we did for $H_ {3}$ gets us</p> \[\begin{align*} :\phi^{4}: =&amp; :\phi^{4}:^{(4)}+:\phi^{4}:^{(2)}+:\phi^{4}:^{(0)}+:\phi^{4}:^{(-2)}+:\phi^{4}:^{(-4)},\\ :\phi^{4}:^{(4)} =&amp; \int \prod_ {i=1}^{4} \frac{d^{3}p_ {i}}{(2\pi)^{3}} \, e^{ -i\vec{x}\cdot \sum_ {i=1}^{4}\vec{p}_ {i} } A^{\ddagger}_ {p_ {1}}A^{\ddagger}_ {p_ {2}}A^{\ddagger}_ {p_ {3}}A^{\ddagger}_ {p_ {4}} ,\\ :\phi^{4}:^{(2)} =&amp; 4\int \prod_ {i=1}^{4} \frac{d^{3}p_ {i}}{(2\pi)^{3}} \, e^{ -i\vec{x}\cdot \sum_ {i=1}^{4}\vec{p}_ {i} } A^{\ddagger}_ {p_ {1}}A^{\ddagger}_ {p_ {2}}A^{\ddagger}_ {p_ {3}} \frac{A_ {-p_ {4}}}{2\omega_ {p_ {4}}} ,\\ :\phi^{4}:^{(0)} =&amp; 6\int \prod_ {i=1}^{4} \frac{d^{3}p_ {i}}{(2\pi)^{3}} \, e^{ -i\vec{x}\cdot \sum_ {i=1}^{4}\vec{p}_ {i} } A^{\ddagger}_ {p_ {1}} A^{\ddagger}_ {p_ {2}} \frac{A_ {-p_ {3}}}{2\omega_ {p_ {3}}}\frac{A_ {-p_ {4}}}{2\omega_ {p_ {4}}} ,\\ :\phi^{4}:^{(-2)} =&amp; 4\int \prod_ {i=1}^{4} \frac{d^{3}p_ {i}}{(2\pi)^{3}} \, e^{ -i\vec{x}\cdot \sum_ {i=1}^{4}\vec{p}_ {i} } A^{\ddagger}_ {p_ {1}} \frac{A_ {-p_ {2}}}{2\omega_ {p_ {2}}}\frac{A_ {-p_ {3}}}{2\omega_ {p_ {3}}}\frac{A_ {-p_ {4}}}{2\omega_ {p_ {4}}} ,\\ :\phi^{4}:^{(-4)} =&amp; \int \prod_ {i=1}^{4} \frac{d^{3}p_ {i}}{(2\pi)^{3}} \, e^{ -i\vec{x}\cdot \sum_ {i=1}^{4}\vec{p}_ {i} } \frac{A_ {-p_ {1}}}{2\omega_ {p_ {1}}}\frac{A_ {-p_ {2}}}{2\omega_ {p_ {2}}}\frac{A_ {-p_ {3}}}{2\omega_ {p_ {3}}}\frac{A_ {-p_ {4}}}{2\omega_ {p_ {4}}} . \end{align*}\] <h3 id="vacuum-states-correction">Vacuum states correction</h3> <p>Let’s look at the 1-meson subspace first. There is no such contribution from $H_ {3}$. What about from $H_ {4}$? For $H_ {4}\left\lvert \Omega_ {0} \right\rangle^{(0)}$ to have one meson, we need $H_ {4}^{(1)}$ to act on $\left\lvert \Omega_ {0} \right\rangle$. We have</p> <p>\(\begin{align*} \left\lvert \Omega_ {2} \right\rangle^{(1)} &amp;= - H_ {2}^{-1} H_ {4}^{(1)} \left\lvert \Omega_ {0} \right\rangle, \\ &amp;= - C_ {1} H_ {2}^{-1} :\phi:\left\lvert \Omega_ {0} \right\rangle \\ &amp;= - \frac{C_ {1}}{m} \left\lvert \vec{p} \right\rangle_ {0}, \quad \vec{p}=0. \end{align*}\) The last condition, $p=0$, is required by momentum conservation.</p> <p>The no-tadpole rule at order $g^{2}$ gives as that</p> \[\text{no-tadpole at }g^{2}: - \frac{C_ {1}}{m^{2}}+ \left\langle \Omega_ {1} \right\rvert\phi \left\lvert \Omega_ {1} \right\rangle=0,\] <p>however $\left\lvert \Omega_ {1} \right\rangle$ has only 3-meson component, thus the second term in the above equation is zero, we have $C_ {1}=0$. This fixes another counter term:</p> \[\boxed{ \delta v_ {2}=0. }\] <p>It simplifies the Hamiltonian to</p> \[\mathcal{H}_ {4}=\frac{g^{2}}{4} \phi^{4} - \frac{\delta m_ {1}^{2}}{2} \phi^{2} + C_ {0}.\] <p>In general the no-tadpole rule require the linear term in $\phi$ to vanish, and the true-vacuum condition require the constant term to vanish, which we will check later.</p> <hr/> <p><strong>Introduce the shorthand notation $\left\lvert \vec{p}_ {-1-2} \right\rangle:=\left\lvert -\vec{p}_ {1}-\vec{p}_ {2} \right\rangle$, $\left\lvert \vec{p}_ {-1’-2’} \right\rangle:=\left\lvert -\vec{p}’_ {1}-\vec{p}’_ {2} \right\rangle$ and $\left\lvert \vec{p}_ {1,2} \right\rangle:=\left\lvert \vec{p}_ {1}\vec{p}_ {2} \right\rangle$. Also let’s write $\omega_ {1}=\omega_ {p_ {1}},\, \omega_ {1’}=\omega_ {\vec{p}_ {1}’},\,\omega_ {1+2}=\omega_ {p_ {1}+p_ {2}}$ and</strong></p> \[\frac{d^{3}p_ {1,2,1',2'}}{(2\pi)^{12}} := \frac{d^{3}p_ {1}}{(2\pi)^{3}}\frac{d^{3}p_ {2}}{(2\pi)^{3}}\frac{d^{3}p'_ {1}}{(2\pi)^{3}}\frac{d^{3}p'_ {2}}{(2\pi)^{3}}.\] <p>The decomposition of $H_ {4}$ by meson numbers reads</p> \[\begin{align*} H_ {4} =&amp; \sum_ {n=0}^{4} H_ {4}(4-2n), \\ H_ {4}^{(4)} =&amp; \frac{g^{2}}{4} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, A^{\ddagger}_ {1}A^{\ddagger}_ {2}A^{\ddagger}_ {3}A^{\ddagger}_ {-1-2-3} ,\\ H_ {4}^{(2)} =&amp; g^{2} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, A^{\ddagger}_ {1}A^{\ddagger}_ {2}A^{\ddagger}_ {3} \frac{A_ {1+2+3}}{2\omega_ {1+2+3}} \\ &amp;- \frac{\delta m_ {1}^{2}}{2} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, A_ {1}^{\ddagger}A_ {-1}^{\ddagger},\\ H_ {4}^{(0)} =&amp; \frac{3g^{2}}{2} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}}\, A^{\ddagger}_ {1} A^{\ddagger}_ {2} \frac{A_ {-3}}{2\omega_ {3}}\frac{A_ {1+2+3}}{2\omega_ {1+2+3}} \\ &amp;- \delta m_ {1}^{2} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{A_ {1}^{\ddagger}A_ {1}}{2\omega_ {1}}+\int d^{3}x \, C_ {0} ,\\ H_ {4}^{(-2)} =&amp; g^{2}\int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}}\, A^{\ddagger}_ {1} \frac{A_ {-2}}{2\omega_ {2}}\frac{A_ {-3}}{2\omega_ {3}}\frac{A_ {1+2+3}}{2\omega_ {1+2+3}} \\ &amp; -\frac{\delta m_ {1}^{2}}{2}\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{A_ {1}A_ {-1}}{(2\omega_ {1})^{2}} , \\ H_ {4}^{(-4)} =&amp; \frac{g^{2}}{4} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}}\, \frac{A_ {-1}}{2\omega_ {1}}\frac{A_ {-2}}{2\omega_ {2}}\frac{A_ {-3}}{2\omega_ {3}}\frac{A_ {1+2+3}}{2\omega_ {1+2+3}} , \end{align*}\] <p>where we have used the condition that $\vec{p}_ {1}+\vec{p}_ {2}+\vec{p}_ {3}+\vec{p}_ {4}=0$.</p> <p>We have now all the ingredients needed to solve</p> \[\boxed{ \left\lvert \Omega_ {2} \right\rangle=-H_ {2}^{-1} H_ {4}\left\lvert \Omega_ {0} \right\rangle - H_ {2}^{-1} H_ {3}\left\lvert \Omega_ {1} \right\rangle, }\] <p>one meson number at a time. We will go from $6$ mesons and down to $0$ mesons.</p> <p>In $6$-meson Fock space we have</p> \[\begin{align*} \left\lvert \Omega_ {2} \right\rangle^{(6)} =&amp; -H_ {2}^{-1} H_ {3}^{(3)}\left\lvert \Omega_ {1} \right\rangle^{(3)} \\ =&amp;\frac{m^{2}g^{2}}{2} \int \frac{d^{3}p_ {1,2,1',2'}}{(2\pi)^{12}} \, \\ &amp;\times \frac{\left\lvert \vec{p}_ {1,2,-1-2}\;\vec{p}'_ {1,2,-1-2} \right\rangle_ {0}}{(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(\omega_ {1}+\omega_ {2}+\omega_ {1+2}+\omega_ {1}'+\omega_ {2}'+\omega_ {1'+2'})} \end{align*}\] <p>where</p> \[\left\lvert \vec{p}'_ {1,2,-1-2} \right\rangle_ {0} := \left\lvert \vec{p}'_ {1},\vec{p}'_ {2},\vec{p}'_ {-1-2} \right\rangle_ {0} := \left\lvert \vec{p}'_ {1},\vec{p}'_ {2},-\vec{p}'_ {1}-\vec{p}'_ {2} \right\rangle_ {0}.\] <p>This is represented by the figure below. Note the manifestation of momentum conservation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kink/p63-480.webp 480w,/img/kink/p63-800.webp 800w,/img/kink/p63-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kink/p63.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Here we have some operator (vertex) acting on a state with three mesons, namele $\left\lvert \Omega_ {1} \right\rangle^{(3)}$. The three momenta $\vec{p}_ {1}, \vec{p}_ {2}$ and $\vec{p}_ {3}=-\vec{p}_ {1}-\vec{p}_ {2}$ in the "initial" states are passed on to the "final" state, that's why we have the three long arrows arossing the left and right panel. Momentum conservation demands $\vec{p}_ {1} + \vec{p}_ {2} +\vec{p}_ {3}=0$. The so-called vertex creats another three mesons, that's why we have three meson lines starting in the middle of the plot. Momentum conservation also applies to primed momenta. </div> <p>In the case of 4-mesons, both $H_ {3}$ and $H_ {4}$ contribute. The contribution from $H_ {4}$ is simpler:</p> \[- H_ {2}^{-1} H_ {4}\left\lvert \Omega_ {0} \right\rangle= - \frac{g^{2}}{4} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \frac{\left\lvert \vec{p}_ {1,2,3,-1-2-3} \right\rangle_ {0}}{\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2+3}} ,\] <p>while the contribution from $H_ {3}\left\lvert \Omega_ {1} \right\rangle$ is more complicated due to the expansion of $\left\lvert \Omega_ {1} \right\rangle$, we have</p> \[\begin{align*} H_ {3}^{(1)}\left\lvert \Omega_ {1} \right\rangle^{(3)} =&amp; -\frac{mg}{\sqrt{2}} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, \frac{3}{2\omega_ {1+2}}A_ {1}^{\ddagger}A_ {2}^{\ddagger}A_ {1+2} \times \frac{mg}{\sqrt{2}} \int \frac{d^{3}p_ {1',2'}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p}_ {1',2',-1'-2'}\right\rangle_ {0} }{(\omega_ {1'}+\omega_ {2'}+\omega_ {3'})}\\ =&amp; - \frac{m^{2}g^{2}}{2}\int \frac{d^{3}p_ {1,2,1',2'}}{(2\pi)^{12}} \, \frac{3A_ {1}^{\ddagger}A_ {2}^{\ddagger}A_ {1+2}A_ {1'}^{\ddagger}A^{\ddagger}_ {2'}A^{\ddagger}_ {-1'-2'}\left\lvert \Omega_ {0} \right\rangle}{2\omega_ {1+2}(\omega_ {1'}+\omega_ {2'}+\omega_ {3'})}\\ =&amp; - \frac{9m^{2}g^{2}}{4}\int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \frac{\left\lvert \vec{p}_ {1,2,3,-1-2-3} \right\rangle_ {0}}{\omega_ {1+2}(\omega_ {1+2}+\omega_ {3}+\omega_ {1+2+3})} \end{align*}\] <p>where all the contractions offer a factor of 3 and $\left\lvert \vec{p}_ {1,2,3,-1-2-3} \right\rangle$ is a short-handed notation for $\left\lvert \vec{p}_ {1} \vec{p}_ {2} \vec{p}_ {3},-\vec{p}_ {1} -\vec{p}_ {2} -\vec{p}_ {3}\right\rangle$.</p> <p>Thus</p> \[\begin{align*} -H_ {2}^{-1} H_ {3}\left\lvert \Omega_ {1} \right\rangle =&amp; \left( \frac{3mg}{2} \right)^{2} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \frac{1}{\omega_ {1+2}(\omega_ {1+2}+\omega_ {3}+\omega_ {1+2+3})} \\ &amp;\times \frac{\left\lvert \vec{p}_ {1,2,3,-1-2-3} \right\rangle_ {0}}{(\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2+3})}. \end{align*}\] <p>Putting them together we have</p> \[\boxed{ \begin{align*} \left\lvert \Omega_ {2} \right\rangle^{(4)} = &amp; \frac{g^{2}}{4} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \left( \frac{9m^{2}}{\omega_ {1+2}(\omega_ {1+2}+\omega_ {3}+\omega_ {1+2+3})} -1\right) \\ &amp;\times \frac{\left\lvert \vec{p}_ {1,2,3,-1-2-3} \right\rangle_ {0}}{\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2+3}} \end{align*} }\] <hr/> <p>We move on to calculate $\left\lvert \Omega_ {2} \right\rangle^{(2)}=-H_ {2}^{-1} H_ {4}^{(2)}\left\lvert \Omega_ {0} \right\rangle - H_ {2}^{-1} H_ {3}^{(-1)}\left\lvert \Omega_ {1}\right\rangle^{(3)}$. We get</p> \[\begin{align*} - \frac{1}{H_ {2}}H_ {4}^{(2)}\left\lvert \Omega_ {0} \right\rangle =&amp; \frac{1}{2}\delta m^{2} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}_ {1,-1} \right\rangle_ {0}}{2\omega_ {1}} \\ - \frac{1}{H_ {2}}H_ {3}^{(-1)}\left\lvert \Omega_ {1}\right\rangle^{(3)} =&amp; \frac{9m^{2}g^{2}}{4} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p}_ {1,-1} \right\rangle_ {0}}{\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})2\omega_ {1}}. \end{align*}\] <p>Putting them together we get</p> \[\left\lvert \Omega \right\rangle^{(2)}_ {2}=\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lbrace \delta m^{2}+\frac{9 m^{2}g^{2}}{2} \int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \right\rbrace \frac{\left\lvert \vec{p}_ {1,-1} \right\rangle_ {0}}{4\omega_ {1}} .\] <p>The integral over $p_ {2}$ has superfacial degree of divergent $3-3=0$, thus adopts logarithmic divergence. <strong>This divergence ought to be canceled by $\delta m^{2}$</strong>, as we will check later. If that’s true then $\left\lvert \Omega \right\rangle^{(2)}_ {2}=0.$</p> <hr/> <p>The HRC1 equation reads</p> \[H_ {4}^{0}\left\lvert \Omega_ {0} \right\rangle+H_ {3}^{(-3)}\left\lvert \Omega_ {1}^{(3)} \right\rangle+H_ {2}\left\lvert \Omega_ {2} \right\rangle^{(0)}=0,\] <p>but $\left\lvert \Omega_ {2} \right\rangle^{(0)}$ does not exist by construction, if $\left\lvert \Omega_ {2} \right\rangle$ has any $\left\lvert \Omega_ {0} \right\rangle$ component we can absorbe it into the $\left\lvert \Omega_ {0} \right\rangle$ itself. So we are left with</p> \[H_ {4}^{(0)}\left\lvert \Omega_ {0} \right\rangle+H_ {3}^{(-3)}\left\lvert \Omega_ {1}^{(3)} \right\rangle=0,\] <p>This fixes the counter terms. The contribution from $H_ {4}^{(0)}$ is a divergent constant:</p> \[H_ {4}^{(0)}\left\lvert \Omega_ {0} \right\rangle = \int d^{3}x \, C_ {0} \left\lvert \Omega_ {0} \right\rangle.\] <p>So the contribution from $H_ {3}$ must cancel it. In order to compare the above result, we need to keep the $\int d^{3}x$ integral, writing $H_ {3}^{(-3)}$ as</p> \[H_ {3}^{(-3)} = -\frac{mg}{\sqrt{2}} \int d^{3}x \, e^{ -i\vec{x}\cdot(\vec{p}_ {1}+\vec{p}_ {2}+\vec{p}_ {3}) } \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}}\, \frac{1}{2\omega_ {1}2\omega_ {2}2\omega_ {3}} A_ {-1}A_ {-2}A_ {-3} .\] <p>Similar to the calculation before, we have</p> \[\begin{align*} H_ {3}^{(-3)} \left\lvert \Omega_ {1} \right\rangle^{3} =&amp; -\frac{mg}{\sqrt{2}} \int d^{3}x \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}}\, \frac{1}{2\omega_ {1}2\omega_ {2}2\omega_ {3}} A_ {-1}A_ {-2}A_ {-3}\\ &amp;\times \frac{mg}{\sqrt{2}} \int \frac{d^{3}p_ {1}'}{(2\pi)^{3}} \frac{d^{3}p_ {2}'}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}_ {1}'\vec{p}_ {2}',-\vec{p}_ {1}'-p_ {2}' \right\rangle_ {0} }{(\omega_ {p_ {1}'}+\omega_ {p_ {2}'}+\omega_ {p_ {1}'+p_ {2}'})}\\ =&amp; -\frac{3g^{2}m^{2}}{8} \int d^{3}x \int \frac{d^{3}p_ {1}d^{3}p_ {2}}{(2\pi)^{6}} \, \frac{\left\lvert \Omega \right\rangle_ {0}}{\omega_ {p_ {1}}\omega_ {p_ {2}}\omega_ {p_ {1}+p_ {2}}(\omega_ {p_ {1}}+\omega_ {p_ {2}}+\omega_ {p_ {1}+p_ {2}})} . \end{align*}\] <p>To cancel the divergence, we obviously need</p> \[C_ {0} = A_ {4}' \equiv \frac{3g^{2}m^{2}}{8} \int \frac{d^{3}p_ {1}d^{3}p_ {2}}{(2\pi)^{6}} \, \frac{1}{\omega_ {p_ {1}}\omega_ {p_ {2}}\omega_ {p_ {1}+p_ {2}}(\omega_ {p_ {1}}+\omega_ {p_ {2}}+\omega_ {p_ {1}+p_ {2}})} .\] <hr/> <h3 id="momentum-states-corrections">Momentum states corrections</h3> <p>Next we turn to the renormalization condition for the momentum eigenstates. At order $g^{2}$ this conditions reads</p> \[(\omega_ {p}-H_ {2})\left\lvert p \right\rangle_ {2} = H_ {4} \left\lvert \vec{p} \right\rangle_ {0} +H_ {3}\left\lvert p \right\rangle_ {1} .\] <p>Again we start from the highest meson number and go downwards. The highest possible particle number comes from $H_ {3}^{(3)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)}$, which is simple since there only exists creation operators. It reads:</p> \[\left\lvert \vec{p} \right\rangle_ {2}^{(7)} = \frac{m^{2}g^{2}}{2} \int \frac{d^{3}p_ {1,2,3,4}}{(2\pi)^{12}} \, \frac{\left\lvert \vec{p},\vec{p}_ {1,2,3,4,-1-2,-3-4} \right\rangle_ {0}}{(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {4}+\omega_ {1+2}+\omega_ {3+4})}.\] <hr/> <p><strong>Next Fock space is $5$</strong>, coming from the following contributions:</p> \[H_ {4}^{(4)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} = \frac{g^{2}}{4} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \left\lvert \vec{p},\vec{p}_ {1,2,3,-1-2-3} \right\rangle_ {0} ,\] \[H_ {3}^{(3)} \left\lvert \vec{p} \right\rangle_ {1}^{(2)} = \frac{3g^{2}m^{2}}{4\omega_ {p}}\int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \frac{\left\lvert \vec{p}-\vec{p}_ {1}, \vec{p}_ {1,2,3,-2-3} \right\rangle_ {0}}{\omega _ {p}-\omega_ {1}-\omega_ {p-p_ {1}}}\] <p>and</p> \[H_ {3}^{(1)} \left\lvert \vec{p} \right\rangle_ {1}^{(4)} = - \frac{3g^{2}m^{2}}{4} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \left( \frac{3\left\lvert \vec{p},\vec{p}_ {1,2,3,-1-2-3} \right\rangle_ {0}}{\omega_ {2+3}(\omega_ {1}+\omega_ {2+3}+\omega_ {1+2+3})} + \frac{\left\lvert \vec{p}_ {1,2,3,-1-2},\vec{p}-\vec{p}_ {3} \right\rangle_ {0}}{\omega_ {p}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \right)\] <p>Does the first term agrees with Jarah’s result? The second term is fine as it is.</p> <p>Altogether these lead to</p> \[\begin{align*} \left\lvert \vec{p}\right\rangle_ {2}^{(5)} =&amp; \frac{g^{2}}{4}\int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \left( \frac{9m^{2}}{\omega_ {2+3}(\omega_ {1}+\omega_ {2+3}+\omega_ {1+2+3})}-1 \right) \frac{\left\lvert \vec{p},\vec{p}_ {1,2,3,-1-2-3} \right\rangle_ {0}}{\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2+3}} \\ &amp;+ \frac{3g^{2}m^{2}}{4\omega _ {p} }\int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \frac{\left\lvert \vec{p}_ {1,2,3,-1-2},\vec{p}-\vec{p}_ {3} \right\rangle_ {0}}{(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(\omega_ {3}+\omega_ {p+p_ {3}}-\omega_ {p})}. \end{align*}\] <hr/> <p><strong>3-Fock space comes from</strong></p> \[(\omega_ {p}-H_ {2})\left\lvert \vec{p} \right\rangle_ {2}^{(3)} = H_ {4}^{(2)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} +H_ {3}^{(1)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)} +H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)}.\] <p>Same as before, we get:</p> \[\begin{align*} H_ {4}^{(2)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} =&amp; \frac{g^{2}}{2\omega_ {p}} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\left\lvert \vec{p}-\vec{p}_ {1}-\vec{p}_ {2},\vec{p}_ {1,2} \right\rangle_ {0} -\frac{\delta m^{2}}{2}\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p},\vec{p}_ {1,-1} \right\rangle_ {0}, \\ H_ {3}^{(1)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)} =&amp; - \frac{9g^{2}m^{2}}{4\omega_ {p}} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p}-\vec{p}_ {1}-\vec{p}_ {2},\vec{p}_ {1,2} \right\rangle_ {0}}{\omega_ {1+2}(-\omega _ {p} +\omega_ {p-p_ {1}-p_ {2}}+\omega_ {1+2})} ,\\ H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)} =&amp; - \frac{9m^{2}g^{2}}{4} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p},\vec{p}_ {1,-1} \right\rangle_ {0}}{\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})}\\ &amp;- \frac{9m^{2}g^{2}}{4\omega _ {p} } \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p}-\vec{p}_ {1}-\vec{p}_ {2},\vec{p}_ {1,2} \right\rangle_ {0}}{\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \end{align*}\] <p>Inverting $\omega _ {p}-H_ {2}$ we get</p> \[\begin{align*} \left\lvert \vec{p} \right\rangle_ {2}^{(3)} =&amp; \frac{g^{2}}{4\omega _ {p} }\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \left( -2+\frac{9m^{2}}{\omega_ {1+2}}\left( \frac{1}{-\omega _ {p} +\omega_ {p-p_ {1}-p_ {2}}+\omega_ {1+2}} + \frac{1}{\omega_ {1}+\omega_ {2}+\omega_ {1+2}} \right) \right)\\ &amp;\times \frac{\left\lvert \vec{p}_ {1,2},\vec{p}-\vec{p}_ {1}-\vec{p}_ {2} \right\rangle_ {0}}{-\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {p-p_ {1}-p_ {2}}} \\ &amp;- \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left( \delta m^{2} + \frac{9m^{2}g^{2}}{2}\int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \right) \frac{\left\lvert \vec{p},\vec{p}_ {1,-1} \right\rangle_ {0}}{4\omega_ {1}}. \end{align*}\] <p>As a consistancy check, this renormalization condition for $\delta m^{2}$ is the same as that we got from $\left\lvert \Omega_ {2} \right\rangle$.</p> <hr/> <p><strong>1-Fock space comes from</strong></p> <p>We have</p> \[(\omega_ {p}-H_ {2})\left\lvert \vec{p} \right\rangle_ {2}^{(1)} = H_ {4}^{(0)} \left\lvert \vec{p} \right\rangle_ {0}^{(1)} +H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)} +H_ {3}^{(-3)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)}.\] <p>During the computation sometimes we need again to keep $d^{3}x$ when dealing with conter terms.</p> <p>we have</p> \[\begin{align*} H_ {4}^{(0)}\left\lvert p \right\rangle_ {0}^{(1)} =&amp; A_ {4}'\int d^{3}x \, \left\lvert \vec{p} \right\rangle_ {0} - \frac{\delta m^{2}}{2\omega _ {p} }\left\lvert \vec{p} \right\rangle_ {0},\\ H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)} =&amp; -\frac{9g^{2}m^{2}}{8\omega _ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p} \right\rangle_ {0}}{\omega_ {1}\omega_ {p+p_ {1}}(-\omega _ {p} +\omega_ {1}+\omega_ {p+p_ {1}})},\\ H_ {3}^{(-3)}\left\lvert p \right\rangle_ {1}^{(4)} =&amp; - \frac{9g^{2}m^{2}\left\lvert \vec{p} \right\rangle_ {0}}{8\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{1}{\omega_ {1}\omega_ {p+p_ {1}}(\omega _ {p} +\omega_ {1}+\omega_ {p+p_ {1}})}\\ &amp;- \frac{3}{8}g^{2}m^{2}\delta^{3}(0) \left\lvert \vec{p} \right\rangle_ {0} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{1}{\omega_ {1}\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})}. \end{align*}\] <p>In the last term if we write $(2\pi)^{3}\delta^{3}(0)=\int d^{3}x$ then it cancels with the $A_ {4}’$ term.</p> <p>Another approach is to use</p> \[\int d^{3}x \, \left( - \frac{\delta m^{2}}{2} :\phi^{2}: \right) = - \frac{\delta m^{2}}{2}\int d^{3}x \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, e^{ -i\vec{x}\cdot(\vec{p}_ {1}+\vec{p}_ {2}) } \left( A^{\ddagger}_ {1,2}+2A^{\ddagger}_ {1} \frac{A_ {-2}}{2\omega_ {1}}+\frac{A_ {-1,-2}}{2\omega_ {1}2\omega_ {2}} \right).\] <p>Now recall that due to the conservation of momentum, $(\omega_ {p}-H_ {2})\left\lvert \vec{p} \right\rangle_ {2}^{(1)} =0$, this allows as to calculate $\delta m^{2}$,</p> \[\begin{align*} \frac{\delta m^{2}}{2\omega _ {p} } =&amp;- \frac{9m^{2}g^{2}}{8\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{1}{\omega_ {1}\omega_ {p+p_ {1}}(-\omega _ {p} +\omega_ {1}+\omega_ {p+p_ {1}})} \\ &amp;- \frac{9g^{2}m^{2}}{8\omega _ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{1}{\omega_ {1}\omega_ {p+ {p_ {1}}}(\omega _ {p} +\omega_ {1}+\omega_ {p+p_ {1}})} \\ =&amp; - \frac{9m^{2}g^{2}}{8\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{1}{\omega_ {1}\omega_ {p+p_ {1}}} \left( \frac{2(\omega_ {1}+\omega_ {p+p_ {1}})}{(\omega_ {1}+\omega_ {p+p_ {1}})^{2}-\omega p^{2}} \right)\\ \approx &amp;- \frac{9g^{2}m^{2}}{16\pi^{2}\omega _ {p} }\ln\left( \frac{\Lambda}{m} \right) + \text{const},\\ \implies \delta m^{2} =&amp; - \frac{9g^{2}m^{2}}{8\pi^{2}}\ln\left( \frac{\Lambda}{m} \right) + \text{const} \end{align*}\] <p>The constant part can be fixed later.</p> <p>As for the orthogonal condition, we can check that $_ {0}\left\langle \vec{p}_ {1,2} \middle\vert \vec{p}\right\rangle_ {2}$ indeed is zero.</p> <h2 id="coupling-counter-term-at-order-g3">Coupling counter term at order $g^{3}$</h2> <p>The 3rd-order Hamiltonian density reads</p> \[\mathcal{H}_ {5} = C_ {3}\phi^{3}+ C_ {1}\phi + C_ {0}.\] <p>where</p> \[\begin{align*} C_ {3}=&amp; \frac{m\delta g}{\sqrt{2}} +\frac{g \delta m_ {1}^{2}}{2\sqrt{2}m} - \frac{m^{2}\delta m_ {1}^{4}}{2\sqrt{2}g}, \\ C_ {1}=&amp; m^{2}\delta v_ {3}-\frac{3gm\mathcal{I}_ {1}}{\sqrt{2}}+\frac{m^{3}\delta g^{2}}{\sqrt{2}g^{3}}-\frac{m \delta g\delta m_ {1}^{2}}{2\sqrt{2}g^{2}} -\frac{\delta m_ {1}^{4}}{8\sqrt{2}gm},\\ C_ {0}=&amp; A_ {5}. \end{align*}\] <p>We have the renormalization condition saying that the interaction vacuum is really a vacuum, $H\left\lvert \Omega \right\rangle=0$, the $\left\lvert \Omega_ {0} \right\rangle$ component of it then requires that $C_ {0}=A_ {5}=0$.</p> <p>Since we have</p> \[\begin{align*} \int d^{3}x : \phi^{3}:^{(3)} =&amp; \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, A_ {1}^{\ddagger}A_ {2}^{\ddagger}A_ {-1-2}^{\ddagger} \\ \int d^{3}x:\phi^{3}:^{(1)} =&amp; \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, \frac{3}{2\omega_ {1+2}}A_ {1}^{\ddagger}A_ {2}^{\ddagger}A_ {1+2}\\ \int d^{3}x:\phi^{3}:^{(-1)} =&amp; \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\,\frac{3}{2\omega_ {2}} \frac{1}{2\omega_ {1+2}} A_ {1}^{\ddagger}A_ {-2} A_ {1+2} \\ \int d^{3}x:\phi^{3}:^{(-3)} =&amp; \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, \frac{1}{2\omega_ {1}2\omega_ {2}2\omega_ {1+2}} A_ {-1}A_ {-2}A_ {1+2} \end{align*}\] <p>we have</p> \[\begin{align*} H_ {5}=&amp; H_ {5}^{(3)}+H_ {5}^{(1)}+H_ {5}^{(-1)}+H_ {5}^{(-3)} ,\\ H_ {5}^{(3)}=&amp; C_ {3}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, A_ {1}^{\ddagger}A_ {2}^{\ddagger}A_ {-1-2}^{\ddagger} ,\\ H_ {5}^{(1)}=&amp; C_ {3}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, \frac{3}{2\omega_ {1+2}}A_ {1}^{\ddagger}A_ {2}^{\ddagger}A_ {1+2} +C_ {1}A_ {0}^{\ddagger} ,\\ H_ {5}^{(-1)}=&amp;C_ {3} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\,\frac{3}{2\omega_ {2}} \frac{1}{2\omega_ {1+2}} A_ {1}^{\ddagger}A_ {-2} A_ {1+2} + C_ {1} \frac{A_ {0}}{2m} ,\\ H_ {5}^{(-3)}=&amp; C_ {3}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, \frac{1}{2\omega_ {1}2\omega_ {2}2\omega_ {1+2}} A_ {-1}A_ {-2}A_ {1+2} ,\\ \end{align*}\] <h3 id="the-vacuum-correction">The vacuum correction</h3> <p>where $C_ {3},C_ {1}$ are parameters to be determined. As before the equation for $\left\lvert \Omega \right\rangle_ {3}$ is :</p> \[\left\lvert \Omega \right\rangle_ {3} = -H_ {2}^{-1}(H_ {5}\left\lvert \Omega \right\rangle_ {0}+H_ {4}\left\lvert \Omega \right\rangle_ {1}+H_ {3}\left\lvert \Omega \right\rangle_ {2}),\] <p>if separate into different Fock spaces we get</p> \[\begin{align*} - H_ {2}\left\lvert \Omega \right\rangle_ {3}^{(9)} =&amp; H_ {3}^{(3)} \left\lvert \Omega \right\rangle_ {2}^{(6)},\\ - H_ {2}\left\lvert \Omega \right\rangle_ {3}^{(7)} =&amp; H_ {3}^{(3)} \left\lvert \Omega \right\rangle_ {2}^{(4)}+H_ {3}^{(1)} \left\lvert \Omega \right\rangle_ {2}^{(6)}+H_ {4}^{(4)}\left\lvert \Omega \right\rangle_ {1}^{(3)},\\ - H_ {2}\left\lvert \Omega \right\rangle_ {3}^{(5)} =&amp; H_ {3}^{(1)}\left\lvert \Omega \right\rangle_ {2}^{(4)} +H_ {3}^{(-1)}\left\lvert \Omega \right\rangle_ {2}^{(6)} +H_ {4}^{(2)}\left\lvert \Omega \right\rangle_ {1}^{(3)}, \\ - H_ {2}\left\lvert \Omega \right\rangle_ {3}^{(3)} =&amp; H_ {3}^{(-1)}\left\lvert \Omega \right\rangle_ {2}^{(4)} +H_ {3}^{(-3)}\left\lvert \Omega \right\rangle_ {2}^{(6)} +H_ {4}^{(0)} \left\lvert \Omega \right\rangle_ {1}^{(3)}, \\ - H_ {2}\left\lvert \Omega \right\rangle_ {3}^{(1)} =&amp; H_ {3}^{(-3)}\left\lvert \Omega \right\rangle_ {2}^{(4)}+H_ {4}^{(-2)}\left\lvert \Omega \right\rangle_ {1}^{(3)}+H_ {5}^{(1)}\left\lvert \Omega \right\rangle_ {0}. &amp; \end{align*}\] <p>Substitute the ingredients and simplify as before, we get the following results.</p> <h4 id="leftlvert-omega-rightrangle_-39">$\left\lvert \Omega \right\rangle_ {3}^{(9)}:$</h4> <p>\(\begin{align*} \left\lvert \Omega \right\rangle_ {3}^{(9)} =&amp; \frac{g^{3}m^{3}}{2\sqrt{2}}\int \frac{d^{3}p_ {1,2,3,4,5,6}}{(2\pi)^{18}} \, \frac{1}{(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {4}+\omega_ {1+2}+\omega_ {3+4})} \\ &amp;\times \frac{\left\lvert \vec{p}_ {1,2,3,4,5,6},-\vec{p}_ {1}-\vec{p}_ {2},-\vec{p}_ {3}-\vec{p}_ {4}，-\vec{p}_ {5}-\vec{p}_ {6} \right\rangle_ {0}}{(\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {4}+\omega_ {5}+\omega_ {6}+\omega_ {1+2}+\omega_ {3+4}+\omega_ {5+6})} \end{align*}\)</p> <h4 id="leftlvert-omega-rightrangle_-37">$\left\lvert \Omega \right\rangle_ {3}^{(7)}:$</h4> \[\begin{align*} H_ {3}^{(3)}\left\lvert \Omega \right\rangle_ {2}^{(4)} =&amp; -\frac{g^{3}m}{4\sqrt{2}}\int \frac{d^{3}p_ {1,2,3,4,5}}{(2\pi)^{15}} \, \frac{ \frac{9m^{2}}{\omega_ {3+4}(\omega_ {3+4}+\omega_ {5}+\omega_ {3+4+5})}-1 }{\omega_ {3}+\omega_ {4}+\omega_ {5}+\omega_ {3+4+5}} \left\lvert \vec{p}_ {1,2,3,4,5},\vec{p}_ {-1-2},\vec{p}_ {-3-4-5} \right\rangle_ {0},\\ H_ {3}^{(1)}\left\lvert \Omega \right\rangle_ {2}^{(6)} =&amp; -\frac{9 g^3 m^3}{4 \sqrt{2}} \int \frac{d^{3}p_ {1,2,3,4,5}}{(2\pi)^{15}} \, \frac{\left\lvert \vec{p}_ {1,2,3,4,5,-1-2-3,-4-5} \right\rangle_ {0}}{\omega_ {1+2}(\omega_ {1+2}+\omega_ {3}+\omega_ {1+2+3})(\omega_ {4}+\omega_ {5}+\omega_ {4+5})}, \\ H_ {4}^{(4)}\left\lvert \Omega \right\rangle_ {1}^{(3)} =&amp; \frac{g^{3}m}{4\sqrt{2}} \int \frac{d^{3}p_ {1,2,3,4,5}}{(2\pi)^{15}} \, \frac{\left\lvert \vec{p}_ {1,2,3,4,5,-1-2-3,-4-5} \right\rangle_ {0}}{\omega_ {4}+\omega_ {5}+\omega_ {4+5}}. \end{align*}\] <p>Put altogether and inverse $-H_ {2}$, we get:</p> \[\left\lvert \Omega \right\rangle_ {3}^{(7)} = \frac{g^{3}m}{4\sqrt{2}}\int \frac{d^{3}p_ {1,2,3,4,5}}{(2\pi)^{15}} \, \frac{\left( \frac{9m^{2}}{\omega_ {1+2}(\omega_ {1+2}+\omega_ {3}+\omega_ {1+2+3})}-1 \right)\left\lvert \vec{p}_ {1,2,3,4,5,-1-2-3,-4-5} \right\rangle_ {0}}{(\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2+3})(\omega_ {4}+\omega_ {5}+\omega_ {4+5})}.\] <h4 id="leftlvert-omega-rightrangle_-35">$\left\lvert \Omega \right\rangle_ {3}^{(5)}:$</h4> <p>we have</p> \[\begin{align*} H_ {3}^{(1)}\left\lvert \Omega \right\rangle_ {2}^{(4)} =&amp; \frac{3 g^3 m}{4 \sqrt{2}} \int \frac{d^{3}p_ {1,2,3,4}}{(2\pi)^{12}} \, \frac{\left\lvert \vec{p}_ {1,2,3,4,-1-2-3-4} \right\rangle_ {0}}{\omega_ {1+2}+\omega_ {3}+\omega_ {4}+\omega_ {1+2+3+4}} \\ &amp;\times \left(2- \frac{9m^{2}}{\omega_ {3+4}(\omega_ {1+2}+\omega_ {3+4}+\omega_ {1+2+3+4})}-\frac{9m^{2}}{\omega_ {1+2+3}(\omega_ {1+2+3}+\omega_ {4}+\omega_ {1+2+3+4})}\right), \\ H_ {3}^{(-1)}\left\lvert \Omega \right\rangle_ {2}^{(6)}=&amp; -\frac{9g^{3}m^{3}}{4\sqrt{2}}\int \frac{d^{3}p_ {1,2,3,4}}{(2\pi)^{12}} \, \left( \frac{\left\lvert \vec{p}_ {1,2,3,-1,-2-3} \right\rangle_ {0}}{\omega_ {4}\omega_ {1+4}(\omega_ {2}+\omega_ {3}+\omega_ {2+3})(\omega_ {1}+\omega_ {4}+\omega_ {1+4})} \right. \\ &amp;\left. + \frac{3\left\lvert \vec{p}_ {2,4,-1-2,1+3,-3-4} \right\rangle_ {0}}{\omega_ {1}\omega_ {3}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(\omega_ {1}+\omega_ {2}+\omega_ {1+2}+\omega_ {3}+\omega_ {4}+\omega_ {3+4})}\right) , \\ H_ {4}^{(2)}\left\lvert \Omega \right\rangle_ {1}^{(3)} =&amp; -\delta m^{2}\frac{gm}{2\sqrt{2}} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \frac{\left\lvert \vec{p}_ {1,2,3,-1,-2-3} \right\rangle_ {0}}{\omega_ {2}+\omega_ {3}+\omega_ {2+3}} \\ &amp;+ \frac{3g^{3}m}{2\sqrt{2}} \int \frac{d^{3}p_ {1,2,3,4}}{(2\pi)^{12}} \, \frac{\left\lvert \vec{p}_ {1,2,3,-3-4,-1-2+4} \right\rangle_ {0}}{\omega_ {4}(\omega_ {3}+\omega_ {4}+\omega_ {3+4})}. \end{align*}\] <p>Next step would be inversing the $-H_ {2}$, put everything together and simplify, but I’ll leave to later.</p> <h4 id="leftlvert-omega-rightrangle_-33">$\left\lvert \Omega \right\rangle_ {3}^{(3)}$</h4> <p>We have</p> \[\begin{align*} H_ {3}^{(-1)}\left\lvert \Omega \right\rangle_ {2}^{(4)} =&amp; \frac{8g^{2}m}{2\sqrt{2}}\int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \left\lvert \vec{p}_ {1,3,-1-3} \right\rangle_ {0} \left( \frac{1}{2\omega_ {2}\omega_ {1+2}(\omega_ {2}+\omega_ {1+2}+\omega_ {3}+\omega_ {1+3})} \right. \\ &amp;+ \frac{3m^{2}}{\omega_ {2}\omega_ {1+2}\omega_ {1+2+3}(\omega_ {2}+\omega_ {3}+\omega_ {1+2}+\omega_ {1+3})(\omega_ {2}+\omega_ {1+3}+\omega_ {1+2+3})} \\ &amp;\left.+\frac{3m^{2}(2\omega_ {1}+\omega_ {2}+\omega_ {1+2}+\omega_ {3}+\omega_ {1+3})}{4\omega_ {1}\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(\omega_ {1}+\omega_ {3}+\omega_ {1+3})(\omega_ {2}+\omega_ {3}+\omega_ {1+2}+\omega_ {1+3})} \right), \\ H_ {3}^{(-3)}\left\lvert \Omega \right\rangle_ {2}^{(6)} =&amp; \\ H_ {4}^{(0)}\left\lvert \Omega \right\rangle_ {1}^{(3)} =&amp; \frac{gm}{4\sqrt{2}}\int d^{3}x \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, \left( \frac{-6\delta m^{2}}{\omega_ {1}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})}+\frac{4A_ {4}'}{(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \right)\left\lvert \vec{p}_ {1,2,-1-2} \right\rangle_ {0} \\ &amp;+ \frac{9mg^{3}}{4\sqrt{2}}\int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \frac{\left\lvert \vec{p}_ {1,2,-1-2} \right\rangle_ {0}}{\omega_ {3}\omega_ {1+2+3}(\omega_ {1+2}+\omega_ {3}+\omega_ {1+2+3})} \end{align*}\] <h4 id="leftlvert-omega-rightrangle_-31">$\left\lvert \Omega \right\rangle_ {3}^{(1)}$</h4> <p>We have</p> \[\begin{align*} H_ {5}^{(1)} \left\lvert \Omega \right\rangle_ {0}^{(0)} =&amp; C_ {1}\left\lvert \vec{p} \right\rangle {\Large\mid}_ {\vec{p}=0} , \\ H_ {3}^{(-3)} \left\lvert \Omega \right\rangle_ {2}^{(4)} =&amp; -\frac{3gm\delta m^{2}}{4\sqrt{2}} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}=0 \right\rangle_ {0}}{\omega_ {1}^{2}(m+2\omega_ {1})}+\frac{3g^{3}}{4\sqrt{2}}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p}=0 \right\rangle_ {0}}{\omega_ {1}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \\ H_ {4}^{(-2)} \left\lvert \Omega \right\rangle_ {1}^{(3)} =&amp; - \delta m^{2} \frac{3gm}{4\sqrt{2}} \int \frac{d^{3}p_ {1}}{(2\pi)} \, \frac{\left\lvert \vec{p}=0 \right\rangle}{\omega_ {1}^{2}(m+2\omega_ {1})}+\frac{3g^{3}}{4\sqrt{2}}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p}=0 \right\rangle}{\omega_ {1}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})}. \end{align*}\] <p>Inverse $-H_ {2}$ and put everything altogether</p> \[\left\lvert \Omega \right\rangle_ {3}^{(1)} = - \frac{C_ {1}\left\lvert \vec{p}=0 \right\rangle_ {0}}{m}-\frac{3gm\delta m^{2}}{2\sqrt{2}} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}=0 \right\rangle_ {0}}{\omega_ {1}^{2}(m+2\omega_ {1})}+\frac{3g^{3}}{2\sqrt{2}}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p}=0 \right\rangle_ {0}}{\omega_ {1}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})}\] <hr/> <p><strong>Let’s apply the no-tadpole condition.</strong> Expand and match the coupling, we find the equation involving $H_ {5}$ is</p> \[_ {3}\left\langle \Omega \right\rvert \phi(\vec{x})\left\lvert \Omega \right\rangle_ {0} = - _ {2}\left\langle \Omega \right\rvert \phi(\vec{x})\left\lvert \Omega \right\rangle_ {1},\] <p>This should allow us to fix a constant parameter ($C_ {1}$ specifically) in $H_ {5}$. The right hand side reduces to</p> \[\begin{align*} \left\langle \Omega_ {2} \right\rvert\phi(\vec{x})\left\lvert \Omega_ {1} \right\rangle =&amp; \frac{e^{ -i\vec{k}\cdot \vec{x} }}{2m} C_ {1} \\ =&amp;\left\langle \Omega_ {2}^{(4)} \right\rvert\phi(\vec{x})\left\lvert \Omega_ {1}^{(3)} \right\rangle \\ =&amp; -\frac{e^{ -i\vec{k}\cdot \vec{x} }3g^{3}m\left\lvert \Omega_ {0} \right\rangle}{2\sqrt{2}}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{-9m^{2}+\omega_ {1}^{2}+\omega_ {1}(\omega_ {2}+\omega_ {1+2})}{\omega_ {1}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})^{2}(m+\omega_ {1}+\omega_ {2}+\omega_ {1+2})}\\ &amp;+ \frac{-9m^{2}+\omega_ {2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})}{\omega_ {2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})^{2}(m+\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \\ &amp;+ \frac{2(-9m^{2}+m\omega_ {1+2}+2\omega_ {1+2}^{2})}{\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(m+\omega_ {1}+\omega_ {2}+\omega_ {1+2})(m+2\omega_ {1+2})} \end{align*}\] <p>Looks like the integral can not be simplified any more.</p> <hr/> <h3 id="momentum-state-correction">Momentum state correction</h3> <p>From $H\left\lvert \vec{p} \right\rangle=\omega _ {p}\left\lvert \vec{p} \right\rangle$, expand it, the $g^{3}$ order part reads</p> \[(H_ {2}-\omega _ {p} )\left\lvert \vec{p} \right\rangle_ {3} = -H_ {3}\left\lvert \vec{p} \right\rangle_ {2}-H_ {4}\left\lvert \vec{p} \right\rangle_ {1} - H_ {5} \left\lvert \vec{p} \right\rangle_ {0},\] <p>separate into different Fock spaces we get:</p> \[\begin{align*} (\omega _ {p} -H_ {2})\left\lvert \vec{p} \right\rangle_ {3}^{(10)} =&amp; H_ {3}^{(3)}\left\lvert \vec{p} \right\rangle_ {2}^{(7)} , \\ (\omega _ {p} -H_ {2})\left\lvert \vec{p} \right\rangle_ {3}^{(8)} =&amp; H_ {3}^{(1)}\left\lvert \vec{p} \right\rangle_ {2}^{(7)} + H_ {3}^{(3)}\left\lvert \vec{p} \right\rangle_ {2}^{(5)} + H_ {4}^{(4)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)} , \\ (\omega _ {p} -H_ {2})\left\lvert \vec{p} \right\rangle_ {3}^{(6)} =&amp; H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {2}^{(7)} + H_ {3}^{(1)}\left\lvert \vec{p} \right\rangle_ {2}^{(5)}+ H_ {3}^{(3)}\left\lvert \vec{p} \right\rangle_ {2}^{(3)} + H_ {4}^{(2)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)}+H_ {4}^{(4)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)} , \\ (\omega _ {p} -H_ {2})\left\lvert \vec{p} \right\rangle_ {3}^{(4)} =&amp; H_ {3}^{(-3)}\left\lvert \vec{p} \right\rangle_ {2}^{(7)} +H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {2}^{(5)} + H_ {3}^{(1)} \left\lvert \vec{p} \right\rangle_ {2}^{(1)} + H_ {4}^{(0)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)}+ H_ {4}^{(2)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)} +H_ {5}^{(3)}\left\lvert \vec{p} \right\rangle_ {0}^{(1)}, \\ (\omega _ {p} -H_ {2})\left\lvert \vec{p} \right\rangle_ {3}^{(2)} =&amp; H_ {3}^{(-3)}\left\lvert \vec{p} \right\rangle_ {2}^{(5)}+ H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {2}^{(3)} + H_ {4}^{(0)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)} + H_ {4}^{(-2)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)} +H_ {5}^{(1)}\left\lvert \vec{p} \right\rangle_ {0}^{(1)}, \\ (\omega _ {p} -H_ {2})\left\lvert \vec{p} \right\rangle_ {3}^{(0)} =&amp;H_ {3}^{(-3)} \left\lvert \vec{p} \right\rangle_ {2}^{(3)} +H_ {4}^{(-4)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)}+H_ {4}^{(-2)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)}+H_ {5}^{(-1)}\left\lvert \vec{p} \right\rangle_ {0}^{(1)}=0. \end{align*}\] <h4 id="leftlvert-vecp-rightrangle_-310">$\left\lvert \vec{p} \right\rangle_ {3}^{(10)}$</h4> <p>we have</p> \[H_ {3}^{(3)}\left\lvert \vec{p} \right\rangle_ {2}^{(7)} = - \frac{g^{3}m^{3}}{2\sqrt{2}} \int \frac{d^{3}p_ {1,2,3,4,5,6}}{(2\pi)^{18}} \, \frac{\left\lvert \vec{p},\vec{p}_ {1,2,3,4,5,6,-1-2,-3-4,-5-6} \right\rangle_ {0}}{\omega_ {3}+\omega_ {4}+\omega_ {5}+\omega_ {5}+\omega_ {3+4}+\omega_ {5+6}}\] <p>inverse $\omega _ {p}-H_ {2}$ we get</p> \[\begin{align*} \left\lvert \vec{p} \right\rangle_ {3}^{10} =&amp;- \frac{g^{3}m^{3}}{2\sqrt{2}} \int \frac{d^{3}p_ {1,2,3,4,5,6}}{(2\pi)^{18}} \, \frac{1}{\omega_ {3}+\omega_ {4}+\omega_ {5}+\omega_ {5}+\omega_ {3+4}+\omega_ {5+6}} \\ &amp;\times \frac{\left\lvert \vec{p},\vec{p}_ {1,2,3,4,5,6,-1-2,-3-4,-5-6} \right\rangle_ {0}}{\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {4}+\omega_ {5}+\omega_ {6}+\omega_ {1+2}+\omega_ {3+4}+\omega_ {5+6}}. \end{align*}\] <h4 id="leftlvert-vecp-rightrangle_-38">$\left\lvert \vec{p} \right\rangle_ {3}^{(8)}$</h4> <p>we get</p> \[\begin{align*} H_ {3}^{(1)}\left\lvert \vec{p} \right\rangle_ {2}^{(7)} =&amp; \\ H_ {3}^{(3)}\left\lvert \vec{p} \right\rangle_ {2}^{(5)} =&amp; \\ H_ {4}^{(4)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)} =&amp; \end{align*}\] <h4 id="leftlvert-vecp-rightrangle_-36">$\left\lvert \vec{p} \right\rangle_ {3}^{(6)}$</h4> <h4 id="leftlvert-vecp-rightrangle_-34">$\left\lvert \vec{p} \right\rangle_ {3}^{(4)}$</h4> <p>We have</p> \[H_ {5}\left\lvert \vec{p} \right\rangle_ {0} = C_ {5,3}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \left\lvert \vec{p},\vec{p}_ {1,2,-1-2} \right\rangle + \frac{3C_ {5,3}}{2\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}-\vec{p}_ {f_ {1}},\vec{p}_ {1} \right\rangle + C_ {5,1} \left\lvert \vec{p},0 \right\rangle .\] <h4 id="leftlvert-vecp-rightrangle_-32">$\left\lvert \vec{p} \right\rangle_ {3}^{(2)}$</h4> <p>In summary, we have</p> \[\begin{align*} H_ {3}^{(-3)}\left\lvert \vec{p} \right\rangle_ {2}^{(5)} =&amp; ,\\ H_ {3}^{(-1)}\left\lvert \vec{p} \right\rangle_ {2}^{(3)} =&amp; ,\\ H_ {4}^{(0)}\left\lvert \vec{p} \right\rangle_ {1}^{(2)} =&amp; \frac{3gm A_ {4}'}{2\sqrt{2}\omega _ {p} }\int d^{3}x \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle}{-\omega _ {p} +\omega_ {1}+\omega_ {p-p_ {1}}} \\ &amp; - \frac{3gm\delta m^{2}}{2\sqrt{2}\omega _ {p} } \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle}{\omega_ {1}(-\omega _ {p} +\omega_ {1}+\omega_ {p-p_ {1}})} \\ &amp;+ \frac{9mg^{3}}{8\sqrt{2}\omega _ {p} } \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle}{\omega_ {2}\omega_ {p+p_ {2}}(-\omega _ {p} +\omega_ {2}+\omega_ {p+p_ {2}})} \\ H_ {4}^{(-2)}\left\lvert \vec{p} \right\rangle_ {1}^{(4)}=&amp; ,\\ H_ {5}^{(1)}\left\lvert \vec{p} \right\rangle_ {0}^{(1)}=&amp; C_ {5,1} \left\lvert \vec{p},\vec{q}=0 \right\rangle + \frac{3C_ {5,3}}{2\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \left\lvert \vec{p}_ {1},\vec{p}-\vec{p}_ {1} \right\rangle_ {0} , \end{align*}\] <h4 id="leftlvert-vecp-rightrangle_-30">$\left\lvert \vec{p} \right\rangle_ {3}^{(0)}$</h4> <h1 id="summary-of-results">Summary of results</h1> <p>I use the shorthand notation $\left\lvert \vec{p}_ {-1-2} \right\rangle:=\left\lvert -\vec{p}_ {1}-\vec{p}_ {2} \right\rangle$, $\left\lvert \vec{p}_ {-1’-2’} \right\rangle:=\left\lvert -\vec{p}’_ {1}-\vec{p}’_ {2} \right\rangle$, $\omega_ {1}=\omega_ {p_ {1}}, \omega_ {1’}=\omega_ {\vec{p}_ {1}’},\omega_ {1+2}=\omega_ {p_ {1}+p_ {2}}$. Multiple indices usually denote tensor product, for example $\left\lvert \vec{p}_ {1,2} \right\rangle:=\left\lvert \vec{p}_ {1} \right\rangle\left\lvert \vec{p}_ {2} \right\rangle=\left\lvert \vec{p}_ {1}\vec{p}_ {2} \right\rangle$, and</p> \[\frac{d^{3}p_ {1,2,1',2'}}{(2\pi)^{12}} := \frac{d^{3}p_ {1}}{(2\pi)^{3}}\frac{d^{3}p_ {2}}{(2\pi)^{3}}\frac{d^{3}p'_ {1}}{(2\pi)^{3}}\frac{d^{3}p'_ {2}}{(2\pi)^{3}}.\] <p>We also simplify $A_ {p_ {1}}^{\ddagger}$ as $A_ {1}^{\ddagger}$ and $A_ {-p_ {1}-p_ {2}}$ as $A_ {-1-2}$.</p> <h2 id="hamiltonians">Hamiltonians:</h2> <p>In the below is the summary of Hamiltonians updated by Hamiltonian Renormalization Conditions (HRC):</p> \[\begin{align*} \mathcal{H}_ {0} =&amp; \mathcal{H}_ {1}=0, \\ \mathcal{H}_ {2} =&amp; \frac{\pi^{2}}{2} + \frac{(\partial \phi)^{2}}{2} + \frac{1}{2}m^{2}\phi^{2} \\ H_ {2} =&amp; \int \frac{d^{3}p}{(2\pi)^{3}} \, \omega_ {p} A^{\ddagger}_ {p} A_ {p} , \\ \mathcal{H}_ {3} =&amp; -\frac{mg}{\sqrt{2}} \phi^{3}, \\ H_ {3} =&amp; H_ {3}^{(3)}+H_ {3}^{(1)}+H_ {3}^{(-1)} + H_ {3}^{(-3)} \end{align*}\] <p>where</p> \[\begin{align*} H_ {3}^{(3)} =&amp; -\frac{mg}{\sqrt{2}} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, A_ {1}^{\ddagger}A_ {2}^{\ddagger}A_ {-1-2}^{\ddagger} \\ H_ {3}^{(1)} =&amp; -\frac{mg}{\sqrt{2}} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, \frac{3}{2\omega_ {1+2}}A_ {1}^{\ddagger}A_ {2}^{\ddagger}A_ {1+2} \\ H_ {3}^{(-1)} =&amp; -\frac{mg}{\sqrt{2}}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\,\frac{3}{2\omega_ {2}} \frac{1}{2\omega_ {1+2}} A_ {1}^{\ddagger}A_ {-2} A_ {1+2} \\ H_ {3}^{(-3)} =&amp; -\frac{mg}{\sqrt{2}}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, \frac{1}{2\omega_ {1}2\omega_ {2}2\omega_ {1+2}} A_ {-1}A_ {-2}A_ {1+2} . \end{align*}\] <p>In each term the momentum conservation is suggested by the fact that, if we treat the subscript of creation operators as plus momenta, that of the annihilation operators as negative momenta, then their summation gives zero, for example in $A_ {1}^{\ddagger}A_ {2}^{\ddagger}A_ {1+2}$ the total momentum would be (1+2-1-2) which is zero.</p> <p>and</p> \[\begin{align*} H_ {4} =&amp; \sum_ {n=0}^{4} H_ {4}^{(4-2n)}, \\ H_ {4}^{(4)} =&amp; \frac{g^{2}}{4} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, A^{\ddagger}_ {1}A^{\ddagger}_ {2}A^{\ddagger}_ {3}A^{\ddagger}_ {-1-2-3} ,\\ H_ {4}^{(2)} =&amp; g^{2} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, A^{\ddagger}_ {1}A^{\ddagger}_ {2}A^{\ddagger}_ {3} \frac{A_ {1+2+3}}{2\omega_ {1+2+3}} \\ &amp;- \frac{\delta m_ {1}^{2}}{2} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, A_ {1}^{\ddagger}A_ {-1}^{\ddagger},\\ H_ {4}^{(0)} =&amp; \frac{3g^{2}}{2} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}}\, A^{\ddagger}_ {1} A^{\ddagger}_ {2} \frac{A_ {-3}}{2\omega_ {3}}\frac{A_ {1+2+3}}{2\omega_ {1+2+3}} \\ &amp;- \delta m_ {1}^{2} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{A_ {1}^{\ddagger}A_ {1}}{2\omega_ {1}}+\int d^{3}x \, A_ {4}' ,\\ H_ {4}^{(-2)} =&amp; g^{2}\int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}}\, A^{\ddagger}_ {1} \frac{A_ {-2}}{2\omega_ {2}}\frac{A_ {-3}}{2\omega_ {3}}\frac{A_ {1+2+3}}{2\omega_ {1+2+3}} \\ &amp; -\frac{\delta m_ {1}^{2}}{2}\int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{A_ {1}A_ {-1}}{(2\omega_ {1})^{2}} , \\ H_ {4}^{(-4)} =&amp; \frac{g^{2}}{4} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}}\, \frac{A_ {-1}}{2\omega_ {1}}\frac{A_ {-2}}{2\omega_ {2}}\frac{A_ {-3}}{2\omega_ {3}}\frac{A_ {1+2+3}}{2\omega_ {1+2+3}} . \end{align*}\] <p>and</p> \[\begin{align*} H_ {5}=&amp; H_ {5}^{(3)}+H_ {5}^{(1)}+H_ {5}^{(-1)}+H_ {5}^{(-3)} ,\\ H_ {5}^{(3)}=&amp; C_ {5,3}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, A_ {1}^{\ddagger}A_ {2}^{\ddagger}A_ {-1-2}^{\ddagger} ,\\ H_ {5}^{(1)}=&amp; C_ {5,3}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, \frac{3}{2\omega_ {1+2}}A_ {1}^{\ddagger}A_ {2}^{\ddagger}A_ {1+2} +C_ {5,1}A_ {0}^{\ddagger} ,\\ H_ {5}^{(-1)}=&amp;C_ {5,3} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\,\frac{3}{2\omega_ {2}} \frac{1}{2\omega_ {1+2}} A_ {1}^{\ddagger}A_ {-2} A_ {1+2} + C_ {5,1} \frac{A_ {0}}{2m} ,\\ H_ {5}^{(-3)}=&amp; C_ {5,3}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}}\, \frac{1}{2\omega_ {1}2\omega_ {2}2\omega_ {1+2}} A_ {-1}A_ {-2}A_ {1+2} ,\\ \end{align*}\] <p>where $C_ {5,i}$ are some combinations of counter terms whose value will be fixed later.</p> <h2 id="vacuum-states-corrections">Vacuum states corrections</h2> <p>The corrections to vacuum states:</p> \[\begin{align*} \left\lvert \Omega \right\rangle_ {1}^{(3)} =&amp; \frac{mg}{\sqrt{2}} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p}_ {1,2,-1-2}\right\rangle_ {0} }{(\omega_ {1}+\omega_ {2}+\omega_ {3})}, \\ \left\lvert \Omega \right\rangle_ {2}^{(6)} =&amp;\frac{m^{2}g^{2}}{2} \int \frac{d^{3}p_ {1,2,1',2'}}{(2\pi)^{12}} \\ &amp;\times \frac{\left\lvert \vec{p}_ {1,2,-1-2}\;\vec{p}'_ {1,2,-1-2} \right\rangle_ {0}}{(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(\omega_ {1}+\omega_ {2}+\omega_ {1+2}+\omega_ {1'}+\omega_ {2'}+\omega_ {1'+2'})} \\ \left\lvert \Omega \right\rangle_ {2}^{(4)} =&amp; \frac{g^{2}}{4} \int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \left( \frac{9m^{2}}{\omega_ {1+2}(\omega_ {1+2}+\omega_ {3}+\omega_ {1+2+3})} -1\right) \\ &amp;\times \frac{\left\lvert \vec{p}_ {1,2,3,-1-2-3} \right\rangle_ {0}}{\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2+3}} ,\\ \left\lvert \Omega \right\rangle_ {3}^{(9)} =&amp; \frac{g^{3}m^{3}}{2\sqrt{2}}\int \frac{d^{3}p_ {1,2,3,4,5,6}}{(2\pi)^{18}} \, \frac{1}{(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {4}+\omega_ {1+2}+\omega_ {3+4})} \\ &amp;\times \frac{\left\lvert \vec{p}_ {1,2,3,4,5,6},-\vec{p}_ {1}-\vec{p}_ {2},-\vec{p}_ {3}-\vec{p}_ {4}，-\vec{p}_ {5}-\vec{p}_ {6} \right\rangle}{(\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {4}+\omega_ {5}+\omega_ {6}+\omega_ {1+2}+\omega_ {3+4}+\omega_ {5+6})}\\ \left\lvert \Omega \right\rangle_ {3}^{(7)} =&amp; \frac{g^{3}m}{4\sqrt{2}}\int \frac{d^{3}p_ {1,2,3,4,5}}{(2\pi)^{15}} \, \frac{\left( \frac{9m^{2}}{\omega_ {1+2}(\omega_ {1+2}+\omega_ {3}+\omega_ {1+2+3})}-1 \right)\left\lvert \vec{p}_ {1,2,3,4,5,-1-2-3,-4-5} \right\rangle}{(\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2+3})(\omega_ {4}+\omega_ {5}+\omega_ {4+5})},\\ \left\lvert \Omega \right\rangle_ {3}^{(5)} =&amp; \\ \left\lvert \Omega \right\rangle_ {3}^{(3)} =&amp; \\ \left\lvert \Omega \right\rangle_ {3}^{(1)} =&amp; - \frac{C_ {1}\left\lvert \vec{p}=0 \right\rangle}{m}-\frac{3gm\delta m^{2}}{2\sqrt{2}} \int \frac{d^{3}p_ {1}}{(2\pi)^{3}} \, \frac{\left\lvert \vec{p}=0 \right\rangle}{\omega_ {1}^{2}(m+2\omega_ {1})}+\frac{3g^{3}}{2\sqrt{2}}\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p}=0 \right\rangle}{\omega_ {1}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \end{align*}\] <p>The momentum eigenstates:</p> \[\begin{align*} \left\lvert \vec{p} \right\rangle_ {1}^{(2)} =&amp; \frac{3mg}{2\sqrt{2}\omega _ {p} }\int \frac{d^{3}p_ {1}}{(2\pi)^{3}}\, \frac{\left\lvert \vec{p}_ {1},\vec{p} -\vec{p}_ {1} \right\rangle^{(2)}_ {0}}{\omega_ {p_ {1}}+\omega_ {p-p_ {1}}-\omega_ {p}} , \\ \left\lvert \vec{p} \right\rangle_ {1}^{(4)} =&amp; \frac{mg}{\sqrt{2}} \int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \frac{\left\lvert \vec{p} \, \vec{p}_ {1,2,-1-2} \right\rangle_ {0}^{(4)}}{\omega_ {1}+\omega_ {2}+\omega_ {1+2}}, \\ \left\lvert \vec{p} \right\rangle_ {2}^{(7)} =&amp; \frac{m^{2}g^{2}}{2} \int \frac{d^{3}p_ {1,2,3,4}}{(2\pi)^{12}} \, \frac{\left\lvert \vec{p},\vec{p}_ {1,2,3,4,-1-2,-3-4} \right\rangle_ {0}}{\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {4}+\omega_ {1+2}+\omega_ {3+4}}, \\ \left\lvert \vec{p}\right\rangle_ {2}^{(5)} =&amp; \frac{g^{2}}{4}\int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \left( \frac{9m^{2}}{\omega_ {2+3}(\omega_ {1}+\omega_ {2+3}+\omega_ {1+2+3})}-1 \right) \frac{\left\lvert \vec{p},\vec{p}_ {1,2,3,-1-2-3} \right\rangle_ {0}}{\omega_ {1}+\omega_ {2}+\omega_ {3}+\omega_ {1+2+3}} \\ &amp;+ \frac{3g^{2}m^{2}}{4\omega _ {p} }\int \frac{d^{3}p_ {1,2,3}}{(2\pi)^{9}} \, \frac{\left\lvert \vec{p}_ {1,2,3,-1-2},\vec{p}-\vec{p}_ {3} \right\rangle_ {0}}{(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(\omega_ {3}+\omega_ {p+p_ {3}}-\omega_ {p})}.\\ \left\lvert \vec{p} \right\rangle_ {2}^{(3)} =&amp; \frac{g^{2}}{4\omega _ {p} }\int \frac{d^{3}p_ {1,2}}{(2\pi)^{6}} \, \left( -2+\frac{9m^{2}}{\omega_ {1+2}}\left( \frac{1}{-\omega _ {p} +\omega_ {p-p_ {1}-p_ {2}}+\omega_ {1+2}} + \frac{1}{\omega_ {1}+\omega_ {2}+\omega_ {1+2}} \right) \right)\\ &amp;\times \frac{\left\lvert \vec{p}_ {1,2},\vec{p}-\vec{p}_ {1}-\vec{p}_ {2} \right\rangle_ {0}}{-\omega _ {p} +\omega_ {1}+\omega_ {2}+\omega_ {p-p_ {1}-p_ {2}}} \end{align*}\] <p>The counter terms:</p> \[\begin{align*} \delta v_ {1}= &amp;- \frac{m}{\sqrt{2}g} \left( \frac{\delta g}{g}- \frac{\delta m_ {1}^{2}}{2m^{2}}- \frac{\delta m_ {1}^{4}}{2g^{2}} \right) \\ \delta v_ {2} =&amp; 0, \\ A_ {4}' \equiv&amp; \frac{3g^{2}m^{2}}{8} \int d^{3}x\,\frac{d^{3}p_ {1}d^{3}p_ {2}}{(2\pi)^{6}} \, \frac{1}{\omega_ {p_ {1}}\omega_ {p_ {2}}\omega_ {p_ {1}+p_ {2}}(\omega_ {p_ {1}}+\omega_ {p_ {2}}+\omega_ {p_ {1}+p_ {2}})} ,\\ \delta m^{2} =&amp; - \frac{9m^{2}g^{2}}{2}\int \frac{d^{3}p_ {2}}{(2\pi)^{3}} \, \frac{1}{\omega_ {2}\omega_ {1+2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \\ =&amp; - \frac{9g^{2}m^{2}}{8\pi^{2}}\ln\left( \frac{\Lambda}{m} \right) + \text{finite}, \\ \frac{\delta g}{g} =&amp; - \frac{9g^{2}\ln \Lambda}{16\pi^{2}}+\text{finite}. \end{align*}\] <p>where $A_ {4}’$ is the constant part of $H_ {4}$.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="#domainWall"/><category term="kink"/><summary type="html"><![CDATA[Spontaneous Symmetry Breaking]]></summary></entry><entry><title type="html">Example of Mathematica Package Quantum</title><link href="https://baiyangzhang.github.io/blog/2024/Example-of-Mathematica-Package-Quantum/" rel="alternate" type="text/html" title="Example of Mathematica Package Quantum"/><published>2024-08-04T00:00:00+00:00</published><updated>2024-08-04T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Example-of-Mathematica-Package-Quantum</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Example-of-Mathematica-Package-Quantum/"><![CDATA[<p>I have put together a short example of using Mathematica’s Quantum package to solve the harmonic oscillator problem <a href="https://github.com/BaiyangZhang/BaiyangZhang.github.io/blob/master/assets/Mathematica/ExampleHarmonicOscillator.nb">here</a>, hopefully I can apply this package to my current project on the quantum corrections of kink mass.</p>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[I have put together a short example of using Mathematica’s Quantum package to solve the harmonic oscillator problem here, hopefully I can apply this package to my current project on the quantum corrections of kink mass.]]></summary></entry><entry><title type="html">Note on The Moral Foundations of Politics</title><link href="https://baiyangzhang.github.io/blog/2024/Note-on-The-Moral-Foundations-of-Politics/" rel="alternate" type="text/html" title="Note on The Moral Foundations of Politics"/><published>2024-07-30T00:00:00+00:00</published><updated>2024-07-30T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Note-on-The-Moral-Foundations-of-Politics</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Note-on-The-Moral-Foundations-of-Politics/"><![CDATA[<h1 id="enlightenment-politics">Enlightenment Politics</h1> <blockquote> <p>f there is a single overarching idea shared in common by adherents to different strands of Enlightenment thinking, it is faith in the power of human reason to understand the true nature of our circumstances and ourselves. Human improvement is measured by the yardstick of <strong>individual rights</strong> that embody, and protect, <strong>human freedom</strong>.</p> </blockquote> <blockquote> <p>Descartes announced that he was in search of propositions that are impossible to doubt. His famous example, known as the <code class="language-plaintext highlighter-rouge">cogito</code>, was ‘‘I think, therefore I am.’’</p> </blockquote> <blockquote> <p>Immanuel Kant defined in <code class="language-plaintext highlighter-rouge">The Critique of Pure Reason</code> (1781), of placing knowledge ‘‘on the secure path of a science.’’</p> </blockquote> <blockquote> <p>These developments in philosophy reflected and reinforced the emergence of modern scientific consciousness.</p> </blockquote> <p>Such ideas, as necessary conditions for the development of natural science (not merely technology), seems to never had appeared in China. Year 1781 is the year 乾隆四十六年 in China, one of the most closed, ignorant, and autocratic era in history.</p> <blockquote> <p>During the seventeenth and eighteenth centuries, when the hallmark of scientific knowledge was indubitable certainty, ethics, political philosophy, and the human sciences were regarded as superior to the natural sciences. This view seems strange from the vantage point of the twenty-first century, when fields like physics, chemistry, astronomy, geology, and biology have all advanced with astonishing speed to discoveries that would have been unimaginable in the eighteenth century.</p> </blockquote> <h2 id="the-workmanship-ideal-of-knowledge">The Workmanship Ideal of Knowledge</h2> <blockquote> <p>The first distinctive feature of the early Enlightenment concerns the range of <code class="language-plaintext highlighter-rouge">a priori knowledge</code>, the kind of knowledge that either follows from definitions or is otherwise deduced from covering principals. This is the kind of knowledge Descartes had in mind when he formulated his cogito and that Kant located in the realm of ‘‘analytic judgments.’</p> </blockquote> <p><strong>Epistemology</strong> is a branch of philosophy that studies the nature, origin, and limits of human knowledge. The term comes from the Greek words “episteme,” meaning knowledge or understanding, and “logos,” meaning study or discourse. Epistemology addresses questions such as:</p> <ul> <li>What is knowledge?</li> <li>How is knowledge acquired?</li> <li>What do people know?</li> <li>How do we know what we know?</li> <li>What are the limits of human knowledge?</li> <li>What makes beliefs justified or rational?</li> </ul> <p>In exploring these questions, epistemology deals with the definition of knowledge and its scope and limits. It often involves debating between different theories of knowledge, such as empiricism (the idea that knowledge comes primarily from sensory experience), rationalism (the idea that reason is the main source of knowledge), and constructivism (the idea that knowledge is constructed by individuals through their interactions with the world).</p> <p>Immanuel Kant distinguished between two types of judgments: <code class="language-plaintext highlighter-rouge">analytic</code> and <code class="language-plaintext highlighter-rouge">synthetic</code>. These distinctions are central to his philosophy, especially in his work “Critique of Pure Reason.”</p> <ol> <li> <p><strong>Analytic Judgments</strong>: An analytic judgment is one where the predicate (the part of the sentence that says something about the subject) is contained within the subject itself. The truth of an analytic judgment is derived from the meanings of the words involved and logical reasoning. They are tautological in nature and do not add any new information about the world. For example, the statement “All bachelors are unmarried” is analytic because the predicate “unmarried” is part of the definition of the subject “bachelor.”</p> </li> <li> <p><strong>Synthetic Judgments</strong>: A synthetic judgment, on the other hand, is one where the predicate adds something to the subject that is not contained within it. The truth of a synthetic judgment is determined through how our concepts relate to the world and cannot be known just by understanding the meanings of the words. They require empirical investigation or intuition. For instance, “The cat is on the mat” is a synthetic judgment because the concept of “the cat” does not inherently include the concept of “being on the mat.”</p> </li> </ol> <p>Kant’s distinction between analytic and synthetic judgments is fundamental to his epistemology, particularly in addressing the question of how human beings can have knowledge about the world. He further introduced the concept of “synthetic a priori” judgments, which are synthetic judgments that are known independently of experience (a priori), like mathematical truths.</p> <hr/> <p>The <code class="language-plaintext highlighter-rouge">creationist</code> or <code class="language-plaintext highlighter-rouge">workmanship</code> theory in political science, often associated with the work of John Locke, is a theory of political obligation. It suggests that political authority and legitimacy derive from the consent of the governed, likening the role of the government or ruler to that of a craftsman or creator who constructs a system with the consent and for the benefit of the people.</p> <p>This theory is rooted in the idea that political and social structures are artificial constructs, made by human beings, unlike natural phenomena. The “creationist” aspect implies that political structures are deliberately created or constructed, rather than organically evolved. The “workmanship” aspect emphasizes the idea that the creators or rulers of these structures have a responsibility to the people they govern, similar to how a craftsman is responsible for the quality and function of their creation.</p> <p>Locke’s theory was revolutionary at its time because it challenged the prevailing notion of the divine right of kings, suggesting instead that political authority is justified only when it serves the interests of the governed and respects their rights. This theory laid the groundwork for modern concepts of democracy, individual rights, and the social contract.</p> <hr/> <p>Thomas Hobbes and John Locke, two prominent philosophers, had distinct views on natural law, reflecting their differing perspectives on human nature and the ideal structure of society.</p> <p>Hobbes, in his work “Leviathan,” presented a rather pessimistic view of human nature. He believed that in the state of nature (a hypothetical condition without government or laws), humans are driven by self-interest and a desire for self-preservation, leading to a “war of all against all” (bellum omnium contra omnes). In this state, life would be “solitary, poor, nasty, brutish, and short.”</p> <p>For Hobbes, natural law is a set of precepts or general rules, <em>discovered by reason</em>, which prohibit anything destructive to one’s own life. <em>It’s based on the right of every individual to preserve their own life</em>, leading to the conclusion that humans should seek peace. This is where his famous concept of the social contract comes into play: individuals surrender some of their freedoms and submit to the authority of a ruler (or a ruling assembly) to ensure their own safety and peace. Thus, Hobbes’s natural law is fundamentally about self-preservation and the avoidance of harm to others as a means of securing one’s own safety.</p> <p>Locke’s view, as articulated in “Two Treatises of Government,” is more optimistic about human nature. He believed that in the state of nature, humans live in a state of equality and freedom, not inherently prone to violence or war. For Locke, the <em>law of nature is a moral guide based on the belief that God has given the world to all people in common</em>. It teaches that, since all are equal and independent, no one ought to harm another in their life, health, liberty, or possessions.</p> <p>Locke’s natural law is grounded in the rights to life, liberty, and property. It includes the idea that people have the obligation to respect the rights of others. His social contract theory suggests that people form governments to protect these natural rights. If a government fails to do so, citizens have the right to overthrow it. This view laid the groundwork for modern democracy and significantly influenced the development of political philosophy in the Western world.</p> <p>So, Hobbes saw natural law as a means of avoiding the brutal state of nature through self-preservation and peace, whereas Locke viewed natural law as a moral guide ensuring equality and the inherent rights of life, liberty, and property.</p> <hr/> <blockquote> <p>A basic issue for Locke and many of his contemporaries was the ontological status of natural law and in particular its relation to God’s will.</p> </blockquote> <p>In this sentence, “ontological status” refers to the fundamental nature or essence of natural law, especially in relation to its existence and its relationship to God’s will. Ontology, in philosophy, is the study of being or existence, and it deals with questions concerning what entities exist or can be said to exist, and how such entities can be grouped, related within a hierarchy, and subdivided according to similarities and differences.</p> <p>So, when discussing the “ontological status of natural law” in the context of John Locke and his contemporaries, the focus is on understanding the very essence of natural law: whether it exists as an objective reality independent of human beings, how it relates to or derives from God’s will, and what its fundamental characteristics are. This was a central topic in the philosophical and theological debates of that era, particularly in the context of determining the basis and legitimacy of moral and legal principles. Locke and many others were engaged in trying to understand whether natural laws were inherent aspects of the universe, ordained by God, or whether they were constructs of human reason and society.</p> <p>“Will-centered” refers to the philosophical position known as voluntarism. This is a theory that emphasizes the role of the will, either divine or human, in various philosophical contexts. In the context of Locke’s moral and political writings, being “will-centered” or a voluntarist means that Locke ultimately leaned towards the view that natural law and moral principles are determined by the will, particularly the will of God, rather than being inherent or objective truths that exist independently of any will.</p> <p>In Locke’s time, the debate about the nature of natural law often centered around whether natural laws were intrinsic to the universe (a position known as intellectualism or rationalism) or whether they were decrees of God’s will (voluntarism). A will-centered or voluntarist approach suggests that moral and legal norms derive their authority from an act of will, particularly the divine will, rather than from reason alone or from the inherent nature of reality. In this view, what is right or wrong, just or unjust, is so because God wills it to be that way, and human beings understand and follow these laws through revelation, religious teachings, or other means of discerning God’s will.</p> <blockquote> <p>Locke distinguished “ectype”’ from “archetype” ideas: ectypes are general ideas of substances, and archetypes are ideas constructed by man.</p> </blockquote> <p>John Locke’s distinction between “ectype” and “archetype” ideas is a crucial aspect of his epistemological theory, which he discusses in his work “An Essay Concerning Human Understanding.” This distinction is part of his broader inquiry into the nature of human knowledge and understanding.</p> <p>In Locke’s philosophy, archetypes are the original models or patterns from which copies are made. They are the fundamental, primary ideas that exist in the mind of God or, in a more secular interpretation, the perfect, abstract forms of things. When Locke refers to archetypes as ideas constructed by man, he means that these are the ideal standards or criteria we hold in our minds for categorizing and understanding the world. They represent our understanding of what the essential characteristics of a particular thing are.</p> <p>For instance, the archetype of a tree would be the idealized concept or mental representation of what a tree is supposed to be. This archetype is not derived from any particular tree but is a kind of composite or abstracted idea of “treeness” that we use to recognize and categorize individual trees.</p> <p>Ectypes, on the other hand, are derivative or secondary ideas. They are the imperfect copies or generalizations that we derive from our experience with individual instances in the world. Ectype ideas are more about the general ideas of substances we form based on our sensory experiences and observations. When we see many individual trees, for example, we form a general idea of what a tree is - this is an ectype. It’s a more practical, experiential idea based on the aggregation of real-world instances.</p> <p>In summary, Locke’s distinction between archetype and ectype ideas can be understood as a differentiation between the idealized, abstract concepts we hold in our minds as standards (archetypes) and the more practical, general ideas we form based on our sensory experience of the world (ectypes). Archetypes are about the essence or ideal form of things, while ectypes are about the general, often imperfect, concepts we derive from actual experiences.</p> <h2 id="the-preoccupation-with-certainty">The Preoccupation with Certainty</h2> <blockquote> <p>The post-Humean Enlightenment tradition has been marked by a fallibilist view of knowledge. All knowledge claims are fallible on this account, and science advances not by making knowledge more certain but by producing more knowledge. Recognizing the corrigibility of all knowledge claims and the possibility that one might always be wrong exemplifies the modern scientific attitude. As Karl Popper (1902-1994) noted, the most that we can say, when hypotheses survive empirical tests, is that they have not been falsified so that we can accept them provisionally.</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">Value judgments</code> are statements or opinions that express an evaluation, typically of something’s worth, beauty, goodness, or morality. Examples include statements like “Lying is wrong,” or “This painting is beautiful.” A.J. Ayer was a key figure in the logical positivist movement, which held that for a statement to be meaningful, it must be either empirically verifiable (i.e., testable by observation or experiment) or analytically true (true by definition, like mathematical or logical statements). In logical positivism, a <code class="language-plaintext highlighter-rouge">proposition</code> is a statement that can be either true or false. It’s a claim about the world that can, <em>at least in principle</em>, be tested and verified or falsified.</p> <p>The Logical Positivist movement, also known as Logical Empiricism, was a philosophical movement that emerged in the early 20th century. It primarily revolved around a group of philosophers associated with the Vienna Circle (<code class="language-plaintext highlighter-rouge">Moritz Schlick</code>, <code class="language-plaintext highlighter-rouge">Hans Hahn</code>, ), along with others like A.J. Ayer in Britain. This movement sought to apply the rigor of scientific methodology to philosophy, with a significant focus on the analysis of language and the verification of statements.</p> <p>Key Features of Logical Positivism include</p> <ol> <li> <p><strong>Verification Principle</strong>: The central tenet of Logical Positivism is the verification principle. This principle asserts that a statement is only meaningful if it can be empirically verified or is analytically true (true by virtue of its meaning, like “All bachelors are unmarried”). The idea was to eliminate metaphysical and abstract discussions that couldn’t be supported by empirical evidence or logical reasoning.</p> </li> <li> <p><strong>Empiricism and Science</strong>: Logical Positivists emphasized the importance of empirical evidence and scientific methods in acquiring knowledge. They viewed science as the model for all true knowledge.</p> </li> <li> <p><strong>Rejection of Metaphysics</strong>: They were critical of metaphysics and other traditional philosophical endeavors, which they saw as meaningless since such statements couldn’t be empirically verified. They believed that many philosophical problems arose from misunderstandings of language and could be resolved by clarifying the language used.</p> </li> <li> <p><strong>Language and Meaning</strong>: A significant focus was placed on the analysis of language, particularly the language of science. They aimed to clarify how language is used in scientific theories and to distinguish between meaningful and meaningless statements.</p> </li> <li> <p><strong>Influence of Wittgenstein</strong>: Although not officially part of the Vienna Circle, Ludwig Wittgenstein’s early work, especially his “Tractatus Logico-Philosophicus,” significantly influenced Logical Positivism. Wittgenstein argued that <em>much of philosophy consists of nonsensical propositions and that the role of philosophy should be to clarify thought and language</em>.</p> </li> <li> <p><strong>Ethical and Aesthetic Statements</strong>: Logical Positivists generally considered ethical and aesthetic statements to be expressions of emotions or subjective preferences, rather than statements that could be true or false.</p> </li> </ol> <p>The “positivism” component is linked to the movement’s commitment to a scientific and empirical approach to knowledge. Positivism, as a philosophical stance, argues that knowledge should be based on positive, observable facts and their logical and mathematical treatment. It rejects introspection and intuition as sources of knowledge and instead emphasizes empirical evidence obtained through observation and experimentation. Logical Positivists extended this approach by asserting that statements must be empirically verifiable (or analytically true) to be meaningful.</p> <hr/> <p>Somewhat to my surprise, Karl Popper is not a member of the Vienna circle even though they shared many intellectual engagements. Furthermore, Karl Popper is even critically oppositional. The Vienna Circle advocated for the verification principle, which held that a statement is meaningful only if it can be empirically <em>verified</em>. Popper challenged this view, proposing <em>falsificationism</em> instead. According to Popper, scientific theories cannot be conclusively verified but can be falsified. He argued that a theory is scientific if it is testable and can potentially be refuted by evidence. This approach places a greater emphasis on the role of empirical refutation rather than verification.</p> <p>Also, Popper was critical of what he called <code class="language-plaintext highlighter-rouge">historicism</code> – the belief that <em>history unfolds according to deterministic laws or principles</em>. He argued that such theories, which <em>were often used to justify authoritarian regimes</em>, are fundamentally flawed. He believed that historicism led to totalitarianism because it promoted the idea that certain individuals or groups had access to inevitable truths about societal development, thus justifying their absolute rule. Popper advocated for what he termed an <code class="language-plaintext highlighter-rouge">open society</code>. An open society, in his view, is characterized by a democratic government, individual freedoms, and a critical attitude towards tradition and authority. It allows for change and improvement through rational and critical discourse, as opposed to the unquestioning acceptance of dogmatic principles.</p> <p>Just as Popper applied the <em>principle of falsifiability</em> to scientific theories, he suggested that political policies should also be subjected to critical scrutiny and should be alterable in the face of new evidence or arguments. He was wary of any political theory or system that claimed to have absolute or final answers.</p> <hr/> <p>According to Ayer, the expression of a value judgment is not a proposition since it can not be judged by right and wrong, the question of truth or falsehood does not here arise.</p> <p>Regarding ethics, Ayer points out that many theorists in ethics tend to treat statements about the causes and characteristics of our ethical feelings as if these statements were definitions of ethical concepts. For example, a theory might claim that an action is good if it promotes happiness. Here, the cause of the ethical feeling (happiness) is used to define the ethical concept (good). Ayer argues that ethical concepts are <em>pseudo-concepts</em>, since ethical concepts, in his view, is neither empirically verifiable or analytically correct.</p> <p>Ayer’s stance is closely associated with <code class="language-plaintext highlighter-rouge">emotivism</code>, a meta-ethical view that suggests <em>ethical statements do not assert propositions but express emotional attitudes</em>. According to emotivism, saying “Stealing is wrong” is akin to expressing one’s disapproval of stealing, rather than making an objective claim about the nature of stealing.</p> <h2 id="the-centrality-of-individual-rights">The Centrality of Individual Rights</h2> <blockquote> <p>In addition to faith in science, the Enlightenment’s central focus on individual rights differentiates its political philosophy from the ancient and medieval commitments to order and hierarchy. This focus brings the freedom of the individual to the center of arguments about politics. This move was signaled in the natural law tradition by a shift in emphasis from the logic of law to the idea of natural right.</p> </blockquote> <p>Hobbes contended that it was customary to conflate “Jus and Lex, law and right”. Yet he made the distinction that right, consisted in liberty to do, or to forbeare, whereas law, determines and binds to one of them. Similarly by Locke.</p> <p>John Locke’s oppinion on natural law is as the following. In his work <em>Essays on the Law of Nature</em>, Locke argues a moral law inherent in the world and discoverable through reason.</p> <p>Key points of Locke’s argument include:</p> <ol> <li> <p><strong>Natural Law and Reason:</strong> Locke posits that natural law is an aspect of the natural world, similar to physical laws. According to him, this <em>moral law can be discovered through the use of reason, without the need for divine revelation</em>.</p> </li> <li> <p><strong>Moral Obligations:</strong> He argues that <em>natural law imposes moral obligations on individuals</em>. These moral principles are universal and apply to all people, regardless of their culture or society.</p> </li> <li> <p><strong>Rights and Duties:</strong> Locke’s view of natural law is closely tied to his ideas about individual rights and duties. He believes that natural law forms the basis for understanding human rights, especially the right to life, liberty, and property.</p> </li> <li> <p><strong>Foundation for Political Theory:</strong> These essays lay the groundwork for Locke’s later political theories, particularly those presented in his famous works, “Two Treatises of Government.” He uses the concept of natural law to argue for the rights of individuals and the limitations of governmental power.</p> </li> <li> <p><strong>Human Equality:</strong> Locke emphasizes the inherent equality of all human beings, derived from their natural state. This idea is a critical aspect of his argument against absolute monarchy and for the formation of governments based on the consent of the governed.</p> </li> <li> <p><strong>Religious Tolerance:</strong> Although not as explicitly developed in these essays as in his later works, Locke’s concept of natural law also leads to his advocacy for religious tolerance, seeing religious belief as a matter of individual conscience.</p> </li> </ol> <p>In summary, Locke’s “Essays on the Law of Nature” propose that there is a moral law inherent in the natural world, understandable through human reason, and that this law underpins human rights and forms the basis for just and ethical governance.</p> <hr/> <p>John Locke’s <code class="language-plaintext highlighter-rouge">voluntarist theology</code> reflects his views on the nature of God and the relationship between divine will and moral law. The emphasis is on <em>the will will (voluntas in Latin, hence the name) of God of God as the primary or sole source of moral law</em>. Locke’s voluntarism posits that <em>moral laws are decrees of God’s will</em>. In this view, what is morally right or wrong is so because God wills it, and not necessarily because it aligns with any intrinsic moral truths or rational principles independent of God’s will. Locke emphasizes the <em>absolute freedom</em> and <em>omnipotence</em> of God. He argues that God’s will is not bound by any external standards or principles. Therefore, moral laws are a product of God’s free choice.</p> <p>While Locke is a proponent of reason and believes that human beings can discover moral truths through rational inquiry, he also upholds the importance of divine revelation. In his voluntarist theology, revelation plays a crucial role in imparting knowledge of God’s will, which might not be entirely accessible through reason alone. Locke’s voluntarism is tied to his rejection of innate ideas, a concept he famously critiques in his “Essay Concerning Human Understanding.” He argues against the notion that <em>moral principles are innately known</em>, instead positing that our understanding of moral laws comes from experience, reason, and revelation. Locke’s voluntarist approach suggests that moral obligations are ultimately grounded in obedience to God’s will. This perspective can lead to a form of ethical subjectivism, where moral truths depend on the decrees of a divine authority.</p> <hr/> <blockquote> <p>In Locke’s formulation, natural law dictates that man is subject to divine imperatives to live in certain ways, but, within the limits set by the law of nature, men can act in a godlike fashion. Man as maker has a maker’s knowledge of his intentional actions, and a natural right to dominion over man’s products. … Provided we do not violate natural law, we stand in the same relation to the objects we create as God stands to us; we own them just as he owns us.</p> </blockquote> <h2 id="tensions-between-science-and-individual-rights">Tensions Between Science and Individual Rights</h2> <p>The two enlightenment values, the preoccupation of science and the commitment to individual rights, seem to be in contradiction with each other. Science is deterministic, concerned with discovering the laws that govern the universe, with human being included. This has potential for conflict with an ethic that emphasizes individual freedom, for now the freedom has to be subjugated to the laws (of nature, of God).</p> <p>In Locke’s theory, the freedom to comprehend natural law by one’s own lights supplied the basis of Locke’s right to resist, which could be invoked against the sovereign. No one is in a higher position to monopolize the right to interpret the scripture.</p> <blockquote> <p>We will see this tension surface repeatedly in the utilitarian, Marxist, and social contract traditions, without ever being fully resolved.</p> </blockquote> <h1 id="classical-utilitarianism">Classical Utilitarianism</h1> <p>Jeremy Bentham famously wrote that</p> <blockquote> <p>Nature has placed mankind under the governance of two sovereign masters, <em>pain</em> and <em>pleasure</em>. It is for them alone to point out what we ought to do, as well as to determine what we shall do. On the one hand the standard of right and wrong, on the other the chain of causes and effects, are fastened to their throne. They govern us in all we do, in all we say, in all we think: every effort we can make to throw off our subjection, will serve but to demonstrate and confirm it. In words a man may pretend to abjure their empire: but in reality he will remain subject to it all the while. The principle of utility recognizes this subjection, and assumes it for the foundation of that system, the object of which is to rear the fabric of felicity by the hands of reason and law. Systems which attempt to question it, deal in sounds instead of senses, in caprice instead of reason, in darkness instead of light.</p> </blockquote> <p>Some regimes indeed deals “in sounds instead of senses, in caprice instead of reason,” yet as long as they get only one thing right, as long as they supress the alternative, their reign will continue.</p> <p>The <code class="language-plaintext highlighter-rouge">principle of utility</code>, as Bentham explains, “approves or disapproves of every action whatsoever, according to the tendency which it appears to have to augment or diminish the happiness of the party whose interest is in question: or, what is the same thing in other words, to promote or to oppose that happiness.”</p> <blockquote> <blockquote> <p>A century later Marx and Engels would write of a uto pian order in which politics could be replaced by administration.≥ Bentham believed that it could be done in eighteenth-century England.</p> </blockquote> </blockquote> <p>Funny enoguh, Marx thought very little of Jeremy Benthem.Marx wrote, in <em>Das Capita</em>, that Bentham was a “panegyrist of bourgeois society,” and “With the driest naiveté he (Bentham) takes the modern shopkeeper, especially the English shopkeeper, as the normal man. Whatever is useful to this queer normal man, and to his world, is absolutely useful. This yard-measure, then, he applies to past, present, and future. The Christian religion, for example, is ‘useful,’ ‘because it forbids in the name of religion the same faults which the penal code condemns in the name of the law.’ Artistic criticism is ‘harmful,’ because it disturbs worthy people in their enjoyment of Martin Tupper, etc.”</p> <p>Bentham’s happiness principal, when applied to governments, requires us to maximize the greatest happiness of the greatest number in the community.</p> <p>Bentham defended an extensive system of political rights, but he saw rights as human artifacts, created by the legal system and enforced by the sovereign. He insisted that there are no rights without enforcement and no enforcement without government, a blunt statement of the view that would subsequently become known as legal positivism.</p> <h2 id="individual-versus-collective-utility-and-the-need-for-government">Individual Versus Collective Utility and the Need for Government</h2> <p>In Bentham’s <em>Principles of the Civil Code</em>, he wrote</p> <blockquote> <p>Law does not say to man, Work and I will reward you but it says: Labour, and by stopping the hand that would take them from you, I will ensure to you the fruits of your labour – its natural and sufficient reward, which without me you cannot preserve. If industry creates, it is law which preserves; if at the first moment we owe everything to labour, at the second, and every succeeding moment, we owe everything to law.</p> </blockquote> <p>Law should limit itself to ensuring that people can pursue utility for themselves.</p> <p>The necessity for having a government comes down to two things.</p> <ol> <li>Selfish behavior can be self-defeating. The problem of funding the provision of public goods is one of a class of market failures, where the market’s invisible hand leads to sub-optimal outcomes for all concerned.</li> </ol>]]></content><author><name>Baiyang Zhang</name></author><category term="politics"/><summary type="html"><![CDATA[Enlightenment Politics]]></summary></entry></feed>