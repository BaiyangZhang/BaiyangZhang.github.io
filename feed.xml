<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://baiyangzhang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://baiyangzhang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-11-16T07:11:01+00:00</updated><id>https://baiyangzhang.github.io/feed.xml</id><title type="html">Baiyang Zhang</title><subtitle>A place dedicated to sharing insights and reflections on mathematics, physics, and social sciences. </subtitle><entry><title type="html">Introduction to Transseries</title><link href="https://baiyangzhang.github.io/blog/2023/Transseries-Lecture/" rel="alternate" type="text/html" title="Introduction to Transseries"/><published>2023-11-08T00:00:00+00:00</published><updated>2023-11-08T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Transseries-Lecture</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Transseries-Lecture/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In history, formal power series are used extensively for finding the resolution of differential equations. If the resulting power series is convergent, it gives rise to a germ which can be analytically continued to (multi-valued) functions on a Riemann surface. However, very often, the power series we found from solving a differential equation is divergent, then it is not clear <em>a prior</em> how to attach reasonable sums to them.</p> <p>The modern theory of <code class="language-plaintext highlighter-rouge">resummation</code> was developed systematically by Stieltjes, Borel and Hardy, who invented some resummation methods which <em>are stable under the common operators of analysis</em>. Later, Poincare established the equivalence between computations with formal power series and asymptotic expansions. Newton, Borel and Hardy were all aware of the systematic aspects of their theories and they consciously tried to complete their framework so as to capture as much of analysis as possible. The great unifying theory nevertheless had to wait until the late 20-th century and Ecalle’s work on transseries and Dulac’s conjecture.</p> <hr/> <p>Transseries have found significant applications in various areas of physics, particularly in high-energy physics. They are employed as algebraic tools to investigate self-consistent Dyson–Schwinger equations, which are integral equations that arise in the field of quantum field theory, specifically in Yukawa theory and quantum electrodynamics <a href="https://www.sciencedirect.com/science/article/pii/S0003491616300793">1</a>. These equations are pivotal in understanding the interactions of particles and fields at a fundamental level.</p> <p>In the realm of general relativity, transseries are applied to asymptotic analysis. General relativity stands as one of the cornerstones of modern physics, governing the laws of gravitation and the dynamics of large-scale structures in the universe . By applying transseries in this domain, researchers can gain insights into the asymptotic behavior of gravitational fields and the dynamics of spacetime.</p> <p>Furthermore, transseries are used in the <em>extraction of non-perturbative physics from perturbation theory through resurgence and alien calculus</em>. Perturbation theory is a fundamental tool in quantum mechanics and quantum field theory, allowing for the approximation of complex systems. The non-perturbative effects are those that cannot be captured by perturbation theory alone, and transseries help to identify and understand these effects <a href="https://www.sciencedirect.com/science/article/pii/S0003491619301691">2</a>..</p> <p>Additionally, in the context of integrable, asymptotically free field theories, transseries have applications in studying the free energy of such systems when coupled to a conserved charge. These studies are significant in high-energy physics, particularly in understanding the thermodynamics and statistical mechanics of particle systems <a href="https://link.springer.com/article/10.1007/JHEP08%282022%29279">3</a>.</p> <p>These examples showcase the versatility and importance of transseries in advancing the understanding of fundamental physics, from the microscale of particle interactions to the macroscale of cosmic phenomena.</p> <p>I hope the information provided has sparked your interest in transseries. Now, let’s delve into the subject itself.</p> <hr/> <p>Define a ordered group ${\frak G}$ (frak G) of transmonomials. Define a differential field $\mathbb{T}$ of <code class="language-plaintext highlighter-rouge">transseries</code>. Transmonomials are generalizations of monomials in polynomials, by including exponential and logarithmic. In this note and that follows, we will consider the limit where $x\to \infty$.</p> <p>Let’s start with exponents first.</p> <p><strong>Log-free transmonomials.</strong> They are of form \(x^{b}e^{ L },\quad b\in \mathbb{R},\; L \in \text{large log-free transseries.}\)</p> <p>For example, the following are all log-free transmonomials, \(x^{-1},\; x^{\pi}x^{x^{\sqrt{ 2 }}-3x},\; e^{ \sum_ {i}x^{-1}e^{ x } },etc.\)</p> <p>The multiplication is defined in the obvious way. The group identity is just $1$.</p> <p>We define a binary relation $\gg$, read “far larger than”. Keep in mind that we assumed $x\to \infty$. So how does this “far larger than” work? We compare the exponents $e^{ L }$ first, whichever with the largest exponent $L$ is far larger than others; if they have same exponents, then we compare the power of $x$, namely $x^{b}$, whichever with larger $b$ is far larger then others. To be specific, \(x^{b_ {1}}e^{ L_ {1} } \gg x^{b_ {2}}e^{ L_ {2} }\quad \text{ if } L_ {1} &gt; L_ {2} \;\lor\; (L_ {1}=L_ {2}\;\land\; b_ {1}&gt;b_ {2} ),\) where $\lor$ is logic or. For example, $x^{-5}\gg x^{20}e^{ -x }$ since $x^{-5}=x^{-5}e^{ 0 }$ and $0&gt;-x$.</p> <p><strong>Log-free transseries.</strong> A log-free transseries $T$ is a formal sum of log-free monomials ${\frak g}$, \(T = \sum_ {i} c_ {i} {\frak g}_ {i},\quad c_ {i} \in \mathbb{R} .\) We require the order of transmonomials be such that, each ${\frak g_ {i}}$ is far smaller than all previous terms, namely they appear in descending orders. This is similar to the case of regular polynomials where we usually put the highest powers at first.</p> <p>The transseries $T$ is said to be <code class="language-plaintext highlighter-rouge">purely large</code> if all transmonomials ${\frak g}_ {i}$ are far larger than $1$ (not $0$), namely ${\frak g_ {i}}\gg 1 \;\forall i$. $T$ is said to be <code class="language-plaintext highlighter-rouge">small</code> if all ${\frak g}_ {i}\ll 1$ (why isn’t it called purely small?). The largest (in the sense of far larger than) transmonomial is called the <code class="language-plaintext highlighter-rouge">dominant term</code>, let’s call it $c_ {0}{\frak g}_ {0}$. If the dominant term has positive coefficients, $c_ {0}&gt;0$, then $T$ is said to be positive. This enables us to compare the size of two transseries $S,T$, we say $S&gt;T$ if $S-T&gt;0$. So we just need to compare their dominant terms.</p> <hr/> <p>We consider only transmonomials and transseries of “finite exponential height”. For example, we don’t want \(e^{ x^{x^{x\dots}} }.\)</p> <p>The <code class="language-plaintext highlighter-rouge">differentiation</code> of $T$ with respect to $x$ is defined the usual way.</p> <hr/> <p>Next let’s include logarithmic. $\log$ acting $m$ times is denoted $\log_ {m}x$ or $\log_ {(m)}x$, namely \(\log_ {(m)}x = \log \dots \log x,\quad m\; \log .\)</p> <p>A general transseries is obtained by substitution of some $\log_ {m}x$ for $x$ in a log-free transseries.</p> <p><em>Every nonzero transseries has a multiplicative inverse.</em> This is similar to formal power series.</p> <p><em>A lot of functions can now be regarded as a transseries</em>. For example, e hyperbolic sine is a two-term transseries.</p> <h2 id="formal-constructions">Formal Constructions</h2> <p>In mathematics, the move towards higher levels of formality entails adopting rigorous and precise language, definitions, and proofs, which brings clarity and precision, ensuring that mathematical concepts are universally understood and applied correctly. It allows for the development of solid, gap-free proofs, having a deeper understanding of mathematical structures and providing a robust foundation for complex theories. This precision in communication is critical in a global context, where scientists from different fields rely on universally recognized formalisms to understand each other effectively.</p> <p>However, this precision comes at a cost. It can make the subject less accessible to beginners (like myself), potentially hindering educational and interdisciplinary work. A focus on stringent formalism might even inhibit creative thinking, as the rigidity of formal proofs could constrain the exploratory, intuitive processes that often drive mathematical discovery. Moreover, the lengthy and detailed nature of formal proofs can make mathematical work less efficient, both in terms of personal understanding and communication with others. There’s also the risk of diminishing intuition, which is a crucial aspect of mathematical thought, particularly in the preliminary stages of research. It is definitely crucial to have a balance between concrete examples and general formalism, what is I hope to achieve in this note.</p> <hr/> <p>The set of monomials ${\frak G}$ form a field, which is also a group if we focus on multiplication alone. ${\frak G}$ is <em>not</em> finitely generated, to see this consider the finitely generated group with generator \(\mu_ {1},\mu_ {2},\dots,\mu_ {n},\) the generated group has elements of form \(\left\{ \mu_ {1}^{k_ {1}}\times \mu_ {2}^{k_ {2}}\times \dots \times \mu_ {n}^{k_ {n}} \,\middle\vert\, k_ {1},\dots,k_ {n} \in \mathbb{Z} \right\} .\) Note that the exponents must be integers.</p> <p>Let’s use capital letters to denote a set of indices, for example define \(K := (k_ {1},k_ {2},\dots,k_ {n})\) then \(\mu_ {1}^{k_ {1}}\dots \mu_ {n}^{k_ {n}} =: \mu^{K}.\)</p> <p>This will save some writing. The problem is that $\mu^{K}$ can also be interpreted as $\mu^{k_ {1}k_ {1}\dots k_ {n}}$, but it should be clear from the context.</p>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Note on Numerical Methods in Solving ODE</title><link href="https://baiyangzhang.github.io/blog/2023/Notes-on-Numerical-Methods-in-Solving-ODE/" rel="alternate" type="text/html" title="Note on Numerical Methods in Solving ODE"/><published>2023-10-31T00:00:00+00:00</published><updated>2023-10-31T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Notes-on-Numerical-Methods-in-Solving-ODE</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Notes-on-Numerical-Methods-in-Solving-ODE/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In my line of work I sometimes need to solve unsolvable ODEs and PDEs, unsolvable in the sense that it is impossible to get an analytical solution. What we do is to turn to numerical methods for help. For instance, just recently I need to solve a modified version of the Skyrme equation (the equation of motion resulted from the Skyrme model). I thought it would be helpful to summarize what I’ve learnt here.</p> <p>Various numerical methods have been developed to tackle different types of ODEs (initial value problems, boundary value problems, linear, nonlinear, etc.). Here are some of the most popular numerical methods for solving ODEs:</p> <ol> <li><strong>Euler’s Method</strong>: <ul> <li>This is the simplest one-step method.</li> <li>It’s based on a linear approximation of the solution.</li> <li>While straightforward and instructive for educational purposes, it’s rarely used in practice due to its low accuracy and stability issues.</li> </ul> </li> <li><strong>Runge-Kutta Methods</strong>: <ul> <li>These are a family of iterative methods.</li> <li>The 4th order Runge-Kutta (often called RK4) is particularly popular due to its balance between accuracy and computational cost.</li> </ul> </li> <li><strong>Leapfrog (or Midpoint) Method</strong>: <ul> <li>A second-order method that is particularly useful in cases where energy conservation is crucial, such as in molecular dynamics simulations.</li> </ul> </li> <li><strong>Predictor-Corrector Methods</strong>: <ul> <li>These methods predict a solution using an explicit method and then correct it with an implicit method.</li> <li>Examples include the Adams-Bashforth (predictor) and Adams-Moulton (corrector) methods.</li> </ul> </li> <li><strong>Backward Differentiation Formulas (BDF)</strong>: <ul> <li>These are implicit multi-step methods.</li> <li>Commonly used for stiff ODEs.</li> </ul> </li> <li><strong>Multistep Methods</strong>: <ul> <li>These methods use values at multiple previous time steps.</li> <li>Examples include the Adams methods.</li> </ul> </li> <li><strong>Symplectic Integrators</strong>: <ul> <li>These are used for Hamiltonian systems where preserving the symplectic structure (related to conservation of energy) is essential.</li> </ul> </li> <li><strong>Implicit Methods</strong>: <ul> <li>Used frequently for stiff equations where explicit methods require prohibitively small time steps.</li> <li>Examples include the backward Euler method and the trapezoidal rule.</li> </ul> </li> <li><strong>Shooting Method</strong>: <ul> <li>Primarily used for boundary value problems (BVPs).</li> <li>Converts a BVP into an initial value problem (IVP) and then solves the IVP.</li> </ul> </li> <li><strong>Relaxation Methods</strong>: <ul> <li>Also for boundary value problems.</li> <li>Iteratively refines an initial guess to the solution.</li> </ul> </li> <li><strong>Finite Difference Method</strong>: <ul> <li>Converts differential equations into <em>difference</em> equations, which can then be solved algebraically.</li> <li>Often used for both ODEs and PDEs.</li> </ul> </li> <li><strong>Collocation Methods</strong>: <ul> <li>This approach seeks an approximate solution by considering values at specific points (collocation points).</li> </ul> </li> <li><strong>Continuation method</strong>, which we will go to detail later.</li> </ol> <hr/> <p>These methods can be adapted or combined in various ways depending on the specific problem at hand. Moreover, the choice of method often depends on the nature of the ODE (e.g., stiffness), desired accuracy, computational cost considerations, and the specific properties that need to be preserved (e.g., conservation laws).</p> <p>Many modern computational packages and software (like MATLAB, Mathematica, and SciPy in Python) provide built-in functions that implement these methods, which makes it easier for users to solve ODEs without delving deeply into the numerical intricacies of each method.</p> <p><strong>Stiffness.</strong></p> <p>Imagine you’re on a winding road with both smooth curves and sharp turns. If you’re driving a car along this road at a constant speed, the smooth curves can be navigated quite easily, but the sharp turns require more caution and precision.</p> <p>Similarly, in the context of differential equations, there can be parts of the solution that change very slowly (smooth curves) and others that change extremely rapidly (sharp turns). When a differential equation has solutions with widely differing rates of change over its domain, we say that the equation is “stiff.” When you’re solving a stiff differential equation using numerical methods (like the Euler method or the Runge-Kutta method), you’ll notice that the rapid changes require very small step sizes for accurate solutions. However, the slow-changing parts don’t need such small steps. If you choose a step size suitable for the rapidly changing sections (very small), the computation can become inefficient because you’re using more steps than necessary for the slow-changing sections. On the other hand, if you choose a larger step size suitable for the slow-changing sections, the solution can become unstable or highly inaccurate in the rapidly changing sections.</p> <p>To efficiently and accurately solve stiff differential equations, specialized numerical methods have been developed, known as “stiff solvers.” These solvers are designed to adaptively handle the challenges posed by stiffness, allowing for stable and efficient computation.</p> <h2 id="numerical-values-of-parameters">numerical values of parameters</h2> <p>I collected the following values from the Adkins:Nappi:1984 paper<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>,<br/> \(m_ {\pi} = 108 \text{ MeV}, \quad e=4.82,\quad F_ {\pi} = \frac{m_ {\pi}}{0.263 e}\) which gives us \(\begin{align} m_ {1} &amp;= 0.526, \\ m_ {2} &amp;= 1.052. \end{align}\)</p> <p>In the chiral case, pion is massless and we have \(\begin{align} m_ {1} &amp;= 0, \\ m_ {2} &amp;= 1.052. \end{align}\)</p> <h2 id="the-shooting-method">The shooting method</h2> <p>The shooting method is a numerical technique used to solve boundary value problems (BVPs) for ordinary differential equations (ODEs). <em>It’s especially useful for second-order ODEs, but can be applied to higher-order equations as well</em>.</p> <p>Here’s a basic overview of the shooting method:</p> <p><strong>The Problem:</strong> Suppose you have a second-order ODE given as: \(y''(x) = f(x, y, y'),\) with boundary conditions: \(y(a) = y_ a\) \(y(b) = y_ b\)</p> <p><strong>The Challenge:</strong> Directly solving the BVP using typical ODE solvers is difficult because standard solvers require initial conditions (values of $y$ and $y’$ at a starting point), rather than boundary conditions at two separate points.</p> <p><strong>The Shooting Method’s Approach:</strong></p> <ol> <li> <p><strong>Guess an Initial Slope</strong>: Choose an initial guess for the derivative $y’(a)$, let’s call it $y’_ a$.</p> </li> <li> <p><strong>Solve as an IVP</strong>: Using the known value $y(a) = y_ a$ and the guessed $y’(a) = y’_ a$ then solve the ODE as an initial value problem (IVP) over the interval $[a, b]$ using standard techniques, like the Runge-Kutta method.</p> </li> <li> <p><strong>Check the Endpoint</strong>: Once you’ve solved the ODE using your initial guess, check the value of $y(b)$ from this solution. Compare it to the desired boundary condition $y_ b$.</p> </li> <li> <p><strong>Adjust the Guess</strong>: If $y(b)$ from your solution is close to $y_ b$, then you’re done. If not, adjust your guess for $y’(a)$ and solve the IVP again. This is typically done using a root-finding algorithm like Newton’s method or the secant method.</p> </li> <li> <p><strong>Iterate</strong>: Repeat steps 2-4 until $y(b)$ from your solution is sufficiently close to $y_ b$, or until a set number of iterations have been reached.</p> </li> </ol> <p>The method’s name comes from the idea that you’re “shooting” from one boundary towards the other. Your first “shot” might miss the target (the second boundary condition). By adjusting your aim (the initial derivative guess) and shoot again, you try to hit the target. The process is repeated until you’re close enough to the target, similar to adjusting one’s aim when firing at a target in marksmanship.</p> <p>While the shooting method can be effective, it’s not guaranteed to work for all BVPs, especially when the underlying ODEs are highly nonlinear or when appropriate initial guesses are hard to ascertain. Unfortunately, the solving of the modified Skyrme equation seems to fall in the category, as I am about to go to details right now.</p> <p>With the parameters listed in the previous chapter, I tried to solve the equation of motion using shooting method with the following codes</p> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bc1</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">Pi</span><span class="p">;</span>
<span class="n">bc2</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="mi">50</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>

<span class="n">approximateSolution</span> <span class="o">=</span> <span class="n">Pi</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Tanh</span><span class="p">[</span><span class="n">r</span><span class="p">]);</span>

<span class="n">initialGuessY</span> <span class="o">=</span> 
  <span class="n">approximateSolution</span> <span class="p">/</span><span class="o">.</span> 
   <span class="n">r</span> <span class="o">-&gt;</span> <span class="mi">0</span><span class="p">;</span>  <span class="p">(</span><span class="o">*</span><span class="n">Evaluate</span> <span class="n">approximate</span> <span class="n">solution</span> <span class="n">at</span> <span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="p">)</span>
<span class="n">initialGuessYPrime</span> <span class="o">=</span> 
  <span class="n">D</span><span class="p">[</span><span class="n">approximateSolution</span><span class="p">,</span> <span class="n">r</span><span class="p">]</span> <span class="p">/</span><span class="o">.</span> <span class="n">r</span> <span class="o">-&gt;</span> <span class="mi">0</span><span class="p">;</span>  <span class="p">(</span><span class="o">*</span><span class="n">Evaluate</span> <span class="n">derivative</span> <span class="n">at</span> <span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="p">)</span>

<span class="n">shootingMethod</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"Shooting"</span><span class="p">,</span> 
   <span class="s2">"StartingInitialConditions"</span> <span class="o">-&gt;</span> <span class="p">{</span><span class="n">f</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">initialGuessY</span><span class="p">,</span> 
   <span class="n">f</span><span class="o">'</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">initialGuessYPrime</span><span class="p">}};</span>

<span class="n">solutionTest1</span> <span class="o">=</span> <span class="n">Module</span><span class="p">[{</span><span class="err">$$</span><span class="n">Eta</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m1</span> <span class="o">=</span> <span class="mf">0.526</span><span class="err">`</span><span class="p">,</span> <span class="n">m2</span> <span class="o">=</span> <span class="mf">1.052</span><span class="err">`</span><span class="p">},</span>
	 <span class="n">shootingMethod</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"Shooting"</span><span class="p">,</span> 
    <span class="s2">"StartingInitialConditions"</span> <span class="o">-&gt;</span> <span class="p">{</span><span class="n">f</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">Pi</span><span class="p">,</span> <span class="n">f</span><span class="o">'</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">6</span><span class="p">}};</span>
  <span class="n">NDSolve</span><span class="p">[{</span><span class="n">eom</span><span class="p">,</span> <span class="n">bc1</span><span class="p">,</span> <span class="n">bc2</span><span class="p">},</span> <span class="n">f</span><span class="p">,</span> <span class="p">{</span><span class="n">r</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">},</span> <span class="n">PrecisionGoal</span> <span class="o">-&gt;</span> <span class="mi">7</span><span class="p">,</span> 
   <span class="n">AccuracyGoal</span> <span class="o">-&gt;</span> <span class="mi">7</span><span class="p">]]</span>
</code></pre></div></div> <p>where eom is short for the equation of motion, given by \(\begin{align} \text{eom} =&amp;-2 r^4 f''(r)-4 \eta r^2 f''(r) \sin ^2(f(r))+4 \eta r^3 f'(r)^3-4 r^3 f'(r)^3-4 r^3 f'(r)-2 \eta r^2 f'(r)^2 \sin (2 f(r)) \\ &amp;+6 \eta r^4 f'(r)^2 f''(r)-6 r^4 f'(r)^2 f''(r)+2 \text{m1}^2 r^4 \sin (f(r))+2 \text{m2}^2 r^4 \sin (f(r)) \\ &amp;-\text{m2}^2 r^4 \sin (2 f(r))+2 r^2 \sin (2 f(r))+\sin (2 f(r))-\sin (2 f(r)) \cos (2 f(r)) \\ &amp;==0. \end{align}\) However the computation takes a long time and yields a nonsensical result, <img src="/img/eom.png" alt=""/></p> <p>which doesn’t make any sense.</p> <hr/> <p>For the following discussions, I found paper <a href="https://arxiv.org/abs/1309.1313">arXiv:1309.1313</a> to be most helpful. Below are some approximation we can adopt at $r\to 0$, \(\frac{\sin(f(r))}{r} \to f'(0), \quad \frac{1}{4}-\frac{\sin^2(f(r))}{r^2} -(f'(r))^2 \to \frac{1}{4}\) \(\frac{r^2}{4} + 2\sin^2(f(r)) = \frac{r^2}{4}\left( 1+8\frac{\sin^2(f(r))}{r^2} \right) \to \frac{r^2}{4}(1+8f'(0)).\)</p> <p>Maybe we can make it work by providing a super accurate initial condition? With this hope I try to solve the equation at the origin, close to $r=0$. Expand $f(r)$ about the origin we get \(f(r) = f(0) + r f'(r) = \pi + rg(r),\quad g(r) := f'(r)\) where we have made use of the initial condition that $f(0)=\pi$, and $r$ is supposed to be very small. Take this to the equation of motion, with some manipulation we get \(\left(-4 r^4 g^{2}(r)-2 r^4\right) g'(r)-2 m_ 1^2 r^5 g(r)-4 m_ 2^2 r^5 g(r)-4 r^3 g^{3}(r) =0\) keep the leading order and NLO in $r$ we have \(\left(2 r g^{2}(r)+r\right) g'(r)+2 g^{3}(r)=0\) In paper arXiv:hep-ph/0106150v2, Ponchiano etc. adopted Pade approximation and it seems to be working good. But it’s not directly useful to me.</p> <p>Well let’s move on to the next method.</p> <h2 id="the-continuation-homotopy-embedding-method">The continuation (homotopy, embedding) method</h2> <p>The core idea behind the “continuation method” is that, instead of trying to solve a super-hard problem right away, we start with a simpler version of it that we can solve. Then, we “continue” from that solution, making small changes step by step, until we reach the solution of the original, harder problem.</p> <ol> <li><strong>Start Simple</strong>: Begin with a version of the problem that’s easy to solve.</li> <li><strong>Make Small Changes</strong>: Adjust the problem little by little, using the solution from the last step as the starting point for the next.</li> <li><strong>Reach the Target</strong>: Continue this process until you’ve transformed your simple problem’s solution into a solution for your original, harder problem.</li> </ol> <p>Let us apply the aforementioned philosophical ideas into practice. Suppose we wish to solve a system of $N$ non-linear equations in $N$ variables, say \(F(x) = 0,\quad F: \mathbb{R}^{n} \to \mathbb{R}^{n}.\) We assume $F$ is $C^{\infty}$. Suppose that we don’t know a lot about the initial value of the derivative, then we can’t effectively adopt the shooting method. As a possible remedy, define a homotopy or deformation $H(x,t)$ which deforms from some simpler equations $G(x)$ to $F(x)$ when $t$ smoothly changes, to be specific define \(H(x,0) = G(x),\quad H(x,1) = F(x).\) Everything is required to be smooth here. Typically, one can choose a so-called <code class="language-plaintext highlighter-rouge">convex homotopy</code> such as \(H(x,t) = t\,F(x) + (1-t)\, G(x).\) $H(x,t)$ is the function we are trying to solve. Our job is to find $G(x)$ with known solution, then the PDE that $H(x,t)$ satisfies, offer the initial condition, then try to solve it.</p> <p>Let’s look at an example. Let’s solve the following non-linear partial differential equation, which is a simplified version of the Ginzburg-Landau equation, a fundamental equation in superconductivity theory: \(\frac{\partial u}{\partial t} = \nabla^2 u + \lambda u - u^3\)</p> <p>Here, $u(x, y, t)$ is the field we want to solve for, $\nabla^2 u$ is the Laplacian operator, $\lambda$ is a parameter, and $t$ represents time.</p> <p>Let’s consider a square domain $[0, L] \times [0, L]$ with periodic boundary conditions. We will solve this equation using the continuation method by gradually increasing the parameter $\lambda$ and using the solution from the previous value of $\lambda$ as the initial condition for the next one.</p> <p>This following code defines the PDE and its boundary conditions, then solves it using <code class="language-plaintext highlighter-rouge">NDSolve</code> for a range of values of $\lambda$, starting from $\lambda = 0$ and going up to $\lambda = 1$. The solution for each value of $\lambda$ is used as the initial condition for the next one. Finally, it plots the solution for $\lambda = 1$.</p> <p>Here is the Mathematica code:</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">(* Define the domain size *)</span><span class="w">
</span><span class="nv">L</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="o">;</span><span class="w">

</span><span class="c">(* Define the grid size *)</span><span class="w">
</span><span class="nv">nx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="o">;</span><span class="w">
</span><span class="nv">ny</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="o">;</span><span class="w">

</span><span class="c">(* Define the time step and final time *)</span><span class="w">
</span><span class="nv">dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.01</span><span class="o">;</span><span class="w">
</span><span class="nv">tmax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="o">;</span><span class="w">

</span><span class="c">(* Define the initial condition *)</span><span class="w">
</span><span class="nv">u0</span><span class="p">[</span><span class="nv">x</span><span class="o">_,</span><span class="w"> </span><span class="nv">y</span><span class="o">_</span><span class="p">]</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">Sin</span><span class="p">[(</span><span class="nb">Pi</span><span class="o">*</span><span class="nv">x</span><span class="p">)</span><span class="o">/</span><span class="nv">L</span><span class="p">]</span><span class="w"> </span><span class="nb">Sin</span><span class="p">[(</span><span class="nb">Pi</span><span class="o">*</span><span class="nv">y</span><span class="p">)</span><span class="o">/</span><span class="nv">L</span><span class="p">]</span><span class="o">;</span><span class="w">

</span><span class="c">(* Define the PDE *)</span><span class="w">
</span><span class="nv">pde</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">D</span><span class="p">[</span><span class="nv">u</span><span class="p">[</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="nv">t</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="nv">t</span><span class="p">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">D</span><span class="p">[</span><span class="nv">u</span><span class="p">[</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="nv">t</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="m">2</span><span class="p">}]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">D</span><span class="p">[</span><span class="nv">u</span><span class="p">[</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="nv">t</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="m">2</span><span class="p">}]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">λ</span><span class="o">*</span><span class="nv">u</span><span class="p">[</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="nv">t</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">u</span><span class="p">[</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="nv">t</span><span class="p">]</span><span class="o">^</span><span class="m">3</span><span class="o">;</span><span class="w">

</span><span class="c">(* Solve the PDE using the continuation method *)</span><span class="w">
</span><span class="err">λ</span><span class="nv">values</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">Range</span><span class="p">[</span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="m">0.1</span><span class="p">]</span><span class="o">;</span><span class="w">
</span><span class="nv">usol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{}</span><span class="o">;</span><span class="w">
</span><span class="nv">uinit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">u0</span><span class="p">[</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">y</span><span class="p">]</span><span class="o">;</span><span class="w">
</span><span class="nb">For</span><span class="p">[</span><span class="nv">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="nb">Length</span><span class="p">[</span><span class="err">λ</span><span class="nv">values</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="err">λ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">λ</span><span class="nv">values</span><span class="p">[[</span><span class="nv">i</span><span class="p">]]</span><span class="o">;</span><span class="w">
  </span><span class="nv">sol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">NDSolve</span><span class="p">[{</span><span class="nv">pde</span><span class="o">,</span><span class="w"> </span><span class="nv">u</span><span class="p">[</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="m">0</span><span class="p">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nv">uinit</span><span class="o">,</span><span class="w"> 
    </span><span class="nb">PeriodicBoundaryCondition</span><span class="p">[</span><span class="nv">u</span><span class="p">[</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="nv">t</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="nb">TranslationTransform</span><span class="p">[{</span><span class="nv">L</span><span class="o">,</span><span class="w"> </span><span class="m">0</span><span class="p">}]]</span><span class="o">,</span><span class="w">
    </span><span class="nb">PeriodicBoundaryCondition</span><span class="p">[</span><span class="nv">u</span><span class="p">[</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="nv">t</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="nv">y</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="nb">TranslationTransform</span><span class="p">[{</span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="nv">L</span><span class="p">}]]}</span><span class="o">,</span><span class="w">
   </span><span class="nv">u</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="nv">L</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="nv">L</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">t</span><span class="o">,</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="nv">tmax</span><span class="p">}</span><span class="o">,</span><span class="w"> 
   </span><span class="nb">Method</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="s">"MethodOfLines"</span><span class="o">,</span><span class="w"> </span><span class="s">"SpatialDiscretization"</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="s">"TensorProductGrid"</span><span class="o">,</span><span class="w"> </span><span class="s">"MaxPoints"</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="nv">nx</span><span class="o">,</span><span class="w"> </span><span class="nv">ny</span><span class="p">}}}]</span><span class="o">;</span><span class="w">
  </span><span class="nb">AppendTo</span><span class="p">[</span><span class="nv">usol</span><span class="o">,</span><span class="w"> </span><span class="nv">sol</span><span class="p">]</span><span class="o">;</span><span class="w">
  </span><span class="nv">uinit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">u</span><span class="p">[</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="nv">tmax</span><span class="p">]</span><span class="w"> </span><span class="o">/.</span><span class="w"> </span><span class="nv">sol</span><span class="o">;</span><span class="w">
</span><span class="p">]</span><span class="o">;</span><span class="w">

</span><span class="c">(* Plot the solution for λ = 1 *)</span><span class="w">
</span><span class="nv">sol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">usol</span><span class="p">[[</span><span class="o">-</span><span class="m">1</span><span class="p">]]</span><span class="o">;</span><span class="w">
</span><span class="nb">Plot3D</span><span class="p">[</span><span class="nb">Evaluate</span><span class="p">[</span><span class="nv">u</span><span class="p">[</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="nv">tmax</span><span class="p">]</span><span class="w"> </span><span class="o">/.</span><span class="w"> </span><span class="nv">sol</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="nv">L</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">y</span><span class="o">,</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="nv">L</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nb">AxesLabel</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="s">"x"</span><span class="o">,</span><span class="w"> </span><span class="s">"y"</span><span class="o">,</span><span class="w"> </span><span class="s">"u"</span><span class="p">}]</span><span class="w">
</span></code></pre></div></div> <p>But before getting our hands dirty let’s consider another similar but different method, for reasons I’ll explain later.</p> <h2 id="the-relaxation-method">The relaxation method</h2> <p>The term “relaxation” refers to the idea that an initial guess at the solution is iteratively refined or “relaxed” until it converges to the true solution.</p> <p><strong>Introduction to the Relaxation Method:</strong></p> <p>The relaxation method is often used for solving elliptic PDEs, such as Laplace’s equation and Poisson’s equation. The general approach of the relaxation method is as follows:</p> <ol> <li>Discretize the domain of the PDE into a grid or mesh.</li> <li>Make an initial guess for the solution at each grid point.</li> <li>Iteratively update the solution at each grid point using a numerical approximation of the PDE.</li> <li>Repeat step 3 until the solution converges to within a specified tolerance.</li> </ol> <p>Let’s consider an example of solving Laplace’s equation on a rectangular domain $[0, a] \times [0, b]$ with Dirichlet boundary conditions.</p> <p>Laplace’s equation is given by: \(\nabla^2 u(x, y) = 0\)</p> <p>Where $u(x, y)$ is the function we are trying to solve for, and $\nabla^2$ is the Laplacian operator.</p> <p>For simplicity, let’s consider the case where $a = b = 1$, and the boundary conditions are: \(u(0, y) = 0, \quad u(1, y) = 0, \quad u(x, 0) = 0, \quad u(x, 1) = \sin(\pi x)\)</p> <p>Here are the steps to solve this problem using the relaxation method in Mathematica:</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">(* Define the domain size *)</span><span class="w">
</span><span class="nv">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="o">;</span><span class="w">
</span><span class="nv">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="o">;</span><span class="w">

</span><span class="c">(* Define the grid size *)</span><span class="w">
</span><span class="nv">nx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="o">;</span><span class="w">
</span><span class="nv">ny</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="o">;</span><span class="w">

</span><span class="c">(* Define the boundary conditions *)</span><span class="w">
</span><span class="nv">u</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">Table</span><span class="p">[</span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">nx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">ny</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="p">}]</span><span class="o">;</span><span class="w">
</span><span class="nb">Do</span><span class="p">[</span><span class="nv">u</span><span class="p">[[</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="nv">j</span><span class="p">]]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">j</span><span class="o">,</span><span class="w"> </span><span class="nv">ny</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="p">}]</span><span class="o">;</span><span class="w">
</span><span class="nb">Do</span><span class="p">[</span><span class="nv">u</span><span class="p">[[</span><span class="nv">nx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="nv">j</span><span class="p">]]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">j</span><span class="o">,</span><span class="w"> </span><span class="nv">ny</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="p">}]</span><span class="o">;</span><span class="w">
</span><span class="nb">Do</span><span class="p">[</span><span class="nv">u</span><span class="p">[[</span><span class="nv">i</span><span class="o">,</span><span class="w"> </span><span class="m">1</span><span class="p">]]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">i</span><span class="o">,</span><span class="w"> </span><span class="nv">nx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="p">}]</span><span class="o">;</span><span class="w">
</span><span class="nb">Do</span><span class="p">[</span><span class="nv">u</span><span class="p">[[</span><span class="nv">i</span><span class="o">,</span><span class="w"> </span><span class="nv">ny</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="p">]]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">Sin</span><span class="p">[</span><span class="nb">Pi</span><span class="o">*</span><span class="p">(</span><span class="nv">i</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="o">/</span><span class="nv">nx</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">i</span><span class="o">,</span><span class="w"> </span><span class="nv">nx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="p">}]</span><span class="o">;</span><span class="w">

</span><span class="c">(* Define the relaxation parameter *)</span><span class="w">
</span><span class="nv">omega</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1.5</span><span class="o">;</span><span class="w">

</span><span class="c">(* Define the tolerance for convergence *)</span><span class="w">
</span><span class="nv">tol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="m">6</span><span class="p">)</span><span class="o">;</span><span class="w">

</span><span class="c">(* Perform the relaxation iteration *)</span><span class="w">
</span><span class="nv">iteration</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="o">;</span><span class="w">
</span><span class="nv">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="o">;</span><span class="w">
</span><span class="nb">While</span><span class="p">[</span><span class="nv">error</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="nv">tol</span><span class="o">,</span><span class="w">
  </span><span class="nv">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="o">;</span><span class="w">
  </span><span class="nb">For</span><span class="p">[</span><span class="nv">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="o">,</span><span class="w"> </span><span class="nv">i</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="nv">nx</span><span class="o">,</span><span class="w"> </span><span class="nv">i</span><span class="o">++,</span><span class="w">
    </span><span class="nb">For</span><span class="p">[</span><span class="nv">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="o">,</span><span class="w"> </span><span class="nv">j</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="nv">ny</span><span class="o">,</span><span class="w"> </span><span class="nv">j</span><span class="o">++,</span><span class="w">
      </span><span class="nv">old</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">u</span><span class="p">[[</span><span class="nv">i</span><span class="o">,</span><span class="w"> </span><span class="nv">j</span><span class="p">]]</span><span class="o">;</span><span class="w">
      </span><span class="nv">u</span><span class="p">[[</span><span class="nv">i</span><span class="o">,</span><span class="w"> </span><span class="nv">j</span><span class="p">]]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">omega</span><span class="p">)</span><span class="o">*</span><span class="nv">u</span><span class="p">[[</span><span class="nv">i</span><span class="o">,</span><span class="w"> </span><span class="nv">j</span><span class="p">]]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">omega</span><span class="o">*</span><span class="m">0.25</span><span class="o">*</span><span class="p">(</span><span class="nv">u</span><span class="p">[[</span><span class="nv">i</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="nv">j</span><span class="p">]]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">u</span><span class="p">[[</span><span class="nv">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="nv">j</span><span class="p">]]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">u</span><span class="p">[[</span><span class="nv">i</span><span class="o">,</span><span class="w"> </span><span class="nv">j</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="p">]]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">u</span><span class="p">[[</span><span class="nv">i</span><span class="o">,</span><span class="w"> </span><span class="nv">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="p">]])</span><span class="o">;</span><span class="w">
      </span><span class="nv">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">Max</span><span class="p">[</span><span class="nv">error</span><span class="o">,</span><span class="w"> </span><span class="nb">Abs</span><span class="p">[</span><span class="nv">u</span><span class="p">[[</span><span class="nv">i</span><span class="o">,</span><span class="w"> </span><span class="nv">j</span><span class="p">]]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">old</span><span class="p">]]</span><span class="o">;</span><span class="w">
    </span><span class="p">]</span><span class="o">;</span><span class="w">
  </span><span class="p">]</span><span class="o">;</span><span class="w">
  </span><span class="nv">iteration</span><span class="o">++;</span><span class="w">
</span><span class="p">]</span><span class="o">;</span><span class="w">

</span><span class="c">(* Display the result *)</span><span class="w">
</span><span class="nv">u</span><span class="w">
</span></code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">u</code> matrix contains the approximate solution to the PDE at each grid point. The <code class="language-plaintext highlighter-rouge">While</code> loop continues iterating until the maximum change in the solution at any grid point is less than the specified tolerance <code class="language-plaintext highlighter-rouge">tol</code>.</p> <p>The relaxation method is a powerful and widely used numerical technique for solving PDEs. It is particularly well-suited for solving elliptic PDEs, such as Laplace’s equation and Poisson’s equation, which arise in various physical applications. The method is relatively simple to implement and can be easily adapted to different types of PDEs and boundary conditions.</p> <p>However, it’s important to note that the convergence of the relaxation method can be sensitive to the choice of the relaxation parameter <code class="language-plaintext highlighter-rouge">omega</code> and the grid size. In some cases, it may be necessary to experiment with different values of these parameters to achieve a satisfactory solution.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Nuclear Physics B233 (1984) 109-115, doi: 10.1016/0550-3213(84)90172-x <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">The Fundamental Group and Covering Spaces</title><link href="https://baiyangzhang.github.io/blog/2023/The-Fundamental-Group-and-Covering-paces/" rel="alternate" type="text/html" title="The Fundamental Group and Covering Spaces"/><published>2023-10-30T00:00:00+00:00</published><updated>2023-10-30T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/The-Fundamental-Group-and-Covering%20-paces</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/The-Fundamental-Group-and-Covering-paces/"><![CDATA[<p><em>Disclaimer: Nothing in this note is original.</em></p> <p>Let $\gamma$ be a loop with base point $p_ {0}$, on a connected (not necessarily simply connected) maniofld $M$. This circle is a map \(\gamma: [0,1]\to M,\quad \gamma(0)=\gamma(1)=p_ {0}.\) Consider all such loops with the same base point. Recall that if we can deform one loop to another in a continuous fashion while preserving the base point, we say they are homotopic, \(\gamma_ {1} \sim \gamma_ {2}.\) If some loop $\gamma$ is homotopic to a point, or a constant, then we call it trivial and write \(\gamma \sim 1.\)</p> <p>Note the requirement that the base point must be fixed, sometimes it happens that a loop can be shrink to a point if you are allowed to move it around in the manifold, but if you have to fix a base point then it is no longer true. Hence, sometimes a loop is <code class="language-plaintext highlighter-rouge">homologically</code> trivial but not homotopically trivial.</p> <p>Given two loops $\gamma_ {1}$ and $\gamma_ {2}$ on M, by reparameterization (so that each loop is traversed with double speed) we may compose them to give a new loop, which is traditionally written from left to right, for example $\gamma_ {1}\gamma_ {1}(\theta)$ means traverse along $\gamma_ {1}$ first then $\gamma_ {2}$. This defines the <em>“multiplication”</em> of loops. This multiplication is defined up to homotopy, under which the loops form a group structure. This is the <code class="language-plaintext highlighter-rouge">fundamental group</code> of $M$, written \(\pi_ {1}(M; p_ {0}).\)</p> <p>If we don’t care about the base point, then we simply write \(\pi_ {1}(M).\)</p> <p>Recall that, by definition, a space is <code class="language-plaintext highlighter-rouge">simply connected</code> if all loops are contractible to a point, that is, if the group $\pi_ {1}(M)$ consists only of the identity.</p> <p>Some famous example include that $\pi_ {1}(S_ {1})=\mathbb{Z}$, $\pi_ {1}(SO(3))=\mathbb{Z}_ {2}$, etc.</p> <hr/> <p><strong>Covering space</strong></p> <p>We shall say that a connected space $\overline{M}$ is a covering of the connected $M$, with covering or projection map \(\pi: \overline{M} \to M,\) if each $x\in M$ has a neighborhood $U$ such that the preimage $\pi ^{-1}(U)$ consists of <em>disjoint open</em> subsets $\left{ U_ {\alpha} \right}$ of $\overline{M}$, each <em>diffeomorphic</em> under $\pi$.</p> <p>For example, we can cover $\mathbb{S}^{1}$ with $\mathbb{R}$ infinite times.</p> <p>The notion of covering space can also be described in terms of fiber bundles as follows:</p> <p>A covering space of a manifold $M$ is a connected space $\overline{M}$ that is a fiber bundle over $M$ with fiber $F$ a discrete set of points.</p> <p>Let $M$ be a connected manifold. The universal covering manifold $\overline{M}$ of $M$ is a covering space that has trivial fundamental group.</p> <hr/> <p>Let $\gamma,\gamma_ {1}$ be two paths form $p_ {0}$ to $p$. An orientable covering is such that if when we translate an orientation along the closed curve $\gamma\gamma ^{-1}$ we return with the original orientation. It is important that we are dealing with homotopy classes: if a closed curve $C$ preserves orientation, and if $C’$ is homotopic to $C$, then $C’$ will also preserve orientation.</p> <p>An orientable covering is in general smaller than the universal covering, but not necessarily.</p> <p>By the same arguments, it can be shown in general that the orientable cover of $M$ is either $M$ itself, if $M$ is orientable, or a 2-sheeted cover of $M$. As an example, a two-sheeted-orientable cover of the Klein bottle is the torus!</p> <p><strong>Lifting paths</strong></p> <p>Given a covering $\pi: \overline{M} \to M$, the lift of a path $\gamma$ in $M$ is roughly speaking a continuous path $\overline{\gamma}$ in $\overline{M}$ whose projection $\pi(\overline{M})$ is $\gamma$ itself.</p> <p>By definition of the universal cover, the points of the fiber $\pi ^{-1}(p_ {0}), p_ {0}\in M$ are in $1:1$ correspondence with the distinct homotopy classes of closed curves in $M$ starting at $p_ {0}$. Summarizing,</p> <p><strong>Theorem.</strong> The universal cover $\overline{M}$ of $M$ is simply connected and the number of sheets in the covering is equal to the number of elements (the <code class="language-plaintext highlighter-rouge">order</code>) of $\pi ^{-1}(p_ {0})$.</p> <p>If a manifold is not orientable, there is some closed curve that reverses orientation. We have the following explanation of the terminology that we have been using:</p> <p><strong>Theorem.</strong> The orientable cover of $M$ is always orientable. The number of sheets is $1$ if $M$ is orientable and $2$ if $M$ is not orientable.</p> <hr/> <p>In homotopy group $\pi_ {1}(M)$, there sometimes exists properties that can be used to define subgroups of $\pi_ {1}$. For example, if $M$ is not orientable, then two curves $\gamma,\gamma_ {1}$ in <em>different</em> homotopy class may preserve the orientation or not. $\gamma$ and $\gamma_ {1}$ may both preserve the orientation, then they belong to the same subgroup of $\pi_ {1}$ that is defined to be the loops that preserve the orientation. The identity of this subgroup is the equivalence class of all the loops that preserves the orientation, while the other element is the class of loops that flip the orientation, if you are in doubt just think of loops on a Mobius strip.</p> <p>There is a relation between the subgroups of $\pi_ {1}$ and the covering space. Given any subgroup $G$ of $\pi_ {1}$, we may associate a covering space $M_ {G}$ of $M$ as follows: We again consider pairs $p\in M,\gamma$, and we identify $(p,\gamma)$ with $p,\gamma_ {1}$ iff the homotopy class of the loop $\gamma \gamma_ {1}^{-1}$ lies in the subgroup $G$.</p> <hr/> <p>What happens if we put universal covering space with Lie group? After all Lie groups are nothing but manifolds themselves. It is more or less intuitive that, for a Lie group $G$, its universal covering space is itself another Lie group $\overline{G}$. For example, $SO(3)\cong \mathbb{R}P^{3}$ is a Lie group, its universal covering space is $SU(2)\cong \mathbb{S}^{3}$, which is another Lie group.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="Geometry"/><category term="Frankel"/><summary type="html"><![CDATA[Disclaimer: Nothing in this note is original.]]></summary></entry><entry><title type="html">Bi-invariant Forms on Compact Groups</title><link href="https://baiyangzhang.github.io/blog/2023/Bi-invariant-Forms-on-Compact-Groups/" rel="alternate" type="text/html" title="Bi-invariant Forms on Compact Groups"/><published>2023-10-29T00:00:00+00:00</published><updated>2023-10-29T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Bi-invariant-Forms-on-Compact-Groups</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Bi-invariant-Forms-on-Compact-Groups/"><![CDATA[<p><em>Disclaimer: Nothing in this note is original.</em></p> <h2 id="bi-invariant-forms-on-compact-groups">Bi-invariant Forms on Compact Groups</h2> <p>Recall that a form or vector field on $G$ is said to be bi-invariant if it is both left and right invariant.</p> <p><strong>Theorem.</strong> if $\alpha$ is a bi-invariant $p$-form, then $p$ is closed, \(d \alpha=0.\) Recall that the Maurer-Cartan form is defined as \(\Omega := g^{-1} dg\)</p> <p>The Maurer-Cartan form is a $\mathfrak{g}$-valued 1-form defined on a Lie group $G$, where $\mathfrak{g}$ is the Lie algebra of $G$. The Maurer-Cartan form encodes the local structure of the Lie group, and it is used to translate between the geometry of the Lie group and the algebra of its Lie algebra.</p> <p>If $G$ is a Lie group and $g \in G$, the Maurer-Cartan form $\Omega$ at $g$ is a linear map from the tangent space $T_gG$ of $G$ at $g$ to the Lie algebra $\mathfrak{g}$ of $G$:</p> \[\Omega_g : T_gG \rightarrow \mathfrak{g}.\] <p>Now, let $v$ be a $\mathfrak{g}$-valued vector field on $G$. Then the action of the Maurer-Cartan form on $v$ is given by \(\Omega(v) : G \rightarrow \mathfrak{g},\)\(g \mapsto \Omega_g(v_g),\) where $v_g$ is the value of the vector field $v$ at $g \in G$.</p> <p>The Maurer-Cartan form has the property that it is left-invariant, i.e., for any $g, h \in G$, we have \(\Omega_{gh} = \mathrm{Ad}(h^{-1}) \cdot \Omega_g,\) where $\mathrm{Ad}$ is the adjoint representation of the Lie group $G$. This reflects the relationship between the geometry of the Lie group and the algebraic structure of its Lie algebra.</p> <p>we can also define the <code class="language-plaintext highlighter-rouge">Cartan p-form</code> \(\Omega_ {p} := \mathrm{Tr}\, \Omega^{p} =\mathrm{Tr}\,\left\{ \Omega \wedge \dots \wedge \Omega \right\} .\) Cartan p-forms are scalar forms with very special properties, 1) they are bi-invariant hence closed, 2) $\Omega_ {2p}=0$ automatically. We will neglect the proof here.</p> <p>The Cartan 3-form plays an especially important role. Since $\Omega(X)=X$ for all $X\in{\frak g}$, we have \((\Omega \wedge \Omega)(X,Y) = \Omega(X)\otimes \Omega(Y) - \Omega(Y)\otimes \Omega(X)=[X,Y],\) When $G$ is compact we have \(\Omega^{3}(X,Y,Z) = 3\mathrm{Tr}\,([X,Y]Z).\)</p> <hr/> <p><strong>Bi-invariant Riemannian Metrics</strong></p> <p>Let $\left\langle -,- \right\rangle$ be the scalar product in ${\frak g}$ that is Ad invariant. The existence of an invariant scalar product is closely related to the concept of compactness for Lie groups. If a Lie group is compact, then it is possible to average any bilinear form on the Lie algebra over the group to obtain an invariant scalar product. This averaging process relies on the existence of a normalized Haar measure, which exists for compact groups. However, if the Lie group is not compact, such a Haar measure does not exist in general, and hence the averaging process cannot be carried out in the same way. As a result, there is no guarantee that a non-compact Lie group will have an invariant scalar product on its Lie algebra. That said, it is possible for non-compact Lie groups to have invariant scalar products on their Lie algebras, but this is not guaranteed, and it depends on the specific properties of the group and its algebra.</p> <p><strong>Theorem.</strong> There is a bi-invariant Riemannian metric on every compact Lie group</p> <p><strong>Theorem.</strong> In any bi-invariant metric on a group, the geodesics are the 1-parameter subgroups and their translates.</p> <p>As a result, in a group with a bi-variant Riemann metric, a geodesic through $e$ with tangent velocity $X$ is given by $e^{ tX }$.</p> <p>One says that a Riemannian manifold $M^{n}$ is <code class="language-plaintext highlighter-rouge">geodesically complete</code> if every geodesic starting anywhere $C(0)\cdot e^{ tX }$ exists for all real value of $t$. It is a fact that if $M$ is compact then it is automatically geodesically complete. Furthermore</p> <p><strong>Hopf-Rinow Theorem.</strong> If $M^{n}$ is geodesically complete, then any pair of points on $G$ can be joined with a geodesic of minimal length.</p> <p>For a proof of these facts see Milnor’s book.</p> <p>In a compact group $G$ we may introduce a bi-invariant metric, and then the 1- parameter subgroups are geodesics. Thus</p> <p><strong>Theorem.</strong> Every point in a compact connected Lie group G lies on at least one 1-parameter subgroup.</p> <hr/> <p>There is a theorem saying that, in a <em>bi-invariant metric</em> on a <em>compact</em> connected Lie group $G$, the bi-invariant forms coincide with the harmonic forms.</p> <p>We will skip the proof here, just mention that it will make use of the fact that the Hodge dual $\star$ commutes with left and right translations $R_ {g\ast}, L_ {g\ast}$.</p> <p><strong>Bi-invariant forms are harmonic in the bi-invariant metric!</strong></p> <hr/> <p>The <code class="language-plaintext highlighter-rouge">center</code> of a group $G$ is the <em>subgroup</em> $Z$ of elements that commute with all the elements of the group. For example, the center of $U(N)$ is $e^{ i\theta }\mathbb{1}_ {N}$, the center of $SU(N)$ is the $n$ scalars $\lambda$ such that $\lambda^{i}=1,\;i=1,\dots,N$.</p> <p><strong>Weyl’s Theorem.</strong> Let $G$ be a compact connected group. Then the first Betti number vanishes, $b_ {1}(G) = 0$, iff the center of $G$ does not contain any $1$-parameter subgroup.</p> <p>In particular, $b_ {1}=0$ for $SU(N)$ but not for $U(N)$.</p> <p>The following plays an important role in gauge theories,</p> <p><strong>Cartan’s Theorem.</strong> If $G$ is a <em>compact</em>, <em>nonabelian</em> Lie group, then the Cartan $3$-form \(\Omega_ {3} = \mathrm{Tr}\,(g^{-1} dg g^{-1} dg g^{-1} dg)\) is a non-trivial harmonic form. In particular $b_ {3}(G)\neq 0$.</p> <h2 id="the-geometry-of-a-lie-group">The Geometry of a Lie Group</h2> <p>Let $G$ be a Lie group endowed with a bi-invariant metric (which exist on every compact Lie group). Let $\mathbf{X},\mathbf{Y},\mathbf{Z},\dots$ be left-invariant vector fields. In a compact Lie group, geodesics are also 1-parameter subgroups and vise versa, so \(\nabla_ {\mathbf{X}}\mathbf{X}=0.\) Like wise, \(\nabla_ {\mathbf{X}+\mathbf{Y}}(\mathbf{X}+\mathbf{Y}) \implies 2\nabla_ {\mathbf{X}}\mathbf{Y}=[\mathbf{X},\mathbf{Y}].\)</p> <p>The center of a Lie algebra ${\frak g}$ is defined to be the elements $\mathbf{X}$ of ${\frak g}$ such that $[\mathbf{X},\mathbf{Y}]=0$ for all $\mathbf{Y}\in{\frak g}$.</p> <p><strong>Weyl’s Theorem.</strong> Let $G$ be a compact Lie group with bi-invariant metric. Suppose that the center of $G$ is trivial, then $G$ is compact and has a finite fundamental group $\pi_ {1}(G)$.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="Geometry"/><category term="Minkowski"/><category term="Frankel"/><summary type="html"><![CDATA[Disclaimer: Nothing in this note is original.]]></summary></entry><entry><title type="html">The Poincare Duality</title><link href="https://baiyangzhang.github.io/blog/2023/Poincare-Duality/" rel="alternate" type="text/html" title="The Poincare Duality"/><published>2023-10-26T00:00:00+00:00</published><updated>2023-10-26T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Poincare-Duality</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Poincare-Duality/"><![CDATA[<h3 id="simplicial-vs-singular-complex">Simplicial vs. Singular complex</h3> <p>I put it here not because it is really related to our discussion about Poincare duality, just because I was confused and need to make some note somewhere. If you are only interested in Poincare duality please jump to the second part directly.</p> <p>A singular complex and a simplicial complex are both concepts from algebraic topology to study a space, however, they are used in different contexts and have some key differences:</p> <p>Simplicial complex is a collection of simplices (points, line segments, triangles, and their n-dimensional analogues) that are glued together in a certain way. The collection must satisfy two conditions:</p> <ol> <li>Every face of a simplex in the collection is also in the collection.</li> <li>The intersection of any two simplices in the collection is either empty or a face of both.</li> </ol> <p>On the other hand, a singular complex is not a collection of simplices, but a collection of singular simplices, which are continuous maps from the standard simplex to a topological space. Singular simplices can be used to study the topology of any space, not just those that can be decomposed into simplices.</p> <p>Simplicial complexes are used in situations where <em>a space can be nicely decomposed into simplices</em>. They are useful in the study of homology and cohomology, as well as in computational topology. Meanwhile, singular complexes are used to define singular homology and cohomology, which <em>can be applied to any topological space</em>, including those that can’t be nicely decomposed into simplices. Singular homology and cohomology are more general than their simplicial counterparts.</p> <p>A simplicial complex has a clear geometric representation as a collection of simplices glued together. On the contrary, a singular complex doesn’t have as clear a geometric representation since it consists of maps from simplices to a space, rather than the simplices themselves. Simplicial complexes are less flexible than singular complexes since they can only be used to study spaces that can be decomposed into simplices</p> <p>In summary, a simplicial complex is a collection of simplices glued together in a specific way, while a singular complex is a collection of continuous maps from the standard simplex to a topological space. Simplicial complexes have a clear geometric representation and are useful in studying spaces that can be decomposed into simplices, while singular complexes are more flexible and can be used to study any topological space.</p> <h2 id="poincares-duality">Poincare’s Duality</h2> <h3 id="introduction">Introduction</h3> <p>Poincaré duality is a fundamental and profound concept in algebraic topology, bridging homology and cohomology, two primary tools we use to probe the topological structure of a manifold. This relationship can provide a richer understanding of the manifold’s geometry and topology.</p> <p>In simple terms, Poincaré duality tells us that for a <em>compact</em>, <em>orientable</em> manifold $M$ of dimension $n$, the $k$-th homology group of $M$ is isomorphic to the $(n-k)$-th cohomology group. Mathematically, this is represented as: \(H_ k(M; \mathbb{Z}) \cong H^{n-k}(M; \mathbb{Z}).\) To truly appreciate Poincaré duality, we must first have a solid grasp of some core concepts in algebraic topology.</p> <p><strong>1. Homology:</strong></p> <ul> <li>Homology provides a way to associate algebraic objects, called homology groups, to topological spaces. These groups capture the essence of “holes” in the space at different dimensions. For example, the $0$-th homology group measures the number of connected components, the $1$-st homology group captures loops, and the $2$-nd homology group represents cavities or voids in the space.</li> </ul> <p><strong>2. Cohomology:</strong></p> <ul> <li>Cohomology, similar to homology, assigns algebraic objects, cohomology groups, to a topological space. However, instead of measuring “holes,” cohomology groups measure the ways in which functions can be integrated over these “holes.” Algebraically speaking, cohomology is a linear operation that turns the elements of homology into numbers.</li> </ul> <p>Now, let’s delve deeper into Poincaré duality. For a compact, orientable $n$-dimensional manifold $M$, we can consider a $k$-dimensional cycle $Z$ (which represents an element of the $k$-th homology group) and a $(n-k)$-dimensional cohomology class $\omega$. Poincaré duality states that we can pair $\omega$ with $Z$ to produce a number, and this pairing is <em>non-degenerate</em>. In simpler terms, for each non-zero $k$-dimensional cycle, there exists a $(n-k)$-dimensional cohomology class that “detects” it, and vice versa.</p> <p>Poincare duality has profound implications for the study of manifolds. It tells us that <em>the topology of a manifold at small dimensions reflects its topology at large dimensions</em>. This symmetry provides an elegant and powerful tool for understanding the manifold’s structure.</p> <p>In our upcoming sections, we will explore the details.</p> <h4 id="review-of-homology">Review of homology</h4> <p><em>Induced homomorphism in homology</em></p> <p>In homology theory, an induced homomorphism is a function between homology groups that is naturally derived from a continuous function between topological spaces. In other words, if you have a <strong>continuous</strong> function $f: X \to Y$ between two topological spaces $X$ and $Y$, then this function will “induce” a homomorphism $f_ *: H_ n(X) \to H_ n(Y)$ between the homology groups of $X$ and $Y$.</p> <p>The induced homomorphism has the following properties:</p> <ol> <li><strong>Functoriality:</strong> <ul> <li>If $f: X \to Y$ and $g: Y \to Z$ are continuous functions, then $(g \circ f)_ \ast = g_ \ast \circ f_ \ast$.</li> <li>If $id_ X: X \to X$ is the identity function, then $(id_ X)_ \ast = id_ {H_ n(X)}$.</li> </ul> </li> <li><strong>Preservation of Homologous Cycles:</strong> <ul> <li>If two cycles are homologous in $X$, then their images under $f_ *$ will be homologous in $Y$.</li> </ul> </li> </ol> <p>Mathematically, if $f: X \to Y$ is a continuous map, then for each non-negative integer $n$, there is an induced homomorphism: \(f_ \ast: H_ n(X) \to H_ n(Y)\) The induced homomorphism is defined by taking a homology class $[z]$ in $H_ n(X)$ and mapping it to the homology class $[f(z)]$ in $H_ n(Y)$, where $f(z)$ is the image of the cycle $z$ under the map $f$. This can be expressed as: \(f_ \ast([z]) = [f(z)]\) The concept of induced homomorphisms allows us to study how the topological properties of spaces are related through continuous functions.</p> <p><strong>Relative homology groups</strong></p> <p>The notation $H_ k(M, N; R)$ denotes the $k$-th relative homology group of a pair of topological spaces $(M, N)$ with coefficients in a ring $R$. Here, $M$ is a topological space, $N$ is a subspace of $M$, and $R$ is a ring (commonly $\mathbb{Z}$ for integer coefficients or $\mathbb{Q}$ for rational coefficients).</p> <p>Relative homology groups are a generalization of homology groups that allow us to study the topology of a space relative to a subspace. The $k$-th relative homology group $H_ k(M, N; R)$ measures the $k$-dimensional holes in $M$ that are not in $N$, with the homology classes represented with coefficients in the ring $R$.</p> <p>The definition of the $k$-th relative homology group involves the following steps:</p> <ol> <li> <p><strong>Chain Complex:</strong> Consider the chain complex of $M$ and the chain complex of $N$ with coefficients in $R$. These are sequences of $R$-modules and boundary homomorphisms that capture the $k$-dimensional simplices in $M$ and $N$, respectively.</p> </li> <li> <p><strong>Quotient Chain Complex:</strong> Form the quotient chain complex $C_ k(M, N; R) = C_ k(M; R) / C_ k(N; R)$.</p> </li> <li> <p><strong>Boundary Homomorphisms:</strong> Define boundary homomorphisms $\partial_ k: C_ k(M, N; R) \to C_ {k-1}(M, N; R)$ on the quotient chain complex.</p> </li> <li> <p><strong>Homology Group:</strong> Finally, the $k$-th relative homology group $H_ k(M, N; R)$ is the quotient group of cycles modulo boundaries in the quotient chain complex.</p> </li> </ol> <h3 id="mikio-nakaharas-approach">Mikio Nakahara’s approach</h3> <p>In Nakahara’s textbook, he approached Poincare duality (Chapter 6, <em>DE RHAM COHOMOLOGY GROUPS</em>) as the following.</p> <p>First he defined the “standard” version of everything, a standard space is just an Euclidean space $\mathbb{R}^{n}$, a standard n-simplex is a generalization of a triangle to n-dimensional space. To define it more formally, a standard $n$-simplex is a set of points ${v_ 0, v_ 1, \ldots, v_ n}$ in $\mathbb{R}^{n+1}$ such that:</p> <ol> <li>Each $v_ i$ is a standard basis vector. This means that it has a 1 in the $i$-th coordinate, and 0 in every other coordinate. For example, in $\mathbb{R}^3$, the second standard basis vector is $(0, 1, 0)$.</li> <li>The convex hull of these points is the standard $n$-simplex. Mathematically, this is defined as: \(\overline{\sigma}_ {r} := \left\{\sum_ {i=0}^n t_ i v_ i \,\mid \, \sum_ {i=0}^n t_ i = 1 \text{ and } 0 \leq t_ i \leq 1 \text{ for all } i\right\}.\)</li> </ol> <p>Here are a few examples of standard simplices:</p> <ul> <li>The standard 0-simplex is a single point.</li> <li>The standard 1-simplex is a line segment.</li> <li>The standard 2-simplex is an equilateral triangle.</li> <li>The standard 3-simplex is a regular tetrahedron.</li> </ul> <p>Then Nakahara goes on to define the integral of a n-form on a standard n-simplex. Here you don’t need to worry about things such as orientation, volume element (without $\sqrt{ \left\lvert g \right\rvert }$ since is it trivial), nice and simple. He uses <em>singular</em> instead of <em>simplicial</em> complex to study the topology of a manifold $M$. After defining the <em>chains, cycles and boundaries</em> (the basic building blocks of homology), Nakahara defined what a homology group is. It is just the classes of cycles up to boundaries, \(B_ {r}(M;\mathbb{R}) := Z_ {r}(M;\mathbb{R}) / B(M;\mathbb{R})\) where $\mathbb{R}$ denotes the coefficient of the homology group, could be replaced by $\mathbb{Z}_ {2}$ or other groups.</p> <p>Recall that Nakahara defined standard simplex $\overline{\sigma}$ and a generic, non-standard simplex $\sigma$ (without the bar) on $\mathbb{R}^{n}$. Mapping continuously a generic simplex $\mathbb{R}^{n}$ to $M$ you have the singular simplex on $M$, singular since the map might be without a inverse. Now we can do integral not only on r-simplexes on $\mathbb{R}^{n}$ but also on r-chains on the manifold $M$, by pulling back the form to be integrated on $M$ to $\mathbb{R}^{n}$, then make use of the linearity of integrals. By the end of the first section, Nakahara introduced the Stokes’ theorem.</p> <p>In the second section about de Rham cohomology, Nakahara first introduced counterparts of cycles and boundaries of differential forms, namely \(\text{cycles} \longleftrightarrow \text{cocycyles = closed forms}\) and \(\text{boundaries} \longleftrightarrow \text{coboundaries = exact forms.}\)</p> <p>The boundaries also form vector spaces with $\mathbb{R}$-coefficients. Then the de Rham cohomology group was introduced as the quotient \(H^{r} (M; \mathbb{R}) = \text{Closed forms} / \text{Exact forms}.\)</p> <p>All this could be better illustrated using a complex chain maybe, but that’s not Nakahara’s approach, maybe considering the physics backgrounds of the readers. By the end of the section he introduced the de Rham’s theorem, which we already covered in another note under the name “de Rham …”.</p> <p>In section 6.3, Nakahara introduces a lemma that can help to decide when is a closed form also exact, we have already covered this again in another note talking about Poincare’s <code class="language-plaintext highlighter-rouge">potential</code>. Roughly speaking, on a patch $U\subset M$, if $U$ is contractible to a point, then a closed form defined on it is exact. <em>Any closed form is exact at least locally.</em> The de Rham cohomology group is regarded as an obstruction to the global exactness of closed forms.</p> <p>In section 6.4, Nakahara starts with Poincare duality, which is the topic of our note today. We will take more time to explain it here.</p> <hr/> <p>Let $M$ be a $m$-dimensional <em>compact</em> m-dimensional manifold and let $\omega \in H^{r}(M)$ and $\eta \in H^{m-r}$. Noting that $\omega \wedge \eta$ is a volume element, so we can construct an inner product \(\left\langle \omega,\eta \right\rangle := \int _ {M} \, \omega \wedge \eta.\) The inner product $\left\langle -,- \right\rangle$ is a map that takes an element from $H^{r}$ and $H^{m-r}$ to a number. This map is both linear (obviously) and non-singular, meaning that if $\omega \neq 0$ then $\left\langle \omega,\eta \right\rangle$ can’t be zero for all $\eta$. That makes the relation between $H^{r}$ and $H^{m-r}$ a <code class="language-plaintext highlighter-rouge">duality</code>.</p> <p>According to Nakahara, this is called the <code class="language-plaintext highlighter-rouge">Poincare duality</code>. The problem is that, the Poincare duality defined here is between two cohomology groups, in the meanwhile the Poincare duality I read about from somewhere else is between homology and cohomology, between $H_ {r}$ and $H^{m-r}$. Maybe the missing connection between $H_ {r}$ and $H^{r}$ is given by the de Rham theorem? To answer this I’ll keep reading Hatcher’s textbook.</p> <hr/> <p>After introducing the Poincare duality, Nakahara introduces the cohomology ring where the role of production is played by wedge. Note that this is a ring regarding cohomology, not the differential forms themselves. This is one big difference between homology and cohomology, that we can define a sensible product for cohomology but not for homology. But I am not sure if this statement only applies to de Rham cohomology or to any cohomology.</p> <p>At the end of this chapter, Nakahara talked about Kunneth formula, which tells</p>]]></content><author><name>Baiyang Zhang</name></author><category term="math"/><category term="quantum"/><category term="field"/><category term="theory"/><category term="confinement"/><summary type="html"><![CDATA[Simplicial vs. Singular complex]]></summary></entry><entry><title type="html">Introduction to Higher Form Symmetry</title><link href="https://baiyangzhang.github.io/blog/2023/Introduction-to-Higher-Form-Symmetry-Lecture-3/" rel="alternate" type="text/html" title="Introduction to Higher Form Symmetry"/><published>2023-10-23T00:00:00+00:00</published><updated>2023-10-23T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Introduction-to-Higher-Form-Symmetry%20Lecture%203</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Introduction-to-Higher-Form-Symmetry-Lecture-3/"><![CDATA[<p>For conventions used in this note, see my blog <a href="http://www.mathlimbo.net/2022/07/17/Conventions-and-Formula/">here</a>.</p> <p>In lecture 2 we have talked about classic symmetry and their re-interpretation using the language of differential (exterior) form. We have made the connection between the so-called symmetry defect operator (SDO) and a charged, point operator. In this note we try to generalized this concept to charged operators defined on manifolds of dimension more than zero, that is, a line, a surface, etc.</p> <p>Let’s start with the ordinary symmetry once again. Recall that we have written the variation of the action as \(\delta S = \int d^{D-1}x \, J^{\mu}\partial_ {\mu}\epsilon(x) \tag{1}\) where $\xi$ is the parameter of the symmetry transformation, under which the charged operators transform as \(\phi(x) \to \epsilon(x) \Delta \phi(x).\) In our convention, $\Delta$ stands for a small but finite change while $\epsilon(x)$ stands for an infinitesimal function. Eq. (1) can be regarded as the definition of the Noether current $J^{\mu}$, which tells us how much the action changed under the transformation in question. But, being a symmetry of the system, of course the action remains unchanged, hence the conservation of the charge \(\partial_ {\mu}J^{\mu} = 0 \Longleftrightarrow d\star J=0.\)</p> <p>In the language of differential forms, we can regard $\epsilon$ as a $0$-form, which is (by definition) a function. Then the variation of action reads \(\delta S = \int_ {M^{(D)}} (\star J)\wedge d\epsilon, \tag{2}\) where $\star J$ is a $(D-1)$-form, $d \epsilon$ is a $1$-form (since $\epsilon$ is a zero form), hence their wedge product is a $D$-form, something can be integrated over $D$-dimensional manifold $M$ (whose boundary is $\Sigma$).</p> <p>The advantage of Eq. (2) is that it can be generalized to higher forms. Assume $\epsilon$ is a 1-form now. Then $d \epsilon$ is a 2-form, as a result $\star J$ is a $(D-2)$-form and $J$ is a $2$-form. The conservation law becomes \(d \star J = 0 \to \partial_ {\mu} J^{\mu \nu}=0.\) Since $(D-2)$-form can be integrated over a $(D-2)$ manifold, we can define the charge operator as \(Q(\Sigma_ {D-2}):= \int_ {\Sigma} \, \star J.\)</p>]]></content><author><name>Baiyang Zhang</name></author><category term="PureMath"/><category term="Notes"/><summary type="html"><![CDATA[For conventions used in this note, see my blog here.]]></summary></entry><entry><title type="html">Noether’s Principal</title><link href="https://baiyangzhang.github.io/blog/2023/Noether's-Principal/" rel="alternate" type="text/html" title="Noether’s Principal"/><published>2023-10-23T00:00:00+00:00</published><updated>2023-10-23T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Noether&apos;s%E2%80%93Principal</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Noether&apos;s-Principal/"><![CDATA[<p><em>Disclaimer: Nothing in this note is original.</em></p> <h3 id="noethers-theorem-revisited">Noether’s theorem revisited</h3> <p>Consider the variation of some integral \(\delta\int _ {M} \, L_ {0}(x,\phi,\phi_ {x}) dx^0 \wedge \dots \wedge dx^{n} =0\) where both the fields and the <em>domain of integration</em> varies. The variation $\delta x$ of the domain is call <code class="language-plaintext highlighter-rouge">external variation</code> and the variation of the field $\delta \phi$ is called the <code class="language-plaintext highlighter-rouge">internal variation</code>.</p> <p>Let $M$ be a $n+1$ dimensional pseudo-Riemannian manifold and $E\to M$ be a $\mathbb{R}^{n}$ vector bundle over $M$. Let $U\subset M$ be a patch of $M$, let $\phi$ be a section over $U$. The <code class="language-plaintext highlighter-rouge">Lagrangian</code> is a functional \(L_ {0} = L_ {0}(x, \phi,\phi'),\quad x \in U.\) We are concerned with the <code class="language-plaintext highlighter-rouge">action</code> integral \(S = \int_ {M} \, L_ {0}(x,\phi,\phi _ {x}) \,d^{n+1}x,\quad d^{n+1}x := dx^{0}\wedge \dots \wedge dx^{n}.\) In order to introduce the volume form, we write \(L_ {0} dx = \mathcal{L}_ {0}(x,\phi,\phi') \sqrt{ g }\, dx^{0}\wedge \dots \wedge dx^{n}.\) where $\sqrt{ g }=\sqrt{ \left\lvert g \right\rvert }$, I just feel lazy to write the whole thing. $\mathcal{L}_ {0}$ is a true scalar, called the Lagrangian <code class="language-plaintext highlighter-rouge">density</code>. We want $\mathcal{L}_ {0}$ to be a scalar, but usually when the fiber bundle is not “flat”, the ordinary derivative is not enough. Let the structure group be $G$ with Lie algebra ${\frak g}$, then the connection is ${\frak g}$-valued 1-form, with which we can construct the covariant derivative $\nabla \phi$. Given the metric $g_ {\mu \nu}$ we can then construct a truly scalar function $\mathcal{L}$, with $\phi’$ replaced with $\nabla_ {\mu}\phi$, \(\mathcal{L} = \mathcal{L}(x,\phi,\nabla \phi).\) However this is not always the case, for example in General relativity the Lagrangian is the scalar curvature $R$, with no covariant derivative but only regular derivative.</p> <p>From $\nabla \phi=\partial \phi+\omega \phi$ and $\delta \partial=\partial \delta$ and from the assumption that the connection is unaffected by a variation of $\delta \phi$, we see immediately that \(\delta (\nabla \phi) = \nabla (\delta \phi).\)</p> <p>It can be shown that if $\phi_ {U}^{a}$ is a section on $E$, then $\partial \mathcal{L} / \partial \phi_ {U}^{a}$ define a section of the <strong>dual bundle</strong> $E^{\ast}$. The difference between $E$ and $E^{\ast}$ can be distinguished from how the transition functions act on the sections, whether it is from the left or from the right.</p> <p>Recall that $\delta \phi$ is a section of $E$, so if $\partial \mathcal{L} / \partial \phi$ is a section of $E^{\ast}$ then \(\frac{\partial \mathcal{L}}{\partial \phi} \delta \phi \text{ is a scalar.}\) If there are more than one $\phi$ we just sum over all of them.</p> <p>Spare the details, we finally arrive at the equation of motion: \(\frac{\delta \mathcal{L}}{\delta \phi} = \frac{\partial \mathcal{L}}{\partial \phi}-\nabla_ {\mu} \frac{\partial \mathcal{L}}{\partial(\nabla_ {\mu}\phi)}=0.\)</p> <p>That is, if we choose the <code class="language-plaintext highlighter-rouge">essential</code> or <code class="language-plaintext highlighter-rouge">imposed</code> boundary condition to be that $\delta \phi$ vanishes on the boundary.</p> <p>Assume the Lagrangian is invariant under the fiber motion $\phi\to \phi(\alpha)$, we shall mainly be interested in the case where there is a 1-parameter subgroup \(g(\alpha) = e^{ \alpha E }\) where $E$ is an element of the Lie algebra of the transition group, $E \in {\frak g}$. The fiber transformation is \(\phi \mapsto g(\alpha) \phi.\) If $\phi$ is a <code class="language-plaintext highlighter-rouge">critical</code> section, that is a section satisfies the equation of motion, let $\delta \phi$ be the variation of some symmetry transformation, then we have the</p> <p><strong>Noether’s theorem for internal symmetry.</strong><br/> \(\text{div} \left[ \frac{\partial \mathcal{L}}{\partial \nabla\phi^{a}}\delta \phi^{a} \right] = 0.\)</p> <p>The <em>principal</em> behind Noether’s theorem is more important. All internal first-variation problems lead to an expression of the form \(\delta \int_ {U} \, \mathcal{L} \text{ Vol} = \int_ {U} \, \left( \frac{\delta \mathcal{L}}{\delta \phi} \right)\delta \phi \text{ Vol} + \int _ {U} \, G(x,\phi,\delta \phi) \text{ Vol} .\) Again, if $\phi$ is critical and $\delta \phi$ be the variation of some internal symmetry, then by the definition of internal symmetry, the action is left alone (free from surface contribution), then $G(x,\phi,\delta \phi)=0$. This is sometimes referred to as the <code class="language-plaintext highlighter-rouge">Noether's principal.</code></p> <h3 id="yangmills-nucleon">Yang–Mills Nucleon</h3> <p>Heisenberg postulated that the proton and the neutron behave identically with respect to the “strong” interactions between nuclei. The isospin group is \(U(2) / U(1) = SU(2),\) but why $U(1)$ should be excluded? It is because given a nucleon state, which is a mixture of proton and neutron state, a global rotation by a constant $e^{ i\alpha }$ gives the same state!</p> <p>Yukawa, in 1935, introduced the idea that one should explain the strong nuclear force between nucleons by assuming that the force arises from the exchange of certain particles, mesons, unobserved at that time, just as the force between electrons results from the exchange of photons. Yang and Mills in 1954 suggested that we can arrive at exchange mesons by assuming that the correct Lagrangian for the nucleon will admit $SU(2)$ as a local symmetry group, rather than the global one of Heisenberg. the nucleon field should be considered not as a $\mathbb{C}^{2}$ function on space–time but rather as a section of a $\mathbb{C}^{2}$ vector bundle, whose structure group is $SU(2)$. Bundles has much advantage over functions, besides that it is defined globally, bundle is the most natural language for gauge theories, bundles contain the information of frames, and the free change of frames (gauge transformation). In a bundle, the different choice of frames will result in a extra piece to the curvature, that is why in comparison with the Riemannian manifold curvature, except for $\omega$ curvature which is there in both cases, there is also a $g^{-1}dg$ in the curvature of a bundle. This really comes from the associated bundle of frames.</p> <h3 id="compact-groups-and-yangmills-action">Compact Groups and Yang–Mills Action</h3> <p>The $U(N)$ group is compact. To see that, consider the components $u_ {ij}$ of $U\in U(N)$. The unitarity translates to \(\mathrm{Tr}\,U^{\dagger} U = N \implies \sum_ {ij} {u}^{\ast }_ {ij} u_ {ij} = N = \sum_ {ij} \left\lvert u_ {ij} \right\rvert ^{2},\) thus all the $u_ {ij}$ live on a complex sphere, hence must be compact.</p> <p>The advantage of compact group is that</p> <p><strong>Theorem.</strong> In a compact Lie group, the left and right Haar measures coincide (the Haar measure is bi-invariant).</p> <p><strong>Proof.</strong> Let \(\omega = \sigma_ {1} \wedge \dots \wedge \sigma_ {n}\) be the <em>left invariant</em> volume form. To say $\omega$ is <em>right invariant</em> is to say the for $g \in G$, \(R_ {g}^{\ast } (\omega {\Large\mid}_ {g} ) = \omega {\Large\mid}_ {1}, \quad 1 \text{ is the identity.}\) Let $e$ be an orthonormal basis of left invariant vector fields and \(\left\langle \omega,e \right\rangle =1.\) If $\omega$ is <strong>not</strong> right invariant, we will have \(\left\langle R_ {g}^{\ast }(\omega), e \right\rangle = \left\langle \omega, R_ {g\ast } e \right\rangle =c \neq 1,\) where we have omitted some notations. Let’s just write \(R_ {g\ast } e = e g.\) This would mean that \(\left\langle \omega, g^{-1} eg \right\rangle =c.\) Thus under this adjoint action $\text{Ad}(g)$, the orthonormal $e$ at the identity is sent into a frame at the identity with volume $c &gt; 1$. Under $Ad(g^{n})$, the frame $e$ is sent into a frame with volume $c^{n}\to \infty$ as $n\to \infty$. This means that a continuous function \(F: G \to \mathbb{R}, \quad g \mapsto \omega(g ^{-1}e g)\) is not bounded on $G$ (think hard about it!). Here comes the key point: <em>any continuous real-valued function on a compact space is bounded</em>! A contradiction. Q.E.D.</p> <p>Leg $\omega_ {h}$ be the volume form at $h$. For any continuous function $f$ and for all $g$ in the compact group $G$ we have</p> <p>\(\int _ {G} \, f(h) \omega_ {h} = \int _ {G} \, f(gh) \omega_ {h} = \int _ {G} \, f(hg) \omega_ {h}.\) This should be intuitive since $g$ is a isomorphism from the group to itself.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="Geometry"/><category term="Frankel"/><summary type="html"><![CDATA[Disclaimer: Nothing in this note is original.]]></summary></entry><entry><title type="html">Excerpt From “On Anarchism” by Noam Chomsky</title><link href="https://baiyangzhang.github.io/blog/2023/Excerpt-From-On-Anarchism/" rel="alternate" type="text/html" title="Excerpt From “On Anarchism” by Noam Chomsky"/><published>2023-10-20T00:00:00+00:00</published><updated>2023-10-20T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Excerpt-From-On-Anarchism</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Excerpt-From-On-Anarchism/"><![CDATA[<h3 id="introduction">Introduction</h3> <p><strong>Ableism:</strong> A term used to describe discrimination, prejudice, or bias against individuals with disabilities.</p> <p><strong>Gender queerness:</strong> Gender queerness is a concept and identity that falls under the broader umbrella of non-binary gender identities. It challenges and rejects the traditional binary understanding of gender as solely male or female. Instead, genderqueer individuals may identify with both genders, neither gender, or a different gender entirely. Here are some key points to understand about gender queerness:</p> <ul> <li><em>Fluidity</em>: Some genderqueer people experience a fluid gender identity, meaning their gender identity can change over time or in different circumstances.</li> <li><em>Beyond the Binary</em>: Genderqueer identities are diverse and can include identities like bi-gender (identifying as two genders), agender (lacking a gender or being gender neutral), and more.</li> <li><em>Personal Expression</em>: The way genderqueer people express their gender can vary widely. Some may choose to present androgynously, some may present in ways that are traditionally associated with men or women, and others may choose an entirely different form of expression.</li> <li><em>Preferred Pronouns</em>: Genderqueer individuals may use a variety of pronouns. This includes he/him, she/her, they/them, or other neopronouns such as ze/hir.</li> </ul> <p><strong>Zapatistas:</strong> The Zapatistas, officially called the Zapatista Army of National Liberation (EZLN), is a revolutionary leftist group that originated in Mexico in 1994. The group is named after Emiliano Zapata, a leading figure in the Mexican Revolution who advocated for land reforms and the rights of peasants. Here are some key aspects of the Zapatistas:</p> <ul> <li><strong>Indigenous Rights</strong>: A significant aspect of the Zapatista movement is its emphasis on the rights and autonomy of indigenous peoples in Mexico. Much of the EZLN’s membership consists of indigenous people from various ethnic groups.</li> <li><strong>Form of Struggle</strong>: After the initial 1994 uprising, the Zapatistas transitioned from a traditional armed struggle to a more peaceful, grassroots, and civil resistance. They prioritize community-based initiatives and projects over direct military action.</li> <li><strong>Autonomous Municipalities</strong>: The Zapatistas established “caracoles” (snails) and autonomous municipalities in Chiapas. These areas are governed by local assemblies and function outside the traditional political system of Mexico.</li> <li><strong>Subcomandante Marcos</strong>: One of the most well-known figures of the Zapatista movement is Subcomandante Marcos (also known as Subcomandante Galeano since 2014). He served as the movement’s spokesperson and is recognized for his writings, which mix revolutionary theory with poetic and mythical elements.</li> </ul> <p><strong>Black blocs:</strong> The term “black bloc” refers to a tactic often used by protesters, rather than a specific group or organization. Participants in a black bloc wear all-black clothing and cover their faces with masks or bandanas to maintain anonymity, create a unified presence, and avoid identification by law enforcement or surveillance.</p> <p><strong>Occupy Wall Street:</strong> Occupy Wall Street (OWS) was a protest movement that began on September 17, 2011, in Zuccotti Park, located in New York City’s Wall Street financial district. It was initiated by the Canadian activist group Adbusters and subsequently spread to cities across the United States and around the world. The primary slogan of the movement was “We are the 99%,” which highlighted the increasing income and wealth inequality in the U.S., drawing attention to the concentration of wealth in the top 1% of the population.</p> <p>Here are some key aspects and outcomes of Occupy Wall Street:</p> <ul> <li><strong>Goals and Grievances</strong>: The movement was largely about economic inequality, corporate influence over the democratic process, and the perceived failures of Wall Street and big banks, especially in the wake of the 2007-2008 financial crisis. Protesters also voiced concerns about issues such as student loan debt, foreclosures, and the influence of money in politics.</li> <li><strong>Leaderless Structure</strong>: OWS was deliberately leaderless and used a consensus-based decision-making process. General Assemblies were held where participants could discuss issues, make decisions, and plan actions.</li> <li><strong>Encampments</strong>: Inspired by the initial occupation of Zuccotti Park, similar encampments sprang up in various cities around the U.S. and the world. These became sites of community, discussion, and often faced police evictions.</li> <li><strong>Impact on Discourse</strong>: While OWS did not have a specific set of demands or a unified platform, it significantly impacted public discourse. Concepts like income inequality and the “1%” became more mainstream topics of discussion in the media, among politicians, and in everyday conversations.</li> <li><strong>Criticism</strong>: The movement faced criticism on various fronts. Some felt that the lack of a clear set of demands or a centralized leadership made the movement ineffective. Others believed that certain actions or behaviors by protesters detracted from the movement’s broader goals.</li> <li><strong>Legacy</strong>: While the encampments and direct actions associated with OWS dwindled by 2012, the movement’s broader impacts can still be seen. It influenced subsequent social movements and brought attention to economic inequality, which remained a significant topic in subsequent political campaigns and policy discussions.</li> <li><strong>Connection to Other Movements</strong>: OWS shared tactics, ideas, and personnel with other movements and causes, such as Black Lives Matter, the Fight for $15 (a campaign for a higher minimum wage), and even the 2016 and 2020 presidential campaigns of Senator Bernie Sanders.</li> </ul> <p> <strong>corporatocracy:</strong> A system in which power effectively rests with a small, elite group of inside individuals, sometimes from a small group of educational institutions, or influential economic entities or devices, such as banks, commercial entities, lobbyists that act in complicity with, or at the whim of the oligarchy, often with little or no regard for constitutionally protected prerogative.</p> <hr/> <p><strong>Principal:</strong> Power that isn’t really justified by the will of the <em>governed</em> should be dismantled.</p> <p>The anarchist historian Rudolf Rocker (a German socialist, 1873-1953) said</p> <blockquote> <p>For the Anarchist, freedom is not an abstract philosophical concept, but the vital concrete possibility for every human being to bring to full development all the powers, capacities, and talents with which nature has endowed him, and turn them to social account.</p> </blockquote> <p>First, what is a syndicalist. A syndicalist is someone who supports or is involved in syndicalism, which is a type of economic and political theory that advocates for the control of industries and businesses by worker unions. The basic idea behind syndicalism is the belief that <em>workers, through their unions, should own and manage industries, rather than being controlled by private owners or the state</em>.</p> <p>Key aspects of syndicalism include:</p> <ol> <li> <p><strong>Direct Action</strong>: Syndicalists often believe in bypassing political systems and advocating for change through strikes, boycotts, and other forms of direct collective action.</p> </li> <li> <p><strong>Workers’ Self-Management</strong>: Workers should manage the workplaces, making decisions that affect them directly rather than having those decisions made by capitalists or distant bureaucrats.</p> </li> <li> <p><strong>Anti-Capitalism</strong>: Syndicalism is inherently anti-capitalist as it opposes the private ownership of the means of production for profit.</p> </li> <li> <p><strong>Solidarity</strong>: A strong emphasis on solidarity among workers, believing that all labor has a common interest against the exploitative nature of capitalist society.</p> </li> <li> <p><strong>Federalism</strong>: Syndicalism promotes a federated structure of organization, from local to regional to national and international levels, with decision-making power remaining as local as possible.</p> </li> </ol> <p>Syndicalism was particularly influential in the early 20th century and played a significant role in labor movements in Europe, notably in Italy, France, and Spain, where it was linked with large-scale social and political upheavals. It is less prominent today but still exists as a current in labor and leftist movements around the world.</p> <p>Rocker would take for granted that</p> <blockquote> <p>the serious, final, complete liberation of the workers is possible only upon one condition: that of the appropriation of capital, that is, of raw material and all the tools of labor, including land, by the whole body of the workers.</p> </blockquote> <p>Anarcho-syndicalist are convinced that a Socialist economic order cannot be created by the decrees and statues of government, but only by the solidaric collaboration of the workers with hand and brain in each special branch of production; that is, through the taking over of the management of all plants by the producers themselves under such form that the separate groups, plants, and branches of industry are independent members of the general economic organism and systematically carry on production and the distribution of the products in the interest of the community on the basis of free mutual agreements.</p> <p>Such ideas had been put into practice in a dramatic way in the <strong>Spanish Revolution</strong>.</p> <p>The Spanish Revolution of <code class="language-plaintext highlighter-rouge">1936</code> was a profound social upheaval triggered by a military coup against the <code class="language-plaintext highlighter-rouge">Second Spanish Republic</code>, which had been grappling with deep political instability and socioeconomic unrest. This period, marked by an audacious experiment in social and economic transformation, arose in the context of a country deeply divided. On one side were the <code class="language-plaintext highlighter-rouge">Nationalists</code>, consisting of conservative elements like monarchists, large landowners, the Catholic Church, and the fascist <em>Falange</em>, led by General <em>Francisco Franco</em>. On the other, stood the <code class="language-plaintext highlighter-rouge">Republicans</code>, a heterogeneous group that included liberal bourgeoisie, socialists, communists, and anarchists. Years of escalating tensions between the wealthy elite and the working classes, a succession of failed political reforms, and the influence of rising European fascist movements had primed Spain for conflict. The attempted coup fractured the nation, with various regions holding out as strongholds of resistance, particularly Catalonia and Aragon, where anarchist and socialist groups were most influential. As the Republic scrambled to contain the rebellion, a vacuum of power enabled radical left-wing elements to assume control of cities like Barcelona, implementing far-reaching collectivization policies that saw factories, utilities, and farms taken over by workers’ collectives.</p> <p>This radical restructuring of society unfolded amidst a brutal civil war that would last until 1939. Workers, armed and organized into militias, not only fought on the front lines against the Nationalist forces but also transformed the rear guard into a laboratory of social innovation. Land was redistributed, industries were self-managed by employees, and many societal structures were reorganized according to libertarian socialist principles, particularly those championed by the Confederación Nacional del Trabajo (CNT) and the Federación Anarquista Ibérica (FAI). However, the revolution was marred by internal conflicts, as the diverse Republican factions had conflicting goals and ideologies. The Communist Party of Spain, backed by Soviet aid, sought to stabilize the Republic and undermine the anarchists’ influence, leading to intra-Republican strife that culminated in the violent May Days of 1937 in Barcelona. Meanwhile, the Nationalists steadily gained ground, aided significantly by military support from Nazi Germany and Fascist Italy, while the policy of non-intervention adopted by democratic powers effectively isolated the Republican forces. The internal dissension, coupled with the steady advance of Franco’s forces, spelled doom for the Republican cause. By 1939, the Nationalist victory had snuffed out the revolution, and Franco established a dictatorship that would endure for decades, rolling back the revolution’s reforms and ruthlessly suppressing any remnants of opposition.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="NoamChomsky"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Lecture Notes on Mathematics in Economics</title><link href="https://baiyangzhang.github.io/blog/2023/Mathematical-Economics/" rel="alternate" type="text/html" title="Lecture Notes on Mathematics in Economics"/><published>2023-10-09T00:00:00+00:00</published><updated>2023-10-09T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Mathematical-Economics</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Mathematical-Economics/"><![CDATA[<h3 id="syllabus">Syllabus</h3> <p><strong>Semester</strong>: Fall 2023</p> <p><strong>Duration:</strong> 40 Real hours (54 teaching hours), 3 real hours per class, 14 classes / 7 weeks</p> <p><strong>Lecturer</strong>: Dr. Baiyang Zhang</p> <p><strong>Office Address:</strong> N/A</p> <p><strong>Email</strong>: <a href="mailto:james.fisher@henu.edu.cn">byzhang@henu.edu.cn</a></p> <p><strong>Lecture Schedule</strong>: Monday and Wednesday, 2:30 - 5:30</p> <p><strong>Classroom:</strong> Monday at Teaching building Room 3502, Wednesday at Room 109 at the School of Economics</p> <p><strong>Textbooks and References:</strong></p> <ol> <li> <p>“<em>Fundamental Methods of Mathematical Economics</em>” by Chiang and Wainwright</p> </li> <li> <p>“<em>Introduction to Probability</em>” by Grinstead and Snell</p> </li> <li> <p>“<em>Introduction to Linear Algebra</em>” by Strang.</p> </li> </ol> <p><strong>Course Objectives:</strong>  This course will include the basics of analysis, derivatives and integration, linear algebra, optimization, and probability, with the goal of preparing students for further course work within the School of Economics.</p> <p><strong>Assessment Policy:</strong> The assessments for this course will one final, in addition to several homeworks. Each item is scored on a percentage basis. The final score for the class is the weighted sum of the items’ scores.  The weights are as follows: final accounts for 70% of the final grade, and the homeworks account for the remaining 30% of the final grade.</p> <p>In general, the final grade is an A when the final score is 85% or better, a B when the final score is between 70% and 84.9%, a C when the final score is between 60% and 69.9%, a D when the final score is between 50% and 59.9%, and an F when the final score is below 50%.  Assessment and final grades, however, may be curved to the benefit of the students.</p> <p><strong>Tentative Weekly Schedule:</strong></p> <p>CW = Chiang and Wainwright, GS = Grinstead and Snell, W = Wooldridge.</p> <p>Additional review sessions may be scheduled in advance of exams.</p> <table> <thead> <tr> <th><strong>Lecture</strong></th> <th><strong>Topics</strong></th> <th><strong>Reading</strong></th> </tr> </thead> <tbody> <tr> <td>1</td> <td>Introduction and Basics of Analysis</td> <td>CW, Ch. 1 and 2</td> </tr> <tr> <td>2-4</td> <td>Linear Algebra</td> <td>CW, Ch. 4 and 5</td> </tr> <tr> <td>5-6</td> <td>Derivatives</td> <td>CW, Ch. 6,7 and 8</td> </tr> <tr> <td>7</td> <td>Integrals</td> <td>CW, Ch. 14</td> </tr> <tr> <td>8-10</td> <td>Unconstrained Optimization</td> <td>CW, Ch. 9, 10, and 11</td> </tr> <tr> <td>11</td> <td>Constrained Optimization with Equality Constraints</td> <td>CW, Ch. 12</td> </tr> <tr> <td>12</td> <td>Probability Distributions and Combinatorics</td> <td>GS, Ch. 1, 2 and 3</td> </tr> <tr> <td>13</td> <td>Common Distributions and Conditional Probability</td> <td>GS, Ch. 4 and 5</td> </tr> <tr> <td>14</td> <td>Expected Values</td> <td>GS, Ch. 6</td> </tr> </tbody> </table> <hr/> <h2 id="lecture-1">Lecture 1</h2> <p><strong>Mathematical Economics versus Econometrics</strong></p> <p>Econometrics is concerned mainly with the measurement of economic data. Hence it deals with the study of empirical observations using statistical methods of estimation and hypothesis testing. Indeed, empirical studies and theoretical analyses are often complementary and mutually reinforcing. On the one hand, theories must be tested against empirical data for validity before they can be applied with confidence. On the other, statistical work needs economic theory as a guide, in order to determine the most relevant and fruitful direction of research.</p> <p><strong>Economic Models</strong></p> <p>A model is essentially and necessarily an abstraction from the real world. The sensible procedure is to pick out what appeals to our reason to be the primary factors and relationships relevant to our problem and to focus our attention on these alone.</p> <p><strong>Mathematics from a bird’s eye view</strong></p> <p>Explain: Algebra, Geometry, and Analysis.</p> <p>Most people who have done some high school mathematics will think of algebra as the sort of mathematics that results when you substitute letters for numbers. Algebra will often be contrasted with arithmetic, which is a more direct study of the numbers themselves.</p> <p>There is, however, a different contrast, between algebra and geometry, which is much more important at an advanced level. The high school conception of geometry is that it is the study of <code class="language-plaintext highlighter-rouge">shapes</code> such as circles, triangles, cubes, and spheres together with concepts such as rotations, reflections, symmetries, and so on. Thus, the objects of geometry, and the processes that they undergo, have a much more visual character than the equations of algebra.</p> <p>Some parts of mathematics involve manipulating symbols according to certain rules: for example, a true equation remains true if you “do the same to both sides.” These parts would typically be thought of as algebraic, whereas other parts are concerned with concepts that can be visualized, and these are typically thought of as geometrical.</p> <p>One is more symbolic and the other more pictorial.</p> <p>The word “analysis,” used to denote a branch of mathematics, is not one that features at high school level. However, the word “calculus” is much more familiar, and differentiation and integration are good examples of mathematics that would be classified as analysis rather than algebra or geometry. The reason for this is that they involve limiting processes. For example, the derivative of a function f at a point x is the limit of the gradients of a sequence of chords of the graph of $f$ , and the area of a shape with a curved boundary is defined to be the limit of the areas of rectilinear regions that fill up more and more of the shape.</p> <p>Thus, as a first approximation, one might say that a branch of mathematics belongs to analysis if it involves limiting processes, whereas it belongs to algebra if you can get to the answer after just a finite sequence of steps.</p> <p><strong>Branches of Mathematics</strong></p> <ul> <li>Algebra. Deals with number systems, polynomials, and more abstract structures such as groups, fields, vector spaces, and rings.</li> <li>Number theory.</li> <li>Algebraic geometry</li> <li>Analysis <ul> <li>The study of PDE, ODE.</li> <li>Dynamics. What happens when you take a simple process and do it over and over again?</li> </ul> </li> <li>Logic <ul> <li>Set theory</li> <li>Category theory</li> </ul> </li> <li>Combinatorics</li> <li>Theoretical Computer Science</li> <li>Probability</li> <li>Mathematical Physics</li> </ul> <hr/> <ol> <li><em>Math as a language with its own vocabulary and syntax.</em></li> <li><em>Introduction of set theory, including the basic concepts and operations that can be done to them.</em></li> <li><em>Introduce the concept of function and functional. They are nothing but various maps from one set to another.</em></li> <li>The number system. Explain integers, rational numbers, real numbers and complex numbers. $\mathbb{R},\mathbb{N}, \mathbb{Z}, \mathbb{Q}$.</li> </ol> <h2 id="lecture-2">Lecture 2</h2> <p>“You can’t add apples and oranges.” In a strange way, this is the reason for vectors. We have two separate numbers $v_ {1}$ and $v_ {2}$. The pair produces a two-dimensional vector $\vec{v}$. Explain the following concepts:</p> <ul> <li>column,</li> <li>components.</li> </ul> <p>We don’t add $v_ {1}$ and $v_ {2}$, but we do add vectors of the same type. Explain vector addition. We want to add apples with apples.</p> <p>Explain what is a scalar, and scalar multiplication.</p> <p>Given two vectors $\vec{v}$ and $\vec{w}$, explain the linear combination of them.</p> <p>This big view, taking all the combinations of $\vec{v}$ and $\vec{w}$, is linear algebra at work.</p> <p>Illustrate the addition of vectors using arrows.</p> <p>Introduce</p> <ul> <li>dot product,</li> <li>length.</li> </ul> <p>The dot product is gonna be needed when defining the action of a matrix on a vector.</p> <p>After introducing the product rules in two different ways, we introduce the linear equations.</p> <h3 id="lecture-3">Lecture 3</h3> <p><strong>Linear combination:</strong></p> <p>Imagine you have a collection of building blocks, and each block represents a different item. A “linear combination” is like creating a new structure using these blocks, where you decide:</p> <ol> <li><strong>How many of each block to use</strong>: This is similar to multiplying the block (or item) by a number.</li> <li><strong>How to combine them</strong>: Essentially, you’re just adding these multiplied blocks together.</li> </ol> <p>Let’s use a simpler example:</p> <p>Imagine you have two types of fruit: apples and bananas.</p> <p>A “linear combination” of apples and bananas could be:</p> <ul> <li>3 apples + 2 bananas</li> <li>5 apples + 1 banana</li> <li>2 apples - 4 bananas (Yes, in mathematics, you can have negative bananas! Just think of it as owing bananas to someone.)</li> </ul> <p>In each of these cases, the number of apples and bananas you decide to use (3, 2, 5, 1, etc.) are called “coefficients”.</p> <p>When it comes to mathematics and vectors, the idea is the same. You’re combining different vectors using certain coefficients to produce a new vector. But the basic idea is just like combining apples and bananas!</p> <hr/> <p>There are many ways to look at a matrix.</p> <ol> <li> <p><strong>Table of Numbers</strong>: At its core, a matrix is like a table or grid filled with numbers. Think of it like a spreadsheet or a bingo card. Each number sits in its own little box, and these boxes are organized into rows and columns.</p> </li> <li> <p><strong>Collection of Column Vectors</strong>: Imagine each column in that table as a list of numbers. This list can be seen as a “column vector”. So, a matrix can be thought of as a collection of these column vectors, standing side by side. For example, a matrix with three columns is like having three lists (or column vectors) put together.</p> </li> <li> <p><strong>Collection of Row Vectors</strong>: Similarly, you can think of each row in the matrix as a separate “row vector”. So, another way to view a matrix is as a stack of these row vectors, one on top of the other.</p> </li> <li> <p><strong>Transformation Machine</strong>(we will go to more details in this class): This is a more advanced way to think about matrices, especially in linear algebra. Imagine you have a point on a graph. A matrix can act as a “machine” where you input your point, and out comes a new point. This new point might be stretched, squished, rotated, or even flipped compared to the original. In essence, the matrix transformed it!</p> </li> <li> <p><strong>System of Equations</strong>(topic of this class too): If you’ve ever dealt with multiple equations at once (like trying to figure out both the price of a burger and fries when given combined costs), matrices can represent these systems. Each row could represent a different equation, and the numbers in that row represent the coefficients of variables in that equation.</p> </li> <li> <p><strong>Storage and Organization</strong>: In computer science and data analysis, matrices can be used to store data. For instance, consider ratings given by users to movies on a streaming platform. Each row might represent a user, each column might represent a movie, and the number in a specific box represents the rating that user gave to that movie.</p> </li> </ol> <p>These are just some of the many ways to look at matrices. Depending on the subject (like physics, computer graphics, or economics), matrices might take on other interesting interpretations!</p> <hr/> <p><strong>The multiplication of matrices</strong></p> <p><strong>The Basics</strong>:</p> <p>Matrix multiplication is not just multiplying numbers. Instead, it’s a combination of multiplication and addition. Remember, the way you multiply matrices is quite different from multiplying regular numbers, so it’s essential to understand the steps and rules.</p> <p><strong>The Key Rule</strong>:</p> <p>For two matrices to be multiplied, the number of columns in the first matrix must be equal to the number of rows in the second matrix. This is a crucial rule.</p> <p>If Matrix A has dimensions of $m \times n$ (meaning $m$ rows and $n$ columns) and Matrix B has dimensions of $p \times q$ (meaning $p$ rows and $q$ columns), then for A and B to be multipliable, $n$ must equal $p$. The resulting matrix will have dimensions $m \times q$.</p> <p><strong>How to Multiply</strong>:</p> <p>Let’s consider two simple matrices:</p> <p>Matrix A: \(\begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\) Matrix B: \(\begin{pmatrix} 2 &amp; 1 \\ 0 &amp; 3 \end{pmatrix}\)</p> <p>To multiply them:</p> <ol> <li><strong>First element of the result (top-left corner)</strong>: <ul> <li>Take the first row of Matrix A: (1, 2).</li> <li>Take the first column of Matrix B: (2, 0).</li> <li>Multiply corresponding elements and add them up: (1×2) + (2×0) = 2.</li> </ul> </li> <li><strong>Second element in the first row (top-right corner)</strong>: <ul> <li>Take the first row of Matrix A: (1, 2).</li> <li>Take the second column of Matrix B: (1, 3).</li> <li>Multiply corresponding elements and add them up: (1×1) + (2×3) = 7.</li> </ul> </li> <li><strong>First element in the second row (bottom-left corner)</strong>: <ul> <li>Take the second row of Matrix A: (3, 4).</li> <li>Take the first column of Matrix B: (2, 0).</li> <li>Multiply and add: (3×2) + (4×0) = 6.</li> </ul> </li> <li><strong>Second element in the second row (bottom-right corner)</strong>: <ul> <li>Take the second row of Matrix A: (3, 4).</li> <li>Take the second column of Matrix B: (1, 3).</li> <li>Multiply and add: (3×1) + (4×3) = 15.</li> </ul> </li> </ol> <p>The resulting matrix is:</p> <p>\(\begin{pmatrix} 2 &amp; 7 \\ 6 &amp; 15 \end{pmatrix}\) <strong>Visualization</strong>:</p> <p>Imagine Matrix A’s rows as horizontal hands reaching out, and Matrix B’s columns as vertical hands reaching up. When these hands “high-five”, they form the elements of the resulting matrix by the rule we just discussed.</p> <p><strong>Practice</strong>:</p> <p>The best way to get comfortable with matrix multiplication is to practice. Start with smaller matrices, understand the patterns, and then work with larger ones.</p> <p>Remember, the rule of matching columns of the first matrix to rows of the second is crucial. If they don’t match, the matrices can’t be multiplied.</p> <hr/> <h3 id="example-production-in-a-shoe-factory">Example: Production in a Shoe Factory</h3> <p>Imagine you run a small shoe factory. You produce two types of shoes: sneakers and boots.</p> <p><strong>Vectors</strong>:</p> <ol> <li><strong>Production Vector</strong> for a given week: <ul> <li>Sneakers: 100 pairs</li> <li>Boots: 50 pairs</li> </ul> <p>We can represent this as: \(\text{Shoes} = \begin{bmatrix} 100 \\ 50 \end{bmatrix}\)</p> </li> <li><strong>Cost Vector</strong> for producing each type of shoe: <ul> <li>Cost to produce one pair of sneakers: $20</li> <li>Cost to produce one pair of boots: $40</li> </ul> <p>This can be represented as: \(\text{Cost} = \begin{bmatrix} 20 \\ 40 \end{bmatrix}\)</p> </li> </ol> <p><strong>Matrix</strong>: Let’s say, to produce each shoe, you need two main raw materials: leather and rubber. We can create a <strong>Material Requirement Matrix</strong> that tells us how much of each material is required to produce one unit of each shoe type.</p> <p>For example:</p> <ul> <li>Each pair of sneakers requires 1 unit of leather and 2 units of rubber.</li> <li>Each pair of boots requires 3 units of leather and 1 unit of rubber.</li> </ul> <p>This matrix is: \(\text{Materials} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 1 \end{bmatrix}\) Where the first column corresponds to the requirements for sneakers and the second column to boots.</p> <p><strong>Matrix Multiplication</strong>:</p> <p>Now, suppose you want to find out how much raw material (leather and rubber) you’ll need for the entire week’s production.</p> <p>To do this, you’d multiply the Material Requirement Matrix by the Production Vector: \(\text{Total Materials} = \text{Materials} \times \text{Shoes}\)</p> <p>Multiplying, we get: \(\text{Total Materials} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 1 \end{bmatrix} \times \begin{bmatrix} 100 \\ 50 \end{bmatrix} = \begin{bmatrix} 200 \\ 350 \end{bmatrix}\)</p> <p>So, you’ll need:</p> <ul> <li>200 units of leather (100 for the sneakers and 150 for the boots)</li> <li>350 units of rubber (200 for the sneakers and 150 for the boots)</li> </ul> <p>This simple example demonstrates the power of vectors and matrices in understanding and organizing economic production.</p> <hr/> <p>Apply the multiplication of matrices to the product of:</p> <ol> <li>row vector times column vector,</li> <li>column vector times row vector (this one is strange).</li> </ol> <hr/> <p>Certainly! Let’s embark on this journey to understand transposes and inverses using clear examples and relatable analogies tailored for students stepping into the realm of mathematical economics.</p> <hr/> <p><strong>Transposes</strong></p> <p><strong>What is a Transpose?</strong> The transpose of a matrix is obtained by flipping the matrix over its main diagonal (the diagonal from the top-left to the bottom-right). In simpler terms, the rows of the matrix become the columns, and the columns become the rows.</p> <p><strong>Visual Analogy</strong>: Imagine you have a bookshelf full of books (your matrix). If you were to tip that bookshelf onto its side (so that it’s lying down), the rows of books would now appear as columns. That’s the transpose!</p> <p><strong>Example</strong>: Given the matrix: \(A = \begin{bmatrix} 2 &amp; 5 \\ 3 &amp; 7 \\ 1 &amp; 4 \\ \end{bmatrix}\)</p> <p>The transpose, denoted as $A^T$, is: \(A^T = \begin{bmatrix} 2 &amp; 3 &amp; 1 \\ 5 &amp; 7 &amp; 4 \\ \end{bmatrix}\)</p> <p><strong>In Mathematical Economics</strong>: Transposing can be useful for various reasons, such as making certain operations or calculations easier or more intuitive. For instance, when working with data sets or in regression analysis, transposes come in handy.</p> <p><strong>Inverses</strong></p> <p><strong>What is an Inverse?</strong> The inverse of a matrix, if it exists, is a matrix that, when multiplied with the original matrix, results in the identity matrix. The identity matrix is a special square matrix with ones on the main diagonal and zeros elsewhere.</p> <p>In symbols, for a matrix $A$, its inverse is denoted $A^{-1}$, such that: \(A \times A^{-1} = I\) where $I$ is the identity matrix.</p> <p><strong>Real-life Analogy</strong>: Think of the process of multiplication and its inverse, division. When you multiply a number by its reciprocal, you get 1. Similarly, in the world of matrices, when you multiply a matrix by its inverse, you get the identity matrix.</p> <p><strong>Properties</strong>:</p> <ol> <li>Not all matrices have inverses. Only square matrices (matrices with the same number of rows and columns) have the potential to have an inverse, and even among them, not all do.</li> <li>A matrix that does not have an inverse is called “singular” or “non-invertible”.</li> </ol> <p><strong>Example</strong>: For a 2x2 matrix: \(A = \begin{bmatrix} a &amp; b \\ c &amp; d \\ \end{bmatrix}\)</p> <p>Its inverse is: \(A^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d &amp; -b \\ -c &amp; a \\ \end{bmatrix}\)</p> <p>However, this inverse exists only if $ad-bc$ is not zero. If $ad-bc = 0$, then the matrix is singular and does not have an inverse.</p> <p><strong>In Mathematical Economics</strong>: The concept of an inverse matrix is fundamental when solving systems of linear equations, which frequently appear in economics. For example, determining equilibrium in markets, analyzing input-output models, or finding solutions to optimization problems often involve the use of matrix inverses.</p> <p>Both transposes and inverses are fundamental tools in the toolbox of mathematical economics. Just as we learn to add, subtract, multiply, and divide with numbers, we learn operations and manipulations with matrices to understand and solve intricate economic phenomena. As students progress, they’ll witness the power and elegance of linear algebra in analyzing economic systems.</p> <hr/> <p><strong>Square Matrix vs. Non-Square Matrix</strong></p> <p><strong>1. Square Matrix</strong>: A matrix is called a “square matrix” if it has the same number of rows and columns. In other words, its dimensions look like $n \times n$, where $n$ is a positive integer. You can visualize it as a perfect square filled with numbers, just like a chess or checkerboard.</p> <p><strong>Example</strong>: A 2x2 matrix: \(\begin{bmatrix} 2 &amp; 5 \\ 3 &amp; 7 \\ \end{bmatrix}\)</p> <p><strong>2. Non-Square Matrix</strong>: Any matrix that doesn’t have the same number of rows and columns is a “non-square matrix”. Its dimensions might look like $m \times n$, where $m$ and $n$ are positive integers, and $m \neq n$.</p> <p><strong>Example</strong>: A 2x3 matrix: \(\begin{bmatrix} 1 &amp; 4 &amp; 7 \\ 2 &amp; 5 &amp; 8 \\ \end{bmatrix}\)</p> <hr/> <p><strong>Square Matrices are Special in Multiplication.</strong></p> <p>When we talk about multiplication in the world of matrices, square matrices have a unique property: they’re “closed under multiplication”. This might sound fancy, but let’s break it down:</p> <p><strong>Closed Under Multiplication</strong>: This means that if you multiply two square matrices of the same size, you’ll get another square matrix of that same size as the result.</p> <p>Let’s say you have two square matrices, both of size $2 \times 2$. When you multiply them, the resulting matrix will also be $2 \times 2$. This property will hold true no matter how big or small the matrices are, as long as they’re square.</p> <p><strong>Economic Analogy</strong>: Imagine each square matrix as a factory machine. When a factory machine (a square matrix) processes another machine of the same size (another square matrix), the result is always a new machine of the same dimensions. This predictable outcome allows for consistent planning and operation, making these “machines” reliable and preferred in many scenarios.</p> <p><strong>Forming a Nice Algebra</strong>: The fact that square matrices are closed under multiplication means they form a consistent system, or a “nice algebra”. In this system, you can perform operations, like multiplication, and always know what kind of result to expect (another square matrix). This consistency is useful in mathematical economics because it provides a stable framework for analysis and predictions.</p> <hr/> <p><strong>What is a Linear Equation?</strong></p> <p><strong>Definition</strong>: A linear equation is an equation of the form: \(a_ 1x_ 1 + a_ 2x_ 2 + ... + a_ nx_ n = b\) where $x_ 1, x_ 2, … x_ n$ are the variables, $a_ 1, a_ 2, … a_ n$ are constants (known as coefficients), and $b$ is another constant.</p> <p><strong>Key Features</strong>:</p> <ol> <li>Each term consists of a variable multiplied by a constant.</li> <li>No term has a variable raised to a power higher than one.</li> <li>There are no products of variables (e.g., $x_ 1 \times x_ 2$).</li> </ol> <p><strong>Simple Example</strong>: Consider the equation $3x + 2y = 12$. Here, $x$ and $y$ are the variables, and the numbers 3 and 2 are their respective coefficients.</p> <p>Imagine you’re graphing this equation on a coordinate plane. For an equation with two variables, the graph would be a straight line. That’s why it’s called “linear” – the graph is a line.</p> <p><strong>What is a System of Linear Equations?</strong></p> <p><strong>Definition</strong>: A system of linear equations is just a collection of two or more linear equations that involve the <em>same set of variables</em>.</p> <p><strong>Simple Example</strong>: \(\begin{align*} 3x + 2y &amp;= 12 \quad \text{(Equation 1)} \\ x - y &amp;= 5 \quad \text{(Equation 2)} \end{align*}\)</p> <p>In this system, you have two linear equations, and you’d typically try to find values for $x$ and $y$ that satisfy both equations simultaneously.</p> <p><strong>Graphical Interpretation</strong>: When you plot both equations on a graph:</p> <ol> <li>If they intersect at a point, that point is the solution to the system (i.e., the values of $x$ and $y$ at that point satisfy both equations).</li> <li>If they never meet (parallel lines), the system has no solution.</li> <li>If the two equations represent the same line, then there are infinitely many solutions - any point on that line is a solution.</li> </ol> <p>Such systems help in understanding multiple interdependencies. For instance, if you have a market with two goods, and each equation represents how demand or supply changes based on the price of both goods, the system helps find an equilibrium where both goods’ demands are satisfied.</p> <p>Think of a linear equation as a single straight path (line) and a system of linear equations as multiple paths. Our goal is often to find where these paths meet or if they never do. In the context of economics, these meeting points can represent equilibrium states, optimal solutions, or any scenario where multiple conditions are satisfied at once. As students dive deeper into mathematical economics, they’ll see that these simple linear systems can be powerful tools for understanding complex economic relationships.</p> <hr/> <p>Certainly! Let’s simplify the concept and lay it out for students transitioning from a high school math background.</p> <hr/> <h3 id="using-matrices-to-represent-equations"><strong>Using Matrices to Represent Equations</strong></h3> <p>Let’s say we have the following system of equations:</p> \[\begin{align*} 2x + 3y &amp;= 8 \\ x - 4y &amp;= -3 \end{align*}\] <p>This can be represented in a matrix format as $AX = B$:</p> <p>Where: \(A = \begin{bmatrix} 2 &amp; 3 \\ 1 &amp; -4 \end{bmatrix}\) (Coefficients of the variables)</p> <p>\(X = \begin{bmatrix} x \\ y \end{bmatrix}\) (Our unknowns)</p> <p>\(B = \begin{bmatrix} 8 \\ -3 \end{bmatrix}\) (Results of the equations)</p> <h2 id="lecture-4">Lecture 4</h2> <h3 id="solving-using-the-inverse"><strong>Solving Using the Inverse</strong></h3> <p>Here’s the magic part: If we multiply both sides of our matrix equation $AX = B$ by the inverse of matrix $A$, which we’ll call $A^{-1}$, we can isolate $X$ (our unknowns).</p> <p>Doing the math: \(A^{-1}AX = X = A^{-1}B\)</p> <p>So, if we can find the inverse of $A$ (remember, not all matrices have inverses!), then we can multiply it with $B$ to get our solution, $X$.</p> <p>In economics, we often deal with many variables and relationships at the same time. Instead of trying to solve each relationship individually, matrices allow us to represent these complex relationships together and solve them in a more streamlined way.</p> <p>For example, imagine you’re studying how the price of one product affects the demand for another, and vice versa. Instead of solving each relationship individually, we can group them in a system of equations, represent them as matrices, and solve them all at once.</p> <p>Using the inverse matrix to solve a system of linear equations is like having a secret decoder ring. It’s a powerful tool that can make solving complex problems more manageable. As students dive deeper into mathematical economics, they’ll find that these tools, while initially seeming abstract, can be invaluable in understanding and analyzing economic relationships and behaviors.</p> <hr/> <p>Let’s think about 2D space for a moment. We’ve all seen the classic X-Y coordinate plane. Imagine you’ve got two vectors (think of them as arrows) on this plane. Sometimes, these two arrows will point in completely different directions. But occasionally, they might just lay flat on top of one another or be exactly opposite.</p> <p>Now, if we use these vectors as rows or columns in a matrix, the question becomes: Does this matrix have a unique way to revert any transformation it causes? Or in other words, can we find its inverse?</p> <p>This is where the idea of a matrix being “singular” comes in. A <strong>singular matrix</strong> doesn’t have an inverse. Visually, if you were to transform the entire 2D space using a singular matrix, some areas would scrunch up so much that they’d be impossible to revert to their original form.</p> <p>To figure out if a matrix is singular, we need a tool, and that tool is the <strong>determinant</strong>.</p> <p>Think of the determinant as a special number associated with a matrix. If the determinant is zero, our matrix is singular (it can’t be inverted). If the determinant isn’t zero, then the matrix can be inverted.</p> <p>For our 2x2 matrices (which are often the starting point in learning), the determinant gives us a sense of the “area scaling factor” when the matrix is used for a transformation. If the determinant is zero, it means the matrix squishes everything down to a line or a point, losing all the original area, making it impossible to revert.</p> <p>For a matrix to be nonsingular (i.e., to have an inverse), each row (like our detectives) has to bring something unique to the table. If even one row is just a repeat or combination of others, it’s like missing out on crucial information. And without that unique contribution from every row, we can’t find an inverse for our matrix.</p> <p><strong>**Rank of a Matrix</strong>**</p> <p>The rank of a matrix is a measure of the “dimension” of the linear space spanned by its rows or columns. In simpler terms, it tells us the number of linearly independent rows or columns in the matrix.</p> <p><em>The Library Analogy:</em></p> <ul> <li>Imagine you have a library of books. Some books might be exactly the same, and some might be different.</li> <li>If you were asked, “How many unique books do you have?”, you would ignore all duplicates and count only the distinct ones.</li> <li>The rank of a matrix is similar: It tells us how many “unique” rows (or columns) there are, ignoring any that can be made by combining others.</li> </ul> <p><em>Determining Rank:</em></p> <ol> <li>If a matrix has all zeros, its rank is 0.</li> <li>If a matrix has some non-zero elements but some rows (or columns) are just scalar multiples or combinations of other rows (or columns), its rank will be less than the total number of rows (or columns).</li> <li>If no row (or column) can be expressed as a combination of any other rows (or columns), the matrix is said to have full rank, meaning its rank is equal to the smaller of the number of rows or columns.</li> </ol> <hr/> <p><strong>Rank of a Matrix:</strong></p> <p>The rank of a matrix is a measure of the “dimension” of the linear space spanned by its rows or columns. In simpler terms, it tells us the number of linearly independent rows or columns in the matrix.</p> <p><strong>Analogies &amp; Insights:</strong></p> <ol> <li> <p><strong>The Library Analogy:</strong></p> <ul> <li>Imagine you have a library of books. Some books might be exactly the same, and some might be different.</li> <li>If you were asked, “How many unique books do you have?”, you would ignore all duplicates and count only the distinct ones.</li> <li>The rank of a matrix is similar: It tells us how many “unique” rows (or columns) there are, ignoring any that can be made by combining others.</li> </ul> </li> </ol> <hr/> <p><strong>Rank of a Matrix:</strong></p> <p>The rank of a matrix is a measure of the “dimension” of the linear space spanned by its rows or columns. In simpler terms, it tells us the number of linearly independent rows or columns in the matrix.</p> <p><strong>Analogies &amp; Insights:</strong></p> <ol> <li> <p><strong>The Library Analogy:</strong></p> <ul> <li>Imagine you have a library of books. Some books might be exactly the same, and some might be different.</li> <li>If you were asked, “How many unique books do you have?”, you would ignore all duplicates and count only the distinct ones.</li> <li>The rank of a matrix is similar: It tells us how many “unique” rows (or columns) there are, ignoring any that can be made by combining others.</li> </ul> </li> </ol> <p><strong>Determinant</strong>:</p> <p>The term “determinant” was used because it can “determine” whether or not a matrix has an inverse.</p> <p>Let’s say you have a 2x2 matrix: \(A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\)</p> <table> <tbody> <tr> <td>The determinant, often denoted as</td> <td>A</td> <td>or det(A), is calculated as:</td> </tr> <tr> <td>$$</td> <td>A</td> <td>= ad - bc $$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>If</td> <td>A</td> <td>equals zero, then A is singular.</td> </tr> </tbody> </table> <p>Introduce the Levi-Civita tensor. Use it to define the determinant.</p> <p>Of course! The Laplace Expansion is an important technique for calculating the determinant of a matrix. It’s especially useful when we have a matrix larger than $3 \times 3$, though it can be used for smaller matrices as well. The method is essentially a recursive process that breaks down a larger matrix into smaller ones.</p> <p><strong>Steps for Laplace Expansion:</strong></p> <ol> <li> <p><strong>Choosing a Row or Column:</strong> You can choose any row or column to expand upon. For the sake of simplicity, we often choose a row or column with the most zeros because it reduces the number of calculations we have to make (since any term multiplied by zero is zero).</p> </li> <li> <p><strong>Calculate Minors:</strong> For each element $a_ {ij}$ of the matrix, remove the i-th row and the j-th column, and compute the determinant of the resulting $(n-1) \times (n-1)$ matrix. This determinant is called the “<code class="language-plaintext highlighter-rouge">minor</code>” of the element, often denoted $M_ {ij}$.</p> </li> <li> <p><strong>Calculate Cofactors:</strong> Associated with each minor is a cofactor, which is defined as: $C_ {ij} = (-1)^{i+j} \times M_ {ij}$. This alternating sign pattern helps ensure the determinant computation is accurate.</p> </li> <li> <p><strong>Compute the Determinant:</strong> The determinant of the matrix is the sum of the products of the elements of your chosen row or column with their respective cofactors. Mathematically, if you chose the i-th row, this can be written as: \(\text{det}(A) = \sum_ {j=1}^{n} a_ {ij} C_ {ij}\)</p> </li> </ol> <p>Alternatively, if you chose the j-th column, it is: \(\text{det}(A) = \sum_ {i=1}^{n} a_ {ij} C_ {ij}\)</p> <p><strong>Example: Determinant of a $3 \times 3$ Matrix using Laplace Expansion:</strong></p> <p>Given matrix A: \(\begin{pmatrix} 1 &amp; 3 &amp; 2 \\ 4 &amp; 1 &amp; 3 \\ 2 &amp; 2 &amp; 1 \\ \end{pmatrix}\)</p> <p>To compute its determinant, let’s expand using the first row:</p> <ol> <li>For the element $a_ {11} = 1$: <ul> <li>Minor $M_ {11}$ is the determinant of: \(\begin{pmatrix} 1 &amp; 3 \\ 2 &amp; 1 \\ \end{pmatrix}\)</li> <li>$M_ {11} = 1 - 6 = -5$</li> <li>Cofactor $C_ {11} = (-1)^{1+1} \times (-5) = 5$</li> </ul> </li> <li>For the element $a_ {12} = 3$: <ul> <li>Minor $M_ {12}$ is the determinant of: \(\begin{pmatrix} 4 &amp; 3 \\ 2 &amp; 1 \\ \end{pmatrix}\)</li> <li>$M_ {12} = 4 - 6 = -2$</li> <li>Cofactor $C_ {12} = (-1)^{1+2} \times (-2) = 2$</li> </ul> </li> <li>For the element $a_ {13} = 2$: <ul> <li>Minor $M_ {13}$ is the determinant of: \(\begin{pmatrix} 4 &amp; 1 \\ 2 &amp; 2 \\ \end{pmatrix}\)</li> <li>$M_ {13} = 8 - 2 = 6$</li> <li>Cofactor $C_ {13} = (-1)^{1+3} \times 6 = -6$</li> </ul> </li> </ol> <p>Combining the results, \(\text{det}(A) = 1 \times 5 + 3 \times 2 + 2 \times (-6) = 5 + 6 - 12 = -1\)</p> <p>And that’s how you can use the Laplace Expansion to compute the determinant of a matrix! This method becomes more cumbersome for larger matrices, but the principles remain the same.</p> <p><strong>Properties of determinants</strong></p> <p>The addition (subtraction) of a multiple of any row to (from) another row will leave the value of the determinant unaltered. The same holds true if we replace the word row by column in the previous statement.</p> <p><em>It preserves the multiplication of matrices.</em> $\left\lvert A \cdot B \right\rvert = \left\lvert A \right\rvert \times \left\lvert B \right\rvert$.</p> <p><strong>Finding the Inverse Matrix</strong></p> <p>Introduce the adjoint of a matrix, and then the inverse.</p> <p><strong>Cramer’s rule</strong></p> <p>Let’s break down Cramer’s rule into a simple-to-understand explanation.</p> <p>Imagine you’re trying to solve a system of equations. This system might represent different scenarios. For instance, let’s say you and a friend are buying apples and bananas. Two different days, two different scenarios:</p> <ol> <li>On Monday, you bought 3 apples and 2 bananas, and it cost you $13.</li> <li>On Tuesday, you bought 4 apples and 5 bananas, and it cost you $31.</li> </ol> <p>From this, you have: 1) 3A + 2B = 13 2) 4A + 5B = 31</p> <p>Where A represents the cost of an apple and B represents the cost of a banana.</p> <p>Cramer’s rule helps you find the cost of A and B using determinants of matrices.</p> <p><strong>Step-by-step with Cramer’s Rule:</strong></p> <ol> <li> <p><strong>Main Determinant (D)</strong>: First, make a matrix of the coefficients of A and B: \(\begin{pmatrix} 3 &amp; 2 \\ 4 &amp; 5 \\ \end{pmatrix}\) Calculate its determinant (D). This determinant represents the “base scenario” of our system.</p> </li> <li> <p><strong>Determinant with respect to A:</strong> Replace the first column (which represents apples) with the numbers on the right side of our equations (13 and 31): \(\begin{pmatrix} 13 &amp; 2 \\ 31 &amp; 5 \\ \end{pmatrix}\) Calculate its determinant. This determinant represents the scenario when we’re focusing just on the apples.</p> </li> <li> <p><strong>Determinant with respect to B</strong>: Replace the second column (which represents bananas) with the numbers on the right side: \(\begin{pmatrix} 3 &amp; 13 \\ 4 &amp; 31 \\ \end{pmatrix}\) Calculate its determinant. This represents the scenario when we’re focusing just on the bananas.</p> </li> <li> <p><strong>Solving for A and B</strong>:</p> <ul> <li>The cost of an apple (A) is found by $A = D_ A / D$</li> <li>The cost of a banana (B) is found by $B = D_ B / D$</li> </ul> </li> </ol> <p>This gives you the individual prices of apples and bananas!</p> <p><strong>In simple words:</strong> Cramer’s rule lets you focus on one variable at a time (like just apples or just bananas) and then combine the results to find out the cost of each. It does this using determinants, which are a special number for matrices, like a fingerprint for the matrix.</p> <p>Remember, Cramer’s rule works best for systems where the number of equations matches the number of unknowns, and the main determinant (D) is not zero. If D were zero, it would be like trying to divide by zero, which we can’t do.</p> <p><strong>Application</strong></p> <p>Let’s walk through a simplified example of Input-Output Analysis using a hypothetical economy with just three industries: Agriculture, Manufacturing, and Services.</p> <p>Imagine the following table showing how each industry’s output is used as input by the others:</p> <table> <thead> <tr> <th> </th> <th>To Agriculture</th> <th>To Manufacturing</th> <th>To Services</th> <th>Final Demand</th> </tr> </thead> <tbody> <tr> <td><strong>From Agriculture</strong></td> <td>10</td> <td>30</td> <td>10</td> <td>50</td> </tr> <tr> <td><strong>From Manufacturing</strong></td> <td>20</td> <td>40</td> <td>20</td> <td>20</td> </tr> <tr> <td><strong>From Services</strong></td> <td>10</td> <td>10</td> <td>30</td> <td>50</td> </tr> </tbody> </table> <p>Each row represents the output of an industry, and each column (excluding the Final Demand column) represents the input to an industry. For example, the number 30 in the ‘From Agriculture’ row and ‘To Manufacturing’ column means that the Manufacturing sector uses 30 units of the Agriculture sector’s output.</p> <ol> <li> <p><strong>Creating the A matrix (Input-Output Coefficient Matrix):</strong> This matrix is obtained by dividing each element of the table by the total output of the corresponding industry. The total output for each industry is the sum of its outputs to all industries plus its final demand.</p> <p>Total output for each sector:</p> <ul> <li>Agriculture: 10 + 30 + 10 + 50 = 100</li> <li>Manufacturing: 20 + 40 + 20 + 20 = 100</li> <li>Services: 10 + 10 + 30 + 50 = 100</li> </ul> <p>Now, construct the A matrix:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|  0.1  0.3  0.1 |
|  0.2  0.4  0.2 |
|  0.1  0.1  0.3 |
</code></pre></div> </div> </li> <li><strong>The Leontief Inverse</strong>: To find the total output required to satisfy a given final demand, we use: \(X = (I - A)^{-1} Y\) Where: <ul> <li>$X$ is the total output vector.</li> <li>$Y$ is the final demand vector.</li> <li>$I$ is the identity matrix.</li> </ul> <p>In our case, $Y$ is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| 50 |
| 20 |
| 50 |
</code></pre></div> </div> <p>And $I$ is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| 1  0  0 |
| 0  1  0 |
| 0  0  1 |
</code></pre></div> </div> <p>Calculating $(I - A)$, and then finding its inverse can be done using a tool or software that supports matrix operations, such as MATLAB, Python (using NumPy), or even specialized calculators.</p> </li> <li><strong>Computing the Result</strong>: Once you have the Leontief inverse, you multiply it by the final demand vector $Y$ to get the total output vector $X$.</li> </ol> <p>The resulting $X$ vector will tell you how much each industry needs to produce in total to meet the given final demand, taking into account not just the direct demand for each industry’s products, but also the indirect demand generated by the need for inputs from other industries.</p> <p>In practice, real-world Input-Output tables are much larger and more complex, often involving hundreds of industries. Still, the basic principles and steps remain the same. Modern software tools make handling and analyzing these large matrices feasible.</p> <h2 id="lecture-5">Lecture 5</h2> <p>Imagine you have a seesaw in a playground. When two people of the same weight sit on each end, the seesaw will be balanced and level. This is similar to equilibrium in economics.</p> <p>In economics, equilibrium is like a balanced seesaw, but instead of people and weight, we are balancing supply and demand. When the amount of goods people want to buy (demand) is equal to the amount of goods available for sale (supply), we have what is called a market equilibrium. This balance determines the price of the good.</p> <p>For example, let’s say you and your friends want to buy lemonade on a hot day. If there’s a lot of lemonade available and not many people want to buy it, the price will likely be low. But if there’s only a little lemonade and a lot of people want it, the price will be high. The point at which the amount of lemonade people want to buy is equal to the amount available, and everyone is happy with the price, is the equilibrium.</p> <hr/> <p>Calculus is a branch of mathematics that deals with the study of change (differential calculus) and accumulation (integral calculus). It provides us with the tools to analyze and understand dynamic processes and systems that change continuously.</p> <p>When we talk about differential calculus, we are primarily concerned with the concept of a derivative, which represents the rate at which a function changes as its input changes. In simpler terms, derivatives help us find the slope of a curve at any given point. This is crucial in economics when we want to understand how one variable responds to changes in another variable, such as the relationship between price and quantity demanded in the market.</p> <p>On the other hand, integral calculus is concerned with the concept of an integral, which represents the accumulation of quantities. Integrals allow us to calculate areas under curves and can be used to find total cost, total revenue, or consumer surplus, given their respective density functions.</p> <p>Together, these concepts from calculus are fundamental tools in mathematical economics, helping us model and analyze the dynamic and complex relationships between different economic variables.</p> <hr/> <p>Review the concept of maps, functions. Then the concept of the slope (of the graph of a function). Then finite change and infinitesimal change. Then the derivative.</p> <h2 id="lecture-6">Lecture 6</h2> <p><strong>Euler’s number $e$</strong></p> <p>The Euler constant $e$ is an irrational and <code class="language-plaintext highlighter-rouge">transcendental</code>(explain) number approximately equal to $2.71828$. It has a beautiful and interesting origin that can be traced back to compound interest.</p> <p><strong>Origin of $e$:</strong></p> <p>Imagine you invest $1 at an interest rate of 100% per year. How much will you have at the end of one year?</p> <ol> <li><strong>No compounding (n = 0):</strong> <ul> <li>You would have $2 at the end of the year.</li> </ul> </li> </ol> <p>Now, consider the interest is compounded, meaning that the interest is added to the principal periodically and the new total then earns interest.</p> <ol> <li><strong>Compounded annually (n = 1):</strong> <ul> <li>You would have $1 \times \left(1 + \frac{1}{1}\right)^1 = $2$.</li> </ul> </li> <li><strong>Compounded semi-annually (n = 2):</strong> <ul> <li>You would have $1 \times \left(1 + \frac{1}{2}\right)^2 = $2.25$.</li> </ul> </li> <li><strong>Compounded quarterly (n = 4):</strong> <ul> <li>You would have $1 \times \left(1 + \frac{1}{4}\right)^4 \approx $2.44$.</li> </ul> </li> </ol> <p>As you increase the frequency of compounding ($n$), the total amount you would have at the end of the year approaches $e$. Mathematically, this is expressed as:</p> \[\lim_ {n \to \infty} \left(1 + \frac{1}{n}\right)^n = e \approx 2.71828...\] <p>This is the origin of Euler’s number $e$ in the context of compound interest. The more frequently interest is compounded, the more money you end up with, and this approaches a limit as the compounding frequency goes to infinity, which is $e$.</p> <p><strong>Importance in Economics:</strong></p> <p>The number $e$ plays an important role in economics, particularly in financial mathematics. Here are some examples:</p> <ol> <li>Compound Interest: <ul> <li>As we’ve seen in the example above, $e$ is crucial in calculating compound interest, which is foundational in economics and finance.</li> </ul> </li> <li>Exponential Growth and Decay: <ul> <li>Many economic processes, like population growth, inflation, and depreciation, can be modeled by exponential functions, and the base of these exponential functions is often $e$.</li> </ul> </li> <li>Option Pricing: <ul> <li>In the Black-Scholes model for option pricing, the exponential function with base $e$ is used to model the behavior of stock prices over time.</li> </ul> </li> <li>Elasticity: <ul> <li>In microeconomics, elasticity, which measures the responsiveness of demand or supply to changes in price or income, often involves exponential functions with base $e$.</li> </ul> </li> </ol> <p>In summary, the number $e$ has a rich and intuitive origin in compound interest, and its applications in economics, particularly in financial mathematics and modeling, make it an essential constant in the field.</p> <p><strong>Understanding the Chain Rule in Calculus</strong></p> <p>To introduce the concept of the chain rule and its applications in finding the derivatives of composite functions.</p> <p>The chain rule is a fundamental principle in calculus that allows us to find the derivative of a composite function. A composite function is a function that combines two or more functions in a specific order. For example, if we have two functions, $f(x)$ and $g(x)$, then a composite function could be $f(g(x))$, where we first apply $g(x)$ and then apply $f(x)$ to the result.</p> <p>The Chain Rule: The chain rule states that if you have a composite function $f(g(x))$, and you want to find its derivative, you would do the following: \(\frac{d}{dx} f(g(x)) = f'(g(x)) \cdot g'(x)\) Here, $f’(g(x))$ is the derivative of $f$ with respect to $g(x)$, and $g’(x)$ is the derivative of $g$ with respect to $x$.</p> <p>Example 1: Let’s consider an example to understand the application of the chain rule. \(h(x) = \sin(x^2)\) Here, we can consider $f(x) = \sin(x)$ and $g(x) = x^2$, so that $h(x) = f(g(x))$. Now, we want to find the derivative of $h(x)$. \(\begin{aligned} h'(x) &amp;= \frac{d}{dx} \sin(x^2) \\ &amp;= \cos(x^2) \cdot \frac{d}{dx} x^2 \\ &amp;= 2x \cdot \cos(x^2) \end{aligned}\)</p> <p>Example 2: Let’s take another example. \(h(x) = \exp(\sin(x))\) Here, $f(x) = \exp(x)$ and $g(x) = \sin(x)$, so $h(x) = f(g(x))$. We want to find the derivative of $h(x)$. \(\begin{aligned} h'(x) &amp;= \frac{d}{dx} \exp(\sin(x)) \\ &amp;= \exp(\sin(x)) \cdot \frac{d}{dx} \sin(x) \\ &amp;= \exp(\sin(x)) \cdot \cos(x) \end{aligned}\)</p> <p>The chain rule is an essential tool in calculus that allows us to find the derivatives of composite functions. It provides a systematic way to break down complex functions into simpler parts, find their derivatives individually, and then multiply them together to get the desired result. As we have seen from the examples, the chain rule is applicable to a wide range of functions, making it a versatile and powerful tool in mathematics.</p> <p>Explain partial derivative, and gradient.</p> <p>Introduce the concept of implicit functions and discuss the implications of the Implicit Function Theorem.</p> <p>In calculus, we often deal with functions that are explicitly defined, where the dependent variable is isolated on one side of the equation. However, in some cases, the dependent variable cannot be easily isolated, and we have to work with implicit functions, where the dependent and independent variables are mixed together. The Implicit Function Theorem provides a way to handle such cases.</p> <p><strong>Implicit Function Theorem:</strong> The Implicit Function Theorem states that if we have an equation in two or more variables, we can often solve for one variable as a function of the others, even if the equation doesn’t explicitly define it as such. In other words, it allows us to find the derivative of an implicitly defined function without explicitly solving for the function.</p> <p><strong>Example 1:</strong> Consider the equation:</p> \[x^2 + y^2 = 1\] <p>This equation defines $y$ implicitly as a function of $x$. We can use the Implicit Function Theorem to find $\frac{dy}{dx}$:</p> \[\begin{aligned} 2x + 2y\frac{dy}{dx} &amp;= 0 \\ \frac{dy}{dx} &amp;= -\frac{x}{y} \end{aligned}\] <p><strong>Example 2:</strong> Consider the equation: \(x^2 - 2xy + y^2 = 3.\) Again, this equation defines $y$ implicitly as a function of $x$. We can find $\frac{dy}{dx}$ by differentiating both sides of the equation with respect to $x$: \(\begin{aligned} 2x - 2y - 2x\frac{dy}{dx} + 2y\frac{dy}{dx} &amp;= 0 \\ (2y - 2x)\frac{dy}{dx} &amp;= 2y - 2x \\ \frac{dy}{dx} &amp;= \frac{y - x}{y - x} \\ \frac{dy}{dx} &amp;= 1 \end{aligned}\)</p> <p>The Implicit Function Theorem is a powerful tool in calculus that allows us to find the derivatives of implicitly defined functions. It is useful in cases where isolating the dependent variable is difficult or impossible. By understanding the Implicit Function Theorem, we can better analyze and interpret complex relationships between variables in mathematics and other related fields.</p> <h3 id="lecture-7">Lecture 7</h3> <p>Integral calculus is an essential mathematical tool in economics for several reasons:</p> <ol> <li> <p><strong>Understanding Accumulation:</strong> Integral calculus helps to model and analyze accumulated quantities, such as total revenue, cost, and profit, over a certain period.</p> </li> <li> <p><strong>Area Under Curves:</strong> It is used to calculate areas under curves, which can represent total income, cost, or other economic variables over time or quantity.</p> </li> <li> <p><strong>Consumer and Producer Surplus:</strong> In microeconomics, integral calculus is used to calculate consumer and producer surplus, which are areas between demand and supply curves.</p> </li> <li> <p><strong>Marginal Analysis:</strong> Integrals can be used to find the total effect of a marginal change, like the total cost from a marginal cost function.</p> </li> <li> <p><strong>Economic Growth and Investment:</strong> In macroeconomics, integral calculus is used to model economic growth and investment over time.</p> </li> <li> <p><strong>Optimization:</strong> Integral calculus is often paired with differential calculus to find optimal solutions, like maximizing profit or minimizing cost.</p> </li> <li> <p><strong>Solving Differential Equations:</strong> Many economic models are described by differential equations, and integral calculus is used to solve these equations.</p> </li> </ol> <p>In summary, integral calculus is a powerful tool in economics that helps to analyze and solve complex problems related to accumulation, areas under curves, optimization, and differential equations. By understanding how quantities accumulate and how to find areas under curves, economists can gain valuable insights into various economic phenomena and make more informed decisions.</p> <hr/> <p>One of the classical models used to describe market growth is the logistic growth model. The logistic growth model is represented by the following differential equation:</p> \[\frac{dP}{dt} = rP\left(1 - \frac{P}{K}\right)\] <p>Where:</p> <ul> <li>$\frac{dP}{dt}$ represents the rate of change of the market size or population over time.</li> <li>$P$ is the market size or population at time $t$.</li> <li>$r$ is the growth rate of the market.</li> <li>$K$ is the carrying capacity of the market, which is the maximum market size that can be sustained given the available resources.</li> </ul> <p>This model describes how the market growth rate is proportional to the current market size, but it also includes a term that slows down the growth rate as the market size approaches the carrying capacity.</p> <p>To use this model, you would need to:</p> <ol> <li>Determine the parameters $r$ and $K$ based on historical data or expert knowledge.</li> <li>Solve the differential equation to get the market size as a function of time, $P(t)$.</li> <li>Analyze the solution to make predictions about future market growth.</li> </ol> <p>For example, if you have a new product and you want to predict how its market will grow over time, you would collect data on the current market size, growth rate, and potential maximum market size. Then, you would use the logistic growth model to predict how the market will evolve over time. This can help you make informed decisions about production, marketing, and other strategic considerations.</p> <hr/> <p>Introduce the initial value problems.</p> <p>Introduce the indefinite integrals. The integral sign, the measure and the integrand.</p> <p>Introduce the definite integrals. Explain the connection between definite and indefinite integrals.</p> <p>Explain the power rule. The exponential rule and the logarithmic rule. Integrals are linear.</p> <p>Introduce the substitution rule. For example, \(\int dx \, 2x (x^{2}+1).\)</p> <p>Integral by parts.</p> <p>Explain the improper integrals.</p> <h3 id="lecture-8-nov-1st">Lecture 8 (Nov 1st)</h3> <p>An economic equilibrium refers to a state where supply equals demand and all agents in the economy are optimizing their decisions given the constraints they face, and there are no tendencies for change. In other words, it is a state where all forces in an economy are balanced, and the economy is stable. There are different types of equilibria in economics, such as market equilibrium, general equilibrium, and Nash equilibrium, each referring to different contexts and conditions of balance and stability.</p> <p>Optimization, meaning “the quest for the best.” For example, a business firm may seek to maximize profit $P$, that is, to maximize the difference between total revenue $R$ and total cost $C$.</p> <hr/> <p><strong>output level</strong></p> <p>In economics, output level refers to the total quantity of goods or services produced by an individual, firm, industry, or entire economy within a specific period. It’s an important indicator of economic activity and is often used to analyze the performance and health of an economy.</p> <p>For an individual firm or industry, the output level would be the total quantity of products or services produced. For an entire economy, the output level is typically represented by the Gross Domestic Product (GDP), which measures the total value of all goods and services produced within a country’s borders over a specific period. Another related measure is the Gross National Product (GNP), which represents the total value of all goods and services produced by the residents of a country, regardless of where they are produced.</p> <p>Understanding output levels is crucial for economists and policymakers because it helps to:</p> <ol> <li> <p><strong>Measure Economic Performance:</strong> The output level indicates how efficiently an economy is using its resources to produce goods and services.</p> </li> <li> <p><strong>Analyze Business Cycles:</strong> By observing changes in output levels over time, economists can identify trends and business cycles, including periods of economic growth and recession.</p> </li> <li> <p><strong>Formulate Economic Policy:</strong> Policymakers use output levels to develop strategies to manage economic activity, such as monetary and fiscal policies.</p> </li> <li> <p><strong>Make Forecasts:</strong> Economists use past output levels to make predictions about future economic activity and growth.</p> </li> </ol> <p>In summary, the output level is a fundamental concept in economics that provides valuable insights into the health and performance of an economy. It is used to analyze trends, develop economic policy, and make forecasts about future economic activity.</p> <hr/> <p><strong>First-Derivative Test</strong></p> <p>How can be tell the local stationary point using first derivatives. What else is needed to tell if it is the local maximum or minimum?</p> <p><strong>Second derivative and higher derivatives.</strong> Concave and convex functions. <strong>Introduce the second derivative test</strong>.</p> <p>Introduce the Taylor expansion.</p> <h3 id="lecture-9-nov-6th">Lecture 9 (Nov 6th)</h3> <p>Introduce exponent and logarithmic functions. Talk about their derivatives and applications.</p> <p>One interesting and significant application of exponential functions in economics is in the concept of the “Multiplier Effect” associated with fiscal policy.</p> <h3 id="lecture-10-nov-8">Lecture 10 (Nov 8)</h3> <p>Consider a consumer with the simple utility (index) function \(U = x_ {1}x_ {2}+2x_ {1}.\) Without any constraints, it is impossible to optimize $U$. But with some budget constrain, for instance \(x_ {1}+2x_ {2}=50,\) then we can optimize it.</p> <p>Introduce the Lagrangian multiplier method.</p> <p>Some examples for the Lagrange multiplier:</p> <p>Ex1. Find the extremum of $z=xy$ subject to $x+y=6$.</p> <p>Ex2. Find the extremum of $z=x_ {1}^{2}+x_ {2}^{2}$ subject to $x_ {1}+4x_ {2}=2$.</p> <h3 id="lecture-11">Lecture 11</h3> <h3 id="lecture-12">Lecture 12</h3> <h3 id="lecture-13">Lecture 13</h3>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[Syllabus]]></summary></entry><entry><title type="html">Note of Nonlinear Quantization</title><link href="https://baiyangzhang.github.io/blog/2023/Note-of-Nonlinear-quantization/" rel="alternate" type="text/html" title="Note of Nonlinear Quantization"/><published>2023-09-26T00:00:00+00:00</published><updated>2023-09-26T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Note-of-Nonlinear-quantization</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Note-of-Nonlinear-quantization/"><![CDATA[<h3 id="the-lagrangian">The Lagrangian</h3> <p>The rescaling works as follows. The length is measured by a length scale $\lambda$ as $x^{\mu} := \lambda x’^{\mu}$ where $x’$ is now dimensionless. I suspect the energy integral is measured by an energy unit $\mu$ by \(E = \int d^{3}x \, \mathcal{E} := \mu\, \int d^{3}x \, \mathcal{E} / \mu =\mu\, \int d^{3}x' \, \lambda^{3} \mathcal{E} / \mu.\) Then $\mathcal{E} / \mu$ is a dimensionless parameter.</p> <p>The term \(\mathcal{L}_ {k} := \frac{F_ {\pi}^{2}}{16}\mathrm{Tr}\,(R_ {\mu}^{2})\) (k for kinetic) is re-expressed as \(\mathcal{L}_ {k} = \frac{F_ {\pi}^{2} \lambda^{-2}}{16}\mathrm{Tr}\,(R_ {\mu'}^{2})\) since $R_ {\mu} \sim \partial_ {\mu}(-) = \lambda ^{-1}\partial_ {\mu’}(-)$, where $\partial_ {\mu’}$ is the derivative with respect to $x’$. Then the according contribution to the energy (Denoted by $E_ {k}$) \(E_ {k} = \int d^{3}x \, -\mathcal{L}_ {k}\) takes the new form \(E_ {k} =\mu \int d^{3}x' \, \mu ^{-1} \lambda^{3} \frac{F_ {\pi}^{2} \lambda^{-2}}{16}\mathrm{Tr}\,(R_ {\mu'}^{2})\) and I guess we want this to give \(E_ {k} =\mu \int d^{3}x' \, \frac{1}{2} \mathrm{Tr}\,(R_ {\mu'}^{2})\) thus we have the relation \(\boxed { 8\mu = F_ {\pi}^{2}\lambda. }\tag{relation 1}\)</p> <p>Now turn the the Skyrme term $\mathcal{L}_ {s}$. Do the same trick, in new unites we have \(\begin{align} E_ {s} &amp;= \mu \int d^{3}x' \, \mu ^{-1} \lambda^{3}\mathcal{L}_ {s} \\ &amp;=\mu \int d^{3}x' \, \mu ^{-1} \lambda^{3} \frac{\lambda^{-4}}{32e^{2}}\mathrm{Tr}\,([R_ {\mu'},R_ {\nu'}]^{2}) \end{align}\) where the factor $\lambda^{-4}$ again comes from re-express the derivative in $R_ {\mu}$. If we identify it with \(E_ {s} = \mu\,\int d^{3}x' \, \frac{2\eta-1}{16}\mathrm{Tr}\,[R_ {\mu'},R_ {\nu'}]^{2}\) where \(\boxed { \frac{1}{e^{2}} := \alpha^{2}(2\eta-1),\quad \beta := 2\alpha^{2}(1-\eta) }\) then we have another relation, \(\boxed { \frac{\alpha^{2}}{2}=\mu \lambda .} \tag{relation 2}\)</p> <p>From relation 1 and 2 we can solve for $\lambda$ and $\mu$ in terms of $\alpha$ and $F_ {\pi}$, \(\begin{cases} \mu = \frac{F_ {\pi}\alpha}{4}, \\ \lambda =\frac{2\alpha}{F_ {\pi}}. \end{cases}\)</p> <p>Then follow the paper draft we can also get \(m_ {1} = \frac{2\alpha m_ {\pi}}{F_ {\pi}}\) and \(m_ {2}=\frac{2\alpha M}{F_ {\pi}}.\)</p> <hr/> <h3 id="the-kinetic-term">The kinetic term</h3> <p>The kinetic terms are the terms in the Lagrangian that contains time derivatives.</p> <p>The most general rotation of a Skyrmion include both spatial rotation and iso-rotation, \(U(x,t) = A_ {1}(t) U_ {0}(D(A_ {2})(x-x_ {0})) A_ {1}(t)^{\dagger}\) where $U_ {0}$ is the static (time-independent) solution, $x_ {0}$ is the center of the Skyrmion, $A_ {1,2}$ are both $SU(2)$ matrices, $A_ {1}$ denotes the iso-rotation and $D(A_ {2})$ the spatial rotation. However, when we consider <em>the special Skyrmion configuration</em>, namely the hedgehog Skyrmion, the iso-rotation is equivalent to spatial rotation (for more details see the other note I made on Skyrmion), so we can just take one $A$ matrix. The convention is that we take $A_ {1}$, neglect the subscript $1$ we have \(U(x,t) = A(t)U_ {0}A(t)^{\dagger}.\) Making use of the specific form of the hedgehog solution, we can translate the time-dependent in $A(t)$ to \(U(\mathbf{x},t) = U_ {0}(R(t) \mathbf{x}),\quad R_ {ij}(t) = \frac{1}{2}\mathrm{Tr}\,(\sigma_ {i} A \sigma_ {j} A^{\dagger}).\)</p> <hr/> <p>Since $A(t)$ can be written as \(A(t) = \alpha_ {0}(t) + i\alpha_ {i}(t)\sigma^{i}, \quad \alpha \in \mathbb{R}\) and $\det(A)=1$ requires that \(\alpha_ {0}^{2}+\alpha_ {i}\alpha_ {i} = 1,\) thus the trajectory of $A(t)$ can be regarded as a curve on $\mathbb{S}^{3}$. Then $\dot{A}(t)$ is perpendicular to $A(t)$, thus we have \(\begin{align} \mathrm{Tr}\,(A ^{\dagger}(t)\dot{A}(t))&amp;=\mathrm{Tr}\,[(\alpha_ {0}\mathbb{1}_ {2}-i\alpha_ {i}\sigma^{i})\cdot(\dot{\alpha}_ {0}\mathbb{1}_ {2}+i \dot{\alpha}_ {j}\sigma^{j})] \\ &amp;=2\alpha_ {0}\dot{\alpha}_ {0}+\alpha_ {i}\dot{\alpha}_ {j}2\delta_ {ij} \\ &amp;= 2(\alpha_ {0},\alpha_ {1},\alpha_ {2},\alpha_ {3})\cdot(\dot{\alpha}_ {0},\dot{\alpha}_ {1},\dot{\alpha}_ {2},\dot{\alpha}_ {3}) \\ &amp;=0. \end{align}\)</p> <p>Furthermore, $A^{\dagger}\dot{A}$ is also skew-hermitian since \((A^{\dagger}\dot{A})^{\dagger}=(\dot{A}^{\dagger} A) = -A^{\dagger}\dot{A}.\)</p> <p>As a traceless anti-hermitian $2\times 2$ matrix, it can be expanded in the basis of three Pauli matrices with pure imaginary coefficients. Writing \(A^{\dagger}\dot{A} = \frac{i}{2} a_ {i} \sigma^{i}\) then the inverse is easily given by \(a_ {i} = -i \mathrm{Tr}\,\left\{ \sigma _ {i} A^{\dagger}\dot{A} \right\} ,\quad t\text{-dependent.}\)</p> <p>To avoid confusion, let us denote the time dependent rignt-currents as $\widetilde{R}_ {\mu}$ and keep $R_ {\mu}$ for time independent versions, that is $\widetilde{R}_ {\mu}:= A R_ {\mu}A^{\dagger}$.</p> <p>Straightforward calculation shows that \(\mathrm{Tr}\,\widetilde{R}_ {0}^{2}=2\mathrm{Tr}\,(M\cdot M-MU_ {0}MU_ {0}^{\dagger}),\quad M:=A^{\dagger}\dot{A}\) and if we define \(T_ {i} =\frac{i}{2}[\sigma _ {i} ,U_ {0}]U_ {0}^{\dagger}\) we have \(\mathrm{Tr}\,\widetilde{R}_ {0}^{2}=\mathrm{Tr}\,(T_ {i} a_ {i} T_ {j} a_ {j} ) = \mathrm{Tr}(T_ {i} T_ {j} ) \,a_ {i} a_ {j} .\)</p> <p>The kinetic energy reads \(T = \int d^3x \left[-\frac12\mathrm{Tr}\,(\widetilde{R}_0^2)-\frac{2\eta-1}{8}\mathrm{Tr}\,\left([\widetilde{R}_0,\widetilde{R}_i]^2\right)-\frac{1-\eta}{8}\left(\mathrm{Tr}\,(\widetilde{R}_0^2)\right)^2+\frac{1-\eta}{4}\mathrm{Tr}\,(\widetilde{R}_0^2)\mathrm{Tr}\,(\widetilde{R}_i^2)\right]\) We can rewrite it in terms of $a_ {i}$’s and regard $a_ {i}$’s as some kind of <strong>angular velocity</strong>. We have \(\boxed { T_ {i} a_ {i} = [A^{\dagger}\dot{A},U_ {0}]U_ {0}^{\dagger} = A^{\dagger}\widetilde{R}_ {0}A }\) and \(\widetilde{R}_ {i} = A (\partial_ {i}U_ {0})U_ {0}^{\dagger}A^{\dagger} = A R_ {i} A^{\dagger},\) Then we can rewrite the Lagrangian as following (to isolate-out the time dependent part).</p> \[\begin{align} \mathrm{Tr}\,\widetilde{R}_ {0}^{2}&amp; = \mathrm{Tr}\,(T_ {i} T_ {j} )a_ {i} a_ {j} , \\ \mathrm{Tr}\,\left([\widetilde{R}_0,\widetilde{R}_i]^2\right)&amp;= \mathrm{Tr}\,([T_ {i},R_ {i} ][T_ {j},R_ {i} ])a_ {i} a_ {j} , \\ \left(\mathrm{Tr}\,(\widetilde{R}_0^2)\right)^2&amp;= \mathrm{Tr}\,(T_ {i} T_ {j} ) \mathrm{Tr}\,(T_ {m} T_ {n} ) a_ {i} a_ {j} a_ {m} a_ {n} \end{align}\] <p>This agrees with Eq.(2.16) in the paper draft, which is \(\begin{align} T &amp;= \int d^3x\left[-\frac12\mathrm{Tr}\,(T_i T_j)-\frac{2\eta-1}{8}\mathrm{Tr}\,\left([T_i,R_k][T_j,R_k]\right)+\frac{1-\eta}{4}\mathrm{Tr}\,(T_i T_j)\mathrm{Tr}\,(R_k^2)\right]a_i a_j \\ &amp;\phantom{=\ }+\int d^3x\left[-\frac{1-\eta}{8}\mathrm{Tr}\,(T_i T_j)\mathrm{Tr}\,(T_k T_l)\right]a_i a_j a_k a_l, \end{align}\)</p> <p>Now what we need to do is to substitute the Hedgehog solution into the above expression. It’s done with the help of Mathematica.</p> <hr/> <p>By the end of the day, we have the expression for kinetic energy where the Hedgehog ansatz is adopted, \(T = \frac{1}{2} \Lambda_ {1} a ^{2} - \frac{1}{4} \Lambda_ {2} a^{4},\quad a^{2}:=a_ {i} a_ {i} , \,a^{4}:= a_ {i} a_ {i} a_ {j} a_ {j} ,\) where $\Lambda_ {1}$ is something like $\text{positive}+ \eta(\text{positive})$, while $\Lambda_ {2}$ is something like $(1-\eta)\text{positive}$.</p> <p>From this we can get the canonical momentum of $a_ {i}$, call it $J_ {i}$ \(J_ {i} = \Lambda_ {1}a_ {i} -\Lambda_ {2}a^{2} a_ {i}\) which squares to \(J^{2} = \Lambda_ {2}^{2}a^{6}-2\Lambda_ {1}\Lambda_ {2}a^{4}+\Lambda_ {1}^{2}a^{2}.\) Define $x:=a^{2}$ to simplify the notations, we want to solve the equation \(y(x) = \Lambda_ {2}^{2} x^{3}-2\Lambda_ {1}\Lambda_ {2}x^{2}+\Lambda_ {1}^{2}x-J^{2}=0,\) the solution will be something like $x=x(J)$.</p> <p>Being a cubic equation there exists three solutions, for details see the other note <a href="http://www.mathlimbo.net/2023/07/07/Quartic-Time-Derivative/">here</a>.</p> <p>In the limit $\eta\to 1$, for a finite positive $J^{2}$, there will always be three roots, all positive. The smallest one goes to a fixed finite value, while the other two goes to infinite. The root that stays finite in this limit is the root that goes to the original Skyrme result. So it makes sense to focus on it and regard it as the correct starting point.</p> <p>Now the question is, what are the kinetic energies given by the three non-degenerate roots, when $\eta&lt;1$. The kinetic energy in terms of $x$ reads \(T = \frac{1}{2} \Lambda_ {1} x - \frac{1}{4} \Lambda_ {2} x^{2},\quad x:=a_ {i} a_ {i}.\)</p> <p>Our goal is to substitute $x$ in kinetic energy from $x$ to $J^{2}$. The problem is that, for each value of $J^{2}$ we have three different values of $x$. Given three real roots $x_ {\text{min}}$, $x_ {\text{mid}}$ and $x_ {\text{max}}$, they will give different value of $T$, however in terms of $J^{2}$ they will give the same $T$ since they give the same $J^{2}$. It is reasonable to adopt $x_ {\text{min}}$ as the correct solution since it is what reproduces the Skyrme result when $\eta\to 0$. Let’s go back to the original equation \(y(x) = \Lambda_ {2}^{2} x^{3}-2\Lambda_ {1}\Lambda_ {2}x^{2}+\Lambda_ {1}^{2}x-J^{2}=0,\) and see if we can express $x$ in terms of $J^{2}$ explicitly. The <code class="language-plaintext highlighter-rouge">discriminant</code> of above equation is \(\Delta = -J^{2} \beta^3 \left(4 \alpha ^3+27 \beta J^{2}\right).\) where \(\alpha :=\Lambda_ {1}&gt;0,\quad \beta:=-\Lambda_ {2}&lt;0\) and $J^{2}&gt;0$. (Note the $\alpha,\beta$ defined here are not the ones appeared earlier.) If the discriminant $\Delta&gt;0$ then we have three non-degenerate roots, the positivity of $\Delta$ means \(\alpha^{3}&gt;- \frac{27}{4} \beta J^{2}.\) In the limit $\eta\to 1$, $\beta\to 0$ so we require $\alpha^{3}&gt;0$, which is automatically satisfied. This confirms that in this limit there will always be three degenerate roots.</p> <p>Define \(x=t-\frac{2\alpha}{3\beta}=:t+\frac{2\sqrt{ -p }}{\sqrt{ 3 }}\) where \(\begin{align} p&amp;= -\frac{\alpha^{2}}{3\beta^{2}} = - \frac{\Lambda_ {1}^{2}}{3\Lambda_ {2}^{2}}, \\ q&amp;= - \frac{2\alpha^{3}}{27\beta^{3}}-\frac{J^{2}}{\beta^{2}}= \frac{2\Lambda_ {1}^{3}}{27\Lambda_ {2}^{3}}-\frac{J^{2}}{\Lambda_ {2}^{2}}, \end{align}\) then the cubic equation adopts the depressed form, \(t^{3}+p t +q=0.\) The discriminant in terms of $p$ and $q$ is \(\Delta = -4 p^3 - 27 q^2, \quad \Delta &gt; 0 \implies 4 p^3 + 27 q^2&lt;0.\) We can see from the definition that $p&lt;0$ but $q$ is not necessarily so. If we solve for the minimum root $x$, we get (from mathematica) \(x_ {\text{min}}=\frac{i \left(\sqrt{3}+i\right) \sqrt[3]{\sqrt{12 p^3+81 q^2}-9 q}}{2 \sqrt[3]{2} 3^{2/3}}+\frac{p+i \sqrt{3} p}{2^{2/3} \sqrt[3]{3} \sqrt[3]{\sqrt{12 p^3+81 q^2}-9 q}}+\frac{2}{3} \sqrt{ -3p } \tag{3}\) which simplifies to (by hand mostly) \(\boxed { x_ {\text{min}} = \frac{4}{\sqrt{ 3 }}\sqrt{ -p }\,\sin ^{2}\left( \frac{\theta}{6}\right)} ,\quad \theta=\arctan \left( \frac{\sqrt{-12 p^3-81 q^2}}{-9q} \right) \tag{4}.\) This is as simple as I can get. I check it numerically, it works out. A natural question to ask is, what is the range of $\theta$? We could choose the principal value of the Arctan function, but the most reliable method to fix the ambiguities is still to compare the numerical results.</p> <hr/> <p><strong>Behavior at $\eta\to 1$</strong></p> <p>Write \(\eta := 1-\epsilon\) where $\epsilon$ is positive and infinitesimal. In this limit, at leading order we have \(\begin{align} \Lambda_ {1} &amp;\equiv\alpha= \frac{16\pi}{3}\int d r\; r^2\left[ \sin^2f +\sin^2(f)(f')^2 +\frac{\sin^4f}{r^2} \right], \\ \Lambda_ {2} &amp;\equiv-\beta= \boxed { \epsilon\; \frac{64\pi}{15}\int d r\;r^2\sin^4f=:-\epsilon K}. \end{align}\) $K$ goes to a constant in the classical Skyrme limit. We have (at $\epsilon\to 0$) \(K&lt;0,\quad \alpha&gt;0, \quad \beta&lt;0.\)</p> <p>Substitute $\beta=\epsilon K$ and leave $\alpha$ as it is, we get \(\begin{align} p&amp;= -\frac{\alpha^{2}}{3\beta^{2}} = \frac{1}{\epsilon^{2}}\left( -\frac{\alpha^{2}}{3K^{2}} \right) , \\ q&amp;= - \frac{2\alpha^{3}}{27\beta^{3}}-\frac{J^{2}}{\beta^{2}}= \frac{2\Lambda_ {1}^{3}}{27\Lambda_ {2}^{3}}-\frac{J^{2}}{\Lambda_ {2}^{2}}=\frac{1}{\epsilon^{3}}\left( -\frac{2\alpha^{3}}{27K^{3}} \right)+\frac{1}{\epsilon^{2}}\left( -\frac{J^{2}}{K^{2}} \right), \end{align}\) up to the NLO we have \(\begin{align} \theta &amp;= \arctan \left(-\sqrt{\epsilon } \sqrt{-\frac{27 J^2 K}{\alpha ^3}}\right)\\ &amp;\approx -\sqrt{\epsilon } \sqrt{-\frac{27 J^2 K}{\alpha ^3}}+ \frac{1}{3} \epsilon ^{3/2} \left(-\frac{27 J^2 k}{\alpha ^3}\right)^{3/2} +\mathcal{O}(\epsilon^{5/2}), \end{align}\) Consequently, up to NLO \(\begin{align} x_ {\text{min}} &amp;= \frac{4}{\sqrt{ 3 }}\sqrt{ -p }\,\sin ^{2}\left( \frac{\theta}{6}\right) \\ &amp;=-\frac{1}{\epsilon} \frac{4\alpha}{ 3K } \left( -\frac{3 J^2 K \epsilon }{4 \alpha ^3}-\frac{219 J^4 K^2 \epsilon ^2}{16 \alpha ^6} \right) \\ &amp;= \frac{J^{2}}{\alpha ^2} +\boxed { \epsilon \frac{73 K J^{4} }{4 \alpha ^5}}. \end{align}\) The boxed term is the correction resulting from the cubic terms. As for how useful this linearized expression is, I don not know, for in theory we already have a analytical expression, which we could use to fit $\eta$ according to various data.</p> <p><strong>The other two roots</strong></p> <p>Now let’s turn to the remaining two roots of the cubic equation. They have no correspondence in the classical Skyrme model, since as $\epsilon\to 0$ both of them goes to infinity, but it is helpful having their closed form written down for $\eta &lt; 1$.</p> <p>Regarding the three positive roots, let’s call them $x_ {\text{min}}, x_ {\text{mid}}$ and $x_ {\text{max}}$. We have already studied $x_ {\text{min}}$ in length in the previous section, some calculation shows that \(\boxed { x_ {\text{mid}} = \frac{4 \sqrt{-p} }{\sqrt{3}}\cos ^2\left(\frac{\theta-\pi }{6}\right), }\) at $\eta=1-\epsilon$ the asymptotic behavior is \(x_ {\text{mid}} =\frac{1}{\epsilon}\frac{\alpha }{K} -\frac{1}{\sqrt{ \epsilon }} \sqrt{-\frac{J^{2}}{\alpha K}} -\frac{27 J^2 K}{\alpha ^3}.\) As we can see, in the Skyrme limit the root diverges.</p> <p>Similarly, the greatest root is \(\boxed { x_ {\text{max}} = \frac{4 \sqrt{-p} }{\sqrt{3}}\cos ^2\left(\frac{\theta+\pi }{6}\right) .}\) Compare with $x_ {\text{mid}}$, we find that the $\theta-\pi$ term under $\cos$ is replace by $\theta+\pi$.</p> <p>The difference between $x_ {\text{max}}$ and $x_ {\text{mid}}$ is \(\boxed { x_ {\text{max}} - x_ {\text{mid}} = -2 \sqrt{-p}\, \sin \left(\frac{\theta }{3}\right), }\) at $\eta=1-\epsilon$ the asymptotic behavior is \(x_ {\text{max}} - x_ {\text{mid}} = \frac{1}{\sqrt{\epsilon }} \frac{-2 \sqrt{ J^2 }}{ \sqrt{-K \alpha}}-\sqrt{\epsilon } \frac{ 13 J^4\, \sqrt{-K}}{4 \,\alpha ^{7/2} \sqrt{J^2}}.\) It shows that close to the Skyrme limit, the distance between those two roots also becomes divergent.</p>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[The Lagrangian]]></summary></entry></feed>