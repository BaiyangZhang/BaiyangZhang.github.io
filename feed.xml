<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://baiyangzhang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://baiyangzhang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-10-22T09:56:56+00:00</updated><id>https://baiyangzhang.github.io/feed.xml</id><title type="html">Baiyang Zhang</title><subtitle>A place dedicated to sharing insights and reflections on mathematics, physics, and social sciences. </subtitle><entry><title type="html">Lecture Notes on Mathematics in Economics</title><link href="https://baiyangzhang.github.io/blog/2023/Mathematical-Economics/" rel="alternate" type="text/html" title="Lecture Notes on Mathematics in Economics"/><published>2023-10-09T00:00:00+00:00</published><updated>2023-10-09T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Mathematical-Economics</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Mathematical-Economics/"><![CDATA[<h3 id="syllabus">Syllabus</h3> <p><strong>Semester</strong>: Fall 2023</p> <p><strong>Duration:</strong> 40 Real hours (54 teaching hours), 3 real hours per class, 14 classes / 7 weeks</p> <p><strong>Lecturer</strong>: Dr. Baiyang Zhang</p> <p><strong>Office Address:</strong> N/A</p> <p><strong>Email</strong>: <a href="mailto:james.fisher@henu.edu.cn">byzhang@henu.edu.cn</a></p> <p><strong>Lecture Schedule</strong>: Monday and Wednesday, 2:30 - 5:30</p> <p><strong>Classroom:</strong> Monday at Teaching building Room 3502, Wednesday at Room 109 at the School of Economics</p> <p><strong>Textbooks and References:</strong></p> <ol> <li> <p>“<em>Fundamental Methods of Mathematical Economics</em>” by Chiang and Wainwright</p> </li> <li> <p>“<em>Introduction to Probability</em>” by Grinstead and Snell</p> </li> <li> <p>“<em>Introduction to Linear Algebra</em>” by Strang.</p> </li> </ol> <p><strong>Course Objectives:</strong>  This course will include the basics of analysis, derivatives and integration, linear algebra, optimization, and probability, with the goal of preparing students for further course work within the School of Economics.</p> <p><strong>Assessment Policy:</strong> The assessments for this course will one final, in addition to several homeworks. Each item is scored on a percentage basis. The final score for the class is the weighted sum of the items’ scores.  The weights are as follows: final accounts for 70% of the final grade, and the homeworks account for the remaining 30% of the final grade.</p> <p>In general, the final grade is an A when the final score is 85% or better, a B when the final score is between 70% and 84.9%, a C when the final score is between 60% and 69.9%, a D when the final score is between 50% and 59.9%, and an F when the final score is below 50%.  Assessment and final grades, however, may be curved to the benefit of the students.</p> <p><strong>Tentative Weekly Schedule:</strong></p> <p>CW = Chiang and Wainwright, GS = Grinstead and Snell, W = Wooldridge.</p> <p>Additional review sessions may be scheduled in advance of exams.</p> <table> <thead> <tr> <th><strong>Lecture</strong></th> <th><strong>Topics</strong></th> <th><strong>Reading</strong></th> </tr> </thead> <tbody> <tr> <td>1</td> <td>Introduction and Basics of Analysis</td> <td>CW, Ch. 1 and 2</td> </tr> <tr> <td>2-4</td> <td>Linear Algebra</td> <td>CW, Ch. 4 and 5</td> </tr> <tr> <td>5-6</td> <td>Derivatives</td> <td>CW, Ch. 6,7 and 8</td> </tr> <tr> <td>7</td> <td>Integrals</td> <td>CW, Ch. 13</td> </tr> <tr> <td>8-10</td> <td>Unconstrained Optimization</td> <td>CW, Ch. 9, 10, and 11</td> </tr> <tr> <td>11</td> <td>Constrained Optimization with Equality Constraints</td> <td>CW, Ch. 12</td> </tr> <tr> <td>12</td> <td>Probability Distributions and Combinatorics</td> <td>GS, Ch. 1, 2 and 3</td> </tr> <tr> <td>13</td> <td>Common Distributions and Conditional Probability</td> <td>GS, Ch. 4 and 5</td> </tr> <tr> <td>14</td> <td>Expected Values</td> <td>GS, Ch. 6</td> </tr> </tbody> </table> <hr/> <h2 id="lecture-1">Lecture 1</h2> <p><strong>Mathematical Economics versus Econometrics</strong></p> <p>Econometrics is concerned mainly with the measurement of economic data. Hence it deals with the study of empirical observations using statistical methods of estimation and hypothesis testing. Indeed, empirical studies and theoretical analyses are often complementary and mutually reinforcing. On the one hand, theories must be tested against empirical data for validity before they can be applied with confidence. On the other, statistical work needs economic theory as a guide, in order to determine the most relevant and fruitful direction of research.</p> <p><strong>Economic Models</strong></p> <p>A model is essentially and necessarily an abstraction from the real world. The sensible procedure is to pick out what appeals to our reason to be the primary factors and relationships relevant to our problem and to focus our attention on these alone.</p> <p><strong>Mathematics from a bird’s eye view</strong></p> <p>Explain: Algebra, Geometry, and Analysis.</p> <p>Most people who have done some high school mathematics will think of algebra as the sort of mathematics that results when you substitute letters for numbers. Algebra will often be contrasted with arithmetic, which is a more direct study of the numbers themselves.</p> <p>There is, however, a different contrast, between algebra and geometry, which is much more important at an advanced level. The high school conception of geometry is that it is the study of <code class="language-plaintext highlighter-rouge">shapes</code> such as circles, triangles, cubes, and spheres together with concepts such as rotations, reflections, symmetries, and so on. Thus, the objects of geometry, and the processes that they undergo, have a much more visual character than the equations of algebra.</p> <p>Some parts of mathematics involve manipulating symbols according to certain rules: for example, a true equation remains true if you “do the same to both sides.” These parts would typically be thought of as algebraic, whereas other parts are concerned with concepts that can be visualized, and these are typically thought of as geometrical.</p> <p>One is more symbolic and the other more pictorial.</p> <p>The word “analysis,” used to denote a branch of mathematics, is not one that features at high school level. However, the word “calculus” is much more familiar, and differentiation and integration are good examples of mathematics that would be classified as analysis rather than algebra or geometry. The reason for this is that they involve limiting processes. For example, the derivative of a function f at a point x is the limit of the gradients of a sequence of chords of the graph of $f$ , and the area of a shape with a curved boundary is defined to be the limit of the areas of rectilinear regions that fill up more and more of the shape.</p> <p>Thus, as a first approximation, one might say that a branch of mathematics belongs to analysis if it involves limiting processes, whereas it belongs to algebra if you can get to the answer after just a finite sequence of steps.</p> <p><strong>Branches of Mathematics</strong></p> <ul> <li>Algebra. Deals with number systems, polynomials, and more abstract structures such as groups, fields, vector spaces, and rings.</li> <li>Number theory.</li> <li>Algebraic geometry</li> <li>Analysis <ul> <li>The study of PDE, ODE.</li> <li>Dynamics. What happens when you take a simple process and do it over and over again?</li> </ul> </li> <li>Logic <ul> <li>Set theory</li> <li>Category theory</li> </ul> </li> <li>Combinatorics</li> <li>Theoretical Computer Science</li> <li>Probability</li> <li>Mathematical Physics</li> </ul> <hr/> <ol> <li><em>Math as a language with its own vocabulary and syntax.</em></li> <li><em>Introduction of set theory, including the basic concepts and operations that can be done to them.</em></li> <li><em>Introduce the concept of function and functional. They are nothing but various maps from one set to another.</em></li> <li>The number system. Explain integers, rational numbers, real numbers and complex numbers. $\mathbb{R},\mathbb{N}, \mathbb{Z}, \mathbb{Q}$.</li> </ol> <h2 id="lecture-2">Lecture 2</h2> <p>“You can’t add apples and oranges.” In a strange way, this is the reason for vectors. We have two separate numbers $v_ {1}$ and $v_ {2}$. The pair produces a two-dimensional vector $\vec{v}$. Explain the following concepts:</p> <ul> <li>column,</li> <li>components.</li> </ul> <p>We don’t add $v_ {1}$ and $v_ {2}$, but we do add vectors of the same type. Explain vector addition. We want to add apples with apples.</p> <p>Explain what is a scalar, and scalar multiplication.</p> <p>Given two vectors $\vec{v}$ and $\vec{w}$, explain the linear combination of them.</p> <p>This big view, taking all the combinations of $\vec{v}$ and $\vec{w}$, is linear algebra at work.</p> <p>Illustrate the addition of vectors using arrows.</p> <p>Introduce</p> <ul> <li>dot product,</li> <li>length.</li> </ul> <p>The dot product is gonna be needed when defining the action of a matrix on a vector.</p> <p>After introducing the product rules in two different ways, we introduce the linear equations.</p> <h3 id="lecture-3">Lecture 3</h3> <p><strong>Linear combination:</strong></p> <p>Imagine you have a collection of building blocks, and each block represents a different item. A “linear combination” is like creating a new structure using these blocks, where you decide:</p> <ol> <li><strong>How many of each block to use</strong>: This is similar to multiplying the block (or item) by a number.</li> <li><strong>How to combine them</strong>: Essentially, you’re just adding these multiplied blocks together.</li> </ol> <p>Let’s use a simpler example:</p> <p>Imagine you have two types of fruit: apples and bananas.</p> <p>A “linear combination” of apples and bananas could be:</p> <ul> <li>3 apples + 2 bananas</li> <li>5 apples + 1 banana</li> <li>2 apples - 4 bananas (Yes, in mathematics, you can have negative bananas! Just think of it as owing bananas to someone.)</li> </ul> <p>In each of these cases, the number of apples and bananas you decide to use (3, 2, 5, 1, etc.) are called “coefficients”.</p> <p>When it comes to mathematics and vectors, the idea is the same. You’re combining different vectors using certain coefficients to produce a new vector. But the basic idea is just like combining apples and bananas!</p> <hr/> <p>There are many ways to look at a matrix.</p> <ol> <li> <p><strong>Table of Numbers</strong>: At its core, a matrix is like a table or grid filled with numbers. Think of it like a spreadsheet or a bingo card. Each number sits in its own little box, and these boxes are organized into rows and columns.</p> </li> <li> <p><strong>Collection of Column Vectors</strong>: Imagine each column in that table as a list of numbers. This list can be seen as a “column vector”. So, a matrix can be thought of as a collection of these column vectors, standing side by side. For example, a matrix with three columns is like having three lists (or column vectors) put together.</p> </li> <li> <p><strong>Collection of Row Vectors</strong>: Similarly, you can think of each row in the matrix as a separate “row vector”. So, another way to view a matrix is as a stack of these row vectors, one on top of the other.</p> </li> <li> <p><strong>Transformation Machine</strong>(we will go to more details in this class): This is a more advanced way to think about matrices, especially in linear algebra. Imagine you have a point on a graph. A matrix can act as a “machine” where you input your point, and out comes a new point. This new point might be stretched, squished, rotated, or even flipped compared to the original. In essence, the matrix transformed it!</p> </li> <li> <p><strong>System of Equations</strong>(topic of this class too): If you’ve ever dealt with multiple equations at once (like trying to figure out both the price of a burger and fries when given combined costs), matrices can represent these systems. Each row could represent a different equation, and the numbers in that row represent the coefficients of variables in that equation.</p> </li> <li> <p><strong>Storage and Organization</strong>: In computer science and data analysis, matrices can be used to store data. For instance, consider ratings given by users to movies on a streaming platform. Each row might represent a user, each column might represent a movie, and the number in a specific box represents the rating that user gave to that movie.</p> </li> </ol> <p>These are just some of the many ways to look at matrices. Depending on the subject (like physics, computer graphics, or economics), matrices might take on other interesting interpretations!</p> <hr/> <p><strong>The multiplication of matrices</strong></p> <p><strong>The Basics</strong>:</p> <p>Matrix multiplication is not just multiplying numbers. Instead, it’s a combination of multiplication and addition. Remember, the way you multiply matrices is quite different from multiplying regular numbers, so it’s essential to understand the steps and rules.</p> <p><strong>The Key Rule</strong>:</p> <p>For two matrices to be multiplied, the number of columns in the first matrix must be equal to the number of rows in the second matrix. This is a crucial rule.</p> <p>If Matrix A has dimensions of $m \times n$ (meaning $m$ rows and $n$ columns) and Matrix B has dimensions of $p \times q$ (meaning $p$ rows and $q$ columns), then for A and B to be multipliable, $n$ must equal $p$. The resulting matrix will have dimensions $m \times q$.</p> <p><strong>How to Multiply</strong>:</p> <p>Let’s consider two simple matrices:</p> <p>Matrix A: \(\begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\) Matrix B: \(\begin{pmatrix} 2 &amp; 1 \\ 0 &amp; 3 \end{pmatrix}\)</p> <p>To multiply them:</p> <ol> <li><strong>First element of the result (top-left corner)</strong>: <ul> <li>Take the first row of Matrix A: (1, 2).</li> <li>Take the first column of Matrix B: (2, 0).</li> <li>Multiply corresponding elements and add them up: (1×2) + (2×0) = 2.</li> </ul> </li> <li><strong>Second element in the first row (top-right corner)</strong>: <ul> <li>Take the first row of Matrix A: (1, 2).</li> <li>Take the second column of Matrix B: (1, 3).</li> <li>Multiply corresponding elements and add them up: (1×1) + (2×3) = 7.</li> </ul> </li> <li><strong>First element in the second row (bottom-left corner)</strong>: <ul> <li>Take the second row of Matrix A: (3, 4).</li> <li>Take the first column of Matrix B: (2, 0).</li> <li>Multiply and add: (3×2) + (4×0) = 6.</li> </ul> </li> <li><strong>Second element in the second row (bottom-right corner)</strong>: <ul> <li>Take the second row of Matrix A: (3, 4).</li> <li>Take the second column of Matrix B: (1, 3).</li> <li>Multiply and add: (3×1) + (4×3) = 15.</li> </ul> </li> </ol> <p>The resulting matrix is:</p> <p>\(\begin{pmatrix} 2 &amp; 7 \\ 6 &amp; 15 \end{pmatrix}\) <strong>Visualization</strong>:</p> <p>Imagine Matrix A’s rows as horizontal hands reaching out, and Matrix B’s columns as vertical hands reaching up. When these hands “high-five”, they form the elements of the resulting matrix by the rule we just discussed.</p> <p><strong>Practice</strong>:</p> <p>The best way to get comfortable with matrix multiplication is to practice. Start with smaller matrices, understand the patterns, and then work with larger ones.</p> <p>Remember, the rule of matching columns of the first matrix to rows of the second is crucial. If they don’t match, the matrices can’t be multiplied.</p> <hr/> <h3 id="example-production-in-a-shoe-factory">Example: Production in a Shoe Factory</h3> <p>Imagine you run a small shoe factory. You produce two types of shoes: sneakers and boots.</p> <p><strong>Vectors</strong>:</p> <ol> <li><strong>Production Vector</strong> for a given week: <ul> <li>Sneakers: 100 pairs</li> <li>Boots: 50 pairs</li> </ul> <p>We can represent this as: \(\text{Shoes} = \begin{bmatrix} 100 \\ 50 \end{bmatrix}\)</p> </li> <li><strong>Cost Vector</strong> for producing each type of shoe: <ul> <li>Cost to produce one pair of sneakers: $20</li> <li>Cost to produce one pair of boots: $40</li> </ul> <p>This can be represented as: \(\text{Cost} = \begin{bmatrix} 20 \\ 40 \end{bmatrix}\)</p> </li> </ol> <p><strong>Matrix</strong>: Let’s say, to produce each shoe, you need two main raw materials: leather and rubber. We can create a <strong>Material Requirement Matrix</strong> that tells us how much of each material is required to produce one unit of each shoe type.</p> <p>For example:</p> <ul> <li>Each pair of sneakers requires 1 unit of leather and 2 units of rubber.</li> <li>Each pair of boots requires 3 units of leather and 1 unit of rubber.</li> </ul> <p>This matrix is: \(\text{Materials} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 1 \end{bmatrix}\) Where the first column corresponds to the requirements for sneakers and the second column to boots.</p> <p><strong>Matrix Multiplication</strong>:</p> <p>Now, suppose you want to find out how much raw material (leather and rubber) you’ll need for the entire week’s production.</p> <p>To do this, you’d multiply the Material Requirement Matrix by the Production Vector: \(\text{Total Materials} = \text{Materials} \times \text{Shoes}\)</p> <p>Multiplying, we get: \(\text{Total Materials} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 1 \end{bmatrix} \times \begin{bmatrix} 100 \\ 50 \end{bmatrix} = \begin{bmatrix} 200 \\ 350 \end{bmatrix}\)</p> <p>So, you’ll need:</p> <ul> <li>200 units of leather (100 for the sneakers and 150 for the boots)</li> <li>350 units of rubber (200 for the sneakers and 150 for the boots)</li> </ul> <p>This simple example demonstrates the power of vectors and matrices in understanding and organizing economic production.</p> <hr/> <p>Apply the multiplication of matrices to the product of:</p> <ol> <li>row vector times column vector,</li> <li>column vector times row vector (this one is strange).</li> </ol> <hr/> <p>Certainly! Let’s embark on this journey to understand transposes and inverses using clear examples and relatable analogies tailored for students stepping into the realm of mathematical economics.</p> <hr/> <p><strong>Transposes</strong></p> <p><strong>What is a Transpose?</strong> The transpose of a matrix is obtained by flipping the matrix over its main diagonal (the diagonal from the top-left to the bottom-right). In simpler terms, the rows of the matrix become the columns, and the columns become the rows.</p> <p><strong>Visual Analogy</strong>: Imagine you have a bookshelf full of books (your matrix). If you were to tip that bookshelf onto its side (so that it’s lying down), the rows of books would now appear as columns. That’s the transpose!</p> <p><strong>Example</strong>: Given the matrix: \(A = \begin{bmatrix} 2 &amp; 5 \\ 3 &amp; 7 \\ 1 &amp; 4 \\ \end{bmatrix}\)</p> <p>The transpose, denoted as $A^T$, is: \(A^T = \begin{bmatrix} 2 &amp; 3 &amp; 1 \\ 5 &amp; 7 &amp; 4 \\ \end{bmatrix}\)</p> <p><strong>In Mathematical Economics</strong>: Transposing can be useful for various reasons, such as making certain operations or calculations easier or more intuitive. For instance, when working with data sets or in regression analysis, transposes come in handy.</p> <p><strong>Inverses</strong></p> <p><strong>What is an Inverse?</strong> The inverse of a matrix, if it exists, is a matrix that, when multiplied with the original matrix, results in the identity matrix. The identity matrix is a special square matrix with ones on the main diagonal and zeros elsewhere.</p> <p>In symbols, for a matrix $A$, its inverse is denoted $A^{-1}$, such that: \(A \times A^{-1} = I\) where $I$ is the identity matrix.</p> <p><strong>Real-life Analogy</strong>: Think of the process of multiplication and its inverse, division. When you multiply a number by its reciprocal, you get 1. Similarly, in the world of matrices, when you multiply a matrix by its inverse, you get the identity matrix.</p> <p><strong>Properties</strong>:</p> <ol> <li>Not all matrices have inverses. Only square matrices (matrices with the same number of rows and columns) have the potential to have an inverse, and even among them, not all do.</li> <li>A matrix that does not have an inverse is called “singular” or “non-invertible”.</li> </ol> <p><strong>Example</strong>: For a 2x2 matrix: \(A = \begin{bmatrix} a &amp; b \\ c &amp; d \\ \end{bmatrix}\)</p> <p>Its inverse is: \(A^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d &amp; -b \\ -c &amp; a \\ \end{bmatrix}\)</p> <p>However, this inverse exists only if $ad-bc$ is not zero. If $ad-bc = 0$, then the matrix is singular and does not have an inverse.</p> <p><strong>In Mathematical Economics</strong>: The concept of an inverse matrix is fundamental when solving systems of linear equations, which frequently appear in economics. For example, determining equilibrium in markets, analyzing input-output models, or finding solutions to optimization problems often involve the use of matrix inverses.</p> <p>Both transposes and inverses are fundamental tools in the toolbox of mathematical economics. Just as we learn to add, subtract, multiply, and divide with numbers, we learn operations and manipulations with matrices to understand and solve intricate economic phenomena. As students progress, they’ll witness the power and elegance of linear algebra in analyzing economic systems.</p> <hr/> <p><strong>Square Matrix vs. Non-Square Matrix</strong></p> <p><strong>1. Square Matrix</strong>: A matrix is called a “square matrix” if it has the same number of rows and columns. In other words, its dimensions look like $n \times n$, where $n$ is a positive integer. You can visualize it as a perfect square filled with numbers, just like a chess or checkerboard.</p> <p><strong>Example</strong>: A 2x2 matrix: \(\begin{bmatrix} 2 &amp; 5 \\ 3 &amp; 7 \\ \end{bmatrix}\)</p> <p><strong>2. Non-Square Matrix</strong>: Any matrix that doesn’t have the same number of rows and columns is a “non-square matrix”. Its dimensions might look like $m \times n$, where $m$ and $n$ are positive integers, and $m \neq n$.</p> <p><strong>Example</strong>: A 2x3 matrix: \(\begin{bmatrix} 1 &amp; 4 &amp; 7 \\ 2 &amp; 5 &amp; 8 \\ \end{bmatrix}\)</p> <hr/> <p><strong>Square Matrices are Special in Multiplication.</strong></p> <p>When we talk about multiplication in the world of matrices, square matrices have a unique property: they’re “closed under multiplication”. This might sound fancy, but let’s break it down:</p> <p><strong>Closed Under Multiplication</strong>: This means that if you multiply two square matrices of the same size, you’ll get another square matrix of that same size as the result.</p> <p>Let’s say you have two square matrices, both of size $2 \times 2$. When you multiply them, the resulting matrix will also be $2 \times 2$. This property will hold true no matter how big or small the matrices are, as long as they’re square.</p> <p><strong>Economic Analogy</strong>: Imagine each square matrix as a factory machine. When a factory machine (a square matrix) processes another machine of the same size (another square matrix), the result is always a new machine of the same dimensions. This predictable outcome allows for consistent planning and operation, making these “machines” reliable and preferred in many scenarios.</p> <p><strong>Forming a Nice Algebra</strong>: The fact that square matrices are closed under multiplication means they form a consistent system, or a “nice algebra”. In this system, you can perform operations, like multiplication, and always know what kind of result to expect (another square matrix). This consistency is useful in mathematical economics because it provides a stable framework for analysis and predictions.</p> <hr/> <p><strong>What is a Linear Equation?</strong></p> <p><strong>Definition</strong>: A linear equation is an equation of the form: \(a_1x_1 + a_2x_2 + ... + a_nx_n = b\) where $x_1, x_2, … x_n$ are the variables, $a_1, a_2, … a_n$ are constants (known as coefficients), and $b$ is another constant.</p> <p><strong>Key Features</strong>:</p> <ol> <li>Each term consists of a variable multiplied by a constant.</li> <li>No term has a variable raised to a power higher than one.</li> <li>There are no products of variables (e.g., $x_1 \times x_2$).</li> </ol> <p><strong>Simple Example</strong>: Consider the equation $3x + 2y = 12$. Here, $x$ and $y$ are the variables, and the numbers 3 and 2 are their respective coefficients.</p> <p>Imagine you’re graphing this equation on a coordinate plane. For an equation with two variables, the graph would be a straight line. That’s why it’s called “linear” – the graph is a line.</p> <p><strong>What is a System of Linear Equations?</strong></p> <p><strong>Definition</strong>: A system of linear equations is just a collection of two or more linear equations that involve the <em>same set of variables</em>.</p> <p><strong>Simple Example</strong>: \(\begin{align*} 3x + 2y &amp;= 12 \quad \text{(Equation 1)} \\ x - y &amp;= 5 \quad \text{(Equation 2)} \end{align*}\)</p> <p>In this system, you have two linear equations, and you’d typically try to find values for $x$ and $y$ that satisfy both equations simultaneously.</p> <p><strong>Graphical Interpretation</strong>: When you plot both equations on a graph:</p> <ol> <li>If they intersect at a point, that point is the solution to the system (i.e., the values of $x$ and $y$ at that point satisfy both equations).</li> <li>If they never meet (parallel lines), the system has no solution.</li> <li>If the two equations represent the same line, then there are infinitely many solutions - any point on that line is a solution.</li> </ol> <p>Such systems help in understanding multiple interdependencies. For instance, if you have a market with two goods, and each equation represents how demand or supply changes based on the price of both goods, the system helps find an equilibrium where both goods’ demands are satisfied.</p> <p>Think of a linear equation as a single straight path (line) and a system of linear equations as multiple paths. Our goal is often to find where these paths meet or if they never do. In the context of economics, these meeting points can represent equilibrium states, optimal solutions, or any scenario where multiple conditions are satisfied at once. As students dive deeper into mathematical economics, they’ll see that these simple linear systems can be powerful tools for understanding complex economic relationships.</p> <hr/> <p>Certainly! Let’s simplify the concept and lay it out for students transitioning from a high school math background.</p> <hr/> <h3 id="using-matrices-to-represent-equations"><strong>Using Matrices to Represent Equations</strong></h3> <p>Let’s say we have the following system of equations:</p> \[\begin{align*} 2x + 3y &amp;= 8 \\ x - 4y &amp;= -3 \end{align*}\] <p>This can be represented in a matrix format as $AX = B$:</p> <p>Where: \(A = \begin{bmatrix} 2 &amp; 3 \\ 1 &amp; -4 \end{bmatrix}\) (Coefficients of the variables)</p> <p>\(X = \begin{bmatrix} x \\ y \end{bmatrix}\) (Our unknowns)</p> <p>\(B = \begin{bmatrix} 8 \\ -3 \end{bmatrix}\) (Results of the equations)</p> <h2 id="lecture-4">Lecture 4</h2> <h3 id="solving-using-the-inverse"><strong>Solving Using the Inverse</strong></h3> <p>Here’s the magic part: If we multiply both sides of our matrix equation $AX = B$ by the inverse of matrix $A$, which we’ll call $A^{-1}$, we can isolate $X$ (our unknowns).</p> <p>Doing the math: \(A^{-1}AX = X = A^{-1}B\)</p> <p>So, if we can find the inverse of $A$ (remember, not all matrices have inverses!), then we can multiply it with $B$ to get our solution, $X$.</p> <p>In economics, we often deal with many variables and relationships at the same time. Instead of trying to solve each relationship individually, matrices allow us to represent these complex relationships together and solve them in a more streamlined way.</p> <p>For example, imagine you’re studying how the price of one product affects the demand for another, and vice versa. Instead of solving each relationship individually, we can group them in a system of equations, represent them as matrices, and solve them all at once.</p> <p>Using the inverse matrix to solve a system of linear equations is like having a secret decoder ring. It’s a powerful tool that can make solving complex problems more manageable. As students dive deeper into mathematical economics, they’ll find that these tools, while initially seeming abstract, can be invaluable in understanding and analyzing economic relationships and behaviors.</p> <hr/> <p>Let’s think about 2D space for a moment. We’ve all seen the classic X-Y coordinate plane. Imagine you’ve got two vectors (think of them as arrows) on this plane. Sometimes, these two arrows will point in completely different directions. But occasionally, they might just lay flat on top of one another or be exactly opposite.</p> <p>Now, if we use these vectors as rows or columns in a matrix, the question becomes: Does this matrix have a unique way to revert any transformation it causes? Or in other words, can we find its inverse?</p> <p>This is where the idea of a matrix being “singular” comes in. A <strong>singular matrix</strong> doesn’t have an inverse. Visually, if you were to transform the entire 2D space using a singular matrix, some areas would scrunch up so much that they’d be impossible to revert to their original form.</p> <p>To figure out if a matrix is singular, we need a tool, and that tool is the <strong>determinant</strong>.</p> <p>Think of the determinant as a special number associated with a matrix. If the determinant is zero, our matrix is singular (it can’t be inverted). If the determinant isn’t zero, then the matrix can be inverted.</p> <p>For our 2x2 matrices (which are often the starting point in learning), the determinant gives us a sense of the “area scaling factor” when the matrix is used for a transformation. If the determinant is zero, it means the matrix squishes everything down to a line or a point, losing all the original area, making it impossible to revert.</p> <p>For a matrix to be nonsingular (i.e., to have an inverse), each row (like our detectives) has to bring something unique to the table. If even one row is just a repeat or combination of others, it’s like missing out on crucial information. And without that unique contribution from every row, we can’t find an inverse for our matrix.</p> <p><strong>**Rank of a Matrix</strong>**</p> <p>The rank of a matrix is a measure of the “dimension” of the linear space spanned by its rows or columns. In simpler terms, it tells us the number of linearly independent rows or columns in the matrix.</p> <p><em>The Library Analogy:</em></p> <ul> <li>Imagine you have a library of books. Some books might be exactly the same, and some might be different.</li> <li>If you were asked, “How many unique books do you have?”, you would ignore all duplicates and count only the distinct ones.</li> <li>The rank of a matrix is similar: It tells us how many “unique” rows (or columns) there are, ignoring any that can be made by combining others.</li> </ul> <p><em>Determining Rank:</em></p> <ol> <li>If a matrix has all zeros, its rank is 0.</li> <li>If a matrix has some non-zero elements but some rows (or columns) are just scalar multiples or combinations of other rows (or columns), its rank will be less than the total number of rows (or columns).</li> <li>If no row (or column) can be expressed as a combination of any other rows (or columns), the matrix is said to have full rank, meaning its rank is equal to the smaller of the number of rows or columns.</li> </ol> <hr/> <p><strong>Rank of a Matrix:</strong></p> <p>The rank of a matrix is a measure of the “dimension” of the linear space spanned by its rows or columns. In simpler terms, it tells us the number of linearly independent rows or columns in the matrix.</p> <p><strong>Analogies &amp; Insights:</strong></p> <ol> <li> <p><strong>The Library Analogy:</strong></p> <ul> <li>Imagine you have a library of books. Some books might be exactly the same, and some might be different.</li> <li>If you were asked, “How many unique books do you have?”, you would ignore all duplicates and count only the distinct ones.</li> <li>The rank of a matrix is similar: It tells us how many “unique” rows (or columns) there are, ignoring any that can be made by combining others.</li> </ul> </li> </ol> <hr/> <p><strong>Rank of a Matrix:</strong></p> <p>The rank of a matrix is a measure of the “dimension” of the linear space spanned by its rows or columns. In simpler terms, it tells us the number of linearly independent rows or columns in the matrix.</p> <p><strong>Analogies &amp; Insights:</strong></p> <ol> <li> <p><strong>The Library Analogy:</strong></p> <ul> <li>Imagine you have a library of books. Some books might be exactly the same, and some might be different.</li> <li>If you were asked, “How many unique books do you have?”, you would ignore all duplicates and count only the distinct ones.</li> <li>The rank of a matrix is similar: It tells us how many “unique” rows (or columns) there are, ignoring any that can be made by combining others.</li> </ul> </li> </ol> <p><strong>Determinant</strong>:</p> <p>The term “determinant” was used because it can “determine” whether or not a matrix has an inverse.</p> <p>Let’s say you have a 2x2 matrix: \(A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\)</p> <table> <tbody> <tr> <td>The determinant, often denoted as</td> <td>A</td> <td>or det(A), is calculated as:</td> </tr> <tr> <td>$$</td> <td>A</td> <td>= ad - bc $$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>If</td> <td>A</td> <td>equals zero, then A is singular.</td> </tr> </tbody> </table> <p>Introduce the Levi-Civita tensor. Use it to define the determinant.</p> <p>Of course! The Laplace Expansion is an important technique for calculating the determinant of a matrix. It’s especially useful when we have a matrix larger than $3 \times 3$, though it can be used for smaller matrices as well. The method is essentially a recursive process that breaks down a larger matrix into smaller ones.</p> <p><strong>Steps for Laplace Expansion:</strong></p> <ol> <li> <p><strong>Choosing a Row or Column:</strong> You can choose any row or column to expand upon. For the sake of simplicity, we often choose a row or column with the most zeros because it reduces the number of calculations we have to make (since any term multiplied by zero is zero).</p> </li> <li> <p><strong>Calculate Minors:</strong> For each element $a_{ij}$ of the matrix, remove the i-th row and the j-th column, and compute the determinant of the resulting $(n-1) \times (n-1)$ matrix. This determinant is called the “<code class="language-plaintext highlighter-rouge">minor</code>” of the element, often denoted $M_{ij}$.</p> </li> <li> <p><strong>Calculate Cofactors:</strong> Associated with each minor is a cofactor, which is defined as: $C_{ij} = (-1)^{i+j} \times M_{ij}$. This alternating sign pattern helps ensure the determinant computation is accurate.</p> </li> <li> <p><strong>Compute the Determinant:</strong> The determinant of the matrix is the sum of the products of the elements of your chosen row or column with their respective cofactors. Mathematically, if you chose the i-th row, this can be written as: \(\text{det}(A) = \sum_{j=1}^{n} a_{ij} C_{ij}\)</p> </li> </ol> <p>Alternatively, if you chose the j-th column, it is: \(\text{det}(A) = \sum_{i=1}^{n} a_{ij} C_{ij}\)</p> <p><strong>Example: Determinant of a $3 \times 3$ Matrix using Laplace Expansion:</strong></p> <p>Given matrix A: \(\begin{pmatrix} 1 &amp; 3 &amp; 2 \\ 4 &amp; 1 &amp; 3 \\ 2 &amp; 2 &amp; 1 \\ \end{pmatrix}\)</p> <p>To compute its determinant, let’s expand using the first row:</p> <ol> <li>For the element $a_{11} = 1$: <ul> <li>Minor $M_{11}$ is the determinant of: \(\begin{pmatrix} 1 &amp; 3 \\ 2 &amp; 1 \\ \end{pmatrix}\)</li> <li>$M_{11} = 1 - 6 = -5$</li> <li>Cofactor $C_{11} = (-1)^{1+1} \times (-5) = 5$</li> </ul> </li> <li>For the element $a_{12} = 3$: <ul> <li>Minor $M_{12}$ is the determinant of: \(\begin{pmatrix} 4 &amp; 3 \\ 2 &amp; 1 \\ \end{pmatrix}\)</li> <li>$M_{12} = 4 - 6 = -2$</li> <li>Cofactor $C_{12} = (-1)^{1+2} \times (-2) = 2$</li> </ul> </li> <li>For the element $a_{13} = 2$: <ul> <li>Minor $M_{13}$ is the determinant of: \(\begin{pmatrix} 4 &amp; 1 \\ 2 &amp; 2 \\ \end{pmatrix}\)</li> <li>$M_{13} = 8 - 2 = 6$</li> <li>Cofactor $C_{13} = (-1)^{1+3} \times 6 = -6$</li> </ul> </li> </ol> <p>Combining the results, \(\text{det}(A) = 1 \times 5 + 3 \times 2 + 2 \times (-6) = 5 + 6 - 12 = -1\)</p> <p>And that’s how you can use the Laplace Expansion to compute the determinant of a matrix! This method becomes more cumbersome for larger matrices, but the principles remain the same.</p> <p><strong>Properties of determinants</strong></p> <p>The addition (subtraction) of a multiple of any row to (from) another row will leave the value of the determinant unaltered. The same holds true if we replace the word row by column in the previous statement.</p> <p><em>It preserves the multiplication of matrices.</em> $\left\lvert A \cdot B \right\rvert = \left\lvert A \right\rvert \times \left\lvert B \right\rvert$.</p> <p><strong>Finding the Inverse Matrix</strong></p> <p>Introduce the adjoint of a matrix, and then the inverse.</p> <p><strong>Cramer’s rule</strong></p> <p>Let’s break down Cramer’s rule into a simple-to-understand explanation.</p> <p>Imagine you’re trying to solve a system of equations. This system might represent different scenarios. For instance, let’s say you and a friend are buying apples and bananas. Two different days, two different scenarios:</p> <ol> <li>On Monday, you bought 3 apples and 2 bananas, and it cost you $13.</li> <li>On Tuesday, you bought 4 apples and 5 bananas, and it cost you $31.</li> </ol> <p>From this, you have: 1) 3A + 2B = 13 2) 4A + 5B = 31</p> <p>Where A represents the cost of an apple and B represents the cost of a banana.</p> <p>Cramer’s rule helps you find the cost of A and B using determinants of matrices.</p> <p><strong>Step-by-step with Cramer’s Rule:</strong></p> <ol> <li> <p><strong>Main Determinant (D)</strong>: First, make a matrix of the coefficients of A and B: \(\begin{pmatrix} 3 &amp; 2 \\ 4 &amp; 5 \\ \end{pmatrix}\) Calculate its determinant (D). This determinant represents the “base scenario” of our system.</p> </li> <li> <p><strong>Determinant with respect to A:</strong> Replace the first column (which represents apples) with the numbers on the right side of our equations (13 and 31): \(\begin{pmatrix} 13 &amp; 2 \\ 31 &amp; 5 \\ \end{pmatrix}\) Calculate its determinant. This determinant represents the scenario when we’re focusing just on the apples.</p> </li> <li> <p><strong>Determinant with respect to B</strong>: Replace the second column (which represents bananas) with the numbers on the right side: \(\begin{pmatrix} 3 &amp; 13 \\ 4 &amp; 31 \\ \end{pmatrix}\) Calculate its determinant. This represents the scenario when we’re focusing just on the bananas.</p> </li> <li> <p><strong>Solving for A and B</strong>:</p> <ul> <li>The cost of an apple (A) is found by $A = D_A / D$</li> <li>The cost of a banana (B) is found by $B = D_B / D$</li> </ul> </li> </ol> <p>This gives you the individual prices of apples and bananas!</p> <p><strong>In simple words:</strong> Cramer’s rule lets you focus on one variable at a time (like just apples or just bananas) and then combine the results to find out the cost of each. It does this using determinants, which are a special number for matrices, like a fingerprint for the matrix.</p> <p>Remember, Cramer’s rule works best for systems where the number of equations matches the number of unknowns, and the main determinant (D) is not zero. If D were zero, it would be like trying to divide by zero, which we can’t do.</p> <p><strong>Application</strong></p> <p>Let’s walk through a simplified example of Input-Output Analysis using a hypothetical economy with just three industries: Agriculture, Manufacturing, and Services.</p> <p>Imagine the following table showing how each industry’s output is used as input by the others:</p> <table> <thead> <tr> <th> </th> <th>To Agriculture</th> <th>To Manufacturing</th> <th>To Services</th> <th>Final Demand</th> </tr> </thead> <tbody> <tr> <td><strong>From Agriculture</strong></td> <td>10</td> <td>30</td> <td>10</td> <td>50</td> </tr> <tr> <td><strong>From Manufacturing</strong></td> <td>20</td> <td>40</td> <td>20</td> <td>20</td> </tr> <tr> <td><strong>From Services</strong></td> <td>10</td> <td>10</td> <td>30</td> <td>50</td> </tr> </tbody> </table> <p>Each row represents the output of an industry, and each column (excluding the Final Demand column) represents the input to an industry. For example, the number 30 in the ‘From Agriculture’ row and ‘To Manufacturing’ column means that the Manufacturing sector uses 30 units of the Agriculture sector’s output.</p> <ol> <li> <p><strong>Creating the A matrix (Input-Output Coefficient Matrix):</strong> This matrix is obtained by dividing each element of the table by the total output of the corresponding industry. The total output for each industry is the sum of its outputs to all industries plus its final demand.</p> <p>Total output for each sector:</p> <ul> <li>Agriculture: 10 + 30 + 10 + 50 = 100</li> <li>Manufacturing: 20 + 40 + 20 + 20 = 100</li> <li>Services: 10 + 10 + 30 + 50 = 100</li> </ul> <p>Now, construct the A matrix:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|  0.1  0.3  0.1 |
|  0.2  0.4  0.2 |
|  0.1  0.1  0.3 |
</code></pre></div> </div> </li> <li><strong>The Leontief Inverse</strong>: To find the total output required to satisfy a given final demand, we use: \(X = (I - A)^{-1} Y\) Where: <ul> <li>$X$ is the total output vector.</li> <li>$Y$ is the final demand vector.</li> <li>$I$ is the identity matrix.</li> </ul> <p>In our case, $Y$ is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| 50 |
| 20 |
| 50 |
</code></pre></div> </div> <p>And $I$ is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| 1  0  0 |
| 0  1  0 |
| 0  0  1 |
</code></pre></div> </div> <p>Calculating $(I - A)$, and then finding its inverse can be done using a tool or software that supports matrix operations, such as MATLAB, Python (using NumPy), or even specialized calculators.</p> </li> <li><strong>Computing the Result</strong>: Once you have the Leontief inverse, you multiply it by the final demand vector $Y$ to get the total output vector $X$.</li> </ol> <p>The resulting $X$ vector will tell you how much each industry needs to produce in total to meet the given final demand, taking into account not just the direct demand for each industry’s products, but also the indirect demand generated by the need for inputs from other industries.</p> <p>In practice, real-world Input-Output tables are much larger and more complex, often involving hundreds of industries. Still, the basic principles and steps remain the same. Modern software tools make handling and analyzing these large matrices feasible.</p> <h2 id="lecture-5">Lecture 5</h2> <p>Imagine you have a seesaw in a playground. When two people of the same weight sit on each end, the seesaw will be balanced and level. This is similar to equilibrium in economics.</p> <p>In economics, equilibrium is like a balanced seesaw, but instead of people and weight, we are balancing supply and demand. When the amount of goods people want to buy (demand) is equal to the amount of goods available for sale (supply), we have what is called a market equilibrium. This balance determines the price of the good.</p> <p>For example, let’s say you and your friends want to buy lemonade on a hot day. If there’s a lot of lemonade available and not many people want to buy it, the price will likely be low. But if there’s only a little lemonade and a lot of people want it, the price will be high. The point at which the amount of lemonade people want to buy is equal to the amount available, and everyone is happy with the price, is the equilibrium.</p> <hr/> <p>Calculus is a branch of mathematics that deals with the study of change (differential calculus) and accumulation (integral calculus). It provides us with the tools to analyze and understand dynamic processes and systems that change continuously.</p> <p>When we talk about differential calculus, we are primarily concerned with the concept of a derivative, which represents the rate at which a function changes as its input changes. In simpler terms, derivatives help us find the slope of a curve at any given point. This is crucial in economics when we want to understand how one variable responds to changes in another variable, such as the relationship between price and quantity demanded in the market.</p> <p>On the other hand, integral calculus is concerned with the concept of an integral, which represents the accumulation of quantities. Integrals allow us to calculate areas under curves and can be used to find total cost, total revenue, or consumer surplus, given their respective density functions.</p> <p>Together, these concepts from calculus are fundamental tools in mathematical economics, helping us model and analyze the dynamic and complex relationships between different economic variables.</p>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[Syllabus]]></summary></entry><entry><title type="html">历史笔记若干</title><link href="https://baiyangzhang.github.io/blog/2023/%E5%8E%86%E5%8F%B2%E7%AC%94%E8%AE%B0%E8%8B%A5%E5%B9%B2/" rel="alternate" type="text/html" title="历史笔记若干"/><published>2023-07-17T00:00:00+00:00</published><updated>2023-07-17T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/%E5%8E%86%E5%8F%B2%E7%AC%94%E8%AE%B0%E8%8B%A5%E5%B9%B2</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/%E5%8E%86%E5%8F%B2%E7%AC%94%E8%AE%B0%E8%8B%A5%E5%B9%B2/"><![CDATA[<p>最近，香港中文大学的中国文化研究所陆续出版了十卷的《中国人民共和国史》，是迄今为止规模最大的非官方共和国史。官方的历史因其在剪裁、解释、罗列事实等种种方面权力不受制约，因此有人将其归类于文学范畴，就像有人讲斯大林指挥下写就的《联共（布）党史》也归类于文学作品一样。这套非官方的著作以史料见长，史料为基础，分析为衍生，时间跨度一九四九至一九八九，四十年虽不能说是沧海桑田，在诸史书中也不见其长，然而这是迷雾重重的四十年，是半遮半露的四十年，是真假掺杂的四十年，对当下很多九零后、八零后乃至于七零后而言，也是几乎空白的四十年。譬如，大家都知道所谓的三年自然灾害，或者当时流行的两种解释，即自然灾害和苏修逼债，然而这三年到底发生了什么？前因后果怎样，多少人受灾，所谓的自然灾害又有哪些，是洪涝呢还是干旱，北戴河会议、成都会议上又都说了些什么？了解这些对了解六零年左右的真相是至关重要的，而如果没有真相，所谓的历史就应该归类于文学作品了。对于这种纵横九百万公里、牵扯六万万公民的历史事件，只知道几个名词，其它一概不知，可谓遗憾。</p> <p>这套书的主编是香港中文大学·当代中国研究中心的主任金观涛，作者包括沈志华、杨奎松、高华等人，都是重史料轻意识形态的真正的历史学家。他们根据各自的专长方向，基本上三年为一卷。出于兴趣，本笔记从沈志华等人撰写的第三卷开始记起，主要讲反右运动。</p> <hr/> <p>1956年1月14日中共中央召开了知识分子问题会议。主要由周恩来和当时的统战部部长李维汉。1955年周恩来就想召开一次知识分子问题会议，解决一下知识分子的使用问题，但是当时正在批判胡风，整体氛围对知识分子不友好，时机不对。1955年下半年，李维汉找到费孝通，提出想摸一摸知识分子的情况，费孝通非常积极，一番调研后提出了知识分子的“六不”问题，即“(对知识分子的水平和作用)估计不足、信任不够、安排不妥、使用不当、待遇不公、帮助不够”。</p> <p>重新使用知识分子也复合毛泽东的主观愿望。自斯大林死后，毛早就觉得世界共运的中心在中国，见毛在一些会议上的讲话和李慎之的回忆。但是中国当时国力孱弱，实在无法与苏俄抗衡，而发展中国就需要知识分子。当时只能求助与苏联专家，因此继续发展自己的专家，包括使用已经有的专家。但是毛又素来不信任乃至于仇视知识分子，参见《毛泽东选集》中的开篇文章《社会各阶层分析》，现在的文章经过学者考证是经过大量修改删节的，删去的部分就包括对知识分子的分析，其中毛将受过高等教育的知识分子称作”极端的反革命派“和”半反革命派“。</p> <p>1955年11月22日，周向毛表示应该考虑对知识分子受到的伤害问题。毛表示，首先应该在党内开个会议考虑一下。第二天，决定成立中共中央研究知识分子的十人小组，由周恩来负责，有彭真、陈毅、李维汉、徐冰、张继春、安子文、周扬、胡乔木和钱俊瑞。</p> <p>到1955年底，在科研、工程技术、医药卫生、教育、文艺五个反面工作的知识分子总共有384万人。其中高级知识分子（一般指大学毕业程度，有几年工作经验，能够独立工作的知识分子）十万人，其中共产党员只占百分之七。全国高等学校毕业生累计21.79万人。全国每一万人中只有五个高校毕业生，相较之下波兰有五十个，苏联有八十六个。</p> <p>建国后中共开展了一系列针对知识分子的运动，包括知识分子思想改造运动、批《武训传》、批判梁漱溟的所谓资产阶级思想、批判俞平伯的《红楼梦研究》和胡适的所谓《资产阶级唯心主义》，批判胡风的所谓《资产阶级文艺思想》，直到在知识分子中展开肃反运动。到1955年，知识分子基本服膺了中共的统治，参见冯友兰、北师大校长陈恒发等人的发言。</p> <p>成立十人小组后，周恩来开始尽心尽力统计知识分子，主要是高级知识分子的情况。周说这是他当时考虑的三个大问题之一。有了毛的首肯和过问，全党上下开始检讨对待知识分子的态度，参见张闻天在外交部全体干部会议上的报告。</p> <p>中共对知识分子的态度是欲待用之，又疑之防之。周恩来在领导人中对知识分子的态度是最友善、最支持的。</p> <hr/> <p><strong>中国知识分子问题会议及其新政策</strong></p> <p>1955年11月周恩来自己起草了会议的报告提纲，胡乔木写出了初稿，与十人领导小组多次讨论、修改。目前没有材料表明在公开前，毛泽东是否看过这份报告或者是什么态度。周恩来的报告中说，此次会议的<strong>目的</strong>在于充分动员和发挥知识分子的力量。</p> <p>对知识分子的政策需要调整，主要基于以下三点：</p> <ol> <li>当今世界科学技术迅猛发展，中国需要奋起直追。</li> <li>1949年以来知识分子在政治上有了很大的进步，其阶级属性发生了重大变化。</li> <li>批评中共党内在知识分子问题上的主要倾向是宗派主义。</li> </ol> <p>会议提出了三项措施：</p> <ol> <li>改善对知识分子的安排与管理，使其能发挥其所长；</li> <li>给他们以充分的信任和尊重；</li> <li>给知识分子工作所必要的条件和待遇。</li> </ol> <p>但是反响一般，杨尚昆就说这次会议讲话一般。而且最最关键的知识分子的言论自由和独立性问题只字不提。</p> <p>知识分子的反应是双重的。1月30日，人民日报全文发表周恩来的《关于知识分子问题的报告》，紧接着的30日-2月7日，周恩来主持召开了政协的二届二次全体会议。会议上当时的中科院院长郭沫若带头代表知识分子表示要“反省自己”等等。</p> <h2 id="毛泽东对苏共二十大的最初反应">毛泽东对苏共二十大的最初反应</h2> <p>在1963年9月6日发表的《苏共领导同我们分歧的由来与发展（一评）》中，中共中央单方面对这一问题做了全面评述。本文断言中苏的关系从苏共二十大（1956年2月）开始，指责苏共主要有两个问题，即彻底否定斯大林和试图通过“议会道路”和平过渡到社会主义。这种说法到今天都是官方说法。</p> <p>苏共二十大的内容并不能概括为“前面否定斯大林”和“和平过渡”问题。现在所有的相关文件几乎都已经公开，从资料来看，苏共二十大从三个方面，1）国际 2）国内和3）党内提出了一些的确与斯大林主持的十九大完全不同的方针、路线。</p> <h3 id="苏共二十大纲领对外政策">苏共二十大纲领：对外政策</h3> <p>提出了三和，即“和平共处、和平过渡、和平竞争”。战争不再是不可避免的。世界或者和平，或者毁于核战争，没有第三条道路。其次，不应该把暴力和内战看作是改造社会的唯一途径。在工人阶级和社会主义力量强大的<em>非社会主义国家</em>，工人阶级可以团结其它力量（中共所谓的“统一战线”）通过议会的方式实现制度的转变。</p> <h3 id="秘密报告与批判个人崇拜的由来">秘密报告与批判个人崇拜的由来</h3> <p>1956年2月25日上午，赫鲁晓夫在只有苏联代表参加的秘密会议上发表了题为《关于个人崇拜及其后果》的演讲。个人崇拜的英语是The cult of personality，cult是邪教的意思，中共本来想翻译成“个人迷信”，后来上层觉得个人崇拜还是需要滴，就该成了个人崇拜这个较为中性的表述。报告已经包含在其它日志中。这里只强调两点，1）报告没有全盘否定斯大林，一开始赫鲁晓夫就说”斯大林在……国内战争……中所起的作用是尽人皆知的“，”毫无疑问……有很大的功劳“。2）报告将斯大林的过错归咎于其个人因素，而不是体制因素。</p> <p>斯大林晚年把与自己同辈的“老近卫军”都赶出了领导核心。国家大事基本在他的晚宴上决定，参加晚宴的只有贝利亚（Beria），马林科夫，赫鲁晓夫等少数第二代领导人。统治阶层人人自危。</p> <p>1953年3月1日斯大林突然中风。</p> <p>最先进行“去斯大林化”，对斯大林个人造成的冤假错案进行甄别、平反的是贝利亚。参见秦晖这方面的讲座。斯大林去世才一星期，贝利亚就成立了若干侦查小组，负责重新审理包括著名的“医生案”在内的一些重大案件。马林科夫和贝利亚都谈到了个人崇拜的危害性。</p> <p>中共，特别是毛的态度是欢迎批判斯大林但是不欢迎批评个人崇拜。</p> <p>就在苏联准备改变过去斯大林时代的政策、路线时，国内却轰轰烈烈地搞起了一个学习苏联的高潮。</p> <hr/> <p>双百方针（百家争鸣、百花齐放）的第一次正式提出是在1956年4月25日至28日的中央政治局扩大会议上。会上陆定一提倡学术自由。而且陆定一把文艺创作也算进学术问题中，还说苏联对文艺创作的限制就太多，“无数的清规戒律”。</p> <p>第二天的会议上，陈伯达概括出来“百花齐放、百家争鸣”的口号，得到了毛的赞同。</p> <p>五月二日，毛泽东在最高国务会议第七次会议上第一次向党外人士宣布了“双百方针”。</p> <h2 id="中共八大">中共八大</h2> <p>八大通过了设定中央政治局常委、多个副主席、新建中央秘书处等决定。</p> <p>关于政治报告，刘少奇认为应该以《论十大关系》为纲。当年（56年）年初苏共召开二十大，做了秘密报告，这进而掀起了国内反个人迷信的思想浪潮。</p> <p>周恩来提出发展经济要稳步前进，突出一个稳，和毛泽东的主张很矛盾。为此，毛周甚至发生了正面冲突。支持反冒进的人主要有周恩来，刘少奇，李先念，李富春，陈云，薄一波等。56年中提出口号《既反保守又反冒进，在综合平衡中稳步前进》。毛泽东不同意但是没有表示反对。</p> <p>56年8月22日至9月13日召开了总共七届七中全会。政治报告初稿长达九万字，直到正式开会前一天晚上十一点才最后改完。</p> <p>中央政治局委员从七大的十三人增加到八大的十七人，并增选候补委员六人。政治局委员里的老委员有毛泽东，刘少奇，周恩来，邓小平，陈云，朱德，林伯渠，董必武，二彭（彭德怀彭真），其中毛是主席，有四个副主席分别是李少奇、周恩来、朱德、陈云。中央委员总书记是邓小平。其他老委员里，任弼时已死，高岗已经蒙冤自杀，康生、张闻天降为候补委员。新委员有邓小平、林彪、罗荣桓，陈毅、李富春、刘伯承、贺龙、李先念。</p> <p>当时李富春是新选国家计委主任。李先念是财政部长。</p> <p>彭德怀和康生的地位都有所下降。尤其是康生，麦克法尔夸怀疑这和赫鲁晓夫秘密报告后中共党党内普遍对秘密警察的反感有关。</p> <p>王光美会议，</p> <blockquote> <p>八大结束后第三天，在天安门城楼上，毛泽东对刘少奇说：八大关于我国基本矛盾的提法不正确。刘少奇很震惊，只答道，“呦，决议已经公布了，怎么办？”</p> </blockquote> <p>八大提出了所谓的三大问题，即主观主义、官僚主义、宗派主义。</p> <p>八大开过后一个月就发生了波匈事件。</p> <hr/> <h3 id="匈牙利事件">匈牙利事件</h3> <p>苏联自主地处理了波兰时间。同年发生了匈牙利事件。拉科西”退休“后，苏联选了拉科西的狗腿子葛罗。拉科西迫害死了比较受爱戴的拉伊克，10月6日为拉伊克举办了重新安葬仪式，民众借机游行示威表达对当局的不满，人群中就有纳吉。</p>]]></content><author><name>Baiyang Zhang</name></author><category term="political"/><category term="history"/><category term="nonfiction"/><category term="book excerpts"/><summary type="html"><![CDATA[最近，香港中文大学的中国文化研究所陆续出版了十卷的《中国人民共和国史》，是迄今为止规模最大的非官方共和国史。官方的历史因其在剪裁、解释、罗列事实等种种方面权力不受制约，因此有人将其归类于文学范畴，就像有人讲斯大林指挥下写就的《联共（布）党史》也归类于文学作品一样。这套非官方的著作以史料见长，史料为基础，分析为衍生，时间跨度一九四九至一九八九，四十年虽不能说是沧海桑田，在诸史书中也不见其长，然而这是迷雾重重的四十年，是半遮半露的四十年，是真假掺杂的四十年，对当下很多九零后、八零后乃至于七零后而言，也是几乎空白的四十年。譬如，大家都知道所谓的三年自然灾害，或者当时流行的两种解释，即自然灾害和苏修逼债，然而这三年到底发生了什么？前因后果怎样，多少人受灾，所谓的自然灾害又有哪些，是洪涝呢还是干旱，北戴河会议、成都会议上又都说了些什么？了解这些对了解六零年左右的真相是至关重要的，而如果没有真相，所谓的历史就应该归类于文学作品了。对于这种纵横九百万公里、牵扯六万万公民的历史事件，只知道几个名词，其它一概不知，可谓遗憾。]]></summary></entry><entry><title type="html">Bundles and Coset Spaces</title><link href="https://baiyangzhang.github.io/blog/2023/Coset-Spaces/" rel="alternate" type="text/html" title="Bundles and Coset Spaces"/><published>2023-07-14T00:00:00+00:00</published><updated>2023-07-14T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Coset-Spaces</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Coset-Spaces/"><![CDATA[<h3 id="cosets">Cosets</h3> <p>Let $G$ be a Lie group and $H$ a subgroup of $G$, denoted as $H&lt;G$. By the way we use $H\lhd G$ to denote that $H$ is a normal subgroup of $G$. Let $g\in G$ and $h \in H$ be the elements. The <code class="language-plaintext highlighter-rouge">left coset space</code> is the space of the equivalent classes $[g]$, where the equivalence relation is defined as \(\sim: g\sim g' \text{ iff } g = g' h \text{ for some } h \in H.\) So left means that $G$ acts from the left to $H$. Thus \(g \sim g' \text{ iff } g^{-1} g' \in H.\) In the case of abelian group we usually write $g+g’$, not $g g’$. We use \(gH := \left\{ gh \,\middle\vert\, h \in H \right\}\) to denote the left translation of $H$ by $g$. The rule is quite straightforward, $H$ is a set thus $gH$ is also a set.</p> <p>Now we’ve got ourselves a bundle, \(\pi: G\to G / H, \quad g\mapsto [g].\)</p> <p>Many familiar spaces are in fact coset spaces! Let’s say that $G$ acts on a space $M$ as a transformation group, provided there is a map \(G \times M \to M,\quad (g,x)\mapsto gx.\) Of course we require the various group properties to be satisfied, for example $g (g’x) = (g g’) x$, etc.</p> <p><strong>Transitivity.</strong> We say that $G$ acts on $M$ <code class="language-plaintext highlighter-rouge">transitively</code> if for all $x,y \in M$ there exists a $g \in G$ such that $g x = y$. $G$ touches every and all the elements of $M$.</p> <p>For example, $SO(3)$ acts transitively on $\mathbb{S}^{2}$.</p> <p><strong>Fundamental Principal.</strong> This principal gives a 1-2-1 correspondence between coset space and $M$. Let $G$ acts transitively on $M$, let $x_ {0}\in M$ and $H &lt; G$ is the group that keeps $x_ {0}$ invariant, $H x_ {0} = x_ {0}$. $H$ is called the <code class="language-plaintext highlighter-rouge">stability, or isotropy, or little</code> (sub)group of $x_ {0}$. Then the points of $M$ are in 1-2-1 correspondence with the left coset $G / H$.</p> <p>The notation $G / H$ does not seem to tell us that it is the left coset, rather than right. I guess the left in left coset is just part of the definition then, a matter of convention maybe. Note that, if $G$ is not abelian, then in general $G / H$ is not itself a group.</p> <p>As an example, convince yourself that \(SO(3) / SO(2) \cong \mathbb{S}^{2}.\)</p> <p>Before we mentioned on the fly the connection between coset space and bundle. The following theorem states the connection in a mathematically strict sense.</p> <p><strong>Theorem.</strong> Let $H &lt; G$ be the subgroup of a <em>Lie group</em> $G$, we require $H$ to be a <em>closed</em> subgroup, namely $H$ includes all its <em>accumulation</em>, or <em>limiting</em> points. Then the coset $G / H$ can be made into a manifold of dimension $\text{dim}G - \text{dim}H$. Furthermore, $G$ is a principal bundle with structure group $H$ and base space $G / H$, the projection $\pi$ sends $g \in G$ to its equivalence class $[g]$.</p> <p>A coset space $G / H$ of a Lie group is called a <code class="language-plaintext highlighter-rouge">homogeneous space</code>.</p> <p>For example, $\mathbb{S}^{2}$ is a homogeneous space. It is the coset space $SO(3) / SO(2)$ of dimension $3-1=2$.</p> <h3 id="grassmann-manifold">Grassmann manifold</h3> <p>The readers should not be unfamiliar with the real projective space $\mathbb{R}P^{2}$, if so please refer to my other notes. We’d like to study the connection between $\mathbb{R}P^{2}$ and $SO(3)$ group. Given a point in $\mathbb{R}P^{2}$, say the line of real axis (recall that a point in a real projective space are unoriented lines passing the origin), what is the little group that keeps it invariant? We have the familiar $SO(2)$ group which is the rotations about $x$-line, but we also have a discrete transformation $z \leftrightarrow -z$ which also keeps $z$-line invariant, so the little group is \(\mathbb{Z}_ {2}\times SO(2),\) So the coset space is \(SO(3) / (\mathbb{Z}_ {2}\times SO(2)),\) which according to the fundamental principal we introduced before is isomorphic to $\mathbb{R}P^{2}$.</p> <p>The dimension of a Cartesian product $G \times G’$ is $\text{dim}(G)+ \text{dim}(G’)$, thus $\text{dim}(\mathbb{Z}_ {2}\times SO(2))=0+1$. Then as you can show that the dimension of $\mathbb{R}P^{2}$ is indeed $2$.</p> <p><strong>Grassmann manifold.</strong> The space of $k$-dimensional un-oriented planes through the origin of $\mathbb{R}^{n}$ is called a Grassmann manifold and is frequently denoted by $\text{Gr(k,n)}$. Thus $\text{Gr}(1,3)=\mathbb{R}P^{2}$.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="Geometry"/><category term="Minkowski"/><category term="Frankel"/><summary type="html"><![CDATA[Cosets]]></summary></entry><entry><title type="html">Fiber Bundles and Principal Bundles</title><link href="https://baiyangzhang.github.io/blog/2023/Fiber-Bundles-and-Principal-Bundles/" rel="alternate" type="text/html" title="Fiber Bundles and Principal Bundles"/><published>2023-07-09T00:00:00+00:00</published><updated>2023-07-09T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Fiber-Bundles-and-Principal-Bundles</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Fiber-Bundles-and-Principal-Bundles/"><![CDATA[<p><em>Disclaimer: Nothing in this note is original.</em></p> <h3 id="fiber-bundles">Fiber bundles</h3> <p>The note is more of a review than introduction, so not so friendly for novices.</p> <p>Recall that a <code class="language-plaintext highlighter-rouge">fiber</code> (or <code class="language-plaintext highlighter-rouge">fibre</code>) <code class="language-plaintext highlighter-rouge">bundle</code> $E$ is a manifold consists of a base space $M$, a fiber $F$, with projection map $\pi: E \to M$. We demand that $E$ is locally production manifold of $F$ and some open patch $U$. This is realize by a <em>diffeomorphism</em> \(\phi_ {U}: F \times U \to \pi^{-1}(U).\)</p> <p>At the intersection of different patches we have the transition function that glues different patches together in a smooth fashion, let \(\phi_ {U} = c_ {UV} \phi_ {V}\) where $\phi_ {U}$ can be regarded as bundle $\pi^{-1}(U)$, the same goes with $V$ patch. Of course the transition function is required to have the following two properties,</p> <ul> <li>$\phi_ {UV} = \phi^{-1}_ {VU}$</li> <li>$\phi_ {UV}\phi_ {VW}\phi_ {WX} = \phi_ {UX}$.</li> </ul> <p>There is a hidden (well maybe not so hidden) group structure in the definition of fiber bundle: The set of diffeomorphisms of the fiber $F$. It need not be a Lie group. If all the maps (transition functions) $\phi_ {UV}$ lie in a <strong>subgroup</strong> $G$ of the group of all diffeomorphism of $F$, we say that $G$ is the <code class="language-plaintext highlighter-rouge">structure group</code>, sometimes just group, of the fiber bundle. For example, the structure group of a Mobius band is $\mathbb{Z}_ {2}$ group.</p> <p>A (local) <code class="language-plaintext highlighter-rouge">cross section</code>, or just a section, is a map $s: U\to E$ such that $\pi \,\circ\, s=\mathbb{1}$. Again we can use the transition function to glue local sections together to form a global section.</p> <hr/> <p>Let $M$ be a $n$-dimensional manifold and let $FM$ (F for frame) be the collection of all <em>orthonormal</em> frames $\mathbf{f}_ {1,\dots,n}$ you could have on $M$. $\mathbf{f}$’s are defined point-by-point. The fiber $\pi^{-1}(p)$ where $p\in M$ is the collection of all the orthonormal frames at $p$. To be specific, given any frame $\mathbf{e}$ at $p$, then the most general frame is given by \(\mathbf{f} = \mathbf{e}g,\quad \text{i.e. } f_ {\beta} = e_ {\alpha} g^{\alpha}_ {\;\; \beta},\quad g\in O(n).\) Thus, <em>after</em> a single frame $\mathbf{e}$ is chosen, $\pi^{-1}(p)$ can be identified with a Lie group $O(n)$, which is also the structure group.</p> <p>The local trivialization $U\times F\to\pi^{-1}(U)$ then becomes \(\phi_ {U} : U\times G\to\pi^{-1}(U)\) and assigns to each $p\in M$ and each $g \in G$ the frame \(\phi_ {U}(p,g) = \mathbf{e}_ {U}(p) \, g.\)</p> <p>In an overlap, $\mathbf{f}(p)$ would have another representation \(f_ {p} = \mathbf{e}_ {V} (p) g_ {V}(p)\) which is connected to $\mathbf{e}_ {U}$ by \(\mathbf{e}_ {V} = \mathbf{e}_ {U} c_ {UV}.\) Then \(\mathbf{f} = \mathbf{e}_ {U}g_ {U} = e_ {V}g_ {V} = e_ {U}c_ {UV}g_ {V}.\) The last relation can be interpreted both actively or passively. The action of $c_ {UV}$ on $F$ is the right action of $O(n)$.</p> <p><strong>Principal bundle.</strong> Given a bundle \(\left\{ E\xrightarrow{\pi} M, F, G \right\}\) where $G$ is the structure group, it is called a principal bundle if the following two conditions are satisfied.</p> <ul> <li>The fiber $F$ is the same as the structure group, for example both of them could be $O(n)$,</li> <li>The <em>transition function</em> $c_ {UV}$ acts on the fiber as the left group action. Note here by “fiber” we mean the trivilization of it, in the example of the frame bundle we mean $g_ {U}$.</li> </ul> <p>Return to our example of frame bundle, the frame bundle is the principal bundle <code class="language-plaintext highlighter-rouge">associated</code> with the tangent vector bundle $TM$.</p> <hr/> <p>The principal bundles have a remarkable property that is not shared among fiber bundles in general: the structure group acts on the bundle in a natural way as a group of transformations. Take the frame bundle for example. Let $g\in G$ be a given matrix and $\mathbf{f}=(\mathbf{f}_ {1},\dots,\mathbf{f}_ {n})$ be a frame at $p\in M$, then $\mathbf{f}$ is nothing but a point the bundle $E=FM$. We can let $g$ send this point $\mathbf{f}$ to a new point $g(\mathbf{f})= g(\mathbf{f})$ by \(g: \mathbf{f} \mapsto \mathbf{f}g.\) Note that $g$ now acts from the right, so it is consistent with the trivialization of the fibers.</p> <p>If we instead have a tangent (vector) bundle, things would not be so smooth and natural. Given a point in $TM$, that is a tangent vector $\mathbf{v}$ at $p\in M$, and given a matrix $g \in G$, what is the action of $g$ on $\mathbf{v}$? We can not have a column form of $\mathbf{v}$ without choosing a specific frame, and only after turning $\mathbf{v}$ into a column can we define the action of $g$ on $\mathbf{v}$! Choosing a specific frame is very unnatural, it ruins the abstractness, or free-of-option side of the tangent vector.</p> <p>For any principal bundle, we have</p> <p><strong>Theorem.</strong> The structure group $G$ of a principal bundle $P$ acts “from the right” on $P$, \(g: \mathbf{f} \mapsto \mathbf{f}\,g\) without fixed points when $g\neq e$, and preserved the fiber ($\pi(\mathbf{f}g)=\pi(\mathbf{f})$).</p> <p>Note that the left translations in $G$ (say by $c_ {VU}$) commute with right translations (say by $g$).</p> <p>Let $A \in {\frak g}$ where ${\frak g}$ is the Lie-algebra of $g$. Introduce the $1$-parameter subgroup $e^{ tA }$, which is a $G$-valued thing, so can serve as an element of the structure group, acting on $\mathbf{f}$ from the right, by definition \(\mathbf{f} \mapsto \mathbf{f}e^{ tA }.\) Define the velocity vector field on $P$ \(A^{\ast }(\mathbf{f}) := \frac{d}{d t} (\mathbf{f}e^{ tA }){\Large\mid}_ {t=0} .\)</p> <p>On a patch $U$, $\mathbf{f}$ is parametrized by $\mathbf{e}_ {U}f_ {U}$ with some base point $\mathbf{e}_ {U}$ and the action is completely described by \(f_ {U} \to f_ {U} e^{ tA },\) whose velocity vector at $f_ {U}\in G$ is \(\frac{d}{dt} (f_ {U}e^{ tA }) {\Large\mid}_ {t=0} = f_ {U} A.\) Recall that $f_ {U}$ is $g$-valued and $A$ ${\frak g}$-valued, $f_ {U}A$ is the left-translate of vector $A$.</p> <p>The vector field $A^{\ast}$, sometimes denoted as $A^{\sharp}$, is called the <code class="language-plaintext highlighter-rouge">fundamental vector field</code> associated to $A$.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="Geometry"/><category term="Minkowski"/><category term="Frankel"/><summary type="html"><![CDATA[Disclaimer: Nothing in this note is original.]]></summary></entry><entry><title type="html">Quartic time derivative in Lagrangian</title><link href="https://baiyangzhang.github.io/blog/2023/Quartic-Time-Derivative/" rel="alternate" type="text/html" title="Quartic time derivative in Lagrangian"/><published>2023-07-07T00:00:00+00:00</published><updated>2023-07-07T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Quartic-Time-Derivative</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Quartic-Time-Derivative/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>One day my postdoc supervisor handed me this function \(T = \frac{\alpha}{2} b^{2}_ {i} + \frac{\beta}{4} (b^{2}_ {i})^{2}\) I guess $b_ {i}$ is something like $\dot{a}_ {i}$. $T$ is probably the kinetic energy.</p> <p>The canonical momentum is \(\pi_ {i} = \frac{\partial T}{\partial b_ {i}} = \alpha b_ {i} + \beta b^{2} b_ {i},\quad b^{2}:=b_ {i}b_ {i}. \tag{5}\)</p> <p>The question is, what is $T(\pi_ {i})$? And the next question, I guess, is how to quantize it?</p> <p>There might be two methods adoptable here, Legendre transform and direct algebraic substitution. The latter is to solve for $b_ {i}$ from Eq.(5) and substitute it back to the expression for $T$.</p> <p>The problem is that, the algebraic substitution will in general loose some information.</p> <p>Functions are information. Different functions contain different functions, some contains more and some less. For example, the constant function contains little information. In general we have the following conclusions.</p> <ul> <li>The information of a function is equivalently contained in its graph. Thus, any operation that preserved the graph up to rotation, stretching, mirroring, etc. also preserved the information. For example, given a function $y=f(x)$, its inverse function, if exists, contains the same information since the graphs are just mirrored along $y=x$.</li> <li>The Fourier transform also preserves the information. In fact, functions are elements, or vectors in some Hilbert space and the Fourier transform corresponds to a change of representation.</li> </ul> <p>So the question I wanna ask myself is, does Legendre transform preserve the information, just as Fourier transform?</p> <h3 id="legendre-transform">Legendre transform</h3> <p>What is a Legendre transform, really? Given a function $y = f(x)$, or $y = y(x)$, sometimes we are more interested in the slope $dy / dx$, so much as to wanting them as the variable instead of $x$. So we define a new variable, \(p:= \frac{dy}{dx} = p(x).\) Now how can we get a function in terms of $p$ without loosing any new information? The obvious way is to solve $x$ in terms of $p$, then substitute $x$ by $p$ in the original function to obtain $f(x(p))$. All seems good, except that this procedure actually causes a loss of information, as shown in the below example.</p> <p>Consider a function $y =\frac{1}{2} (x-x_ {0})^{2}$, follow the procedure we introduced before, define \(p := \frac{dy}{dx} = (x-x_ {0}) \implies x = p+x_ {0}\) take it back to the original function we get \(y = \frac{1}{2} (p+x_ {0}-x_ {0})^{2} = \frac{1}{2}p^{2},\) $x_ {0}$ is gone! This is bad news since now we have loss the information of $x_ {0}$. Another way to say it is that, now two function with different $x_ {0}$ will be transformed to the same function $y=\frac{1}{2} p^{2}$. Bad news indeed. If it is a good transform, we should be able to transform it back to the original function, just as Fourier transform or Laplace transform or Borel transform, etc. Or, you could say now the information has been split to two places, the final function $\frac{p^{2}}{2}$ and the specific transform process, that is we used $p=x-x_ {0}$. Well, we don’t want the transform process itself to contain any information! When we are doing Fourier transform or anything, we don’t need to remember anything about the original function, do we?</p> <p>Well there is a solution but only works for convex and concave functions. Recall that roughly speaking a convex function is a functions curves downwards and a concave function is a function that curves upwards. They are assumed to be almost-everywhere differentiable, if you don’t remember what “almost-everywhere” means please refer to my other notes.</p> <p>Now, the solution of course is the Legendre transformation. Denote $\widetilde{y}$ the Legendre transformed function, we have \(\widetilde{y}(p) = \begin{cases} \text{sup }\left\{ px - y(x) \right\} &amp; y \text{ is convex}, \\ \text{inf }\left\{ px - y(x) \right\} &amp; y \text{ is concave}. \end{cases}\) If the function is not convex/concave only in localized regions, then the transform still makes sense. Sometimes it is defined with an extra minus sign.</p> <p>Let’s apply Legendre transform to $y=\frac{1}{2} (x-x_ {0})^{2}$, we have $p=dy / dx = x-x_ {0}$ and \(\widetilde{y} = px-y = (x-x_ {0})x -\frac{1}{2} (x-x_ {0})^{2} = (x-x_ {0})\left( x-\frac{x}{2} + \frac{x_ {0}}{2} \right) = \frac{1}{2}(x-x_ {0})(x+x_ {0}),\) $x_ {0}$ is still there!</p> <hr/> <p>Now we can do the Legendre transform twice to prove that for certain functions \(\widetilde{\widetilde{y}} = y.\) To show it, first perform once Legendre transform \(\widetilde{y}(p)=p x(q)-y(q),\quad p = dy / dx.\) That is, assume you can solve for $x$ from $p$. Now, do it again we have \(\widetilde{\widetilde{y}} = qp - \widetilde{y} = y(q)\) where I’ve omitted some details, but you can reproduce it, no problem.</p> <hr/> <p>Back to \(T = \frac{\alpha}{2} b^{2} + \frac{\beta}{4} (b^{2})^{2},\quad b^{2}:= b_ {i}b_ {i}\) and the canonical momentum reads \(\pi_ {i} = \frac{\partial T}{\partial b_ {i}} = \alpha b_ {i} + \beta b^{2} b_ {i},\quad b^{2}:=b_ {i}b_ {i}. \tag{5}\)</p> <p>Perform the Legendre transform to $T$ we get \(H:= \pi_ {i}b_ {i}-T = \frac{\alpha}{2}b^{2} - \frac{3}{4}(b^{2})^{2}.\) So far it seems to lead to nowhere, so I’ll postpone the discussion on this matter. Now we directly jump to the algebraic substitution method. Does it cause any loss of information? If so, is the lost information important? I don’t know.</p> <h3 id="algebraic-substitution">Algebraic substitution</h3> <p>So, let’s try to solve Eq. (5). Since only $\pi^{2}$ appears in the Lagrangian, we can square the equation on both sides \(\pi^{2} = \alpha^{2}b^{2} + \beta^{2} (b^{2})^{3} + 2\alpha \beta (b^{2})^{2}\) This suffices as long as we don’t have to deal with $b_ {i}$ components individually. Let $t := \pi^{2}$ and $x:= b^{2}$ to simplify the notation we have \(t = \beta^{2} x^{3}+2\alpha \beta x^{2}+\alpha^{2} x, \tag{6}\) which is \(x^{3}+\frac{2\alpha }{\beta}x^{2}+\frac{\alpha^{2}}{\beta^{2}}x-\frac{t}{\beta^{2}}=0.\) Keep in mind that $x,t$ are positive by construction.</p> <p>The graph of the right hand side of equation (6) can be illustrated using mathematica codes <code class="language-plaintext highlighter-rouge">Manipulate[Plot[\[Beta]^2 x^3 + 2 \[Alpha] \[Beta] x^2 + \[Alpha]^2 x, {x, 0, 15}, AxesLabel -&gt; {"x", "t"}], {\[Alpha], -5, 5}, {\Beta], -5, 5}]</code></p> <p>For example, for a specific choice of value for $\alpha,\beta$ we have <img src="/img/txgraph.png" alt="txgraph"/></p> <p>As the plot shows (also can be read-off from the equation itself), there is always the solution $t=x=0$. This is of no interests for us. What about the other root,which seems to be degenerate?</p> <p>Equation (6) can be simplified to \(t = \beta^{2} x \left( x+ \frac{\alpha}{\beta} \right)^{2}, \quad x,t&gt;0.\) From this form things are much clearer, we see again $x=t=0$ is always a solution, there is another degenerate solution at \(t=0,\quad x = - \frac{\alpha}{\beta}.\) Recall that we are only interested in $x,t\geq 0$, we will discuss to situations according to whether $\alpha$ and $\beta$ have the same sign or not.</p> <h4 id="1-alpha-betaleq-0">1. $\alpha \beta\leq 0$</h4> <p>The local maximum is obtained at \(x_ {\star } = - \frac{\alpha}{3\beta},\quad t_ {\star} = - \frac{4\alpha^{3}}{27\beta}.\) Thus there are three inequivalent real roots for \(\alpha \beta &lt; 0, \quad 0&lt; t &lt; -\frac{4\alpha^{3}}{27\beta},\) where if $t=0$ or $-4\alpha^{3} / 27 \beta$ then there will be two degenerate roots.</p> <p>If \(t &gt; - \frac{4\alpha^{3}}{27\beta},\quad \text{one real root. }\)</p> <p>The situation is summarized in the plot below. <em>The light blue area is where the real solution is unique to the cubic equation</em>. We have marked the local maximum also. <img src="/img/cubic.png" alt="cubic"/></p> <h4 id="2-alpha-beta-0">2. $\alpha \beta&gt; 0$</h4> <p>In this case, since $\alpha$ is fixed to be positive for physics reasons, $\beta$ is also fixed to be positive definite. From the graph it is easy to see that not there exist only one real root for $x=x(t)$ when both $x,t$ are required to be positive. The root formula gives by Mathematica reads</p> \[\begin{align} x =&amp; -\frac{(-2)^{2/3}}{6 \beta ^2}\sqrt[3]{-2 \alpha ^3 \beta ^3+3 \sqrt{3} \sqrt{\beta ^7 t \left(4 \alpha ^3+27 \beta t\right)}-27 \beta ^4 t}\\ &amp;\frac{\alpha}{3 \beta }\left(2-\frac{\sqrt[3]{-2} \alpha \beta }{\sqrt[3]{-2 \alpha ^3 \beta ^3+3 \sqrt{3} \sqrt{\beta ^7 t \left(4 \alpha ^3+27 \beta t\right)}-27 \beta ^4 t}}\right) \end{align}\] <p>As a reminder to <em>myself</em>, you can’t always trust the factors of $i$ in the output of mathematica Solve command, sometimes the solution explicitly contains $i$ but it is actually real, sometimes the other way around. Numerical method is our best friend in this case.</p> <h4 id="the-ancient-way">The ancient way</h4> <p><strong>The standard 16-th century approach.</strong> To really solve the cubic equation, we need to turn the equation to the the standard form, this equations reads \(x^3+px=q\) where \(\begin{align} p &amp;=\frac{\alpha^{2}}{\beta^{2}} - \frac{1}{3}\left(\frac{2\alpha }{\beta}\right)^2, \\ q &amp;=\frac{1}{3} \frac{2\alpha^{3}}{\beta^{3}} + \frac{t}{\beta} -\frac{2}{27}\left( \frac{2\alpha}{\beta} \right)^{3}. \end{align}\) According to some 16th century mathematics, we can then perform the so-called <code class="language-plaintext highlighter-rouge">Vieta's substitution</code> and get the three root. Vieta’s substitution reads \(x =: w- \frac{p}{3w}\) which reduces the equation to \(w^{3}-\frac{p^{3}}{27w^{3}}-q =0\) which can be easily turned into a quadratic equation in $w^{3}$ \((w^{3})^{2}-q(w^{3})-\frac{p^{3}}{27}=0.\) The quadratic formula says that \(w^{3} = \frac{1}{2} q \pm \sqrt{ \frac{q^{2}}{4} + \frac{p^{3}}{27} }=: R\pm \sqrt{ R^{2}+Q^{3} }.\) Sometimes it is more useful to deal with $R$ and $Q$ instead of $p$ and $q$. There are therefore six (complex) solutions for $w$, which gives us three solutions for $x$.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="Skyrmion"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">What is a transseries?</title><link href="https://baiyangzhang.github.io/blog/2023/Transseries-Lecture-I/" rel="alternate" type="text/html" title="What is a transseries?"/><published>2023-07-05T00:00:00+00:00</published><updated>2023-07-05T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Transseries-Lecture-I</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Transseries-Lecture-I/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>In history, formal power series are used extensively for finding the resolution of differential equations. If the resulting power series is convergent, it gives rise to a germ which can be analytically continued to (multi-valued) functions on a Riemann surface. However, very often, the power series we found from solving a differential equation is divergent, then it is not clear <em>a prior</em> how to attach reasonable sums to them.</p> <p>The modern theory of <code class="language-plaintext highlighter-rouge">resummation</code> was developed systematically by Stieltjes, Borel and Hardy, who invented some resummation methods which <em>are stable under the common operators of analysis</em>. Later, Poincare established the equivalence between computations with formal power series and asymptotic expansions. Newton, Borel and Hardy were all aware of the systematic aspects of their theories and they consciously tried to complete their framework so as to capture as much of analysis as possible. The great unifying theory nevertheless had to wait until the late 20-th century and Ecalle’s work on transseries and Dulac’s conjecture.</p> <hr/> <p>Define a ordered group ${\frak G}$ (frak G) of transmonomials. Define a differential field $\mathbb{T}$ of <code class="language-plaintext highlighter-rouge">transseries</code>. Transmonomials are generalizations of monomials in polynomials, by including exponential and logarithmic. In this note and that follows, we will consider the limit where $x\to \infty$.</p> <p>Let’s start with exponents first.</p> <p><strong>Log-free transmonomials.</strong> They are of form \(x^{b}e^{ L },\quad b\in \mathbb{R},\; L \in \text{large log-free transseries.}\)</p> <p>For example, the following are all log-free transmonomials, \(x^{-1},\; x^{\pi}x^{x^{\sqrt{ 2 }}-3x},\; e^{ \sum_ {i}x^{-1}e^{ x } },etc.\)</p> <p>The multiplication is defined in the obvious way. The group identity is just $1$.</p> <p>We define a binary relation $\gg$, read “far larger than”. Keep in mind that we are assume $x\to \infty$. So how does this “far larger than” work? We compare the exponents $e^{ L }$ first, whichever with the largest exponent $L$ is far larger than others; if they have same exponents, then we compare the power of $x$, namely $x^{b}$, whichever with larger $b$ is far larger then others. To be specific, \(x^{b_ {1}}e^{ L_ {1} } \gg x^{b_ {2}}e^{ L_ {2} }\quad \text{ if } L_ {1} &gt; L_ {2} \;\lor\; (L_ {1}=L_ {2}\;\land\; b_ {1}&gt;b_ {2} ),\) where $\lor$ is logic or. For example, $x^{-5}\gg x^{20}e^{ -x }$ since $x^{-5}=x^{-5}e^{ 0 }$ and $0&gt;-x$.</p> <p><strong>Log-free transseries.</strong> A log-free transseries $T$ is a formal sum of log-free monomials ${\frak g}$, \(T = \sum_ {i} c_ {i} {\frak g}_ {i},\quad c_ {i} \in \mathbb{R} .\) We require the order of transmonomials be such that, each ${\frak g_ {i}}$ is far smaller than all previous terms, namely they appear in descending orders. This is similar to the case of regular polynomials where we usually put the highest powers at first.</p> <p>The transseries $T$ is said to be <code class="language-plaintext highlighter-rouge">purely large</code> if all transmonomials ${\frak g}_ {i}$ are far larger than $1$ (not $0$), namely ${\frak g_ {i}}\gg_ {1} \;\forall i$. $T$ is said to be <code class="language-plaintext highlighter-rouge">small</code> if all ${\frak g}_ {i}\ll 1$. The largest (in the sense of far larger than) transmonomial is called the <code class="language-plaintext highlighter-rouge">dominant term</code>, let’s call it $c_ {0}{\frak g}_ {0}$. If the dominant term has positive coefficients, $c_ {0}&gt;0$, then $T$ is said to be positive. This enables us to compare the size of two transseries $S,T$, we say $S&gt;T$ if $S-T&gt;0$. So we just need to compare their dominant terms.</p> <hr/> <p>We consider only transmonomials and transseries of “finite exponential height”. For example, we don’t want \(e^{ x^{x^{x\dots}} }.\)</p> <p>The <code class="language-plaintext highlighter-rouge">differentiation</code> of $T$ with respect to $x$ is defined the usual way.</p> <hr/> <p>Next let’s include logarithmic. $\log$ acting $m$ times is denoted $\log_ {m}x$, namely \(\log_ {m}x = \log \dots \log x,\quad m\; \log .\)</p> <p>A general transseries is obtained by substitution of some $\log_ {m}x$ for $x$ in a log-free transseries.</p> <p><em>Every nonzero transseries has a multiplicative inverse.</em> This is similar to formal power series.</p> <p><em>A lot of functions can now be regarded as a transseries</em>. For example, e hyperbolic sine is a two-term transseries.</p> <p>In the next note, we will talk about formal constructions of transseries. Then in the third, I will show some applications and close this topic.</p>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">The Electromagnetic Connection</title><link href="https://baiyangzhang.github.io/blog/2023/The-Electromagnetic-Connection/" rel="alternate" type="text/html" title="The Electromagnetic Connection"/><published>2023-07-03T00:00:00+00:00</published><updated>2023-07-03T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/The-Electromagnetic-Connection</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/The-Electromagnetic-Connection/"><![CDATA[<p><em>Disclaimer: Nothing in this note is original.</em></p> <p>Is this note we discuss the profound connection between the electromagnetic field and the parallel transportation of wave functions. This will endow the electromagnetic field a geometric interpretation.</p> <h3 id="classical-case">Classical case</h3> <p>In the presence of electromagnetic fields, the charged particle will receive forces not only from a potential $-\nabla V$, but also from the electromagnetic force $q(\mathbf{E}+\mathbf{v}\times \mathbf{B})$. The latter can not be written as a covariant derivative of some potential thanks to the magnetic contribution, so we can’t simply include it into a new potential form $V’$.</p> <p>Recall that the energy-momentum covector defined on the cotangent bundle of the configuration space is \(p_ {\mu} dq^{\mu} = g_ {\mu \nu}p^{\mu}dq^{\nu} = p_ {i}dx^{i}-Edt.\) We add to it the electromagnetic covector \(\phi dt+A_ {i} dx^{i},\) then we have a new total energy-momentum 1-form, from which we can construct a new Lagrangian by Legendre transform, which will tell us how a charged particle moves in EM field.</p> <h3 id="quantum-case">Quantum case</h3> <p>Ignore the spin of a electron, it is described by a wave function $\psi(x,t)$. Schrodinger’s equation states that the wave function evolves in time according to \(i\partial_ {t}\psi = H \psi,\quad H = T + V = \frac{p^{2}}{2m} + V.\)</p> <p>In <em>Cartesian coordinates</em>, \(p_ {i} := -i \frac{\partial}{\partial x^{i}}.\)</p> <p>With the presence of electromagnetism, the following replacement is needed, \(p \to p-eA, \quad V \to V-e\phi.\)</p> <p>Furthermore, we can write the Schrodinger function in terms of the covariant derivatives if we introduce connections by \(\nabla_ {\mu} := \frac{\partial}{\partial x^{\mu}} -ie A_ {\mu}.\) The gauge potential has a compact $1$-form expression, \(A = A_ {\mu}dx^{\mu} = \phi dt+A_ {i}dx^{i}.\)</p> <hr/> <p>The vector potential is not uniquely defined. We may adopt different gauge potentials in different patches, and at the overlapping region the gauge fields (connections) can be glued via gauge transformation. This is precisely the picture of complex line bundle we introduced before, whose covariant derivative is defined via the connection $\omega$ by \(\nabla_ {i}\psi = \partial_ {i} \psi + \omega_ {i}\psi.\)</p> <p>We find that $\omega_ {i}$ is nothing but the gauge field!</p> <p>We can again confirm that gauge transformation for $A_ {\mu}dx^{\mu}$ coincides with how a connection should change under a change of coordinates, if we choose the transition function to be \(c_ {VU} = \exp \left\{ -ie f(x) \right\} ,\quad f(x) \text{ is some function.}\)</p> <p>In summary, <em>a wave function is not to be considered as a single-valued complex function of $\mathbb{R}^{4}$ but rather a collection of functions $\psi_ {U,V, \dots}$ of functions such that in an overlap $U\cap V$ we have</em> \(\psi_ {V} = c_ {VU} \psi(U) = \exp(ief_ {VU})\psi_ {U}.\) This brings us back to the starting point of gauge theories in quantum mechanics, namely</p> <p><strong>Weyl’s principal of gauge invariance.</strong> If $\psi$ satisfies the Schrodinger equation with $A$, then \(\exp(ief_ {UV})\psi\) satisfies the Schrodinger equation with $A\to A+df$.</p> <p>A summary.</p> <ul> <li>Fiber bundle: complex line bundle.</li> <li>Transition function: $U(1)$.</li> <li>Structure group: $U(1)$.</li> <li>Connection: $A = A_ {\mu}dx^{\mu}$.</li> <li>Curvature: $F = dA = -ie (E\wedge dt+B)$.</li> </ul> <hr/> <p>In curvilinear coordinates, what form should the Schrodinger equation take? Recall that when there is no electromagnetic field and the spacetime is flat, the Schrodinger equations is simply \(i\partial_ {t}\psi = H\psi = \left( \frac{1}{2m} g^{\alpha \beta}p_ {\alpha }p_ {\beta}+V \right) \psi.\) In a curvilinear coordinates on an Riemannian manifold $M$, we replace the momentum operator $p_ {\alpha}$ by \(p_ {\alpha}:= -i \nabla_ {\alpha},\) where $\nabla$ is the covariant derivative. We can also replace the Laplacian with its corrected form in curvilinear coordinates, \(\boxed { \nabla^{2} = \frac{1}{\sqrt{ g }} \frac{\partial}{\partial x^{\mu}}\left( \sqrt{ g } \,g^{\mu \nu}\frac{\partial}{\partial x^{\nu}} \right) }\) refer to my blog <a href="http://www.mathlimbo.net/2023/04/10/Electrodynamics-in-Terms-of-Forms/">here</a>. Inserting this into the Schrodinger equation we get the curvilinear Schrodinger equation, which is left as a homework (to myself).</p> <h3 id="global-potentials">Global Potentials</h3> <p>If the space is $\mathbb{R}^{4}$, the second Betti number is zero, then de Rham’s theorem assures us that there is a potential form $A$ for the closed 2-form $F$ such that $F=dA$. However, there are usually singularities of $F$, for example at the point charge $F$ blows up, so when talking about geometry we need to take such singularities out, and this will change the topology of the spacetime. The second Betti number will in general be non-zero, but we still we have the following.</p> <p><strong>Theorem.</strong> Consider a region $U$ of a general relativistic space-time $M^{4}$ that has a <em>global time coordinate</em>. That is, \(U \cong V^{3} \times \mathbb{R}\) where $V^{3}$ is a spacelike hypersurface. Suppose that the magnetic field $B$ vanishes at time $t=0$, Then $F$ has a global potential $A$ such that $F=dA$ on all of $U$.</p> <h3 id="dirac-monopole">Dirac monopole</h3> <p>When there is a monopole in spacetime, then we are forced to introduce the bundle and connection.</p> <p>Assume there is a monopole sitting at the origin. The magnetic field generated is \(\mathbf{B} = \frac{q}{r^{2}} \partial_ {r}\) where $\partial_ {r}$ is the radial vector basis. In the language of forms, $B$ is a two form \(B = i_ {\mathbf{B}}\text{Vol}^{3} = d[q(1-\cos \theta)d\phi],\) which is obviously not defined on the negative $z$-axis, thus the potential is given by \(A_ {U} = q(1-\cos \theta)d\phi\) on $\mathbb{R}^{3}$ minus the negative z-axis. In the complementary region \(V = \mathbb{R}^{3} - \text{positive }z\text{-axis}\) the potential is given by $\phi\to-\phi,\theta\to\pi-\theta$ thus \(A_ {V} = -q(1+\cos \theta)d\phi\) and Maxwell’s equation holds everywhere on $\mathbb{R}^{3}-\left{ 0 \right}$. The transition function is \(C_ {VU} = \exp\left( - \frac{2ieq\phi}{\hbar} \right)\) where $\hbar$ is usually set to zero.</p> <p>Now the potential is not single valued unless \(2eq / \hbar = \mathbb{Z}\) is satisfied.</p> <p>In the language of curvature, we have \(\frac{i}{2\pi}\int d\theta \, = \mathbb{Z}\) where $\theta$ is connected to the magnetic field by \(\theta = -ie B.\)</p> <p>In summary, if there exists a monopole, then we <strong>must</strong> regard the wavefunction as a section of a complex line bundle.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="Geometry"/><category term="Curvature"/><category term="Frankel"/><summary type="html"><![CDATA[Disclaimer: Nothing in this note is original.]]></summary></entry><entry><title type="html">Note on the Skyrme Model, Classical and Quantum</title><link href="https://baiyangzhang.github.io/blog/2023/Note-on-Quantization-of-Skyrme-Model/" rel="alternate" type="text/html" title="Note on the Skyrme Model, Classical and Quantum"/><published>2023-06-30T00:00:00+00:00</published><updated>2023-06-30T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Note-on-Quantization-of-Skyrme-Model</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Note-on-Quantization-of-Skyrme-Model/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>In his 1973 paper “A planar diagram theory for strong interactions”, ‘t Hooft treated the parameter $N$ in the gauge group $SU(N)$ or $U(N)$ as a free parameter, and explained that in the $N\to \infty$ limit, the Feynman diagrams arrange into sets with different topological structure according to its contribution in powers of $1 / N$. For the sake of simplicity he didn’t require the trace of the ${\frak g}$-valued gauge field $A$ to be traceless, thus instead of $SU(N)$ the gauge group is actually $U(N)$. As a result there will be an extra photon corresponding to the $U(1)$ component of $U(1)$, which will be discarded at the last step. ‘t Hooft introduced the double-line notation and each closed group index circle will generate a factor of $\delta^{i}_ {i}=N$, making it convenient to count the factor of $N$ of a given Feynman diagram. Based on this, ‘t Hooft made use of the Euler theorem saying that $\chi=2-2g$ where $\chi$ is the Euler characteristic and $g$ the genus of a manifold, and reached the conclusion that, if $g^{2}N$ is kept fixed at large $N$, then <em>the planar diagrams with genus zero and one dominate</em>. That is, to treat the double-line Feynman diagrams as the boundary of a manifold, then we can talk about the genus of it. <em>Regarding non-planar diagrams, each “handle” pay a price $1 / N^{2}$</em>.</p> <p>Large $N$ really plays an important rule in SU($N$) gauge theory. Large $N$ limit turns out to be pretty helpful in understanding the nonperturbative and phenomenological aspect of field models. In large N limit, the coupling is made $N$-dependent in such a way that the large $N$ limit exists and is nontrivial, duh.</p> <p>Speaking of QCD specifically, we have \(\beta(g) = - \frac{1}{16\pi^{2}}\left( \frac{11}{3}N - \frac{2}{3}N_ {f} \right)g^{3}\) and the ‘t Hooft coupling, which is required to be fixed at large $N$, is \(\lambda = g^{2}N.\) Usually the ‘t Hooft expansion is defined to be the limit $N\to \infty$ with $\lambda$ kept fixed.</p> <p>It is said that <em>counting rules suggest that baryons may emerge as solitons in large QCD theory.</em> The large $N$ theory of mesons, at low energy, reduces to non-linear sigma model, but how? And what does the Wess-Zumino coupling has to do with it?</p> <h2 id="the-non-linear-sigma-model">The Non-linear Sigma Model</h2> <p>The non-linear sigma model, sometimes written as NL$\sigma$M or NLSM, is a scalar field theory where the target space (where the field takes value) is a non-linear manifold. It’s called $\sigma$ model because originally Gell-Mann and Levy used $\Sigma$ to denote the scalar field. NLSM is a precursor of modern day QCD, even now it still serves as a good low-energy approximation, namely low energy effective theory of QCD, where quarks are negligible. NLSM is an effective of spin-zero scalar, in real world pions.</p> <p>There are three types of pions, \(\begin{cases} \pi^{+}: &amp; u\overline{d} \\ \pi^{0}: &amp; \frac{1}{\sqrt{ 2 }}(u\overline{u}+d\overline{d}) \\ \pi^{-}: &amp; \overline{u}d \end{cases}\) which will be the degree of freedoms in the NLSM.</p> <p>There is a clever way to combine the three scalars together to form a compact model. Let $U(x)$ be a $SU(2)$ matrix, we can “stuff” the pion fields into $U(x)$ by writing \(U(x) = \exp(i\pi^{a}\sigma^{a} / f_ {\pi})\) where $\sigma^{a}$ are the Pauli matrices, and $f_ {\pi}\sim 92 \text{ Mev}$ is the pion decay constant. In 4D the scalar field has dimension \([\pi] = \frac{d-2}{2} = 1\) therefore $f_ {\pi}$ should also has the dimension of mass, such that the argument of the exponential function is dimensionless.</p> <p>This is an example of so-called <code class="language-plaintext highlighter-rouge">chiral Lagrangian</code>, which combines the three pions into a single matrix with the help of Pauli matrices.</p> <p>We can Taylor expand the exponential function, and by making use of the properties of Pauli matrices such as \(\sigma^{a} \sigma^{b} = \delta^{ab} + i \epsilon^{abc} \sigma^{c}\) we get \(\exp\{ i\pi \cdot \sigma / f_ {\pi} \} = \cos(\pi / f_ {\pi})\mathbb{1} + i \sin(\pi / f_ {\pi}) \hat{\pi}\cdot \sigma,\quad \pi := \left\lvert \mathbf{\pi} \right\rvert .\) This form makes the symmetry of the Lagrangian manifest.</p> <p>We also want the NLSM to adopt the chiral symmetry, that is the $SU(2)_ {L} \times SU(2)_ {R}$ global symmetry. Let $L,R\in SU(2)$ then the chiral symmetry acting on $U$ is \(U \to L U R^{\dagger}.\) But why? Some people might tell you not to worry about it, it doesn’t matter for the calculation, but did you decide to do physics because you want to understand or because you love calculation? So here I’ll try to explain it briefly. Feel free to skip the next chapter entirely if you are not interested.</p> <h3 id="chiral-transformation">Chiral transformation</h3> <p>The story begins with what happens when some <em>global, continuous symmetry</em> is <em>spontaneously broken</em>. Let $J_ {\mu}$ be the Noether current associated to the symmetry. Assume that the conservation law $\partial_ {\mu} J^{\mu}$ holds in both classical and quantum situation, that is to say this symmetry is not anomalous. In the quantum theory, the conserved charge \(Q := \int d^{3}x \, J_ {0},\quad J_ {0} = \sum \frac{\partial \mathcal{L}}{\partial \dot{\phi}}\Delta \phi\) where $\Delta \phi$ is the variation of $\phi$ under the symmetry at question, the summation is over all kinds of fields. The important thing is that now $Q$ is an <strong>operator</strong>, because $\phi$’s are operators. Recalling the procedure of canonical quantization, first we define the canonical momentum \(\pi := \frac{\partial \mathcal{L}}{\partial \dot{\phi}}\) with the canonical commutation relation \([\phi(x),\pi(y)] = i\delta^{(3)}(x-y)\) where I have omitted some possible indices. We then find that \([Q,\phi_ {n}(y)] = -i \frac{\delta \phi(y)}{\delta \alpha}\) where the RHS is the variation of the field under the symmetry in question, therefore $Q$ <em>generates the symmetry transformation.</em> Now, by definition, spontaneous symmetry breaking happens if the symmetric vacuum (or staple point, whatever name you like) with \(Q\left\lvert{\Omega}\right\rangle=0\) is unstable, but the true, stable vacuum has non-zero charge \(Q\left\lvert{0}\right\rangle\neq 0.\) Here it is best to think of $Q\left\lvert{0}\right\rangle$ not as calculating the charge of $\left\lvert{0}\right\rangle$, but the infinitesimal form of transforming of $\left\lvert{0}\right\rangle$. If you have doubt, just think of the Mexico hat potential and rotational symmetry, once chosen a vacuum $\left\lvert{0}\right\rangle$, one can rotate it by an infinitesimal angle \(e^{ i\epsilon Q }\left\lvert{0}\right\rangle = \left\lvert{0}\right\rangle +1\epsilon Q \left\lvert{0}\right\rangle =\left\lvert{0'}\right\rangle\) which is a new vacuum. Then Obviously $Q\left\lvert{0}\right\rangle\neq 0$.</p> <p>It can be shown that each spontaneously broken symmetry (SSB) gives rise to a massless mode, namely a massless particle. One way to do it is to start with the defining relation \(\left\langle{0}\right\rvert [Q,\phi] \left\lvert{0}\right\rangle \neq 0,\) introduce the eigenstates of momentum $\left\lvert{\pi(\mathbf{p})}\right\rangle$ and the corresponding identity operator \(\mathbb{1} = \int_{-\infty}^{\infty} \frac{d^{3}p}{(2\pi)^{3}} \, \left\lvert{\pi(\mathbf{p})}\right\rangle \left\langle{\pi(\mathbf{p})}\right\rvert ,\) substitute them into the vev of $[Q,\phi]$, write everything in the form of integral, make use of the fact that both the vacuum and the momentum eigenstates have very good property under the translation operator, we arrive at an formula which looks like \(\left\langle{0}\right\rvert [Q,\phi] \left\lvert{0}\right\rangle = \int \frac{d^{3}p}{(2\pi)^{3}} \, e^{ iE(\mathbf{p}=0)t } \times (t\text{-independent})\) but we know that the LHS is time independent, since $Q$ is a conserved charge! Thus the energy of zero momentum state must be \(E(\mathbf{p}=0) = 0\) which means that this state is gapless, physically it means it is the state of a massless particle.</p> <p>Another way to see it is to notice that we can construct the pion state of 3-momentum $\mathbf{p}$ from the vacuum via \(\left\lvert{\pi(\mathbf{p})}\right\rangle := -\frac{2i}{F} \int d^{3}x \, e^{ i\mathbf{p} \cdot \mathbf{x}} J_ {0}(x)\left\lvert{0}\right\rangle ,\) where $\pi$ in $\pi(\mathbf{p})$ is <em>not</em> the canonical momentum, but the pion field, so $\pi(\mathbf{p})$ is the pion field with momentum $\mathbf{p}$. This will be more obvious if you write things explicitly in ladder operators. Anyway, the energy of this state is \(E = E(\mathbf{p}) + 0,\) where $0$ is the vacuum energy. Since $\left\lvert{\pi(0)}\right\rangle$ has energy $0$, energy must goes to zero as the momentum goes to zero, leaving us a massless particle.</p> <p>We present here the famous <code class="language-plaintext highlighter-rouge">Goldstone's Theorem</code>,</p> <p><strong>Goldstone’s Theorem.</strong> Spontaneous breaking of <em>continuous global symmetries</em> implies the existence of massless particles, called Goldstone bosons.</p> <p>But, what does it has to do with chiral transformation? As we will see, massive pions can be approximately seen as the Goldstone bosons, the spontaneously broken symmetry being the chiral symmetry, as we will see later.</p> <p>Usually the first connection between pion and SSB is made via the linear sigma model, which is of a complex scalar field with global $U(1)$ symmetry, with a Mexican hat potential, for more details refer to M. D. Schwartz’s <em>Quantum field theory and the standard model</em>, section <code class="language-plaintext highlighter-rouge">28.2.1</code>. There, the massless scalar field is identified as the pion. But there is no quarks in the model, you can’t introduce chiral symmetry without fermions. So we turn to a more realistic model, concerning the two lightest quarks.</p> <hr/> <p>Consider the QCD Lagrangian with up and down quarks only, both set to be massless, \(\mathcal{L} = -\frac{1}{2} tr F^{2} + \overline{u}i\gamma^{\mu} D_ {\mu} u + \overline{d}i\gamma^{\mu} D_ {\mu} d\) where $u,d$ are the Dirac fermions for up and down quarks respectively. Two things are important for our concern,</p> <ol> <li><em>isospin rotation is a symmetry of the Lagrangian</em>. Recall that isospin treats $(u,d)$ as a doublet under global $SU(2)$ transformation.</li> <li>The <em>separation of left-handed and right-handed components of fermions</em>. Recall that the left-handedness and right-handedness, or chirality, is defined by the different Lorentz transformation behavior. In the massless case, there is no explicit coupling between the left- and right-handed components of the Dirac fermion, therefore we can treat them as independent particles.</li> </ol> <p>In summary, we can now perform the isospin rotation separately, \(\begin{pmatrix} u^{L} \\ d^{L} \end{pmatrix} \to L \begin{pmatrix} u^{L} \\ d^{L} \end{pmatrix},\quad \begin{pmatrix} u^{R} \\ d^{R} \end{pmatrix} \to R \begin{pmatrix} u^{R} \\ d^{R} \end{pmatrix}\) where $L,R$ are different $SU(2)$ matrices. This is the so-called $SU(2)_ {L}\times SU(2)_ {R}$ <code class="language-plaintext highlighter-rouge">chiral symmetry</code> of QCD. Actually the Lagrangian is invariant under not $SU(2)_ {L}\times SU(2)_ {R}$ but $U(2)_ {L}\times U(2)_ {R}$, however we usually extract the two $U(1)$ components of $U(2)$ to form two global transformation, namely the <em>vector</em> and <em>axial vector</em> transformation, the vector transformation rotates both the up and down quark by the same angle, while the axial vector transformation rotates them by opposite angle. All together we can write \(U(2)\times U(2) = SU(2)_ {L} \times SU(2)_ {R}\times U(1)_ {V}\times U(1)_ {A}\) where $V$ stands for vector and $A$ axial vector.</p> <p>There is a rich story about $U(1)_ {V,A}$ symmetries, involving the quantum corrections that breaks the conservation law, etc. But that’s another topic for another day. Right now we focus on the chiral symmetry.</p> <p>You can’t get this from QCD theory itself, but the ground states in nature clearly has a non-zero vev of $\overline{u}u$ or $\overline{d}d$ operator, \(\left\langle \overline{u}u \right\rangle = \left\langle \overline{d}d \right\rangle =: V^{3}\neq 0.\) This is justified by the fact that it implies a spectrum of hadrons. The equivalence between $\left\langle \overline{u}u \right\rangle$ and $\left\langle \overline{d}d \right\rangle$ is also an empirical result. This phenomenological fact is referred to as the `quark condensate.</p> <hr/> <p>Quark condensate, also known as the <code class="language-plaintext highlighter-rouge">chiral condensate</code> is one of the two dominate phenomena, namely <em>confinement</em> and <code class="language-plaintext highlighter-rouge">quark condensate</code>, at low energy. Actually when we wrote $\left\langle \overline{u}u \right\rangle$ we missed some information about the chirality, it should be $\left\langle \overline{u}_ {L} u_ {R} \right\rangle$. But how does this condensate form? I don’t have any precise answer to that question though. Instead, I’ll just list some heuristic reasons.</p> <p>The existence of quark condensate, or equivalent the non-zero $\overline{q}_ {l}q_ {R}$ ($q$ stands for generic quarks) tells us that the vacuum is full of antiquark-quark pairs, for that is what the operators $\overline{q}q$ measure. This is analogous to what happens in a super conductor, where pairs of electrons condensate. In vacuum ,the quark-antiquark pairs must be produced in fluctuation, and is responsible for the confinement since it can break the flux tube. Of course another effect of quark condensate is that they break the chiral symmetry. To see this, recall that under chiral transformation the left-handed and right-handed quarks transform differently, in components we have \(\psi_ {Li}\to \psi'_ {Li} = L_ {ij} \psi_ {Lj},\quad \psi_ {Ri} \to\psi'_ {Ri}=R_ {ij} \psi_ {Rj}\) where $i,j$ labels the flavor. Then the condensate operator $\overline{\psi}_ {Li}\psi_ {Rj}$ transforms as \(\overline{\psi}_ {Li}\psi_ {Rj} \to \overline{\psi'}_ {Li}\psi'_ {Rj} = L^{\dagger}_ {mi}\overline{\psi}_ {Lm} R_ {jn} \psi_ {Rn} ,\) where we better think of $\overline{\psi}$ as $\psi ^\ast$. The quark condensate tells us that \(\left\langle \overline{\psi}_ {Li}\psi_ {Rj} \right\rangle = V^{3}\delta_ {ij}\) Hence \(V^{3}=\left\langle \overline{\psi}_ {Li}\psi_ {Rj} \right\rangle \to \left\langle \overline{\psi}_ {Lm}\psi_ {Rn} \right\rangle L^{\dagger}_ {mi}R_ {jn}= V^{3}(RL^{\dagger})_ {ji},\) we see that the vev is only the same if $L=R$, otherwise the chiral transformation gives it a “phase”. But if $L=R$ then the left-handed and right-handed fermions are transformed the same way, $SU(2)_ {L}\times SU(2)_ {R}$ degenerates to $SU(2)_ {L+R}$, which is just $SU(2)$.</p> <h3 id="back-to-the-lagrangian">Back to the Lagrangian</h3> <p>The pattern that the vacuum breaks the chiral symmetry shows us what the vacuum manifold is like, since \(\left\langle \overline{\psi}_ {Li}\psi_ {R_ {j}} \right\rangle = V^{3}(RL^{\dagger})_ {ji}\) and $RL^{\dagger}\sim SU(2)$, we have \(\left\langle \overline{\psi}_ {Li}\psi_ {R_ {j}} \right\rangle = V^{3}U_ {ij}, \quad U \in SU(2).\)</p> <p>This also answers our previous question about why $U(x)$ transforms as \(U(x) \to L^{\dagger}U(x)R\) under the chiral transform. Note that we have been a little careless when identifying $L$ and $R$, in fact we can <em>define</em> $L^{\dagger}$ and $R$ according to the above formula.</p> <p>Next, Goldstone’s theorem tells us that there exists a massless boson for each broken symmetry. So how many symmetries are broken? The SSB is \(SU(2)\times SU(2) \to SU(2)\) and the number of generator is broken down from $2\times 3=6$ to $3$, so three symmetries (generators of symmetries, to be specific) are broken. Therefore, Goldstone’s theorem tells us that there exists three massless Goldstone bosons.</p> <p>What is the physical nature of these massless bosons? They can only be bound states of quarks. They arise from the fact that the vacuum is “charged” under the quark condensate operator, which simply says that the vacuum gives non-zero expectation value of quark condensate operators. Hence it is better to think of the massless boson as a result of the slow-varying of $U(x)$ in $\left\langle \overline{\psi}\psi \right\rangle=V^{3} U$, where we have neglected as many indices as possible.</p> <p>Then, as mentioned before, we parametrized $U(x)$ by pion fields, \(U(x) = \exp \left( \frac{i}{F_ {\pi}}\pi^{a}\sigma^{a} \right).\) This will be our basic degree of freedom, or building block, of the low-energy effective theory for pions. This seems like a leap of faith to me. It also shows how powerful is the tool of SSB, it enables us to say a lot about pion with very little knowledge about the actual structure of them.</p> <hr/> <p>Now let’s proceed to construct a effective theory of Goldstone modes $U(x)$. This effective model should respect all the low-energy symmetries that survived the SSB, in our case the chiral symmetry.</p> <p>The simplest dynamic, renormalizable Lagrangian in terms of $U$ satisfying the chiral symmetry reads \(\mathcal{L} = \frac{f_ {\pi}^{2}}{4} \mathrm{Tr}\,(\partial_ {\mu}U \partial^{\mu}U^{\dagger}).\) This is the so-called <code class="language-plaintext highlighter-rouge">chiral Lagrangian</code>.</p> <p>The normalization coefficient chosen such that, if you expand $U$ in terms of sin and cos functions, the pion fields adopt the correct dynamic term, \(\mathcal{L} = \frac{1}{2}\partial_ {\mu}\phi \partial^{\mu}\phi + \dots.\) Keep in mind that we are interested in a low energy effective theory, and in low energy the higher derivative terms are always suppressed since the momenta can’t be too high, otherwise we wouldn’t be talking about low energy. However, <em>we can use this fact to expand our Lagrangian terms in terms of momenta</em>, or so-called derivative expansion. At leading order of derivative expansion, except for the kinetic term, we also have \(\mathrm{Tr}\,(U^{\dagger}\partial_ {\mu}U)^{2},\quad (\mathrm{Tr}\,U^{\dagger}\partial_ {\mu}U)^{2}.\) However $\mathrm{Tr}\, U^{\dagger}\partial_ {\mu}U$ vanishes because $U^{\dagger}\partial_ {\mu}U$ takes value in the Lie algebra ${\frak su}(N)$, which has zero trace. Or you can just write $U$ in terms of generators then it become apparent. Furthermore, we can use the property \(U^{\dagger}U=\mathbb{1}\) to write $\mathrm{Tr}\,(U^{\dagger}\partial_ {\mu}U)^{2}$ in terms of $\mathrm{Tr}\,(\partial_ {\mu}U^{\dagger} \partial^{\mu}U)$, so at the leading order of the derivative expansion, we just need the kinetic term.</p> <p>The interesting about the chiral Lagrangian \(\mathcal{L} = \frac{f_ {\pi}^{2}}{4} \mathrm{Tr}\,(\partial_ {\mu}U \partial^{\mu}U).\) is that, albeit the resemblance, it is <strong>not a free theory</strong>. This is because the target space, namely the space where the fields take value in, is not free, it is under some constraint. For example, you can never set $U=0$ since then it wouldn’t be in $SU(2)$. Algebraically speaking, the theory is not free because $U$ are fixed to be special, unitary matrices. Geometrically speaking, the theory is not free because the target space of $U$ is $\mathbb{S}^{3}$, since $SU(2)\cong\mathbb{S}^{3}$.</p> <hr/> <h1 id="classical-skyrme-model">Classical Skyrme Model</h1> <p>Let $N_ {f}=2$. We already mentioned that at low energy, the more derivatives a term has, the less contribution it has. The chiral model has only second order derivatives, but it is poised by the fact that there is no stable soliton solution, something referred to as Derrick’s theorem, as we will discuss below. But first, before we present the problem let me first present the solution, that is to introduce higher-derivative-order corrections. There are only two linearly independent terms like that, depending on <em>whether you take the contraction first than trace later or the other way around</em>, \((\mathrm{Tr}\,(\partial_ {\mu}U\partial^{\mu}U^{\dagger})) ^{2}\tag{term-I}\) and \((\mathrm{Tr}\,(\partial_ {\mu}U\partial_ {\nu}U^{\dagger})) ^{2}\tag{term-II}.\)</p> <p>Skyrme introduced a combination of these two terms, namely term-$\text{II}$ minus term-$\text{I}$, \(\mathrm{Tr}\,\left\{ \partial_ {\mu}U\partial_ {\nu}U^{\dagger} \partial^ {\mu}U\partial^ {\nu}U^{\dagger} - \partial_ {\mu}U\partial^{\mu}U^{\dagger} \partial_ {\nu}U\partial^{\nu}U^{\dagger} \right\} = \frac{1}{2} \mathrm{Tr}\,[\partial _ {\mu}U U^{\dagger},\partial _ {\nu}U U^{\dagger}]^{2}\)</p> <p>We have the simplest Skyrme model \(L = \frac{1}{16}F_ {\pi}^{2} \,\mathrm{Tr}\,(\partial _ {\mu}U\partial ^{\mu}U^{\dagger})+\frac{1}{32e^{2}}\mathrm{Tr}\,[\partial _ {\mu}U U^{\dagger},\partial _ {\nu}U U^{\dagger}]^{2} \tag{1}\) where we have adopted the convention that the derivative acts on the first term followed by it only.</p> <p>The second term is to stabilize the Skyrmion solution. Now, why we need to stabilize it in the first place? It is because, Derrick’s scaling argument, or Derrick’s theorem says that, if the Lagrangian of a scalar field can be written as \(\mathcal{L} = \frac{1}{2} \partial_ {\mu}\phi \partial^{\mu}\phi - V(\phi)\) which is very general, then given a localized wave-pack like classical solution to the equation of motion, call it $\phi_ {0}(x)$, for dimension $\geq 2$, we can always shrink the solution to \(\phi_ {\lambda} (x) = \phi_ {0}(\lambda x), \quad \lambda&gt;1,\) and reduce the energy, thus $\phi_ {0}$ can not be stable. In the case of Skyrme model we don’t have a scalar field but this conclusion still holds, as can be shown by writing down a solution and the energy functional, and study what happens if the soliton is shrunk. Therefore, to stabilize the soliton solution, we need extra terms in the Lagrangian involving the derivatives.</p> <p><strong>Remarks.</strong></p> <ol> <li>There is no first principal reason for including the Skyrme term. It seems quite <em>ad hoc</em>.</li> <li>It is a little counter-intuitive, but the Skyrme term in fact contains only <em>second order time derivative</em>. This can be seen by expanding the bracket, and making use of the properties of $SU(2)$. This is important for second order time derivative is what we want in deriving the equation of motion. And the Skyrme term is unique in this sense.</li> <li>The coupling $e^{2}$ is dimensionless.</li> </ol> <hr/> <p>Recall that $U$ is a SU(2) matrix, under the chiral transformation \(U \to LUR^{-1}, \quad L, R \in SU(2).\) $F_ {\pi}=186 \text{ MeV}$ is the pion decay constant, $e^{2}$ is a dimensionless parameter. The last term, the trace of a commutator, was first introduced by Skyrme to stabilize the soliton solution.</p> <p>To simplify the notation, we denote the Noether current associated to the $SU(N_ {f})_ {L}$ by \(L_ {\mu} = U^{\dagger}\partial_ {\mu}U.\) The Noether current associated to the right-handed $SU(2)$ is left as a homework, you’ll see it is related to $L_ {\mu}$ by a hermitian conjugation.</p> <p>With some manipulation, you can see that the <em>static</em> energy can be written entirely in terms of $L_ {\mu}$, \(E = \frac{F^{2}_ {\pi}}{4} \int d^{3}x \, \left( L_ {i}L^{\dagger}_ {i} - \frac{1}{4e^{2}F^{2}_ {\pi}}(\epsilon_ {ijk}L_ {j}L_ {k}) (\epsilon_ {imn}L^{\dagger}_ {m}L^{\dagger}_ {n}) \right).\)</p> <p>We now use Bogomolnyi trick, namely to complete the square, \(E = \frac{F^{2}_ {\pi}}{4} \int d^{3}x \, \mathrm{Tr}\,\left\lvert L_ {i}\mp \frac{1}{2eF_ {\pi}}\epsilon_ {ijk}L_ {j}L_ {k} \right\rvert ^{2} \pm \frac{F_ {\pi}}{4e}\int d^{3}x \, \epsilon_ {ijk}L_ {i}L_ {j}L_ {k}.\) Then, follow the standard procedure, we need to show that the second term is actually a <em>topological constant</em> (or topological charge), which will justify the statement that the solution to the EOM is given by the field configuration that sets the square term in $E$ zero.</p> <hr/> <p>A field configuration is a map \(U(x): \mathbb{R}^{3}\to SU(2)\) where $\mathbb{R}^{3}$ is the physical space. If we insist that the field goes to the same vacuum at the spatial boundary, then we can compactify $\mathbb{R}^{3}$ to $\mathbb{S}^{3}$. This is a reflection of the idea that the kinds of function you define on a space will in tern decide the topological structure of the supporting space itself. So now the field configuration is a map \(U(x): \mathbb{S}^{3}\to SU(2)\cong \mathbb{S}^{3},\) which is classified by the homotopy group $\pi_ {3}(\mathbb{S}^{3})\cong\mathbb{Z}$. With some mathematics we can show that, the winding number $B$ is given by \(B = \frac{1}{24\pi^{2}} = \int d^{3}x \, \epsilon_ {ijk}\mathrm{Tr}\, (U^{\dagger}\partial_ {i} U U^{\dagger}\partial_ {j} UU^{\dagger}\partial_ {k} U)\) where the derivative acts on the next matrix only.</p> <p>We can go further and write down a local current associated to $B=\int \,J^0$, \(J^{\mu} = \frac{1}{24\pi^{2}}\epsilon^{\mu \nu \rho \sigma} \mathrm{Tr}\,(U^{\dagger}_ {\nu}UU^{\dagger}_ {\rho}UU^{\dagger}_ {\sigma}U)\) which obeys $\partial_ {\mu}J^{\mu}=0$ due to the antisymmetric properties.</p> <p>What is the physical interpretation of $J^{\mu}$? The only candidate seems to be the vector transformation $U(1)$. <em>If we identify $J^{\mu}$ with $J_ {V}$, then we are identifying the solitons with baryons, for $J_ {V}$ is the baryon number current</em>. This is probably why we regard soliton in Skyrme model as candidates for baryons.</p> <p><strong>Remarks.</strong></p> <ol> <li>The baryon number changes the sign under $U \leftrightarrow U^{\dagger}$ transformation. Something similar goes with the left and right current $L_ {\mu},R_ {\mu}$.</li> <li>We state without proof that $B[U_ {1}U_ {2}]=B[U_ {1}]+B[U_ {2}]$.</li> </ol> <hr/> <p>Now having the expression for $B$, we can write the static energy in terms of it, \(E \geq \frac{6\pi^{2}F_ {\pi}}{e}B.\) When the equality is taken, we say the solution saturates the Bogomolnyi condition, and corresponding solitons (with nontrivial topology, namely $B\neq 0$) are called <code class="language-plaintext highlighter-rouge">Skyrmions</code>.</p> <hr/> <p>To find the Skyrmion solution, one would naively try to solve the first-order differential equation. \(0=L_ {i}\mp \frac{1}{2eF_ {\pi}}\epsilon_ {ijk}L_ {j}L_ {k}.\) There is nothing wrong with this approach, actually it is the standard approach in solving solitons in general, however in our particular case this doesn’t seem to help. <em>One can show that there is no solution to the firsts order differential equation.</em> Instead we must turn to the full equation of motion, which is of second order, \(\boxed{ \partial_ {\mu}L^{\mu} = \frac{1}{4F^{2}_ {\pi}e^{2}}\partial_ {\mu}[L_ {\nu},[L^{\mu},L^{\nu}]]. } \tag{EoM for Skyrmion}\)</p> <p>We adopt the “spherical symmetric” ansatz (<code class="language-plaintext highlighter-rouge">hedgehog ansatz</code>) \(\boxed{ U_ {0}(x) = \exp[iF(r) \vec{\tau} \cdot \hat{x}], \quad F(r){\Large\mid}_ {r=0} =\pi,\,F(r){\Large\mid}_ {r=\infty} =0. } \tag{hedgehog}\) where $\hat{x}$ is the unit vector in the direction of $\mathbf{x}$. The reason why I put quotes around “spherical symmetric” is that, the hedgehog solution is not really spherical symmetric like two sphere. To make it easier to visualize it, imagine an actual prickly hedgehog whose spines gives the direction of $U_ {0}(x)$, $\hat{x}$ in the exponent to be more specific.</p> <p><img src="/img/hedgehog.png" alt=""/></p> <p>When speaking of rotation there are two things we could rotate,</p> <ul> <li>the physical space $\mathbf{x}$, a point in space is rotated to another point. They corresponds to the position of the spines of a hedgehog;</li> <li>the inner space, or isospace of the $SU(2)$ matrix $U = \exp(i\theta_ {a}\tau_ {a})$ where $\theta_ {a}$’s are angle-like parameters that parameterize the matrix. They corresponds to the directions of hedgehog spines.</li> </ul> <p>When rotating a hedgehog, to fully realized the invariance, we must rotate both the position of the spines and their directions at the same pace! Of course this is taken care of automatically when you rotate a real hedgehog, because both the spines and their directions live in the same space, the physical space. However, regarding a hedgehog solution, the spines live in a different space, so remember to rotate it as well if you want the invariance!</p> <p>We proceed by substituting the hedgehog ansatz into the Lagrangian, Legendre transform the Lagrangian to energy functional, we have the expression of the soliton energy in terms of the profile function $F(r)$: \(M = 4\pi \int_{0}^{\infty} dr \, r^{2}\left\{ \frac{1}{8}F_ {\pi}^{2}\left[ (\partial _ {r}F)^{2}+2\frac{\sin ^{2}F}{r^{2}} \right] + \frac{1}{2e^{2}} \frac{\sin ^{2}F}{r^{2}} \left[ \frac{\sin ^{2}F}{r^{2}}+2(\partial _ {r}F)^{2} \right]\right\} , \tag{2}\)</p> <p>Then we can obtain the variational equation from it, then we can further solve it numerically. Introduce the dimensionless parameter $\widetilde{r}:= eF_ {\pi}r$, the equation of motion reads \(\left( \frac{1}{4}\widetilde{r}^{2}+2\sin ^{2}F \right)F'' + \frac{1}{2} \widetilde{r} F' + \sin (2F) F'^{2}- \frac{1}{4}\sin(2F) - \frac{\sin ^{2}F \sin 2F}{\widetilde{r}^{2}}=0.\) Next, we can use numerical methods to solve it, with suitable boundary conditions, which are</p> <ul> <li>$F(x){\Large\mid}_ {x=0}=\mathbb{Z}\pi$, to make sure the Skyrmion is well defined at the origin,</li> <li>$F(x){\Large\mid}_ {x=\infty}=0$, otherwise the total energy won’t be finite.</li> </ul> <p>$F(x)$ is a monotonically decreasing function, and <em>the energy of this solution turns out to be about 25% higher than the Bogomolnyi bound</em>. So the Bogomolnyi bound can not be saturated for Skyrmions.</p> <p>The winding number can also be written in terms of $F$, \(B = - \frac{2}{\pi}\int_{0}^{\infty} dr \, F' \sin ^{2}F.\) In deriving the above formulae, it is helpful to adopt the angle parametrization of $\mathbb{S}^{3}$. Take the hedgehog solution to above formula we get $B=1$, as expected.</p> <hr/> <p>Besides the SSB of chiral symmetry, there is another level of symmetry breaking here, where the original $SU(2)_ {L}\times SU(2)_ {R}$ symmetry is broken by the Skyrmion solution. If we insist that \(U(x) {\Large\mid}_ {x\to \infty }= \mathbb{1}\) then the symmetry is broken down to \(SU(2)_ {L}\times SU(2)_ {R} \to SU(2)_ {V},\) for if left-handed and right-handed rotations are different, then $\mathbb{1}$ will not be preserved.</p> <hr/> <p>We can also add the mass term to the pion field. The physical vacuum is set to be $\mathbb{1}$, thus the mass term should vanish at $U=1$. Then of course it should respect the $SU(2)_ {L}\times SU(2)_ {R}$ symmetry. The mass term with correct normalization is \(\mathcal{L}_ {\text{mass}} = \frac{m^{2}_ {\pi} F^{2}_ {\pi}}{2}\mathrm{Tr}\,(U+U^{\dagger}-2\cdot\mathbb{1}).\)</p> <h3 id="how-to-parametrized-the-rotation">How to parametrized the rotation</h3> <p>Recall that in the presence of a Skyrmion, the symmetry is broken to $SU(2)_ {V}$, to preserve $U(X)=\mathbb{1}$ at spatial infinity. We can include the spatial rotational symmetry, we have the symmetry group \(SU(2)_ {\text{rot}} \times SU(2)_ {V}. \tag{Skyrmion symmetry group}\) The single Skyrmion is not invariant under either of these $SU(2)$ groups separately. However, <em>it is invariant under the diagonal $SU(2)$ which acts simultaneously as a spatial and flavor rotation</em>. To be specific, a spatial rotation will change the Skyrmion configuration (think of it as a hedgehog), but we can use isospin rotation to rotate it back, for both the spatial and isospin rotations will rotate the hedgehog.</p> <p>The subgroup of the Skyrmion symmetry group which acts non-trivially on the Skyrmion solution can be used to generate new solutions. These are trivially related to the original, and just change its embedding in the target space. Nonetheless, they have important consequences, as we will see below.</p> <p>Now we can make use of the <em>global</em> $SU(2)_ {\text{rot}}$ symmetry of the Lagrangian, namely if $U_ {0}$ is a soliton solution, then \(U = A U_ {0} A^{-1}\) is also a soliton solution. Thus we have a space of solutions parametrized by $A$, by parametrization we mean that there is a map \(A\in SU(2) \to \text{rotational space of soliton solutions}.\)</p> <hr/> <p>A generic solution would have nine collective coordinates, or moduli, including 1) three spatial translation, 2) three spatial rotation and 3) three isospin rotation. The situation is a little special for $B=1$ Skyrmion hedgehog solution because there the spatial rotation and isospin rotation coincide and we just have $6$ moduli in total.</p> <p>The <code class="language-plaintext highlighter-rouge">numerical</code> configuration for low charged Skyrmions can be found at <a href="https://ncatlab.org/nlab/show/skyrmion">Skyrmion nLab</a>, where the baryon density is plot. I will not dwell into details, just a few comments.</p> <p><strong>Remarks.</strong></p> <ul> <li>For $B\geq 2$ there exists only discrete symmetries. For instance $B=3$ has tetrahedral symmetry and $B=4$ has cubic symmetry.</li> <li>They do not saturate the Bogomolnyi bound.</li> </ul> <h3 id="the-rational-map-ansatz">The rational map ansatz</h3> <p>The rational map method surely looks interesting and important, but maybe it is not needed for my current project, so I’ll postpone it to later. This chapter is a place-holder.</p> <h3 id="some-mumble-jumble">Some mumble-jumble</h3> <p>This part is to supposed to be overlooked by readers. I wrote it only for fun, I don’t think it is leading anywhere.</p> <p>Since the $A$ parameterize the isospin space, maybe we can regard it as some kind of moduli space, we now have a means to describe the motion of a soliton. Every point in the moduli space corresponds to a specific soliton solution, then a trajectory of a soliton would ideally be denoted by a curve in the moduli space. The converse would also hold in the ideal case, namely a curve in the moduli space would give us uniquely a soliton trajectory. It would be nice if there exists a one-to-one correspondence between the curves and the soliton trajectory. It should indeed be the case if no discrete symmetry jumps out and ruins everything. To be more specific, one way to ruin this bijection between curves in the moduli space and actual soliton trajectory is that if some discrete symmetry is modded out in constructing the moduli space, then there may exist special points where the discrete symmetry can make the soliton trajectory not uniquely defined, these are called <code class="language-plaintext highlighter-rouge">stacky points</code>, put it short, there exists more than one way for a curve to cross a stacky point. <em>Could there be stacky points in the moduli space of Skyrmions?</em> Maybe a related question is, what if the gauge group is $SU(N) / Z_ {N}$ instead of $SU(N)$? Changing the gauge group to $SU(N) / Z_ {N}$ has many consequences, one of them is we have introduced an extra <em>discrete symmetry</em>, $Z_ {N}$ symmetry of the gauge group, therefor to the moduli space, and this could make the moduli space not fine anymore, assuming the moduli space of Skyrmion is fine in the first place. Recall that</p> <p><strong>Fine moduli space.</strong> A <code class="language-plaintext highlighter-rouge">fine moduli space</code> is a space $M$, such that</p> <ol> <li>the points in $M$ are in 1-2-1 correspondence with <em>isomorphism classes</em> of the objects we are studying;</li> <li>(<code class="language-plaintext highlighter-rouge">technical condition</code>) For every family $\mathcal{F}$ over a topological (parameter) space $T$, denoted by $\mathcal{F} / T$, the associated moduli map $T\to M$, which maps the point $t \in T$ to the isomorphism class of the family member $\mathcal{F}_ {t}$, is <em>continuous</em>;</li> <li>Every <em>continuous map</em> from $T$ to $M$ is the moduli map of some family parametrized by $T$;</li> <li>if the two families have the same moduli map, they are isomorphic families.</li> </ol> <h1 id="quantization-of-skyrme-model">Quantization of Skyrme Model</h1> <p>Before quantizing the theory, let’s take a look at the symmetry of the Lagrangian.</p> <ul> <li>Charge symmetry $U\to U^{\dagger}$. This is guaranteed by the cyclic property of the trace in the Lagrangian.</li> <li>Parity symmetry $x\leftrightarrow (-x)$.</li> <li>Time reversal symmetry, $t\leftrightarrow(-t)$.</li> </ul> <p>So, all of the $C,P$ and $T$ symmetries are separately preserved. This is actually not a good thing, because we learnt from experiments that in real world this is not the case! In real life only $CPT$ together is preserved. So how can we modify our model to fit it?</p> <p>To my surprise, the solution has to be found in 5-dimension spacetime.</p> <hr/> <p>To break the respective C,P,T conservation but preserve the CPT symmetry, in 1971, J. Wess and B. Zumino introduced a new term to the Skyrmion in 5-dimension.</p> <p><strong>Wess-Zumino correction.</strong> \(S_ {\text{WZ}}:= \alpha \Gamma, \quad \Gamma = - \frac{i}{240\pi^{2}}\int _ {M^{5}} d^{5}x\, \epsilon^{\mu \nu \rho \sigma \tau}\mathrm{Tr}\,(U^{\dagger} \partial_ {\mu}U \partial_ {\nu}U^{\dagger} \partial_ {\rho}U \partial_ {\sigma}U^{\dagger} \partial_ {\tau}U)\) where $\alpha$ is a factor first introduced by Witten in 1983. The integral takes place on $M^{5}$, some 5-dimensional manifold. The boundary of $M^{4}$ is set to be our familiar 4-dimensional Minkowski space $M^{4}$. Now, the integrand is actually a total derivative $d(\dots)$, so the integral can be rewritten as a surface integral, namely an integral in the 4-dimensional Minkowski space, \(\Gamma \propto \int _ {\partial M^{5}=M^{4}} d\Sigma_ {\tau} \, \epsilon^{\mu \nu \rho \sigma \tau}\mathrm{Tr}\,(\partial_ {\mu}U\partial_ {\nu}U^{\dagger}\partial_ {\rho}U\partial_ {\sigma}U^{\dagger}).\) The variation of the above action will generate a term in 4-dimension Minkowski space which looks like \(\epsilon^{\mu \nu \rho \sigma \tau}\mathrm{Tr}\,(\partial_ {\mu}U\partial_ {\nu}U^{\dagger}\partial_ {\rho}U\partial_ {\sigma}U^{\dagger})\) and this is what we needed to break respective $C,P$ and $T$ symmetry.</p> <p>Now the new action will be like \(S_ {\text{new}} = S_ {\text{Skyrme}} \pm S_ {\text{WZ}}.\)</p> <p>I will not go into the details but just point out that, in order to make the new-introduced 5 dimensional bulk irrelevant to our theory, we need our theory unchanged when moving from one bulk $M^{5}$ to another $M’^{5}$, and to do that we can choose $\alpha \in\mathbb{Z}$ and $M^{5}\cup M’^{5}=\mathbb{S}^{5}$.</p> <p>The Wess-Zumino term is topological. It seems that a typical characteristic of topological operators is that, they don’t contribute to the dynamics, they don’t change the equation of motion, they have no contribution to the locally conserved currents, but they do change the discrete symmetry properties of the model. For example, it can be shown (although we won’t do it here) that the Wess-Zumino term has no contribution to the baryon current.</p> <p>I’ll stop here because to be honest I don’t understand how this business really goes.</p> <hr/> <p>Furthermore, the anomaly matching between QCD (UV theory) and Skyrme model (IR theory) tells us that $\alpha=N_ {c}$ where $N_ {c}$ is the color number. Therefore, if we set the baryon charge of a single quark to be $\frac{1}{N_ {c}}$ then one can show that the topological number is actually the same as the baryon number. This gives us yet another “justification” to identify the Skyrme number to the baryon number.</p> <p>However, there is another issue regarding the Skyrmion-baryon identification, namely the spin statistics issue. Are we sure that the topological soliton, Skyrmion, has the same spin-statistics with a baryon? Baryons are fermions (consisting of three quarks), in the mean while it is not straightforward (at least for me) to see whether the Skyrmion is a boson or fermion. To answer this question, we need to take a closer look at the group structure.</p> <p>We worked with $SU(2)_ {f}$ flavor group, now we need to go to a larger group $SU(\geq3)$ to fully exploit the Wess-Zumino term. The hedgehog ansatz still works if we regard $SU(2)$ as a subgroup of $SU(\geq3)$, \(U_ {\text{Hedge}} = \begin{pmatrix} U_ {\text{Hedge},SU(2)} &amp; 0 \\ 0 &amp; \mathbb{1}_ {N_ {f}-2} \end{pmatrix}.\)</p> <p>Surprisingly, the method Witten used has a lot to do with the rotation, or spin, property of a $B=1$ Skyrmion, I will only list the gist of it in the next section.</p> <h3 id="forward-spin">Forward, Spin!</h3> <p>The method Witten adopted to discern the spin-statistics of a single Skyrmion works as follows.</p> <ul> <li>We know that after a rotation of $2\pi$, bosons return to itself but fermions gets an extra factor of $-1$. The latter fact is always illustrated by rotating your palm, since the palm is connected to your shoulder (I hope), rotating it by $2\pi$ will end up with a strange twist of your arm, but you can restore your arm by rotating your arm by another $2\pi$. Interested readers can look for illustration videos on line. So we can rotate the Skyrmion by $2\pi$, see what happens.</li> <li>In order to tell whether Skyrmion rotation gives an extra minus one or not, we need to look at the partition function \(Z = \int D\phi \, e^{ iS } = \sum_ {n\in \mathbb{Z}} \left\langle{\text{Skyr}}\right\rvert e^{ iE_ {n}t }\left\lvert{\text{Skyr}}\right\rangle\) where $t$ is the total time that the system exists. In Euclidean spacetime, we can neglect the higher energy contributions if we take $t$ to be large enough, which it usually is. Then Witten changed the final state to Skyrmions rotated by $2\pi$, let’s denote it by $\left\lvert{\text{Skyr}}\right\rangle_ {2\pi}$, therefore he calculated \(Z_ {\text{twist}} = \int_ {\dots} D\phi \, e^{ iS }= _ {2\pi}\left\langle{\text{Skyr}}\right\rvert e^{ iE_ {0}t }\left\lvert{\text{Skyr}}\right\rangle\) where the path integral is given by integrating over the correct field configuration, which starts from a Skyrmion and ends up with a rotated (by $2\pi$) Skyrmion. Then you can tell from the final result whether $\left\lvert{\text{Skyr}}\right\rangle_ {2\pi}=\left\lvert{\text{Skyr}}\right\rangle$ or $\left\lvert{\text{Skyr}}\right\rangle_ {2\pi}=-\left\lvert{\text{Skyr}}\right\rangle$, the minus sign is everything.</li> <li>The above step is realized by constructing a time-dependent field configuration, which represents a Skyrmion slowly, adiabatically rotates. This is again achieved by cleverly substitute the spatial rotation by a isospin rotation, or inner rotation if you will. This is done by introducing a matrix that sandwiches the Skyrmion, \(U(t) = A(t) U_ {0} A(t)^{-1}.\)</li> </ul> <p><em>All of the above can’t be done without Wess-Zumino term.</em></p> <p>Eventually, Witten found that if $N_ {c}$ is even then the Skyrmion is a boson, if $N_ {c}$ is odd then fermion. In QCD we have $N_ {c}=3$ thus the Skyrmion is to be interpreted as a fermion. This is yet another justification to identify the Skyrmion with baryons.</p> <p>I have to say that, even with all the justifications we introduced before, I am still not convinced that Skyrmions are baryons, for they have entirely different inner structure, electric charges, etc. Bite me.</p> <h3 id="zero-mode-quantization">Zero-mode quantization</h3> <p>There are various different ways to quantize a Skyrmion at different level, the zero-mode quantization may be the simplest one. This is the quantization on the zero mode space, the “coordinates” are upgraded to operators.</p> <p>Zero modes are roughly speaking the “flat” directions where you can deform the Skyrmion without increasing its energy. So you can imagine it’s like rolling a ball (Skyrmion) on a flat ground, where all the three directions corresponds to zero modes. Let’s take the by-now familiar hedgehog solution for the starting point. It has three translational zero modes, but we are not yet interested in them. It also has three spatial rotation symmetries and three isospin rotation symmetries, they are kind of the same thing because you can always achieve the same result by using either one of them, so effectively we only have three independent rotational symmetry. We will work with the isospin, or internal rotation symmetry $SU(2)_ {V}$, for they are easier to be realized.</p> <p>The iso-rotation is parametrized by three collective coordinates $\theta_ {1,2,3}$, in the form \(U_ {H}(r,t) = A(t)U_ {H}(r)A^{\dagger}(t)\) where $H$ for hedgehog, $A=\exp(i \theta_ {i}T^{i})$ and $T^{i}$ are the generators.</p> <p>Given $U(t)$ in terms of $A(t)$, in theory we can substitute it in the Lagrangian, obtaining a Lagrangian (Hamiltonian, whatever) in terms of $A(t)$. We can then try to diagonalize the Hamiltonian, find the eigenstates. We can then study the spin and isospin of these eigenstates by acting the corresponding operators on it. The eigenstates with proper spin and isospin will correspond to the nucleon and delta.</p> <p>To be honest, beside having the same quantum numbers, I don’t see any reasons why the soliton solutions should be regarded as nucleons, or for that matter any particle we observe in real life. This whole soliton-nucleon identification is supposed to be taken with a grain of salt, I guess.</p> <p>So, substituting $U = A(t)U_ {0}A^{-1}(t)$ in Eq. (1), by the end of the day we get \(L = -M + \lambda \mathrm{Tr}\,(\partial _ {0}A \partial _ {0}A^{-1}), \tag{3}\) where $M$ is the soliton mass defined in Eq. (2) and \(\lambda = \frac{2}{3} \pi \left( \frac{1}{e^{3}F_ {\pi}} \right)\Lambda,\quad \Lambda = \int d\widetilde{r} \, \widetilde{r}^{2} \sin ^{2} F \left[ 1+4\left( F'^{2} + \frac{\sin ^{2}F}{\widetilde{r}^{2}} \right) \right] .\) Numerically, Adkins et al. found that $\Lambda = 50.9$. $\Lambda$ is the equivalence of the moment of inertia in classical mechanics.</p> <p>Recall the canonical quantization of quantum mechanics is to endow the generalized coordinate $q$, its canonical momentum $p = \partial L / \partial \dot{q}$ a canonical commutation relations, $[p,q]=\text{something}$. How we do it to $A$, a $SU(2)$ matrix?</p> <p>We can write \(A = a_ {0} + i \vec{a}\cdot \vec{\tau},\quad a_ {\mu}a_ {\mu}=1,\quad \mu=0,1,2,3.\) where the summation convention has been adopted. It is a general property for all $SU(2)$ matrices. In terms of $a$’s the Lagrangian becomes \(L = -M + 2\lambda \dot{a}_ {\mu}\dot{a}_ {\mu}\tag{4}.\) Now we can introduce the canonical momentum in the standard way, \(\boxed { \pi := \frac{\partial L}{\partial \dot{a}} \implies \pi_ {i} = 4\lambda \dot{a}_ {i} }\) and the Hamiltonian becomes \(H = \pi_ {i}a_ {i}-L = M + \frac{1}{8\lambda}\pi_ {i}\pi_ {i}.\) Recall that in quantum mechanics, during canonical quantization we re-wrote the canonical momentum as a difference operator with respect to $x$, $p := -i \partial_ {x}$, we can do the same to the $a_ {i}$ and $\pi_ {i}$, writing \(\boxed { \pi_ {i} \to -i \partial /\partial a_ {i}. }\) then we can write down $H$ in the “moduli-space representation” \(\boxed { H = M + \frac{1}{8\lambda} \sum_ {i=0}^{3}\left( -\frac{\partial^{2}}{\partial a_ {i}^{2}} \right),\quad a_ {i}a_ {i}=1. }\)</p> <p>The constraint suggests that $\sum_ {i}\partial^{2} / \partial a_ {i}^{2}$ should be interpreted as the Laplacian $\nabla^{2}$ on $\mathbb{S}^{3}$. And, Laplacian on a sphere is just the angular-momentum operator in disguise!</p> <p><strong>Laplacian on a $\mathbb{S}^{3}$</strong></p> <p>The Laplacian on a three-sphere, also known as the Laplace-Beltrami operator, acts on a scalar function $f(x_ {1}, x_ {2}, x_ {3}, x_ {4})$ with the constraint $x_ {1}^2 + x_ {2}^2 + x_ {3}^2 + x_ {4}^2 = 1$. This operator is usually defined in terms of the spherical coordinates on the 3-sphere, rather than the Cartesian coordinates $(x_ {1}, x_ {2}, x_ {3}, x_ {4})$.</p> <p>When the Laplacian is applied to a scalar function on a three-sphere, it is most commonly written in a form like this (in spherical coordinates):</p> <p>\(\begin{align} \Delta f &amp;= 1/\sin²(χ) [(\sin²(\chi) f')' - \sin(\chi) \cos(\chi) f'] + 1/(\sin²(\chi) \sin²(\theta)) (\sin(\theta) f_ {\phi \phi}) \\ &amp;\;\;\;\, + 1/(\sin(\chi) \sin(\theta)) f_ {\theta \theta} \end{align}\) where:</p> <ul> <li>$f’$ denotes the derivative of f with respect to $\chi$</li> <li>$f_ {\phi \phi}$ denotes the second derivative of f with respect to $\phi$</li> <li>$f_ {\theta \theta}$ denotes the second derivative of f with respect to $\theta$</li> <li>$\chi$ is the “azimuthal” angle ranging from 0 to $\pi$</li> <li>$\theta$ is the polar angle ranging from 0 to $\pi$</li> <li>$\phi$ is the azimuthal angle ranging from 0 to $2\pi$</li> </ul> <p>The eigenvalue problem for the Laplacian on the three-sphere is then given by: \(\Delta f = \lambda f\) The eigenvalues $\lambda$ of the Laplacian operator on a 3-sphere are given by: \(\lambda=l(l+2)\) where $l$ is a nonnegative integer. These eigenvalues can be derived by solving the eigenvalue problem using separation of variables and spherical harmonics.</p> <p>Note that the eigenfunctions (the functions f that satisfy the above equation) are the spherical harmonics on the 3-sphere, which are the generalization of the usual spherical harmonics on a 2-sphere. The parameter l is the degree of the spherical harmonics, which corresponds to the total angular momentum quantum number in quantum mechanics.</p> <hr/> <p>I state without truly understanding that, by analogy with usual spherical harmonies (where you go to the polar coordinates system and solve the eigenfunction equation, blahblahblah), the eigenfunctions to the Hamiltonian are <em>traceless symmetric polynomials</em> in the $a_ {i}$. (I don’t know how to prove this) For example, one eigenfunction to the Laplacian is $(a_ {0}+ia_ {1})^{l}$, with \(-\nabla^{2}(a_ {0}+ia_ {1})^{l} = l (l+2) (a_ {0}+ia_ {1})^{l}.\) Such a wave function has spin and isospin equal to $l / 2$. To see this, consider the spin operator \(I_ {k} = \frac{1}{2} i \left( a_ {0} \frac{\partial}{\partial a_ {k}}-a_ {k} \frac{\partial}{\partial a_ {0}} -\epsilon_ {klm}a_ {l} \frac{\partial}{a_ {m}} \right)\) which generalized the 3D angular momentum operator \(\mathbf{l} = \mathbf{x} \times \mathbf{p}, \quad l_ {k} = -i \epsilon_ {klm}x_ {l} \frac{\partial}{\partial a_ {m}}.\) The isospin operator is defined as \(J_ {k} = \frac{1}{2} i \left( -a_ {0} \frac{\partial}{\partial a_ {k}} + a_ {k} \frac{\partial}{\partial a_ {0}} -\epsilon_ {klm}a_ {l} \frac{\partial}{a_ {m}} \right)\) where the terms involving zero component gets an extra minus sign in comparison to the spin operator.</p> <p>Then the rest of the story is just standard QM, not even QFT now. Having the operator, we find the eigenstates and eigenvalues. The eigenstates are $\left\lvert{j,m,m’}\right\rangle$ with eigenvalues \(I^{2}\left\lvert{j,m,m'}\right\rangle = -m(m+2)\left\lvert{j,m,m'}\right\rangle\)</p>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">The Phase Space</title><link href="https://baiyangzhang.github.io/blog/2023/The-Phase-Space/" rel="alternate" type="text/html" title="The Phase Space"/><published>2023-06-21T00:00:00+00:00</published><updated>2023-06-21T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/The-Phase-Space</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/The-Phase-Space/"><![CDATA[<p>The first step to understand quantization fully, especially the geometric quantization, is to understand the familiar phase space but from a more geometric point of view.</p> <h3 id="the-phase-space-in-mechanics">The Phase Space in Mechanics</h3> <p>Let $M$ be the n-dimensional <code class="language-plaintext highlighter-rouge">configuration space</code> of a dynamical system, with local coordinates $q^{1},\dots,q^{n}$ which are the generalized coordinates. The Lagrangian is a function of $q$ and $\dot{q}$, which are the generalized velocity. For example $q$ would be an angle and the generalized velocity would be the angular velocity. We have \(L = L(q,\dot{q}).\)</p> <p>But there is a more beautiful way to say the same thing, if you are familiar with tangent bundles $TM$, then you see that the <em>Lagrangian is nothing but a real-valued function defined on the tangent bundle to $M$</em>. A point in the tangent bundle is given by a $2n$-tuple, consisting of a $n$-tuple $q^{i}$, which pinpoint a position on the base space, and another $n$-tuple of generalized velocity $\dot{q}$, which gives as an element of the fiber at that point. In face $\dot{q}$ in this sense should be written as \(\dot{q} = \frac{dq}{dt} \partial _ {q}, \quad \dot{q}(f) = \frac{df}{dt} = \frac{dq}{dt} \partial _ {q}(f),\) then it is obvious that $\dot{q}$ is a tangent vector to $M$.</p> <p>In summary \(L : TM \to \mathbb{R}.\)</p> <p>Note that $q$ and $\dot{q}$ are regarded us two <em>independent variables</em>. When given a specific path $q(t)$ then we can connect $\dot{q}$ with $q$ by the time derivative.</p> <p>Now consider the transition from the Lagrangian to the <code class="language-plaintext highlighter-rouge">Hamiltonian formulation</code> of dynamics. Hamilton defined the functions, so-called <code class="language-plaintext highlighter-rouge">canonical momenta</code> or <code class="language-plaintext highlighter-rouge">generalized momenta</code> \(p_ {i}(q,\dot{q}) := \frac{\partial L}{\partial \dot{q}^{} {i}},\) note that the position of indices are balanced on both sides. This is <strong>not</strong> merely a change of basis of $TM$, one way to see it is to notice that $p^{i}$ has an upper index but $p_ {i}$ has a lower index, they got different transition functions. $p_ {i}$ actually live in the <code class="language-plaintext highlighter-rouge">cotangent space</code>, the $n$-tuple $p_ {1},\dots,p_ {n}$ form a <code class="language-plaintext highlighter-rouge">covector</code>! When put together, the $p$s and $q$s are the local coordinates of a <em>cotangent bundle to $M$</em>. The construction of canonical momentum can be regarded as a map \(p: TM \to T^{\ast }M.\)</p> <p>This space $T^{\ast}M$ in mechanics is called the <code class="language-plaintext highlighter-rouge">phase space</code> of the dynamical system. A short summary:</p> <ul> <li>$M$, the $n$-dimensional base space, is the <code class="language-plaintext highlighter-rouge">configuration space</code>;</li> <li>$TM$, the <code class="language-plaintext highlighter-rouge">tangent bundle</code> to $M$, is the space of all the generalized velocities at all the point of $M$, is called the tangent bundle. The Lagrangian is a $\mathbb{R}$-valued functions on $TM$;</li> <li>$T^{\ast}M$, the <code class="language-plaintext highlighter-rouge">cotangent bundle</code> to $M$, is called the <code class="language-plaintext highlighter-rouge">phase space</code>. The Hamiltonian can be regarded as a $\mathbb{R}$-valued function on the phase space, similar to the Lagrangian.</li> </ul> <p>$M$ and $TM$ exist as soon as the base manifold $M$ exists, but the identification between vectors (elements of $TM$) and covectors can not be done without introducing the Lagrangian $L$, since $p := \partial L / \partial \dot{q}$. We usually call the Lagrangian the dynamics.</p> <p>Since we have lower and upper indices, a natural question to ask is, what can be used to lower or raise the indices? Recall that for a generic manifold with vectors and covectors, the <code class="language-plaintext highlighter-rouge">metric</code> $g_ {ij}$ or its inverse $g^{ij}$ can be used to do that, then we can guess there also exists some kinds of metric on the configuration space $TM$. Indeed, some simple mathematical manipulations show that the metric can be read off from \(p_ {i}=g_ {ij} \dot{q}^{j} \implies g_ {ij} = m \delta_ {ij}\text{ provided } L = \frac{1}{2} m \dot{q}^{2}.\) The Lagrangian can also be written in terms of the metric \(L = T - V = \frac{1}{2} g_ {ij}\dot{q}^{i}\dot{q}^{j}-V.\) In short, the metric is given by the mass matrix.</p> <p>For massless particles, there is a clever way to write down the Lagrangian, but it is out of the scope of this note, interested readers can refer to A. Zee’s book on general relativity.</p> <p>In general, given a manifold $M$, both the tangent bundle $TM$ and the cotangent bundle $T^{\ast}M$ comes for free. However there is no natural way to identify a vector with a covector. For this identification, we need some extra structure, namely the metric or the dynamics, a.k.a. the Lagrangian.</p> <h3 id="the-poincare-1-form">The Poincare $1$-form</h3> <p>There are geometric objects that live naturally on $T^{\ast}M$, not on $TM$. Before introducing a metric, or Lagrangian, such things will only exist on $T^\ast M$, not $TM$.</p> <p>Recall that $1$-form is simply another name for covector. We now think of $T^\ast M$, the entire cotangent bundle as a $2n$ dimensional manifold. This is quite a change of view, for $T^\ast M$ consists of a base space $M$ with local coordinates $(q^{1},\dots,q^{n})$, and on each point of $M$ is glued another space which is all the covectors at that point, with bases $(dq^{1},\dots,dq^{n})$; however we have introduced $p_ {i}$’s, which function as bases of the the cotangent space as well, for they are linear independent covectors. A generic point in $T^\ast M$ is locally given by $2n$-tuple $(p_ {1},\dots,p_ {n},q^{1},\dots,q^{n})$. A covector in this space is spanned by bases \(dp_ {1},\dots,dp_ {n}, dq^{1},\dots,dq^{n},\) but for Poincare form we need only half of these bases.</p> <p><strong>Poincare 1-form.</strong> It is a globally defined 1-form $\lambda$ on every cotangent bundle $T^\ast M$. In local coordinates $(p,q)$ it is given by \(\boxed{ \lambda = \sum_ {i} p_ {i} dq^{i} },\quad \lambda \in \Omega^{1}(T^\ast M).\) Just ignore the second part if it does not make any sense to you. You can show that $\lambda$ is indeed well-behaved on an overlap of local coordinate patches, hence making it globally well defined. It can also be guess from the fact that all the indices are contracted in the definition.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="phaseSpace"/><category term="quantization"/><summary type="html"><![CDATA[The first step to understand quantization fully, especially the geometric quantization, is to understand the familiar phase space but from a more geometric point of view.]]></summary></entry><entry><title type="html">Note on Meson-Kink Scattering</title><link href="https://baiyangzhang.github.io/blog/2023/Note-on-Meson-Kink-Scattering/" rel="alternate" type="text/html" title="Note on Meson-Kink Scattering"/><published>2023-06-20T00:00:00+00:00</published><updated>2023-06-20T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2023/Note-on-Meson-Kink-Scattering</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2023/Note-on-Meson-Kink-Scattering/"><![CDATA[<h3 id="sech2-as-an-example-of-reflectionless-potential">Sech$^2$ as an example of reflectionless potential</h3> <p>The one-dimensional potential \(V(x) = - \frac{\hbar^2}{2m a^{2}} \frac{\nu(\nu+1)}{\cosh ^{2}\left( \frac{x}{a} \right)}\) if reflectionless, at any energy, if $\nu$ is a positive integer number. Since there is no reflection of any eigenfunction, the wave packet formed by superposition of the energy eigenfunctions are also reflectionless. A numerical study shows that a Gaussian wave packet will be narrower by passage of the reflectionless $\text{sech}^{2}$ potential, and they can travel faster than a free space packet.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p> <hr/> <p>For positive energy energies we write $E = \frac{\hbar^{2}k^{2}}{2m}$, where $k$ is the momentum. The Schrodinger equation becomes \(\frac{d^{2}}{dx^{2}}\Psi +\left[ k^{2}+\frac{\nu(\nu+1)}{a^{2}\cosh ^{2}\left( \frac{x}{a} \right)} \right]\Psi = 0.\) Since the equation is parity invariant, name the equation is invariant under the interchange $x\to-x$, the solutions will also be eigenfunctions of the parity operator, they either stay unchanged, or obtain a minus sign, under $x\to-x$. In other words, there exists even or odd parity solution. The solution is found to be \(\begin{align} \psi^e_{\nu} &amp;= \left( \cosh \frac{x}{a} \right)^{\nu+1}F\left( \alpha,\beta, \frac{1}{2},-\sinh ^{2}\left( \frac{x}{a} \right) \right), \\ \psi^o_{\nu} &amp;= \left( \cosh \frac{x}{a} \right)^{\nu+1} \sinh\left( \frac{x}{a} \right) F\left( \alpha+ \frac{1}{2},\beta+\frac{1}{2}, \frac{3}{2},-\sinh ^{2}\left( \frac{x}{a} \right) \right) \end{align}\) where \(\alpha = \frac{1}{2}(\nu+1+ika),\quad \beta=\frac{1}{2}(\nu+1-ika)\) are complex conjugate to each other the $F$ is the hypergeometric function which can be represented by the Gauss hypergeometric series \(F(\alpha,\beta,\gamma,\zeta) = \frac{\Gamma(\gamma)}{\Gamma(\alpha)\Gamma(\beta)}\sum_{0}^\infty \frac{\Gamma(\alpha+n)\Gamma(\beta+n)}{\Gamma(\gamma+n)} \frac{\zeta^n}{n!}\) within the unit circle $\zeta=1$. Recall that $F(\alpha,\beta,\gamma,\zeta)$ is symmetric under the interchange of $\alpha$ and $\beta$.</p> <p>When $\nu=0$ there is no potential, so the solution $\psi_{0}^{\text{even,odd}}$ are plane waves, the trivial solution. To be exact, \(\begin{align} \psi^e_{0} &amp;= \cos kx, \\ \psi^e_{0} &amp;=\frac{\sin kx}{ka}. \end{align}\) The integer $\nu$ solutions rapidly become more and more complicated, but remain elementary functions, expressible in terms of $\cos, \sin$ and tanh functions. For example, \(\begin{align} \psi^e_{1} &amp; = \cos kx - \tanh \left( \frac{x}{a} \right) \frac{\sin kx}{ka},\\ \psi^o_{1} &amp; = \frac{1}{1+(ka)^{2}}\left[ ka\sin kx + \tanh\left( \frac{x}{a} \right) \cos kx \right] \\ \psi^e_{2} &amp; = \frac{1}{1+(ka)^{2}} \left\{ \left[ 1+(ka)^{2}-3\tanh ^{2}\left( \frac{x}{a} \right) \right] \cos kx -3ka \tanh\left( \frac{x}{a} \right)\sin kx\right\}, \\ \psi^o_{2} &amp; = \frac{1}{ka(4+(ka)^{2})^{2}}\left\{ \left[ 1+(ka)^{2}-3\tanh ^{2}\left( \frac{x}{a} \right) \right] \sin kx + 3ka\tanh\left( \frac{x}{a} \right)\cos kx\right\} . \end{align}\) Note that above expressions are different from the ones we are using by somewhere a different factor.</p> <hr/> <p>The eigenstates we listed above can be used to construct propagating waves. For example, the waves moving to the $+x$ direction can be defined by \(\psi_{0}^+ = \psi_{0}^e + ika\psi_{0}^o = e^{ ikx },\) which is the plane wave moving to the plus $x$ direction if you put back the $e^{ -i\omega t }$ factor, and \(\psi_{1}^+ = \psi_{1}^e + i \frac{1+(ka)^{2}}{ka}\psi_{i}^o = \left[ 1+\frac{i}{ka}\tanh\left( \frac{x}{a} \right) \right]e^{ ikx },\) It is not in the form of a shape-preserving wave packet, which should be $f(x-vt)$ for some function $f$, does it mean that during the propagation the shape changes? The complex conjugate of $\psi^+$ will be $\psi^-$, the wave packet that moves to the $-x$ direction.</p> <hr/> <p>Kiriushcheva and Kuzmin studied the propagation of the wave packet numerically for $\nu=1$ sech square potential. [^2] They found that the wave packet narrows and accelerates. Their wave packet was taken to be the Gaussian shape at the initial time,<br/> \(\Phi_{0}(x,t=0) = \exp \left\{ ik_{0}(x-x_{0})-\frac{(x-x_{0})^2}{2b^2} \right\} \tag{1}\) where $x_{0}$ is the center position of the wave packet and $b$ the width. The reason for the imaginary part in the exponential is such that the wave packet has momentum centered at $k_{0}$, as we shall see below. To be specific, the momentum is $k_{0} \pm b$ where $\pm b$ denotes the error. The same wavefunction in the momentum space looks like \(\tilde{\Phi}_{0}(p) = \int dx \, \Phi_{0}(x) e^{ -ipx },\) for the convention refer to note [[2022-7-17-Conventions-and-Formula]]. We have \(\tilde{\Phi}_{0}(p) = \sqrt{ 2\pi }b \exp \left\{ -\frac{b^2}{2}(p-k_{0})^{2}-ipx_{0} \right\} . \tag{2}\) We can now evolve the state with the Hamiltonian, for a given wavefunction $\psi(x,t)$, in terms of momentum eigenstates we have \(\psi(x,t) = \int \frac{dp}{2\pi} \, \phi(p) e^{ -iH(p)t+ipx }\) where $H(p)$ is the Hamiltonian. Apply it to $\Phi(x,t=0)$ we have \(\Phi(x,t) = \frac{b}{\sqrt{ b^{2}+\frac{it}{m} }}\exp \left\{ -\frac{1}{2} \frac{(x-x_{0}-vt)^{2}}{b^{2}+\frac{it}{m}} + ip\left( x-x_{0}-\frac{1}{2}vt \right) \right\} .\)</p> <p>Now, to see how the wave packet moves through time, we need to know how the density evolves, which is \(\left\lvert \Phi(x,t) \right\rvert ^2 = \frac{b}{\sqrt{ b^{2}+\frac{t^{2}}{b^{2}m^{2}} }}\exp \left\{ - \frac{(x-x_{0}-vt)^{2}}{b^{2}+\frac{t^{2}}{b^{2}m^{2}}} \right\} ,\) we can see clearly that the peak of the Gaussian wave packet moves with velocity $v= p / m$, since the time-dependent position of the peak is $x_{0}+vt$. Taking into consideration the error, the center of position will be $x_{0}+vt \pm \sqrt{ b^{2}+t^{2} / b^{2}m^{2} }$.</p> <p>The initial position is $x_{0} \pm b$, the momentum is $k_{0}\pm \frac{1}{b}$, which can be read off from Eq. (2).</p> <p>We notice two things,</p> <ol> <li>Heisenberg uncertainty principle is saturated in this case, $\Delta x \Delta p = b \times\frac{1}{b} =1$ (note we have set $\hbar=1$),</li> <li>the variance in the initial position and velocity are independent, thus can be compounded by squares, the variance in position at time $t$ will be \(\Delta x(t)^{2}=(\Delta x_{0})^{2}+(t\Delta v)^{2}=b^{2}+\left( \frac{t}{mb} \right)^{2}.\) [^2]: N. Kiriushcheva and S. Kuzmin, “Scattering of a Gaussian wave packet by a reflectionless potential,” Am. J. Phys. 66, 867–872 (1998)</li> </ol> <hr/> <h3 id="11-dimensional-qft-with-kinks">1+1 dimensional QFT with kinks</h3> <p>Consider a $1+1$ dimensional model with scalar fields $\phi$ only, in the Schrodinger picture. Schrodinger picture means that operator are constants in time while the wave function evolves. Since the scalar field is a operator in quantum field theory, $\phi(x)$ is constant in time so $\partial_{t}\phi$ makes no sense, hence we need the canonical momentum $\pi(x)$. The Hamiltonian is \(H=\int d x: \mathcal{H}(x):_a, \quad \mathcal{H}(x)=\frac{\pi^2(x)}{2}+\frac{\left(\partial_x \phi(x)\right)^2}{2}+\frac{V(\sqrt{\lambda} \phi(x))}{\lambda}.\) The notation $:\bullet:_{a}$ denotes the normal ordering with respect to ladder operators $A,A^{\dagger}$, we will have more to say about it later. The parameter of potential is written as $\sqrt{ \lambda }\phi(x)$ not because we like to make our lives complicated, but for semi-classical expansion reasons. By semi-classical expansion we mean the to take the limit $\hbar\to 0$. Putting $\hbar$ back to the action we have roughly \(Z = \int \mathcal{D}\phi \mathcal{D}\pi \, e^{ iS/\hbar }, \quad S = \int dx^2 \,\mathcal{L} .\) We can stick to the natural units and consider $\hbar$ not the Plank constant with non-trivial dimension, but just a parameter which is dimensionless. Now we ask, if we want the action kept unchanged as we vary the value $\hbar$, how would various parameters in the Lagrangian vary? The field operator $\phi$ multiplies $\hbar$ as $\frac{\phi^2}{\hbar}$, thus when we change $\hbar$, \(\text{const action} \implies \frac{\phi^2}{\hbar} \text{ is const} \implies \phi \sim \sqrt{ \hbar },\) where tilde means the $\phi$ scales as $\sqrt{ \hbar }$. Similarly we have $\lambda \sim \hbar^{-1 / 2}$. We find that the combination \(\sqrt{ \lambda }\phi = \text{const} \text{ under the variatio of } \hbar,\) which means $\sqrt{ \lambda }\phi$ is const in the semi-classical limit, and that’s why we define the potential in that specific combination.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>N. Kiriushcheva and S. Kuzmin, “Scattering of a Gaussian wave packet by a reflectionless potential,” Am. J. Phys. 66, 867–872 (1998). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Baiyang Zhang</name></author><category term="QuantumFieldTheory"/><category term="Kink"/><category term="Meson"/><summary type="html"><![CDATA[Sech$^2$ as an example of reflectionless potential]]></summary></entry></feed>