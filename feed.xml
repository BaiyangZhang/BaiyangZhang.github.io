<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://baiyangzhang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://baiyangzhang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-30T05:32:34+00:00</updated><id>https://baiyangzhang.github.io/feed.xml</id><title type="html">Baiyang Zhang</title><subtitle>A place dedicated to sharing insights and reflections on mathematics, physics, and social sciences. </subtitle><entry><title type="html">A Passive Perspective of Linearized Soliton Perturbation Theory</title><link href="https://baiyangzhang.github.io/blog/2025/Passive-Perspective-of-LSPT/" rel="alternate" type="text/html" title="A Passive Perspective of Linearized Soliton Perturbation Theory"/><published>2025-03-31T00:00:00+00:00</published><updated>2025-03-31T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2025/Passive-Perspective-of-LSPT</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2025/Passive-Perspective-of-LSPT/"><![CDATA[<h1 id="quantization-and-haag-theorem">Quantization and Haag Theorem</h1> <p><em>This section can be entirely skipped for readers who are not interested in Haag’s theorem.</em></p> <h2 id="quantization">Quantization</h2> <p>Start with a classical theory with Lagrangian $\mathcal{L}$, or equivalently Hamiltonian $H$. There may be some symmetry group $G$ acting on the Lagrangian. The theory can be separated into free part and interacting part. Upon quantization, the interaction will introduce an UV energy scale $\Lambda_ {\text{UV}}$ above which the theory becomes invalid. The symmetry group $G$ might break, if it is solely due to the quantization procedure then we say the symmetry is anomalous. The classical Hamiltonian are functions of canonical variables $q$ and $p$. The canonical variables satisfy certain Poisson brackets, which generates the dynamics of the model.</p> <p>Let’s ignore the interaction for now and deal with a free theory only. We will introduce interaction later and see that they cause fundamental difficulties in quantization. But without interaction, things are under control… more or less. To quantize a classical model we need to construct a Hilbert space in a systemic way. The simplest procedure is to generalize the generalized coordinates and canonical momenta to operators, and map the Poisson bracket to the so-called canonical commutation relation (CCR),</p> \[[q,p] = i\hbar.\] <p>The Hilbert space we are looking for would be a representation of CCR. there exist different possible representations, for example $q,p$ could be represented by matrices; or we could let $q=q$ and $p=-i\hbar \frac{ \partial }{ \partial q }$, the corresponding Hilbert space is the Lebesgue space $L^{2}(\mathbb{R}^{d})$, which is the square-integrable functions quotient functions with measure zero almost everywhere, such as the famous Dirichlet function.</p> <p>Classical functions $f(q,p)$ are quantized in a straightforward way, by replacing $q$ and $p$ by their corresponding operators.</p> <p>This procedure has a number of problems. For example, it is not consistent in the sense that the quantization of $f(q,p)$ some times is not unique due to possible different orders of $p,q$. Later people proposed better, more sophisticated quantization methods, such as geometry quantization and deformation quantization. So it is safe to say that, given a free classical theory, we can quantize it to obtain a Hilbert space for free quantum theory.</p> <p>Since given a CCR we can find multiple different irreducible representations, then the question is, which is the correct one? In quantum mechanics we deal with finite dimensional CCR and this question is answered by the Stone-von Neumann theorem, which claims that all the representations are equivalent up to a unitary transform. This makes sure that whatever representation we choose, we will get the same prediction for observables. However, it turned out later that if $U(1)$ gauge field is included, this claim is no longer true. I am still not sure about this claim, but it seems to suggest that even in the finite dimensional case, a single irreducible representation, up to unitary transform, is already not enough when there is electro-magnetic field around.</p> <hr/> <p>One way to think of the quantization of quantum field theory is to start from quantum mechanics, generalize $p$ and $q$ to $\phi(\vec{x})$ and $\pi(\vec{x})$, going from finite dimensional CCR of quantum mechanics to infinite dimensional CCR of quantum field theory. This way we start from a well-established CCR algebra and generalize its dimension to infinity. Another way is to start from a classical field theory, say, a scalar classical Landau-Ginsberg model, with real scalar field $\phi(\vec{x})$. Let $M$ be the space manifold, the configuration space of a classical field theory is denoted as $\mathcal{C}(M,\mathbb{R})$ where $\mathbb{R}$ is the time direction. Regard $\vec{x}$ as continuous indices as in quantum field theory, then $\vec{x}$ labels different field operator, and each $\phi(\vec{x})$ can be considered as a functional, it is the evaluation functional on the configuration space, explicitly let $f\in\mathcal{C}$ then</p> \[\phi(\vec{x}): \mathcal{C} \to \mathbb{R},\quad f \mapsto f(\vec{x}).\] <p>A general field is a function $F$ of the basic field $\phi$, $F=F(\phi)$. Then $F(\phi)$ is also a functional on $\mathcal{C}$. The advantage of this point of view is that, first, in particular in view of deformation quantization, it is a main advantage of our approach that the fields of both the classical and the quantum theory are defined in terms of the same space of functionals. Second, many of the most fundamental and profound phenomenon in quantum field theory, such as renormalization group flow or critical behavior, already takes place in classical field theory level. This approach greatly increases our understanding of quantum field theory.</p> <p>For quantum field theory, we can borrow the Hilbert space constructed in quantum mechanics and expand it to Fock space, sometimes know as second quantization. Compare to coming up to a Hilbert space from nothing, the construction of Fock space is quite straight forward, that’s why Edward Nelson said something like the first quantization is a mystery and the second quantization is a functor.</p> <p>However, recall that this Fock space is built up from free states, there is no interaction involved yet. The eigen states are supposed to be that of free particles with mass $m$, whose energy is $\sqrt{\vec{p}^{2}+m^{2}}$. To deal with interaction, the standard procedure is to turn to interaction picture, or Dirac picture. In interaction picture, the operators are evolved by free Hamiltonian without interaction, and the states alone are evolved by interact. This allows us to easily write down the time dependence of field operators, then it is easy to write down the Feynman propagators, etc.</p> <p>There are some fundamental issues regarding the good old interaction picture that I haven’t figured out, and I mean the so-called Haag’s theorem, or more accurately Haag-Hall-Wightman (HHW) theorem, which roughly says that interaction picture is inconsistent, even in the case of neutral free scalar field with different masses. <strong>It claims that the set of assumptions required to construct the interaction picture is only consistent when there is no interaction.</strong> It also claims that <strong>non-Fock representations have an important rule to play in QFT</strong> (Earman and Fraser, 2006). Our work on kinks will provide an explicit example.</p> <hr/> <p>As we mentioned before, quantum theories, no matter quantum field theory or quantum field theory are described by a bunch of canonical commutation relations (here we only consider bosonic case), such as $[x,p]=i\hbar$ or $[\phi(\vec{x}),\pi(\vec{y})]=(2\pi)^{d}\delta^{d}(\vec{x}-\vec{y})$. In quantum mechanics we have finite sets of independent CCR’s (canonical commutation relation), while in quantum field theory we have infinite dimension. Things usually get quite messy when going from finite to infinity, properties that hold in the finite case may not hold for infinite case, for example a sum of finite positive numbers are always positive, but a sum of infinite positive numbers may not be positive anymore. The same thing happens with going from quantum mechanics to quantum field theory.</p> <h1 id="haag-hall-wightman-theorem">Haag-Hall-Wightman theorem</h1> <p>Fock space is only one representation among an uncountable infinity of Hilbert space representations (Garding &amp; Wightman, 1954). So why Fock representation? Haag was motivated by this choice question to formulate his theorem. Later turns out that there exists so-called <code class="language-plaintext highlighter-rouge">strange representation</code>, which by definition is any representation of free field that is unitarily inequivalent form Fock space. <strong>The determination of representation is a dynamical problem.</strong> Like Florig and Summers (2000) said,</p> <blockquote> <p>The kinematical aspects determine the choice of CCR-algebra, whereas the dynamics fix the choice of the representation.</p> </blockquote> <hr/> <p>The vacuum state can be specified in two ways: (1) it is the minimal energy state; (2) it is the unique state annihilated by particle number operator $a^{\dagger}a$. If these two specification don’t coincide, the vacuum is said to be <code class="language-plaintext highlighter-rouge">polarized</code>. As we will see later, <strong>the presence of a kink will polarize the trivial vacuum</strong>. <strong>Vacuum polarization lies at the core of HHW theorem, any interacting quantum field operator, or free fields of different masses, polarizes the vacuum</strong>.</p> <p>Back to the equal time CCR (canonical commutation relation), which is roughly</p> \[[\phi(\vec{x}),\pi(\vec{x})]=i\delta^{3}(\vec{x}-\vec{x}'), \quad 0 \text{ otherwise.}\] <p>Sometimes the field operator $\phi$ and $\pi$ are “smeared” in space, namely integrated against some controlled functions $f$</p> \[\phi(f) := \int d^{3}x \, f(\vec{x})\phi(\vec{x})\] <p>so that quantities of interest are well defined. $f$ is called the test function. For example, we can smear the field to get rid of the Dirac delta function if necessary, and we get</p> \[[\phi(f),\pi(g)] = \left\langle f,g \right\rangle\] <p>where $\left\langle f,g \right\rangle$ is the inner product of $f$ and $g$. Even though we are not gonna use this “regularized” commutation relation here, it is useful to know this trick.</p> <p><strong>Here should put a short mathematical explanation of the gist of Haag’s theorem.</strong></p> <p>HHW theorem does not imply that it’s impossible to treat interactions in the standard way (such as that adopted in Peskin&amp;Schroeder), it just has to be unitarily inequivalent to the free Fock space. The HHW theorem shows that, a single, universal Hilbert space representation does not suffice for describing both free and interacting fields. Instead, unitarily inequivalent representations must be employed.</p> <p><strong>Put the example in $\phi^{4}_ {1+1}$ example by Jaffe.</strong></p> <hr/> <p>With the reasons stated above, in our calculation we will adopt Schrodinger’s picture instead of standard interaction picture.</p> <h1 id="vacuum-sector-of-quantum-theory">Vacuum sector of quantum theory</h1> <p>We will work in Schrodinger picture, where operators are dependent on position only. From time to time we will come back to interaction picture as a consistency check. We will only consider real scalar field $\phi$ in $d+1$ dimension. In Schrodinger picture, a generic operator $\mathcal{O}(\phi(\vec{x}),\pi(\vec{x}))$ is given as a function of the field $\phi(\vec{x})$ and the canonical momentum density $\pi(\vec{x})$, where $\vec{x}$ is the spatial coordinate. Note that since it is not the interaction picture, we don’t deal with the Lorentz covariant four-vector $x^{\mu}$.</p> <p>Without loss of generality, we take phi-fourth theory for example, denoted $\phi^{4}_ {d+1}$. What we do to phi-fourth theory can be done to any potential. The Hamiltonian is normal ordered at mass scale $m_ {0}$, which is the bare mass of the free particle. Normal-ordering is regarded by me as a part of the definition of the theory, different people might have different views. Naturally people might ask, why normal order at the bare mass, not the renormalized mass $m$? Well, it doesn’t matter what mass scale we choose, after renormalization the ambiguity introduced by the mass scale is fixed by the renormalization conditions, and the observables are independent of this choice.</p> <p>There is a $\mathbb{Z}_ {2}$ symmetry of the Hamiltonian which breaks spontaneously. The Hamiltonian before the Spontaneous Symmetry Breaking (SSB) reads:</p> \[\begin{align*} \hat{H}(\vec{x}) &amp;= \int d^{d}x: \hat{\mathcal{H}}(\vec{x}) :_ {m_ {0}} \\ \hat{\mathcal{H}}(\vec{x}) &amp;= \frac{1}{2}(\pi(\vec{x})^{2}+(\partial_ {i}\phi(\vec{x})^{2}) - \frac{1}{4}m_ {0}^{2}\phi^{2} + \frac{\lambda_ {0}}{4}\phi^{4}+A. \end{align*}\] <p>We use the convention that parameters with a naught downstairs represent the bare parameters. $\hat{H}$ is the Hamiltonian before the spontaneous symmetry breaking (SSB).</p> <p>To be more specific, the Hamiltonian undergoes spontaneous symmetry breaking since the minimum of the Hamiltonian is obtained at $\phi=\pm v_ {0}$ where $v_ {0}=\frac{m_ {0}}{\sqrt{2\lambda_ {0}}}$. Later we will shift the field operator from $\phi$ to $\phi’$, so that the vacuum is obtained at $\phi’=0$, the Hamiltonian in terms of $\phi’$ is denoted $H$ without hat. Note the factor $-m_ {0}^{2} /4$ in the mass term, we have $\frac{1}{4}$ instead of $\frac{1}{2}$ so that in the symmetry broken phase the particle has a canonical mass term. The last $A$ in the Hamiltonian is a c-number that cancels the zero point energy in the vacuum sector. We also define coupling $g_ {0}$ as</p> \[g_ {0}^{2} := \lambda_ {0}.\] <p>The reason why we introduce a new symbol $g$ for $\sqrt{\lambda}$ is that, when we do calculations in the symmetry broken phase, we will be directly dealing with $\sqrt{\lambda}$ and define it to be $g$ just makes sense.</p> <p>In the Schrodinger picture, the field operator and canonical momentum operator can be decomposed in terms of plane waves as usual,</p> \[\phi(\vec{x}) = \int \frac{d^{d}p}{(2\pi)^{d}} \, \frac{1}{\sqrt{2 \omega_ {p} }} (a_ {p} \, e^{ i\vec{p}\cdot \vec{x} } + a^{\dagger}_ {p}\,e^{ -i\vec{p}\cdot \vec{x} }).\] <p>The canonical commutation relation reads</p> \[[a_ {p},a^{\dagger}_ {k}] = (2\pi)^{d}\delta^{d}(\vec{p}-\vec{k}).\] <p>But $a_ {p}$ and $a_ {p} ^{\dagger}$ are not Lorentz invariant, so we decided to introduce a Lorentz version:</p> \[\phi(\vec{x}) = \int \frac{d^{d}p}{(2\pi)^{d}} \, \frac{1}{2 \omega_ {p} } (A_ {p} \, e^{ i\vec{p}\cdot \vec{x} } + A^{\dagger}_ {p}\,e^{ -i\vec{p}\cdot \vec{x} }).\] <p>The price to pay is that $A,A^{\dagger}$ no longer satisfy CCR. We can fix that by defining $A^{\ddagger}:= A^{\dagger} / 2\omega _ {p}$, then $A$ and $A^{\ddagger}$ satisfy CCR. In summary we have</p> \[\boxed{ \begin{align*} A_ {p} &amp;\equiv \sqrt{ 2\omega_ {p} }\,a_ {p},\quad A^{\dagger}_ {p} \equiv \sqrt{ 2\omega_ {p} }\,a^{\dagger}_ {p}, \\ A^{\ddagger}_ {p} &amp;:= \frac{A^{\dagger}_ {p}}{2\omega_ {p}} = \frac{a^{\dagger}_ {p}}{\sqrt{ 2\omega_ {p} }}, \\ [A_ {p},A^{\ddagger}_ {k}] &amp;= (2\pi)^{d}\,\delta^{d}(\vec{p}-\vec{k}). \end{align*} }\] <hr/> <p>The renormalized quantities are define as</p> \[m^{2}=m_ {0}^{2} + \delta m^{2}, \quad \sqrt{ \lambda } = \sqrt{ \lambda_ {0} } + \delta \sqrt{ \lambda }.\] <p>or equivalently</p> \[g = g_ {0} + \delta g.\] <p>We also define $v$ to be the vev of $\phi$ in broken vacuum:</p> \[v = \frac{m}{\sqrt{ 2\lambda }} + \delta v.\] <p>All the counter terms can be perturbatively expanded:</p> \[\begin{align*} \delta m^{2}&amp; =\sum_ {i=1}^{\infty} \delta m^{2}_ {i} , \quad \delta m^{2}_ {i}\sim \mathcal{O}(g^{i+1}), \\ \delta g &amp;= \sum_ {i=1}^{\infty} \delta g_ {i}, \quad \delta g_ {i} \sim \mathcal{O}(g^{i+2}),\\ H &amp;= \sum_ {i=0}^{\infty} H_ {i},\quad H_ {i} \sim \mathcal{O}(g^{i-2}) , \\ A &amp;= \sum_ {i=0}^{\infty}A_ {i}, \quad A_ {i} \sim \mathcal{O}(g^{i-2}) , \\ \delta v &amp;= \sum_ {i=1}^{\infty} \delta v_ {i} ,\quad \delta v_ {i} \sim \mathcal{O}(g^{i}), \\ \left\lvert{\Psi}\right\rangle &amp;= \sum_ {i=0}^{\infty} \left\lvert{\Psi_ {i}}\right\rangle , \quad \left\lvert{\Psi}\right\rangle _ {i} \sim \mathcal{O}(g^{i}), \\ I &amp;\sim \mathcal{O}(g^{2}). \end{align*}\] <p>Where $I$ is some variable which will be define later, I put it here for the sake of completeness.</p> <h2 id="normal-ordering-revisited">Normal ordering revisited</h2> <p>Reference: Sidney Coleman’s <a href="http://users.physik.fu-berlin.de/%7Ekamecke/ps/coleman.pdf">1975 paper</a> and his lecture note title <em>Aspects of Symmetry</em>, the chapter about classical and quantum lumps.</p> <p>In Coleman’s 1975 paper, he studied the sine-Gordon model</p> \[\mathcal{L} = \frac{1}{2} (\partial_ {\mu}\phi)^{2} + \frac{\alpha_ {0}}{\beta^{2}}\cos \beta \phi+\gamma_ {0}\] <p>and famously pointed out:</p> <ol> <li>As for any theory of a <strong>scalar field</strong> with <strong>nonderivative interaction</strong> in <strong>2 dimensional spacetime</strong>, normal ordering the Hamiltonian alone is enough to cancel all the divergences at any loop order;</li> <li>If the $\beta$-parameter exceeds $8\pi$, the Hamiltonian density is not bounded below.</li> <li>If $\beta&lt;8\pi$, the model is equivalent to the charge-zero sector of almost-massive Thirring model.</li> <li>The fermion in the Thirring model is massless if $\beta=4\pi$.</li> </ol> <p>Well, point 2,3 and 4 are not relevant to our discussion, I just put it here for the sake of completeness. My point is, turns out there is more to normal ordering than I thought. In textbooks such that by Peskin&amp;Schroeder, or Mark Srednicki, normal ordering is regarded as a simple, if not trivial, ad hoc procedure that puts all the creation operators to the left of all the annihilation operators in order to eliminate the zero point energy of the trivial vacuum, and it was only done to the free part of the Hamiltonian. But when also applied to higher degree polynomial, the normal ordering can also generate counter terms. For example, normal ordering the $\phi^{4}$ term will generate a counter term for $\phi^{2}$ term. So I feel like it is necessary to revisit normal order.</p> <p>The defining property of normal ordering is that expectation values of normal-ordered operators vanish in the free theory:</p> \[\left\langle{0}\right\rvert : \mathcal{O} :\left\lvert{0}\right\rangle =0\] <p>for any operator $\mathcal{O}$ and free vacuum $\left\lvert{0}\right\rangle$. There are various formulations of this notion <sup id="fnref:Polchinski"><a href="#fn:Polchinski" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> , such as <code class="language-plaintext highlighter-rouge">creation-annihilation operator normal ordering</code>, <code class="language-plaintext highlighter-rouge">conformal normal ordering</code>, <code class="language-plaintext highlighter-rouge">functional integral normal ordering</code>, etc. In this note we will only talk about creation-annihilation operator normal ordering.</p> <p>What is less often discussed is that normal ordering also depends on a mass scale mmm. In most cases, it is quite clear which value of mmm to choose, typically the mass of the free particle, known as the free mass. However, to determine what the free mass is, we first need to know the form of the Hamiltonian. For example if we can unambiguously separate the Hamiltonian into $\mathcal{H}_ {0}+U(\phi)$, where $\mathcal{H}_ {0}=\pi^{2} / 2+(\partial_ {x}\phi)^{2} /2+m^{2}\phi^{2} /2$, then $m$ is the free mass. Sometimes, however, the situation is more complex. For instance, it may be more convenient to include $m^{2}\phi^{2} /2$ in the interaction term, as in the case of sine-Gordon model. In this case, the free Hamiltonian only contains the kinetic part, and the free particle appears massless. In general, for a quadratic term of the form $a \phi^2$, where $a$ is a real parameter, we can choose how much of it goes into the free Hamiltonian, with the remainder contributing to the interaction. For example, we can pick a number such as $m_ {e}=0.511$ MeV and say that $m_ {e}^{2}\phi^{2}/2$ goes to the free Hamiltonian, such that the free particle has mass $m_ {e}$, then $(a-\frac{m^{2}}{2})\phi^{2}$ will go to the interaction. This is just a matter of convenience, the observables should not depend on such arbitrary choices.</p> <p>So, how does all this relate to normal ordering? Normal ordering is a procedure that rearranges creation and annihilation operators, and these operators inherently depend on the free mass of the particle. $a^{\dagger}_ {p}$ creates a particle of momentum $\vec{p}$ <strong>with mass $m$</strong>, the energy of the particle is $\sqrt{ \vec{p}^{2}+m^{2} }$. I repeat, <strong>the choice of $m$ depends on what you choose to call the free Hamiltonian, and that choice is somehow arbitrary.</strong> The result of changing $m$ can be absorbed in a redefinition of the theory parameters.</p> <p>My assumption is, the field operator $\phi(x)$ itself is independent of the aforementioned choice of mass parameter $m$, while the stuff that do depend on $m$ are</p> <ol> <li>creation and annihilation operators $a_ {p,m}$ and $a^{\dagger}_ {p,m}$,</li> <li>the energy $\omega=\sqrt{ \vec{p}^{2}+m^{2} }$, we will write it as $\omega_ {p,m}$ explicitly,</li> <li>states in Fock space. For example, the free vacuum $\left\lvert{0,m}\right\rangle$ is by definition annihilated by $a_ {p,m}$. Since the states in the Fock space are constructed by applying $a^{\dagger}_ {p,m}$ consecutively, a re-definition of $a^{\dagger}_ {p,m}$ to, say $a^{\dagger}_ {p,\mu}$ will also change the states themselves.</li> </ol> <p>Recall that the field operator can be expanded in terms of energy and ladder operators,</p> \[\phi(\vec{x}) = \int \frac{d^{d}k}{(2\pi)^{d}}\, \frac{1}{\sqrt{ 2\omega_ {k,m} }} (a_ {k,m} \, e^{ i\vec{k}\cdot \vec{x} } + a^{\dagger}_ {k,m}\,e^{ -i\vec{k}\cdot \vec{x} })\] <p>and the $m$-dependence in $\omega_ {k,m}$, $a_ {k,m}$ and $a^{\dagger}_ {k,m}$ should cancel out so that $\phi(\vec{x})$ itself is $m$-independent.</p> <hr/> <p>We can define the <strong>normal ordering at $m$</strong> by decomposing the Schrodinger picture fields and momenta into creation and annihilation part:</p> \[\begin{align*} \phi^{+}(\vec{x}) &amp;= \int \frac{d^{3}p}{(2\pi)^{3}} \, e^{ -i\vec{p}\cdot \vec{x} } A^{\ddagger}_ {p,m} \\ \phi^{-}(\vec{x}) &amp;= \int \frac{d^{3}p}{(2\pi)^{3}} \, e^{ i\vec{p}\cdot \vec{x} } \frac{A_ {p,m}}{2\omega_ {p,m}}, \end{align*}\] <p>similarly for the canonical momenta</p> \[\pi^{\pm }(\vec{x}) = \pm i\sqrt{ -\nabla^{2}+m^{2} }\phi^{\pm }(\vec{x}) ,\] <p>which implies</p> \[\begin{align*} \pi^{-}&amp;= - \frac{i}{2} \int \frac{d^{3}p}{(2\pi)^{3}} \, e^{ i\vec{p}\cdot \vec{x} } A_ {p,m} ,\\ \pi^{+} &amp;= i \int \frac{d^{3}p}{(2\pi)^{3}} \, e^{ -i\vec{p}\cdot \vec{x} } \omega_ {p,m}A^{\ddagger}_ {p,m} . \end{align*}\] <p>It is easy to verify that</p> \[\begin{align*} \phi(\vec{x}) &amp;= \phi^{+}(\vec{x}) + \phi^{-}(\vec{x}) , \\ \pi(\vec{x}) &amp;= \pi^{+}(\vec{x}) + \pi^{-}(\vec{x}) . \end{align*}\] <p>We can arrange a sting of field operators using the Wick’s theorem, which says that a string of product of field operators can be rewritten as the sum of all the possible contractions of operators, all of them normal ordered. For example,</p> \[\phi(\vec{x})\phi(\vec{x}) = :\phi(\vec{x})\phi(\vec{x}): + \text{ contraction}\left\lbrace \phi(\vec{x})\phi(\vec{x}) \right\rbrace .\] <p>The contraction is where different choice of $m$ generates different results. Let’s write the contraction at $m$ as $C_ {m}\left\lbrace \cdots \right\rbrace$, we have</p> \[\boxed{ \begin{align*} \phi^{2}(\vec{x}) &amp;= :\phi^{2}(\vec{x}):_ {m} + C_ {m}\left\lbrace \phi^{2}(\vec{x}) \right\rbrace , \\ C_ {m}\left\lbrace \phi^{2}(\vec{x}) \right\rbrace &amp;= [\phi^{-},\phi^{+}] = \int \frac{d^3p}{(2\pi)^{3}} \, \frac{1}{2\omega_ {p,m}}. \end{align*} }\] <p>Recall that $\phi^{2}$ is $m$-independent, while the normal ordering and the contraction are $m$-dependent. This gives us a means to connect normal ordering at different mass $m$ and $m_ {0}$, since</p> \[\phi^{2}(\vec{x}) = :\phi^{2}(\vec{x}):_ {m} + C_ {m}(\phi^{2}(\vec{x})) = :\phi^{2}(\vec{x}):_ {m_ {0}} + C_ {m_ {0}}(\phi^{2}(\vec{x})),\] <p>which implies that</p> \[\begin{align*} :\phi^{2}(\vec{x}):_ {m_ {0}} &amp;= :\phi^{2}(\vec{x}):_ {m} + C_ {m}-C_ {m_ {0}} \\ &amp;= :\phi^{2}(\vec{x}):_ {m} + \frac{1}{2} \int \frac{d^{3}p}{(2\pi)^{3}} \, \left( \frac{1}{\omega_ {p,m}} - \frac{1}{\omega_ {p,m_ {0}}} \right) . \end{align*}\] <p>where $C_ {m}$ is short for $C_ {m}(\phi^{2})$, a short-handed notation we will use extensively for the rest of the note. $C_ {m}$ is a c-number, usually given by a divergent integral.</p> <p>Similarly, we can get normal order all the terms in the Lagrangian at all the mass scales. For details see my other notes.</p> <p>If we start with a Hamiltonian normal ordered at $m_ {0}$, we can shift it to that normal ordered at the physical mass $m$, the difference is</p> \[\begin{align*} \hat{\mathcal{H}}(\vec{x})-\hat{\mathcal{H}}^{0}(\vec{x})&amp;= \frac{3}{2}\lambda_ {0} I \phi^{2}(\vec{x}) + \frac{3}{4} \lambda_ {0} I^{2} - \frac{m_ {0}^{2}}{4}I \\ &amp;\;\;\;\; + \frac{1}{4} \int \frac{d^{3}p}{(2\pi)^{3}} \, \left( \frac{2\vec{p}^{2}+m^{2}}{\omega_ {p,m}} - \frac{2\vec{p}^{2}+m_ {0}^{2}}{\omega_ {p,m_ {0}}} \right) \end{align*}\] <p>where $\hat{\mathcal{H}}^{0}$ is normal ordered at $m_ {0}$.</p> <h2 id="perturbative-expansion-of-the-hamiltonian">Perturbative Expansion of the Hamiltonian</h2> <p>Since we want to cancel divergences in a perturbative manner, that is, order-by-order in (renormalized) coupling $\lambda$. When $\Lambda$ is involved, we should count the order with respect to $\lambda$ first, treating terms such as $\lambda \Lambda,\,\lambda \Lambda^{2}, \cdots\lambda \Lambda^{n}$ all as $\mathcal{O}(\lambda)$, and group things of the same order together. Only then do we take the limit $\Lambda\to\infty$, and cancel the divergences.</p> <p>For the rest of this note, unless explicitly said otherwise, $\lambda$ stants for the <em>renormalized coupling</em> and $\lambda_ {0}$ the bare coupling.</p> <p>I copy the Hamiltonian here:</p> \[\begin{align*} \hat{H}(\vec{x}) &amp;= \int d^{3}x \, :\left\lbrace \hat{\mathcal{H}}^{0}(\vec{x}) + \frac{3}{2}\lambda_ {0} I \phi^{2}(\vec{x}) + \frac{3}{4} \lambda_ {0} I^{2} - \frac{1}{4}I\,m_ {0}^{2} \right\rbrace :_ {m} \\ &amp;\;\;\;\; + \int d^{3}x \,\frac{1}{4} \int \frac{d^{3}p}{(2\pi)^{3}} \, \left( \frac{2\vec{p}^{2}+m^{2}}{\omega_ {p,m}} - \frac{2\vec{p}^{2}+m_ {0}^{2}}{\omega_ {p,m_ {0}}} \right) . \end{align*}\] <p>A simple Taylor expansion shows that</p> \[\omega_ {p,m} = \omega_ {p,m_ {0}} + \frac{1}{2} \frac{\delta m^{2}}{\omega_ {p,m_ {0}}} + \mathcal{O}(\delta m^{4}),\] <p>thus</p> \[\omega_ {p,m} - \omega_ {p,m_ {0}} \sim \mathcal{O}(\lambda)\] <p>since $\delta m^{2}=(\cdots)\lambda+(\cdots)\lambda^{2}+\cdots$. Similarly,</p> \[\begin{align*} I &amp;= \frac{1}{2} \int \frac{d^{3}p}{(2\pi)^{3}} \, \left( \frac{1}{\omega_ {p,m}}-\frac{1}{\omega_ {p,m_ {0}}} \right) \\ &amp;= \frac{1}{2} \int \frac{d^{3}p}{(2\pi)^{3}} \, \left( - \frac{\delta m^{2}}{2 \omega^{3}_ {p,m_ {0}}} +\cdots \right)\\ &amp;= -\frac{\delta m^{2}}{4} \int \frac{d^{3}p}{(2\pi)^{3}} \, \frac{1}{\omega^{3}_ {p,m_ {0}} } + \cdots\\ &amp;\sim \mathcal{O}(\lambda) \end{align*}\] <p>The last term in the Hamiltonian density reads</p> \[\frac{1}{4} \int \frac{d^{3}p}{(2\pi)^{3}} \, \left( \frac{2\vec{p}^{2}+m^{2}}{\omega_ {p,m}} - \frac{2\vec{p}^{2}+m_ {0}^{2}}{\omega_ {p,m_ {0}}} \right) = \frac{m_ {0}^{2}\delta m^{2}}{8} \int \frac{d^{3}p}{(2\pi)^{3}} \, \frac{1}{\omega^{3}_ {p,m_ {0}}} +\cdots.\] <p>There is another constant term that is of order $\lambda$:</p> \[- \frac{1}{4}I\,m_ {0}^{2} = \frac{m_ {0}^{2}\,\delta m^{2}}{16}\int \frac{d^{3}p}{(2\pi)^{3}} \, \frac{1}{\omega^{3}_ {p,m_ {0}}} +\cdots.\] <p>This is too bad, since I had hoped that the above equation will cancel the above above equation. But we can rearrange the terms proportional to $m_ {0}^{2}$ such that the integrals cancel each other and leaves us only one single term: $-\frac{3}{4} Im_ {0}^{2}$.</p> <p>The term $\frac{3}{2}\lambda_ {0} I \phi^{2}(\vec{x})$ needs some extra work. It seems to be of order $\lambda_ {0}I\sim \mathcal{O}(\lambda^{2})$, however, often times we need to replace the field operator $\phi$ by $\left\langle \phi \right\rangle+\phi’$, where $\left\langle \phi \right\rangle$ is the expectation valued for $\phi$ under certain circumstances, and it is usually of order $\left\langle \phi \right\rangle\sim 1 / g$ where $g\equiv \sqrt{ \lambda }$. Thus this quadratic term is actually of order $\mathcal{O}(\lambda)$ and we need to keep it.</p> <p>Putting everything together. At the leading order of $\lambda$ we have</p> \[\begin{align*} \hat{H}(\vec{x}) &amp;= \int d^{3}x \, \left\lbrace :\hat{\mathcal{H}}^{0}(\vec{x}) + \frac{3}{2}\lambda_ {0} I \phi^{2}(\vec{x}) - \frac{3}{4} I m_ {0}^{2} :_ {m} \right\rbrace \end{align*}\] <p>Then we write the bare parameters in terms of renormalized ones (recall that $g:= \sqrt{ \lambda }$ for both bare and renormalized parameters), at order $\mathcal{O}(\lambda)$ we have</p> \[\begin{align*} \hat{H}(\vec{x}) &amp;\supset \int d^{3}x \, : \frac{1}{2}\pi^{2}+\frac{1}{2}(\partial_ {i}\phi)^{2} - \frac{m^{2}}{4}\phi^{2}+\frac{g^{2}}{4}\phi^{4}:_ {m} \\ &amp;\;\;\;\; + \int d^{3}x \, : \frac{\delta m^{2}}{4}\phi^{2}-\frac{1}{2}g \delta g\phi^{4}+\frac{3}{2}g^{2}I\phi^{2}-\frac{3}{4}m^{2}I +A :_ {m}. \end{align*}\] <p>We have neglected higher order terms, for example a term $3g\delta g\sim \mathcal{O}(g^{4}) \sim\mathcal{O}(\lambda^{2})$ was dropped from the expression.</p> <h1 id="kink-sector">Kink sector</h1> <h2 id="classic-kink-solution">Classic kink solution</h2> <h2 id="displacement-operator">Displacement operator</h2> <p>In order to find the approximate Hilbert state with correct vacuum expectation value of $\phi$, we introduce the displacement operator. Given a function $f(\vec{x})$, the associated displacement operator is defined as</p> \[\mathcal{D}_ {f} := \exp \left\lbrace -i\pi(f) \right\rbrace , \quad \pi(f):=\int d^{3}x \, f(\vec{x})\pi(\vec{x}).\] <p>$\pi(f)$ is the <strong>smeared</strong> canonical momentum operator. This definition is similar to the space translation operator</p> \[T_ {\vec{x}} = \exp \left\lbrace -i\hat{P}\cdot \vec{x} \right\rbrace .\] <p>The translation operator changes the position, while the displacement operator changes the vev of the field operator itself. In other words, $T_ {\vec{x}}$ translates in the configuration space, while $\mathcal{D}_ {f}$ translates in the function space. To be more specific, we have</p> \[[\phi(\vec{x}),\mathcal{D}_ {f}] = e^{ -i\pi(f) } [\phi(\vec{x}),-i\pi(f)] =f(\vec{x}) \mathcal{D}_ {f},\] <p>hence</p> \[\mathcal{D} \phi \mathcal{D}^{\dagger}=\phi-f.\] <p>Since $\mathcal{D}_ {f}^{-1}=\mathcal{D}_ {-f}$, we have the following comparison:</p> \[\begin{align*} T_ {\vec{x}}^{\dagger}\, \phi(\vec{y})\, T_ {\vec{x}} &amp;= \phi(\vec{y}+\vec{x}),\\ \mathcal{D}_ {f} ^{\dagger} \, \phi(\vec{x})\, \mathcal{D}_ {f} &amp;= \phi(\vec{x})+f(\vec{x}). \end{align*}\] <p>The point is that <strong>we can use the unitary $\mathcal{D}_ {f}$ to translate the basis of the Hilbert space.</strong></p> <p>I find it helpful to take $T_ {\vec{x}}$ as an analogy to understand $\mathcal{D}_ {f}$. Let $\left\lvert{\psi}\right\rangle$ be any state, the resulting state of $T_ {\vec{x}}$ acting on $\left\lvert \psi \right\rangle$, namely the state $T_ {\vec{x}}\left\lvert{\psi}\right\rangle$, has two interpretations: the passive view and the active view. In the passive view, the translation is seen as a change to the coordinate system or reference frame, rather than a change to the state itself. The state of the system remains unchanged, but the coordinates used to describe it are shifted. In contrast, in the active view, the translation actively shifts the state, while the coordinate system remains the same. The coordinates stay fixed, but the state (or field configuration) changes.</p> <p>The same applies to the displacement operator $\mathcal{D}_ {f}$, given any state (now we are working in the context of quantum field theory, rather than quantum mechanics) $\left\lvert{\Psi}\right\rangle$, $\mathcal{D}_ {f}\left\lvert{\Psi}\right\rangle$ can be interpreted either as a new state (active perspective), or as the same state but in different basis (passive perspective).</p> <p>When it comes to the discussion about kink states and trivial vacuum states, my personal taste is to think of $\mathcal{D}_ {f}$ passively. This approach allows us to discuss the same states in different bases, or “frames”.</p> <p>$\mathcal{D}_ {f}$ “shifts” $\left\lvert{\varphi}\right\rangle$ by $f(\vec{x})$ as</p> \[\mathcal{D}_ {f}\left\lvert{h(\vec{x})}\right\rangle = \left\lvert{h(\vec{x})+f(\vec{x})}\right\rangle .\] <p>As you can verify, we also have</p> \[\left\langle{\Psi}\right\rvert \mathcal{D}_ {f}^{\dagger} \hat{\phi}(\vec{x}) \mathcal{D}_ {f}\left\lvert{\Psi}\right\rangle = \psi(\vec{x})+f(\vec{x}),\] <p>as we showed before. We can regard $\mathcal{D}_ {f}\left\lvert{\Psi}\right\rangle$ as the same state as $\left\lvert{\Psi}\right\rangle$, just measured with different basis: instead of $\left\langle \varphi \middle\vert \Psi \right\rangle$, we have $\left\langle \varphi+f \middle\vert \Psi \right\rangle$.</p> <hr/> <p>From the expansion</p> \[\phi(\vec{x}) = \int \frac{d^{d}p}{(2\pi)^{d}\sqrt{2\omega_ {p}}} \, (a_ {p} e^{ i\vec{p}\cdot \vec{x} }+h.c.)\] <p>and $\mathcal{D}\phi \mathcal{D}^{\dagger}=\phi-f$, we can determine how $\mathcal{D}$ sandwiches $a_ {p}$:</p> \[\mathcal{D}_ {f} a_ {p} \mathcal{D}_ {f}^{\dagger} = a_ {p} - \widetilde{a}_ {p}\] <p>where $\widetilde{a}$ is the coefficient of the kink solution,</p> <p>\(f(\vec{x}) = \int \frac{d^{d}p}{(2\pi)^{d}\sqrt{2\omega _ {p} }} \, (\widetilde{a}_ {p}e^{ i\vec{p}\cdot \vec{x} }+\widetilde{a}_ {p}^\ast e^{ -i\vec{p}\cdot \vec{x} }) .\)</p> <h2 id="field-operator-in-the-kink-sector">Field operator in the kink sector</h2> <p>In the language of Lagrangian or path integral, perturbation method is relatively straightforward. Consider the scalar $\phi^{4}$ theory with spontaneous symmetry breaking,</p> \[\mathcal{L} = \frac{1}{2} (\partial_ {\mu}\phi)^{2} + \frac{1}{4} m^{2} - \frac{\lambda}{4}\phi^{4}.\] <p>After the spontaneous symmetry breaking the vacuum is shifted from $\left\langle \phi \right\rangle=0$ to $\left\langle \phi \right\rangle= m / \sqrt{2\lambda}$. Now, we want to study the quantum fluctuation of real scalar field $\phi$ in the back ground $\phi=\frac{m}{\sqrt{2\lambda}}$, what we do is redefine the field operator as $\phi\to \frac{m }{\sqrt{ 2\lambda }}+\phi’$, so that the new vacuum is obtained at $\phi’$ equals zero. We treat $\phi’$ as the fundamental quantum field. The small fluctuation about $\frac{m }{\sqrt{ 2\lambda }}$ gives the quantum effects via partition function</p> \[Z[J] = \int D\phi' \, \exp \left\lbrace iS[\phi']+i \int \phi' J \right\rbrace ,\] <p>about $\phi’\sim 0$. We say that this partition function is perturbative about $\phi’=0$.</p> <hr/> <p>Above is in Lagrangian formalism, we know there is also Hamiltonian formalism, and anything that can be done with one formalism can equally be done in the other, it is just a matter of convenience.</p> <p>The fundamental idea is similar to the perturbative calculation we did in quantum mechanics, but generalized to quantum field theory. The key idea is that to perform the perturbation calculation about a state $\left\lvert \Psi \right\rangle$, we need to find the basic field operator $\phi’$ that has zero vev, $\left\langle \Psi \right\rvert\phi’\left\lvert \Psi \right\rangle=0$. Then and only then can we apply the standard perturbation method to $\phi’$ and $\left\lvert \Psi \right\rangle$.</p> <p>Turns out, the desired basic field operator $\phi’$ differs from the original field operator just by an unitary transformation, given by the displacement operator. Recall the defining property of displacement operator, $\mathcal{D}_ {f}^{\dagger}\phi \mathcal{D}_ {f}=\phi+f$.</p> <p>Let’s start with a trivial case. We know that the phi fourth model admits SSB. Assume we want to perform perturbative calculation in the symmetry broken phase $\left\lvert 0^{+} \right\rangle$, where the original field operator $\phi$ has vev $\left\langle 0^{+} \right\rvert \phi \left\lvert 0^{+} \right\rangle=-v$, where</p> \[v = -\frac{m }{\sqrt{ 2\lambda}}.\] <p>Use the displacement operator to transform $\phi$, we have (at least at leading order)</p> \[\left\langle{0^{+}}\right\rvert \mathcal{D}_ {v }^{\dagger} \phi \mathcal{D}_ {v }\left\lvert{0^{+}}\right\rangle = \left\langle{0^{+}}\right\rvert \phi+v \left\lvert{0^{+}}\right\rangle = \frac{m }{\sqrt{ 2\lambda }}-\frac{m }{\sqrt{ 2\lambda }}=0,\] <p>hence</p> \[\boxed{ 0=\left\langle{0^{+}}\right\rvert \phi'\left\lvert{0^{+}}\right\rangle, \quad \phi':= \mathcal{D}_ {v }^{\dagger}\phi \mathcal{D}_ {v }. }\] <p>It inspires us to define a new Hamiltonian exactly in the same fashion,</p> \[\boxed{ \mathcal{H} := \mathcal{D}_ {v }^{\dagger} \hat{\mathcal{H}} \mathcal{D}_ {v }=\hat{\mathcal{H}}(\mathcal{D}^{\dagger}\phi \mathcal{D}) = \hat{\mathcal{H}}(\phi'). }\] <p>We are interested in the spectrum of the Hamiltonian. The good news is that, a unitary transformation preserves all the observables! Thus, in order to study the quantum correction, instead of working with the triplet $\hat{H}$, $\phi$ and $\left\lvert{0^{+}}\right\rangle$ where perturbative method fails, we can work with $H, \phi’$ and $\left\lvert{0^{+}}\right\rangle$ where $\phi’$ can be dealt with perturbatively.</p> <p>Generally speaking, let $\mathcal{O}$ be any operator and $\left\lvert{\psi}\right\rangle$ its eigenstate. If $\mathcal{O}’=\mathcal{D}^{\dagger}\mathcal{O} \mathcal{D}$ is the unitary transformation of $\mathcal{O}$, then its eigenstate is $\mathcal{D}^{\dagger}\left\lvert{\psi}\right\rangle$, not $\left\lvert{\psi}\right\rangle$. It may seem weird to some people to put $\mathcal{O}’$ together with $\left\lvert{\psi}\right\rangle$ rather than $\mathcal{D}^{\dagger}\left\lvert{\psi}\right\rangle$. In fact, it is exactly the point of our method! If we pair $\mathcal{O}’$ with $\mathcal{D}^{\dagger}\left\lvert{\psi}\right\rangle$ it would be a trivial transformation, we will get nothing new! Paring up $\mathcal{O}’$ and $\left\lvert{\psi}\right\rangle$ makes it possible for perturbation methods.</p> <p>However, since $\left\lvert{\psi}\right\rangle$ is not the eigen state of $\mathcal{O}’$, it indeed raise some troubles, the first and foremost is that now $\mathcal{O}’$ is in general not diagonalized by $\left\lvert{\psi}\right\rangle$, we need to find a way to diagonalized it. In our current case, since the displacement operator only shifts the operators by a constant, it actually preserves the diagonalization, meaning if $\hat{\mathcal{H}}$ is diagonalized in $\left\lvert{\psi}\right\rangle$’s then $\mathcal{D}^{\dagger} \hat{\mathcal{H}}\mathcal{D}$ is also diagonalized in $\left\lvert{\psi}\right\rangle$’s. But for a generic $\mathcal{D}_ {f}$ with non-trivial $f$, this will no longer be true. That would be the topic for the other half the the note, after we introduce the kink solution.</p> <p>A comparison between displacement operator method to the case of regular functions might be helpful.</p> <table> <thead> <tr> <th style="text-align: center">Functions</th> <th style="text-align: center">perturbative QFT</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">A functions $f(x)$, a neighborhood about a special locus $x_ 0$ where the interesting things happen.</td> <td style="text-align: center">A Hilbert space, a quantum state $\left\lvert{\Psi}\right\rangle$ of interest represented by a functional $\left\langle \Psi(x),- \right\rangle$, and some operator $\mathcal{O}(\phi)$.</td> </tr> <tr> <td style="text-align: center">We want to study the function $f(x )$ at $x_ 0$ perturbatively,</td> <td style="text-align: center">We want to study the $\left\langle{\Psi}\right\rvert\mathcal{O}\left\lvert{\Psi}\right\rangle$ perturbatively,</td> </tr> <tr> <td style="text-align: center">but $x_ 0$ too far from the origin, so Maclaurin expansion (Taylor expansion at the origin) fails.</td> <td style="text-align: center">but $\left\langle{\Psi}\right\rvert\phi \left\lvert{\Psi}\right\rangle=:f(x)$ is too large for $\phi$ to be treated perturbatively.</td> </tr> <tr> <td style="text-align: center"><strong>In a passive perspective, we shift the origin</strong> to $x_ 0$, then small deviation from $x_ 0$ can now be studied using Maclaurin expansion perturbatively.</td> <td style="text-align: center">In a passive perspective, we shift the operators, especially the field operator $\phi$ since it is usually the building block of other operators. The expectation value of the new, shifted operator $\phi’$ should be zero, $\left\langle{\Psi}\right\rvert \phi’\left\lvert{\Psi}\right\rangle =0$. Now we can treat $\phi$ perturbatively.</td> </tr> <tr> <td style="text-align: center">Shifted coordinate system to study the same old functions $f(x)$.</td> <td style="text-align: center">Shifted operators $\mathcal{D}^{\dagger}\mathcal{O}\mathcal{D}$ to study the same old states $\left\lvert{\Psi}\right\rangle$.</td> </tr> </tbody> </table> <p>In summary, $\mathcal{H}=\mathcal{D}^{\dagger}_ {v}\hat{\mathcal{H}}\mathcal{D}_ {v}$ is a operator-valued function of $\phi’ = \mathcal{D}_ {v}^{\dagger} \phi \mathcal{D}_ {v}$, and $\phi’$ can be dealt with perturbatively.</p> <hr/> <p>Now let’s apply this go kink background. Denote the generic kink solution as $f(\vec{x})$ in $d$ dimensional space, let the kink state be $\left\lvert K \right\rangle$ and the original field operator be $\phi$. We have</p> \[\left\langle K \right\rvert \phi(\vec{x}) \left\lvert K \right\rangle = f(\vec{x})\] <p>at the leading order. In general $f(\vec{x})$ is inversely proportional to the coupling, for example for $\phi^{4}$ model we have $f\propto 1 / \sqrt{\lambda}$.</p> <p>Denote the perturbative field operator in the kink sector as $\varphi$, it should satisfy</p> \[\left\langle K \right\rvert\varphi \left\lvert K \right\rangle=0.\] <p>You can verify that</p> \[\boxed{ \varphi = \mathcal{D}_ {f}^{\dagger} \phi \mathcal{D}_ {f}, }\] <p>we will neglect the subscript $f$ is it is clear from the context.</p> <p>As you can easily check, the kinky field operator $\varphi$ satisfies the same commutation relation with $\mathcal{D}$ as $\phi$:</p> <p>\([\varphi,\mathcal{D}_ {f}] = f \mathcal{D}_ {f}.\)</p> <h2 id="kink-field-operator-and-kink-hamiltonian">Kink field operator and kink Hamiltonian</h2> <p>If we write the same Hamiltonian (note that it has to be the same Hamiltonian no matter what perturbative basic field we choose to use, otherwise we would be solving a different problem) in terms of $\varphi$, we have</p> \[H[\phi] = H[\mathcal{D} \varphi \mathcal{D}^{\dagger}] = \mathcal{D}H[\varphi]\mathcal{D}^{\dagger} =: H'[\varphi].\] <p><strong>Things to put here:</strong> <strong>Define the free (quadratic) Hamiltonian of kink Hamiltonian.</strong></p> <hr/> <p>Assume the kink $f(x)$ as a static, classical background, about which we expand the field operator as fluctuation. Write the fluctuation as $\mathfrak{g}(x,t)$ and assume that we can separate the time ans position coordinates,</p> \[\phi(x,t) =: f(x) + {\mathfrak g}(x) e^{-i\omega t},\] <p>The equation of motion of ${\mathfrak g}$ reads</p> \[[-\omega^{2}-\partial_ {x}^{2}+V^{(2)}(\sqrt{ \lambda }f(x))]\, {\mathfrak g}(x) = 0,\] <p>Which is the Sturm-Liouville equation. A general Sturm-Liouville problem is typically written in the form:</p> \[\frac{d}{dx}\left[ p(x) \frac{dy}{dx} \right] - q(x)y + \lambda r(x)y = 0\] <p>Here, $y$ is the function of the variable $x$ that we are solving for, and $p(x)$, $q(x)$, $r(x)$ are known functions that specify the particular Sturm-Liouville problem. The parameter $\lambda$ is often referred to as the eigenvalue.</p> <p>A few more words on the equation. Key characteristics and applications of the Sturm-Liouville equation include:</p> <ol> <li> <p><strong>Eigenvalue Problem</strong>: The Sturm-Liouville equation is an eigenvalue problem. The solutions $y(x)$ are eigenfunctions, and the associated values of $\lambda$ are eigenvalues. These eigenvalues are typically discrete and can be ordered as a sequence $\lambda_1, \lambda_2, \lambda_3, \ldots$, where each $\lambda_n$ corresponds to a particular eigenfunction $y_n(x)$.</p> </li> <li> <p><strong>Orthogonality and Completeness</strong>: The eigenfunctions of a Sturm-Liouville problem are orthogonal with respect to the weight function $r(x)$. This property is crucial in solving partial differential equations, as it allows the expansion of functions in terms of these eigenfunctions (similar to Fourier series).</p> </li> <li> <p><strong>Boundary Conditions</strong>: Sturm-Liouville problems are typically accompanied by boundary conditions that the solutions must satisfy. These conditions are usually specified at the endpoints of the interval in which the equation is defined.</p> </li> </ol> <p>In our case, the weight function is trivial.</p> <p>We will denote the zero mode (eigenfunction with eigenvalue zero) by ${\mathfrak g}_ {B}$ and the shape mode (the next level up to zero mode) by ${\mathfrak g}_ {S}$. The $B$ in ${\mathfrak g}_ {B}$ has a historical reason (B stands for bounded state), but in our note it is just part of the name. These modes include a continuum</p> \[{\mathfrak g} _ {k}(x) = \frac{e^{-ikx}}{\omega_ {k} \sqrt{m^2+4k^2}}\left[2k^2-m^2+(3/2)m^2\text{sech}^2(m x/2)-3im k\tanh(m x/2)\right]\] <p>with eigenvalue $\omega_ {k} = \sqrt{ k^{2}+m^{2} }$, a zero mode</p> \[{\mathfrak g}_ {B} = -\sqrt{ \frac{3m}{8} } \text{sech}^{2}\left( \frac{mx}{2} \right)\] <p>with eigenvalue $0$, and a shape mode</p> \[{\mathfrak g} _ {S} = \frac{\sqrt{ 3m }}{2} \tanh \frac{mx}{2} \text{sech} \frac{mx}{2},\quad \omega_ {S} = \frac{\sqrt{ 3 }}{2}m\] <p>with eigenvalue less then the rest mass of excited particle.</p> <p>The normalization conditions are</p> \[\int dx \, {\mathfrak g}_ {S}^{2} = \int dx \, {\mathfrak g}_ {B}^{2} = 1 , \quad \int dx \, {\mathfrak g}_ {B}{\mathfrak g}_ {S} = 0 ,\quad \int dx \, {\mathfrak g}_ {k}(x){\mathfrak g}_ {p}(x) = 2\pi i\delta(p-k).\] <p>The completion condition reads</p> \[\sum\!\!\!\!\!\!\!\!\int \; \frac{dk}{2\pi} \, \mathfrak{g} _ {k} (x)\mathfrak{g} _ {k} ^\ast (y)=\delta(x-y).\] <p>and $\mathfrak{g}_ {k}^\ast(x)=\mathfrak{g}_ {-k}(x)$.</p> <p>The sign of ${\mathfrak g}_ {B}$ is fixed using</p> \[{\mathfrak g}_ {B}(x) = - \frac{f'(x)}{\sqrt{ Q_ {0} } },\] <p>where $f$ is again the kink solution.</p> <p>This can be generalized to more than one spatial dimension. Take two dimension for example, without loss of generality, we can choose to lie the kink in the $x$ direction.</p> <p>The important thing is that, the free kink Hamiltonian is diagonalized by normal mode. what I mean is that, if we expand the kink field in terms of normal modes:</p> \[\begin{align*} \varphi(x) &amp;= \sum\!\!\!\!\!\!\!\!\int \frac{ dk}{(2\pi )\sqrt{2\omega _ {k} } } \, \left( b_ {k} {\mathfrak g}_ {k} (x)+h.c. \right),\\ \pi(x) &amp;= \sum\!\!\!\!\!\!\!\!\int \frac{dk}{(2\pi ) } \frac{-i\sqrt{\omega_ {k}}}{\sqrt{2}}\, \left( b_ {k}\mathfrak{g}_ {k} (x) - b_ {k}^{\dagger}\mathfrak{g}_ {k} ^\ast (x) \right){\mathfrak g}_ {k} (x), \end{align*}\] <p>where</p> \[\sum\!\!\!\!\!\!\!\!\int \, dk := \sum_ {B,S}+\int \frac{dk}{2\pi}.\] <p>it can be verified that</p> \[[b_ {k} ,\mathcal{D}_ {f}] = \mathcal{D}_ {f} \frac{\mathfrak{f} _ {k}+\mathfrak{f} _ {-k}^\ast }{2},\] <p>where $\mathfrak{f}_ {k}$ is the coefficient in spanning $f(x)$ in terms of $\mathfrak{g}_ {k}(x)$:</p> \[f(x)=\sum\!\!\!\!\!\!\!\!\int \; \frac{dk}{2\pi} \, \frac{1}{\sqrt{2\omega _ {k} }}(\mathfrak{f} _ {k} \mathfrak{g} _ {k} (x)+\text{c.c.}),\] <p>where c.c. stands for complex conjugation. From this commutation relation we can see that if $\left\lvert 0 \right\rangle$ is a state annihilated by $b_ {k}$, then $\mathcal{D}_ {f}\left\lvert 0 \right\rangle$ is a coherent state regarding $b_ {k}$ since</p> \[b_ {k} \mathcal{D}_ {f}\left\lvert 0 \right\rangle = \mathcal{D}_ {f}\mathcal{D}_ {f}^{\dagger}b_ {k} \mathcal{D}_ {f} \left\lvert 0 \right\rangle= \mathcal{D}_ {f}(b_ {k} +(\cdots))\left\lvert 0 \right\rangle =(\cdots) \mathcal{D}_ {f}\left\lvert 0 \right\rangle,\] <p>where $(\cdots)=\frac{\mathfrak{f} _ {k}+\mathfrak{f} _ {-k}^\ast }{2}$ is a c-number.</p> <hr/> <p>in normal modes (in the kink background), while expanding in the $y$ direction in plane waves. The 2D momentum is $\vec{k}=\left\lbrace k_ {x},k_ {y} \right\rbrace$, where $k_ {x}=\left\lbrace B,S,k \right\rbrace$, $B$ for the zero mode (bounded solution), $S$ for the shape mode (also bounded) and $k$ for the continuum. A nice illustration of normal modes in the background of kink is shown in the figure below, which I shamelessly copied from Tanmay Vachaspati’s book, all the credits goes to Vachaspati.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/kinkLevel-480.webp 480w,/img/kinkLevel-800.webp 800w,/img/kinkLevel-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/kinkLevel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A trivial potential on a periodic space with period $L$ is shown on the left, while the normal modes in the background of a kink solution is shown on the right. What used to be the $n=0$ mode in the trivial potential (on the left) becomes the lowest bound state, the zero mode, in the non-trivial potential. Similarly a linear combination of the $n=\pm 1$ modes in the trivial case may become the second bound state ($n=+1$ in the illustration), and the other states remain unbounded but shift in form. </div> <p>We have omitted the vector sign (or bold font) in $r$ since it would not cause any misunderstanding. We assume (quite reasonably) the separation of variables $x$ and $y$ for 2D normal modes ${\mathfrak g}(r)$,</p> \[{\mathfrak g}(r) = {\mathfrak g}_ {x}\times g_ {y},\quad {\mathfrak g}_ {x} = \text{kink normal modes},\, {\mathfrak g}_ {y} = \text{plane waves.}\] <p>The quantization in terms of $\phi$ and $\pi$ reads</p> \[[\phi(r),\pi(r')] = i\delta^{(d)}(r-r').\] <p>This represents the fundamental quantization relation, unaffected by the selection of sectors. We haven’t given a formal definition of sectors, roughly speaking, within each sector, there exists a distinct set of normal modes for expanding both $\phi$ and $\pi$. Each mode must conform to the aforementioned relation, namely the quantization relation given in space-time positions $r$. Ultimately, the difference across different sectors lies in the diverse backgrounds (regarded as classical functions) used for field expansion. However, as we are analyzing the same theory within the same space-time, the theory should be quantized only once, and, all sectors must consistently align with the same quantization process.</p> <h2 id="need-for-a-new-set-of-ladder-operators">Need for a new set of ladder operators</h2> <h2 id="squeeze">Squeeze!</h2> <h1 id="linearized-soliton-perturbation-method-lspt">Linearized soliton perturbation method (LSPT)</h1> <p>Here we summarize the linearized soliton perturbation theory associated with generic solitonic classic solutions $f_ {\text{sol}}$</p> <hr/> <h1 id="appendix">Appendix</h1> <h2 id="loop-expansion-semi-classical-expansion-and-coupling-expansion">Loop expansion, semi-classical expansion and coupling expansion</h2> <p>There are three different but expansions that we usually don’t extinguish:</p> <ul> <li>Loop expansion, the expansion parameter is the number of loop. The leading contributions are tree diagrams.</li> <li>Semi-classical expansion, the expansion parameter is the Plank constant $\hbar$. At leading order, $\hbar=0$, we are left with classical contributions.</li> <li>Coupling expansion, the expansion parameter is the coupling $g,\lambda, \cdots$.</li> </ul> <p>Set the speed of light $c=1$. The action functional $S[\phi]$ has the unit of $[\hbar]=ET$, where $E$ is the energy unit and $T$ the time. The textbook proof of the equivalence between <em>loop expansion</em> and <em>semi-classical expansion</em>, i.e. $\hbar$-expansion usually goes as follows. First write the partition function (with the source term) with $\hbar$:</p> \[Z[J(x)] = \int \mathcal{D}\phi \, \exp \left\lbrace \frac{i}{\hbar} \int d^{d}x \, (\mathcal{L}+ J \phi) \right\rbrace ,\] <p>where $J=J(x)$ is the source term. Note that the source term is on the same basis of $\mathcal{L}$. Splitting the Lagrangian into free and interacting part, we can rewrite the interaction using functional derivative and put it in the front of the free partition function:</p> \[Z[J] = \exp \left\lbrace \frac{i}{\hbar}\mathcal{L}_ {\text{int}}\left( \frac{1}{i} \frac{\delta}{\delta J} \right) \right\rbrace Z_ {0}[J],\] <p>where $\mathcal{L}_ {\text{int}}$ is the interacting Lagrangian and</p> \[Z_ {0}[J] = \int \mathcal{D}\phi \, \exp \left\lbrace \frac{i}{\hbar}\int d^{d}x\, (\mathcal{L}_ {0}+ J\phi) \right\rbrace\] <p>is the free partition function with the source. It can be integrated out in the closed form (for the level of rigorous for a physicist) to give</p> \[Z_ {0}[J] = N \exp \left\lbrace - \frac{i}{2}\hbar \int d^{d}x d^{d}y \, J(x) \Delta_ {F}(x-y)J(y) \right\rbrace,\] <p>where $\Delta_ {F}$ is the Feynman propagator. It shows that each propagator contributes a factor of $\hbar$.</p> <p>Next, since each diagrammatic vertex is a result of a functional derivative $\frac{i}{\hbar }\mathcal{L}_ {\text{int}}\left( \frac{\delta}{\delta J} \right)$, each vertex contributes a factor of $\frac{1}{\hbar}$. Then, using some relations between the numbers of loops, propagators and vertices (it depends on the specific model), <strong>amputate the external legs</strong>, it can be shown that: a general graph with $L$ loops is proportional to $\hbar^{L-1}$. Thus an expansion in the number of loops is also an expansion in powers of $\hbar$.</p> <hr/> <p>It turns out that there are more than one way to assign $\hbar$-dependence to parameters in the Lagrangian, such as mass $m$, the coupling $g,\lambda$, etc. A criterion for the “right” choice is that, at $\hbar\to 0$, the quantum theory agrees with the classical theory. As an example, Stanley Brodsky and Paul Hoyer in their <a href="https://arxiv.org/pdf/1009.2313.pdf">paper</a> used the quantum mechanical harmonic oscillator as an example in Eq.(1). The gist is that you can rescale $x$ to $x / \sqrt{ \hbar }$, then the propagator is formally independent of $\hbar$. However, this will change how we view distance, in the $\hbar\to 0$ limit, for a fixed distance $L$, the “length” measure will increase as $1 / \sqrt{ \hbar }$, hence we are going to smaller and smaller area.</p> <p>It is generally understood that each loop contribution to amplitudes is associated with one factor of $\hbar$. However, to fully define the $\hbar\to 0$ limit one need to specify the $\hbar$ dependence of various quantities in the Lagrangian as mentioned before, such as the field operator, the mass, the coupling, etc. This is not as straightforward as one might think, for $\hbar$ not only appears in the action $iS / \hbar$ but also appears in the Lagrangian. In Brodsky’s paper mentioned above, the authors proposed a way to establish the $\hbar$ dependence such that the loop and $\hbar$ expansions are equivalent. We will go to more details in the following.</p> <p><strong>First, regard $\hbar$ as a constant of nature with certain dimension, use $\hbar$ to make terms in the Lagrangian dimensionless.</strong></p> <p>Again let’s work with the assumption that $c = \epsilon_ {0} = 1$. Require $[S]=\hbar$, and $\alpha_ {s} = g^{2} / 4\pi \hbar$ is dimensionless, the latter implies that $[g]=[\sqrt{ \hbar }]=\sqrt{ ET }=\sqrt{ EL }$. From the self-energy of gluons $G_ {\mu \nu}G^{\mu \nu}$ where $G = \partial A - \partial A +ig / \hbar [A,A]$ we have</p> \[[A] = \sqrt{ \frac{E}{L} }.\] <p>Similarly, in scalar QED model the classical electric charge $e$ and mass $m$ are divided by $\hbar$,</p> \[S_ {\text{sQED} } = \int d^{4}x \, \left\lbrace \left\lvert D\phi \right\rvert ^{2}-\frac{m^{2} }{\hbar^{2} }\left\lvert \phi \right\rvert ^{2} \right\rbrace , \quad D = \partial +i \frac{e}{\hbar }A.\] <p>The boson field dimension</p> \[[\phi]=[A]= \sqrt{ \frac{E}{L} }.\] <p>Fermion fields are more complicated, since they have no classical counterparts, their dimensions are convention-dependent. We will deal with fermions in a different note perhaps.</p> <p><strong>Second step is to specify $\hbar$ dependence of all quantities appearing in the action.</strong></p> <p>The choice made by Brodsky and Hoyer is as following. Define</p> \[\widetilde{A}:= \frac{A}{\sqrt{ \hbar } },\quad \tilde{\phi}:= \frac{\phi}{\sqrt{ \hbar } }\] <p>where $\widetilde{A},\tilde{\phi}$ are $\hbar$-independent. Similarly, define the following $\hbar$-independent quantities</p> \[\widetilde{g}:= \frac{g}{\hbar},\quad \widetilde{e}:= \frac{e}{\hbar},\quad \widetilde{m}:= \frac{m}{\hbar}.\] <p>Then one can write the Lagrangian in terms of these $\hbar$-independent quantities to check the $\hbar$ dependence explicitly. It turns out that, at least in the simple models discussion in the paper, $\hbar$ always appears in the combination</p> \[\widetilde{g}\sqrt{ \hbar } \quad \text{and}\quad \widetilde{e}\sqrt{ \hbar }\] <p>that is, with the coupling. Hence loop correction of $\mathcal{O}(g^{2},e^{2})$ will be of order $\hbar$.</p> <p>This derivation is equivalent to the standard textbook treatments, for example in Mark Srednicki’s textbook, which associates a factor $\hbar$ to each propagator and $h^{-1}$ with each vertex, and assume that the parameters appearing in the action are independent of $\hbar$.</p> <p>Fore more details refer to Brodsky and Hoyer’s paper mentioned above. The takeaway is that, the correspondence between loop expansion and $\hbar$ expansion is widely accepted, taken for granted, but convention dependent.</p> <p>The semi-classical approximation is also a result of small fluctuation about certain classical background. This can be seen from the <code class="language-plaintext highlighter-rouge">Steepest descent method</code>. For an amazing introduction, see chapter 6 of the book by Carl Bender and Orszag <a href="http://link.springer.com/10.1007/978-1-4757-3069-2">Advanced Mathematical Methods for Scientists and Engineers I</a>. The gist is that the integral can be approximated by an asymptotic power expansion, the leading term is given by its stationary value:</p> \[\int_ {-\infty}^{\infty} dx \, e^{ -S(x) / \hbar } f(x) \sim e^{ -S(x_ {0}) / \hbar }f(x_ {0}) \frac{\sqrt{2\pi \hbar}}{\sqrt{\partial^{2}S{\Large\mid}_ {x_ {0}} }} \, A(\hbar)\] <p>where $x_ {0}$ is the stationary point and $A(\hbar)$ is an asymptotic power series in $\hbar$,</p> \[A(\hbar) = 1+A_ {1} \hbar + A_ {2} \hbar^{2} + \cdots\] <p>which, being an asymptotic series, eventually diverges. The coefficients, $A_ {1}, A_ {2}$, etc, can be calculated using loop expansion.</p> <h2 id="some-mathematical-preliminaries">Some Mathematical Preliminaries</h2> <p>An <code class="language-plaintext highlighter-rouge">algebra</code> over a field $k$ is a vector space $A$ over $k$, equipped with an additional structure: a bilinear multiplication $\cdot: A \times A \to A$ that makes $A$ a ring (not necessarily commutative). That is, $A$ has:</p> <ul> <li>Addition $+$ and scalar multiplication $\cdot$, making $A$ a $k$-vector space,</li> <li>A bilinear multiplication satisfying associativity.</li> </ul> <p>More formally, an algebra $A$ is a vector space over field $k$ together with a map:</p> \[\mu: A \times A \to A, \quad (a, b) \mapsto ab\] <p>that satisfies the ring axioms and is $k$-bilinear, meaning:</p> \[(ab + c)d = ab \cdot d + c \cdot d, \quad a(b + c)d = ab \cdot d + ac \cdot d\] <p>for all $a, b, c, d \in A$. Examples of Algebras include Matrix algebra (the space of $n \times n$ matrices over a field $k$, with matrix addition and multiplication), Polynomial algebra and Function algebra.</p> <p>An algebra $A$ is said to be <code class="language-plaintext highlighter-rouge">unital</code> if it contains a multiplicative unit $1$.</p> <hr/> <p>Denote by $\mathcal{E}$ a complex Hilbert space, with inner product $\left\langle - \middle\vert- \right\rangle$, and $\mathcal{B}(\mathcal{E})$ the set of all bounded operators. In is a Banach algebra with the <code class="language-plaintext highlighter-rouge">operator norm</code>, which is defined as</p> \[\left\lVert A \right\rVert = \text{sup}\, \left\lVert Av \right\rVert \quad \;\forall\; \left\lVert v \right\rVert =1.\] <p>Let $\xi_ {1},\xi_ {2} \in\mathcal{E}$. For $a\in \mathcal{B}(\mathcal{E})$, the adjoint operator $a^{\ast}$ is defined by</p> \[\left\langle a^{\ast }\xi_ {1} \middle\vert\xi_ {2} \right\rangle = \left\langle \xi_ {1} \middle\vert a \xi_ {2} \right\rangle .\] <p>The adjoint operation is an <code class="language-plaintext highlighter-rouge">involution</code>, i.e. it is anti-linear (meaning $(\lambda a)^{\ast}=\overline{\lambda} a^{\ast}$, where $\overline{\lambda}$ is the complex conjugate of $\lambda$) and satisfies $(ab)^{\ast}=b^{\ast}a^{\ast}$.</p> <p>For it to be a $\mathbb{C}^{\ast}$-algebra, it must satisfy the $\mathbb{C}^{\ast}$-identity</p> \[\left\lVert a^{\ast }a \right\rVert = \lVert a \rVert ^{2}.\] <p>If the $\mathbb{C}^{\ast}$-algebra is Cauchy-complete under the operator norm, then it is said to be a concrete $\mathbb{C}^{\ast}$-algebra. By default all the $\mathbb{C}^{\ast}$-algebras we talk about will be concrete, so we will ignore it.</p> <hr/> <p>A <code class="language-plaintext highlighter-rouge">module</code> over an algebra $A$ is a generalization of a vector space, where the scalars used for multiplication come from the algebra $A$ rather than a field. Te be precise, an $A$-module $M$ is an abelian group (under addition) together with a multiplication map:</p> \[A \times M \to M, \quad (a, m) \mapsto a \cdot m\] <p>satisfying the following conditions for all $a, b \in A$, $m, n \in M$, and $\lambda \in k$:</p> <ol> <li>Distributivity: $(a + b) \cdot m = a \cdot m + b \cdot m$,</li> <li>$a \cdot (m + n) = a \cdot m + a \cdot n$,</li> <li>$(\lambda a) \cdot m = \lambda (a \cdot m)$,</li> <li>$(ab) \cdot m = a \cdot (b \cdot m)$.</li> </ol> <p>Roughly speaking, a vector space is just a module over a field. A module over a commutative algebra $A$ can be thought of as a generalization of a vector space where scalars come from $A$ instead of $k$.</p> <p>Examples of modules over algebras include Module over matrix algebras, Polynomial modules, etc.</p> <p>A module is said to be <code class="language-plaintext highlighter-rouge">simple</code> if it does not have nonzero proper submodule.</p> <hr/> <p><strong>Definition.</strong> <code class="language-plaintext highlighter-rouge">Endomorphism algebra</code>. If $V$ is a $k$-vector space, then $\text{End}(V)$ is an algebra formed of the the set of all linear maps from $V$ to $V$ itself (endomorphisms), where the multiplication of two elements is given by composition.</p> <p>For example, let $V$ be $\mathbb{R}^{n}$, then the endomorphism algebra $\text{End}(V)$ is the $n\times n$ real matrices.</p> <p><strong>Definition.</strong> <code class="language-plaintext highlighter-rouge">Algebra representation</code>. Let $A$ be a $k$-algebra, a representation of $A$ is a homomorphism between algebras that maps $A$ to $\text{End}(V)$ for some $k$-module $V$.</p> <p>In human’s language, to define a representation of an abstract algebra, we first choose a module that the algebra act on, then define how exactly each element of the algebra acts on $V$.</p> <hr/> <p>An introduction of category theory can be found in my blogs <a href="https://www.mathlimbo.net/blog/2023/Basic-Category-Theory-Lecture-1/">here</a> and <a href="https://www.mathlimbo.net/blog/2023/Basic-Category-Theory-Lecture-2/">here</a>. A blog explicitly devoted to Yoneda lemma which is sometime useful in dealing with gauge theory can be found <a href="https://www.mathlimbo.net/blog/2024/Yoneda-Lemma/">here</a>, but I doubt we will need it in this note.</p> <hr/> <p>Quantum mechanics is defined over the spatial coordinates $\vec{x}$. The position eigenstates $\left\lvert{\vec{x}}\right\rangle$ form a complete basis of the Hilbert space. However, $\left\lvert{\vec{x}}\right\rangle$ is represented by a Dirac $\delta$-function, which does not belong to the Hilbert space of $L^{2}(\mathbb{R}^{d})$, where $d$ is the dimension of the space, as usually. It is a challenge to mathematical rigor of quantum mechanics.</p> <p>Recall that the Hilbert space $L^{2}(\mathbb{R}^{d})$ consists of all square-integrable functions defined on $\mathbb{R}^{d}$, with an inner product $\left\langle f,g \right\rangle=\int dx \, f^{\ast}g$. This inner product also defines a norm, and with norm we can talk about Cauchy completeness. The space $L^{2}$ is indeed Cauchy complete, since we do not require functions in it to be smooth. The Cauchy completeness makes it a Hilbert space, not a pre-Hilbert space.</p> <p>To deal with mathematical objects such as the Dirac $\delta$-function, quantum mechanics often uses the concept of <code class="language-plaintext highlighter-rouge">rigged Hilbert space</code>, also known as a <code class="language-plaintext highlighter-rouge">Gelfand triplet</code>. A rigged Hilbert space involves three components:</p> <ol> <li>Schwartz space $\mathcal{S}$, the space of <strong>rapidly decreasing smooth</strong> functions. It is a dense subspace of the Hilbert space that we talk about in the next paragraph. By working with $\mathcal{S}$, we can rigorously define unbounded operators and their domains. States here are smooth and decays faster than any polynomial, making them suitable for rigorous operator definitions.</li> <li>Hilbert space $L^{2}$. The Schwartz space $\mathcal{S}$ is a dense subset of $L^{2}$. Here is where all the quantum states reside. These states are normalizable and have finite norm.</li> <li>Dual space $\mathcal{S}^{\ast}$. It includes functionals, also called generalized states. The Dirac delta function represents the position eigenstate, while plane waves represent momentum eigenstates, we can’t fit them into the Hilbert space $L^{2}$, here is where these pathological states reside.</li> </ol> <p>The relationship can be written as</p> \[\mathcal{S} \subset L^{2} \subset \mathcal{S}^{\ast }.\] <p>Actually, I don’t think it makes a lot of sense to regard $\left\lvert{\vec{x}}\right\rangle$ as a physical state, since in real life, due to the uncertainty principal, a particle will never be localized at a specific point. Instead, the position always smears about a certain region, so is its momentum. The role of $\left\lvert{\vec{x}}\right\rangle$ is more of giving us the value of the wave function at position $\vec{x}$, in other words, what naturally appears is the dual version $\left\langle{\vec{x}}\right\rvert$ of $\left\lvert{\vec{x}}\right\rangle$. $\left\langle{\vec{x}}\right\rvert$ can be regarded as map that takes a state in the Hilbert space of quantum states, and spits out a complex number:</p> \[\begin{align*} \left\langle{\vec{x}}\right\rvert : \quad \mathcal{H} &amp;\to \mathbb{C} \\ \left\lvert{\psi}\right\rangle &amp;\mapsto \psi(\vec{x}), \end{align*}\] <p>where $\mathcal{H}$ is not the Hamiltonian but the Hilbert space of quantum states.</p> <p>For a state $\left\lvert{\psi}\right\rangle$ to be a physical state, it should be sensible to talk about</p> \[\left\langle{\psi}\right\rvert \mathcal{O} \left\lvert{\psi}\right\rangle ,\quad \mathcal{O}\text{ is an observable.}\] <p>$\left\lvert{\vec{x}}\right\rangle$ fails this requirement since $\left\langle{\vec{x}}\right\rvert \hat{x} \left\lvert{\vec{x}}\right\rangle=\infty$, this is another reason to say that $\left\lvert{\vec{x}}\right\rangle$ is not a physical state.</p> <hr/> <p>We distinguish two concepts, <code class="language-plaintext highlighter-rouge">states</code> and <code class="language-plaintext highlighter-rouge">wave function</code>. In quantum mechanics, they are sometimes used interchangeably, but they have distinct meanings. A quantum state contains all the information about the system. It can be described in different representations, depending on what information we want to know. For example, if we want to know information about the position, we go to the physical space representation expanded by $\left\lvert{\vec{x}}\right\rangle$. Quantum states can be represented as vectors in a Hilbert space, and there exists pure and mixed states. On the other hand, the wave function is a specific representation of a quantum state in the position (or sometimes momentum) basis. It is a complex-valued function that gives the probability amplitude of finding a particle in a particular position in space. The wave function of a state $\left\lvert{\psi}\right\rangle$ is denoted by $\psi(x)$, where $x$ is the position.</p> <p>In the passive perspective, a state before and after spatial translation is the same state, they just have different wave functions, because in order to get the wave function we need two things: bases and a state, even though the state is unchanged but the bases are changed now, thus the final expression is also changed.</p> <p>Consider a scalar field theory. All the possible configuration of $\phi$ form the configuration space $\mathcal{C}$, each point represents a specific field function $(\mathcal{M}\times\mathbb{R})\to \mathbb{C}$, where $\mathcal{M}$ is the space manifold and $\mathbb{R}$ the time dimension. The wave functional $\Psi[f(x)]$ describes the quantum state at a given time, we have</p> \[\Psi: \mathcal{C} \to \mathbb{C}, \quad f (\vec{x}) \mapsto \Psi[f].\] <p>Similar to quantum mechanics where $\left\lvert{\vec{x}}\right\rangle$ is the eigen state of $\hat{x}$ and $\hat{x}\left\lvert{\vec{x}}\right\rangle=\vec{x}$, in QFT the eigen state of field operator $\hat{\phi}$ satisfy</p> \[\hat{\phi}(\vec{x})\left\lvert{\phi}\right\rangle = \phi(\vec{x})\left\lvert{\phi}\right\rangle ,\] <p>Any wave functional can be given as a superposition of such eigen states. Let $\left\lvert{\Psi}\right\rangle$ be a well-behaved wave functions, it can be formally written as</p> \[\left\lvert{\Psi}\right\rangle = \int D\phi(\vec{x}) \, \Psi[\phi(\vec{x})] \left\lvert{\phi(\vec{x})}\right\rangle ,\] <p>We will simply write $\phi$ and omit the spatial coordinate.</p> <p>Given the quantum state $\left\lvert{\Psi}\right\rangle$, the amplitude of a certain configuration $\phi$ is</p> \[\Psi [\phi(\bullet)] = \left\langle \phi \middle\vert \Psi \right\rangle \in \mathbb{C}.\] <p>As a consistency check, let’s see if we can reproduce the result $\left\langle{\Psi}\right\rvert\hat{\phi}(\vec{x}) \left\lvert{\Psi}\right\rangle=\psi(\vec{x})$ for some classical field $\psi(\vec{x})$. We have</p> \[\begin{align*} \left\langle{\Psi}\right\rvert \hat{\phi}(\vec{x})\left\lvert{\Psi}\right\rangle &amp;= \int D\varphi \, \Psi ^\ast [\varphi]\left\langle{\varphi}\right\rvert\, \hat{\phi}(\vec{x})\,\int D\varphi' \, \Psi[\varphi']\left\lvert{\varphi'}\right\rangle \\ &amp;= \int D\varphi \, \Psi^\ast [\varphi] D\varphi' \, \Psi[\varphi']\left\langle{\varphi}\right\rvert\, \varphi'(\vec{x})\left\lvert{\varphi'}\right\rangle \\ &amp;= \int D\varphi \, \Psi^\ast [\varphi] D\varphi' \, \Psi[\varphi'] \varphi'(\vec{x}) \delta(\varphi-\varphi') \\ &amp;= \int D\varphi \, \Psi^\ast [\varphi]\, \Psi[\varphi] \varphi(\vec{x}) \\ &amp;= \int D\varphi \, \left\lvert \Psi[\varphi] \right\rvert^{2} \varphi(\vec{x}), \end{align*}\] <p>and recall that $\left\lvert \Psi[\varphi] \right\rvert^{2}$ is the probability of finding $\varphi$ in $\Psi$, thus indeed we have</p> \[\int D\varphi \, \left\lvert \Psi[\varphi] \right\rvert^{2} \varphi(\vec{x}) = \left\langle \hat{\phi}(\vec{x}) \right\rangle =: \psi(\vec{x}).\] <p>Time evolution is generated by the Hamiltonian, yielding a functional Schrodinger equation:</p> \[i\hbar \left\lvert{\Psi[\phi]}\right\rangle = \hat{H}\left\lvert{\Psi[\phi]}\right\rangle ,\] <hr/> <p>For the ground state (or vacuum state) of the free scalar field, the wave functional $\Psi_0[\phi]$ has a Gaussian form, it is basically an infinite lattice of harmonic oscillators. It can be written as:</p> \[\Psi_0[\phi] = N \exp \left( -\frac{1}{2} \int d^3x \, d^3y \, \phi(x) K(x,y) \phi(y) \right)\] <table> <tbody> <tr> <td>where $N$ is a normalization constant, and $K(x,y)$ is a kernel that depends on the mass $m$ of the scalar field and the spatial separation $</td> <td>x - y</td> <td>$.</td> </tr> </tbody> </table> <p>For a free scalar field, $K(x,y)$ can be written in terms of the Fourier transform:</p> \[K(x,y) = \int \frac{d^3k}{(2\pi)^3} \, \omega_k \, e^{i k \cdot (x - y)}\] <p>with $\omega_k = \sqrt{k^2 + m^2}$.</p> <p>The wave functional $\Psi_0[\phi]$ provides the probability amplitude for the field configuration $\phi(x)$. The exponential form indicates that the ground state is a Gaussian distribution centered around $\phi(x) = 0$, reflecting the fact that the vacuum state has no preferred field configuration (zero field on average).</p> <p>For more information refer to this <a href="https://physics.stackexchange.com/questions/746099/schroedinger-equation-for-wave-functional-qft">post</a>.</p> <h2 id="kink-modes-mathfrakgvecx-in-2d">Kink modes $\mathfrak{g}(\vec{x})$ in 2D</h2> <p>Since we assumed the domain wall to be lying flat in the $y$-plane, the normal modes in 2-d space can be <em>factorized</em> into $x$ and $y$ components,</p> \[{\mathfrak g} _ {k_ {x}k_ {y}} (x,y) = {\mathfrak g}_ {k_ {x}}(x) \times e^{ -i y k_ {y}}.\] <p>The normal modes are the solution of the equation of motion in the kink background, or Poschl-Teller potential. These modes include a continuum</p> \[{\mathfrak g} _ {k}(x) = \frac{e^{-ikx}}{\omega_ {k} \sqrt{m^2+4k^2}}\left[2k^2-m^2+(3/2)m^2\text{sech}^2(m x/2)-3im k\tanh(m x/2)\right]\] <p>with eigenvalue $\omega_ {k} = \sqrt{ k^{2}+m^{2} }$, a zero mode</p> \[{\mathfrak g}_ {B} = -\sqrt{ \frac{3m}{8} } \text{sech}^{2}\left( \frac{mx}{2} \right)\] <p>with eigenvalue $0$, and a shape mode</p> \[{\mathfrak g} _ {S} = \frac{\sqrt{ 3m }}{2} \tanh \frac{mx}{2} \text{sech} \frac{mx}{2},\quad \omega_ {S} = \frac{\sqrt{ 3 }}{2}m\] <p>with eigenvalue less then the rest mass of excited particle.</p> <p>Momentum in the $y$-direction also contributes to the total energy, putting them together with the zero modes, shape modes and continuum in the $x$ direction we have</p> \[\boxed { \omega_ {k_ {B}k_ {y}} = \left\lvert k_ {y} \right\rvert ,\quad \omega_ {k_ {S}k_ {y}} = \sqrt{ \frac{3m^{2}}{4}+k_ {y}^{2} },\quad \omega_ {k_ {x} k_ {y}} = \sqrt{ m^{2}+k_ {x}^{2}+k_ {y}^{2} }. }\] <p>Note that</p> <ul> <li>there is a zero mode corresponding to $k_ {B},k_ {y}=0$,</li> <li>the mass gap disappears, due to the mass-gap-less of $y$-momentum.</li> </ul> <p>The formalism we developed in generic dimension $d$ surely also applies to $d=2$. Let $\vec{p}=(p_ {x},p_ {y})$ and $\vec{k}=(k_ {x},k_ {y})$. The Fourier transform is</p> \[\tilde{\mathfrak{g} }_ {k_ {x},k_ {y}} (\vec{p})= \int d^{2}x \, \mathfrak{g} _ {k}(\vec{x}) e^{ -i\vec{k}\cdot \vec{x} } = (2\pi)\delta(k_ {y}+p_ {y}) \times \tilde{\mathfrak{g}} _ k (p_ {x}).\] <p>Again we see the factorization in $x$ and $y$, the $\delta$-function in $y$ direction is due to the plane wave expansion.</p> <p>Note that the infinite volume of a flat $\mathbb{R}$ can be written as Dirac-$\delta$ function $\delta(0)$. To see it, recall the Fourier transform of a function is written as</p> \[\widetilde{f}(\vec{k}) = \int d^{d}x \, e^{ -i\vec{k}\cdot \vec{x} } f(\vec{x})\] <p>which means if we set $f(\vec{x})=1$ then</p> \[\tilde{f}(\vec{k}) = \int d^{d}x \, e^{ -i \vec{k}\cdot\vec{x} } = (2\pi)^{d} \delta^{d}(k),\] <p>If we further set $k=0$ then the integral becomes</p> \[\int d^{d}x \, 1\, = \text{Vol}^{d} = (2\pi)^{d}\delta^{d}(0).\] <p>In the case of 1-dimension, say coordinated by $y$, the total length would be $2\pi \delta(0)$.</p> <p>Define the Fourier transformation $\tilde{f}$ of function $f(x)$ to be</p> \[\tilde{f}(\vec{p}) := \int d^{d}x \, f(\vec{x})e^{ -i\vec{p}\cdot \vec{x} }.\] <p>The Fourier transformation of normal modes reads</p> \[\tilde{ {\mathfrak g} }_ {k}(p) = \int d^{d} x \, {\mathfrak g}_ {k}( \vec{x} ) e^{-i\vec{p} \cdot \vec{x} }\] <p>which satisfies relation</p> \[\tilde{ {\mathfrak g} }_ {k}^{\ast}(\vec{p}) = \tilde{ {\mathfrak g} }_ {-k}(-\vec{p}) .\] <p>Sometime this relation can help to make the numerical calculation easier.</p> <p>The normalization relations for ${\mathfrak g}$ reads</p> \[\begin{align*} \int d^{d}x \, {\mathfrak g}^{\ast }_ {k}(\vec{x}){\mathfrak g}_ {k'}(\vec{x}) &amp;=(2\pi)^{d}\delta ^{d}(\vec{k}-\vec{k}'), \\ \int \frac{d^{d}p}{(2\pi)^{d} } \tilde{ {\mathfrak g} }_ {k}(\vec{p}) \tilde{ {\mathfrak g} }_ {k'}(\vec{p}) &amp;= (2\pi)^{d}\delta ^{d}(\vec{k}+\vec{k}'), \\ \int \frac{d^{d}p}{(2\pi)^{d} } \tilde{ {\mathfrak g} }_ {k}(\vec{p}) \tilde{ {\mathfrak g} }^{\ast }_ {k'}(\vec{p}) &amp;= (2\pi)^{d}\delta ^{d}(\vec{k}-\vec{k}') , \end{align*}\] <p>and the completeness condition (in both spacetime and momentum space)</p> \[\begin{align*} \int \frac{d^{d}k}{(2\pi)^{2}} \, {\mathfrak g}_ {k}(\vec{x}) {\mathfrak g}^{\ast }_ {k}(\vec{y}) &amp;=\delta ^{d}(\vec{x}-\vec{y}), \\ \sum\!\!\!\!\!\!\!\!\int \; \frac{d^{d}k}{(2\pi)^{d}} \, \tilde{ {\frak g} }_ {k}(\vec{p}_ {1}) \tilde{ {\frak g} }^{\ast} _ {k}(\vec{p}_ {2}) &amp;= (2\pi)^{d} \delta^{d}(\vec{p}_ {1} - \vec{p}_ {2}). \end{align*}\] <p>Note that in our convention of Fourier transformation, instead of $+i\vec{p}\cdot \vec{x}$ we have minus sign. This is only the spatial part of the Fourier transformation (recall that our spacetime is $d+1$ dimensional).</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:Polchinski"> <p>J. Polchinski, String Theory. Vol. 1: An Introduction to the Bosonic String. Cambridge Univ. Pr., UK, 1998. <a href="#fnref:Polchinski" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Baiyang Zhang</name></author><category term="kink"/><summary type="html"><![CDATA[Quantization and Haag Theorem]]></summary></entry><entry><title type="html">Bare Minimum of Abelian Decomposition and Monopole</title><link href="https://baiyangzhang.github.io/blog/2025/Abelian-Decomposition-and-Monopole/" rel="alternate" type="text/html" title="Bare Minimum of Abelian Decomposition and Monopole"/><published>2025-03-20T00:00:00+00:00</published><updated>2025-03-20T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2025/Abelian-Decomposition-and-Monopole</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2025/Abelian-Decomposition-and-Monopole/"><![CDATA[<h1 id="convention">Convention</h1> <p>The gauge Lie group $G$ has Lie algebra $\mathfrak{g}\in T_ {e}G$, where $e$ is the unit element of $G$. The generators $T^{a}$ of $G$ satisfy</p> \[[T^a,T^b] = i f^{ab}_ {c} T^c\] <p>where $f^{abc}$ is the structure constant. The indices can be lowered or raised by the Cartan-Killing metric $g^{ab} = \text{Tr }{T^a T^b}$, for our discussion $g^{ab} = \delta^{ab}$, so it doesn’t matter where we put the indices.</p> <p>The generators in the fundamental representations satisfy normalization condition</p> \[\text{Tr }{T^a T^b} = \frac{1}{2} \delta^{ab}.\] <p>Gauge field is a $\mathfrak{g}$-valued field,</p> \[A_ \mu = A_ \mu^a T^a, \, T^a \in \mathfrak{g}.\] <p>The $\mathfrak{g}$-valued field strength is</p> \[F_ {\mu\nu} = \partial_ \mu A_ \nu - \partial_ \nu A_ \mu -i[A_ \mu, A_ \nu],\] <p>our convention for covariant derivative acting on a field in the fundamental representation is</p> \[D_ {\mu}\psi := \partial_ \mu \psi -i A_ \mu \psi,\] <p>while acting on a field in the adjoint representation is</p> \[D_ {\mu}\phi \equiv \partial_ \mu \phi -i [A_ {\mu},\phi],\] <p>and</p> \[\begin{align*} [D_ {\mu},D_ {\nu}] \psi &amp;= -iF_ {\mu \nu}\psi,\\ [D_ {\mu},D_ {\nu}]\phi &amp;= -i[F_ {\mu \nu},\phi]. \end{align*}\] <p>where again $\psi,\phi$ are fields in the fundamental and adjoint representation respectively.</p> <p>The gauge transformation for various fields are</p> <ul> <li><strong>gauge field</strong>: $A_ \mu \to \Omega (A_ \mu + i \partial_ \mu) \Omega^\dagger,\, \Omega=e^{i \omega^i T^i} \in SU(N)$</li> <li><strong>fundamental scalar field</strong>: $\phi \to \Omega \phi$</li> <li><strong>fundamental spinor field</strong>: $\psi \to \Omega \psi$</li> <li><strong>adjoint scalar field</strong>: $\phi \to \Omega \phi \Omega^\dagger$</li> </ul> <p>It is sometimes useful to know the infinitesimal form of gauge transformation. In such case</p> \[\Omega = 1 + i\omega^a(x) T^a\] <p>and the infinitesimal change of the gauge field is</p> \[\delta A = \partial_ \mu \omega - i [A_ \mu,\omega] = D_ \mu \omega\] <p>where $\omega = \omega^a T^a$ is a matrix, a $\mathfrak{g}$-valued term. And the infinitesimal change of the field strength is</p> \[\delta F_ {\mu\nu} = i [\omega,F_ {\mu \nu}].\] <p>Another useful identity is</p> \[\boxed{ \delta F_ {\mu\nu} = D_ \mu \delta A_ \nu - (\mu \leftrightarrow \nu) }.\] <p>The action for pure Yang-Mills field is</p> \[S_ {YM} = - \frac{1}{2g^2} \int d^4 x\, \text{Tr }{F_ {\mu\nu}F^{\mu\nu}},\] <p>where $g^2$ is the Yang-Mills coupling constant, but $g$ is not a constant at all. The fact that it appears in the denominator instead of numerator (like in Peskin&amp;Schroeder), is due to a rescaling of the Yang-Mills field $A$. We can reproduce the Yang-Mills action used in Peskin&amp;Schroeder by the following substitution in our convention:</p> \[A \to g A,\, F \to g\partial A -g\partial A -ig^{2}[A,A].\] <p>The advantage of putting the coupling in the front of action is that $g$ is factored out from the Lagrangian, and sits where $\hbar$ does, it implies that in the weak coupling limit, the paths that contribute most to the path integral are the solution to the classical equation of motion, we have the classical limit. If $g\to \infty$, then all path contribute almost equally, we would be living in an entirely quantum world.</p> <p>Introduce the Hodge star operator on the field strength,</p> \[\star F_ {\mu\nu} = \frac{1}{2} \epsilon_ {\mu\nu\rho\sigma}F^{\rho\sigma},\] <p>We have Bianchi identity</p> <p>\(d\star F=0.\)</p> <h1 id="t-hooft-polyakov-monopole">‘t Hooft-Polyakov Monopole</h1> <p>Magnetic Monopole solutions in Yang-Mills theory with adjoint bosons appears more naturally than in U(1) theory. Whenever the <strong>non-abelian</strong> gauge group is broken to its <strong>Cartan subgroup</strong> (Cartan subalgebra is spanned by a maximal set of Lie algebra) by adjoint Higgs bosons, we have magnetic monopoles. The <strong>adjoint</strong> Higgs fields have non-zero vacuum expectation values (VEV for short, denoted by $\left\langle \bullet \right\rangle$), breaking the gauge symmetry. Those gauge transformations that leave $\left\langle \phi \right\rangle$ invariant are symmetries not spontaneously broken by the Higgs bosons. Loosely speaking there are two kinds of symmetries:</p> <ul> <li>the symmetry of the Lagrangian,</li> <li>the symmetry of the vacuum.</li> </ul> <p>Gauge symmetry is not really a symmetry but a redundancy, a change of basis, so here we don’t count it. Sometimes, a symmetry is preserved in the Lagrangian but not the vacuum, meaning that the symmetry will change the vacuum, e.g. the double well potential in quantum mechanics, the $x \leftrightarrow -x$ symmetry doesn’t change the Lagrangian, but it turn one vacuum to the other, thus it is not a symmetry of the vacuum. In this case we say this symmetry is spontaneously broken. Of course, since the spontaneously broken symmetry is still a symmetry of the Hamiltonian, it only takes one vacuum to another. In more mathematical terms, let $H$ be the broken symmetry and $\phi_ {0}$ be any vacuum configuration, the orbit of $H$ acting on $\phi_ {0}$ is the vacuum manifold, or at least a connected submanifold of vacuum.</p> <p>Why should we start with $SU(2)$ monopole? because</p> <ul> <li>it is the simplest monopole in Yang-Mills theory, and</li> <li>$SU(2)$ monopole solution is the building block for monopole solutions in larger groups.</li> </ul> <p>In $SU(2)$ Yang-Mills theory, the generator is $T^a = \frac{1}{2}\sigma^a$, where $\sigma$’s are the Pauli matrices, with inner product defined by the trace:</p> \[\left\langle T^{a} \middle\vert T^b \right\rangle := \text{Tr }{T^a T^b} = \frac{1}{2}\delta^{ab}\] <p>Besides the gauge field, let us add the triplet Higgs scalar in the adjoint representation,</p> \[\phi = \frac{\sigma^a}{2}\phi^a\] <p>with convention $\left\lvert \phi \right\rvert^{2} := \phi^a \phi^a = 2\text{Tr }(\phi^a T^a)^2$. In our convention, if $\phi$ without absolute value notation is squared, we are simply squaring the matrix field itself, while if $\left\lvert \phi \right\rvert $ is squared, we are doing the module square.</p> <p>The action is</p> \[S = \int d^4 x \, \left\lbrace {-\frac{1}{2e^2}\text{Tr }F_ {\mu\nu}F^{\mu\nu}+ \frac{1}{e^2}\mathrm{Tr}\,\left\lvert D_ \mu \phi \right\rvert ^2-V(\phi)}\right\rbrace,\] <p>where</p> \[V(\phi) = - \mu^2 \text{Tr }{\phi^2} + \lambda(\text{Tr }{\phi^2})^2 = -\frac{\mu^2}{2}\phi^a \phi^a +\frac{\lambda}{4}(\phi^a \phi^a )^2,\] <p>the vacuum manifold is $\mathbb{S}^{2}$. To see this we can complete the square</p> \[V = \frac{\lambda}{4} \left( \phi^a\phi^a-v^2 \right)^2-\frac{\mu^4}{4\lambda}, \quad v^2 = \frac{\mu^2}{\lambda},\] <p>If $v^{2}&lt;0$, the model is in confined phase. We will assume that $v^{2}&gt;0$, so the theory sits in the Higgs phase. One possible vacuum is obtained at</p> \[\left\langle \phi^{1} \right\rangle = \left\langle \phi^2 \right\rangle = 0,\, \left\langle \phi^3 \right\rangle = v,\, \left\langle A \right\rangle = 0.\] <p>The VEV of $\vec{\phi}$ field points to the $z$-direction. Now the gauge symmetry $SU(2)$ is broken to $U(1)$, since the vacuum for the Higgs is $\left\langle \phi \right\rangle=\phi^{3}T^{3}$ and only the gauge transformation given by $e^{i\theta^3 T^3}$ preserves the vacuum:</p> \[SU(2) \to U(1).\] <p>It is often useful to borrow the terminologies from QED, separating the field strength into “magnetic” and “electric” part,</p> \[\begin{align*} B_ i &amp;= -\frac{1}{2} \epsilon_ {ijk}F_ {jk},\\ E_ i &amp;= F_ {0i}. \end{align*}\] <p>Note that in Yang-Mills theory, $B$ and $E$ are matrices and they are not gauge invariant as in $U(1)$ theory. It means that $B,E$ are no longer physical observables which must be gauge invariant, such as the trace of $F_ {\mu\nu}^2$ or the Wilson loops.</p> <p>Since we have chosen the vacuum to be $\left\langle \phi \right\rangle = v T^3$, in the unitary gauge, the degrees of freedom are</p> <ul> <li>two massive gauge bosons, $W_ \mu^\pm = \frac{1}{\sqrt{2}}(A_ \mu^1 \pm i A_ \mu^2)$. The mass comes solely from the covariant derivative $\left\lvert D_ \mu \phi \right\rvert^2$, after shifting $\phi^3$ to $\phi^3 + v$, the coupling between A and $\phi^3$ is</li> </ul> \[\frac{1}{2}\frac{v^2}{e^2}A_ \mu^a A^{a\mu}\equiv \frac{1}{2}m_ W^2 A^2,\quad a = 1,2\] <ul> <li>$A^3$ remains massless, it is the “photon” decoupled from $\phi^3$</li> <li>$\phi^3$ particle is electrically neutral.</li> </ul> <p>For the total energy to be finite, we must have that at spatial infinity the field configuration goes to a vacuum configuration fast enough:</p> \[\left\lvert \phi \right\rvert \to v,\quad \left\lvert D_ \mu \phi \right\rvert ^2 \to 0, \quad A \to i \Omega \partial_ \mu\Omega^{-1}.\] <p>Note that $\left\lvert D_ \mu \phi \right\rvert^2$ must drop to zero faster than $r^{-3/2}$ to make the energy integral finite.</p> <p>The topology of the space boundary is $\mathbb{S}^2$ since $\partial \mathbb{R}^3 = \mathbb{S}^2$. The vacuum manifold of the Higgs field is also $\mathbb{S}^2$, thus the Higgs field at $r \to \infty$ is a map $\mathbb{S}^2 \to \mathbb{S}^2$, which is classified by homotopy group $\pi_ 2(S^2) = \mathbb{Z}$, where $\mathbb{Z}$ is the addition group of integers. We can use homotopy to classify topologically different solutions, labeling any finite energy Higgs configuration by an integer $n$, which counts the winding number of Higgs field at infinity. Given a $\phi$ field configuration, Let $\hat{\phi}^{i} \equiv \phi^{i} / \left\lvert \phi \right\rvert$ be the unit field vector, the winding number give by integrating the pullback of the volume in $\phi$ configuration space:</p> \[\boxed{ n = \frac{1}{8\pi}\epsilon_ {ijk} \epsilon_ {abc} \int d^2S_ i \, \hat{\phi}^{a}\partial_ {j} \hat{\phi}^{b}\partial_ {k} \hat{\phi}^{c} }\] <p>The trivial vacuum where $\phi = \text{const}$ everywhere obviously has $n = 0$.</p> <p>Consider the case winding number $n=1$. $\left\langle \phi \right\rangle$ at infinity depends on the direction. The residual U(1) symmetry of $SU(2)$ consists of the elements that commutes with $\left\langle \phi \right\rangle$, thus U(1) also depends on the direction. Recall that $A_ \mu = A_ \mu^a T^a$ as a $\mathfrak{su}(2)$-valued field, the components of $A_ {\mu}$ that commutes with $\phi$ can be projected out (in any direction) via</p> \[a_ \mu = \frac{2}{v} \text{Tr }{\phi A_ \mu},\] <p>for example, if $\phi = v T^3, \, a_ \mu = A_ \mu^3$.</p> <p>The unbroken U(1) component is a vector in the Lie-algebra whose direction, after choosing a $\phi$ vacuum, is parallel to $\phi$. For example, if $\phi$ vacuum is chosen to be $vT^3$, then the unbroken U(1) group is proportional to $T^3$. In the case of a monopole solution, the U(1) direction is position-dependent.</p> <p>Let’s look at the covariant derivative. In order to make sure that $\left\lvert D_ \mu\phi \right\rvert^{2} = \left\lvert \partial_ \mu\phi-i[A_ \mu,\phi] \right\rvert^{2} \to 0$, we need to find a corresponding $A_ \mu$ that can cancel the $\partial_ \mu \phi$, for $\phi = v \hat{\phi}$.</p> <p>The following identities might be helpful:</p> \[\phi^2 = \phi^a T^a \phi^b T^b = \frac{1}{2}\phi^a\phi^b \left\lbrace T^a ,T^b \right\rbrace= \frac{v^2}{4}.\] <p>For Pauli matrices we have</p> \[\sigma^i \sigma^j = \delta^{ij} + i \epsilon^{ijk}\sigma^k.\] <p>If $\phi^{2}$ is constant, we have</p> \[\partial_ \mu \phi^2 = 0 = \left\lbrace \partial_ \mu \phi,\phi \right\rbrace\] <p>thus</p> \[[\partial_ \mu \phi,\phi] = -2\phi\partial_ \mu \phi\] <p>and</p> \[\left[ [\partial_ \mu \phi,\phi],\phi \right] = v^2 \partial_ \mu \phi.\] <p>Let</p> \[\boxed{ A_ \mu \to -\frac{i}{v^2} [\partial_ \mu \phi,\phi]+\frac{a_ \mu}{v}\phi, }\] <p>then</p> \[-i [A_ \mu,\phi] = - \partial_ \mu \phi,\] <p>$a_ \mu$ above is the same as the unbroken U(1) field.</p> <p>Knowing the asymptotic form of the gauge field, we can work out the asymptotic form of $F_ {\mu\nu}$, however we are mostly interested in the U(1) part, that is the field strength in the same direction of $\phi$,</p> \[\begin{align} \notag F_ {\mu\nu} &amp;= \partial_ \mu A_ \nu-\partial_ \nu A_ \mu-i[{A_ \mu},{A_ \nu}]\\ &amp;= \frac{\phi}{v}(\partial_ \mu a_ \nu - \partial_ \nu a_ \mu )+\frac{i}{v^2}[{\partial_ \mu \phi},{\partial_ \nu \phi}] + \frac{a_ \mu }{v} \partial_ \nu \phi - \frac{a_ \nu }{v} \partial_ \mu \phi \end{align}\] <p>The last two terms are orthogonal to $\phi$ thus we can discard them. Writing</p> \[F_ {\mu\nu} = \cdots + f_ {\mu\nu}\frac{\phi}{v}\] <p>where</p> \[f_ {\mu\nu} = \partial_ \mu a_ \nu - \partial_ \mu a_ \nu +\frac{2i}{v^3}\text{Tr }{\phi[{\partial_ \mu \phi},{\partial_ \nu \phi}]}\] <p>is the U(1) field strength. The corresponding magnetic field is</p> \[B_ i = -\frac{1}{2} \epsilon_ {ijk}f_ {jk} = \cdots + \frac{1}{2v^3}\epsilon_ {ijk}\epsilon_ {abc}\phi^a \partial_ j \phi^{b}\partial_ {k} \phi^{c}\] <p>The last term is a surprise since it comes from the non-abelian part, whose contribution to the magnetic charge is proportional to the topological charge</p> \[m = \int d^2S_ i\, B_ i = \frac{1}{2} \int d^2S_ i\, \epsilon_ {ijk}\epsilon_ {abc}\hat{\phi}^a \partial_ j \hat{\phi}^{b}\partial_ {k} \hat{\phi}^{c} = 4\pi n\] <p>where n is the winding number. The $\partial_ \mu a_ \nu - \partial_ \mu a_ \nu$ term doesn’t contribution to the magnetic charge since its contribution to the magnetic field is a curl, whose divergence vanishes. Such solution with a quantized U(1) magnetic charge goes by the name ‘t Hooft-Polyakov monopole.</p> <hr/> <p>Above we have given a general analyses, to find the explicit monopole solution, we introduce the hedgehog ansatz for winding number $n = 1$, a natural guess for the solution for $\phi$:</p> \[\phi^i = v \hat{x}^i h(r), \quad h(r) \to \begin{cases} 1 &amp; r \to \infty \\ 0 &amp; r \to 0 \end{cases}.\] <p>We can find corresponding $A_ \mu$ at spacial infinity by solving</p> \[D_ i \phi = 0 \implies \partial_ i \hat{x}^a + \epsilon_ {abd}A_ \mu^b \hat{x}^c = 0\] <p>by multiplying both sides by $x_ j \epsilon_ {adj}$. The final results is</p> \[A_ i^a(x) = a(r) \epsilon_ {aij}x_ j/er^2, \quad a(r) \to \begin{cases} 1 &amp; r \to \infty \\ 0 &amp; r \to 0 \end{cases}.\] <p>The explicit form of $h(r)$ and $a(r)$ can be worked out by solving the equation of motion, numerically if not analytically.</p> <p>The SU(N) gauge symmetry can be maximally broken to $U(1)^{N-1}$ abelian components, they are the sub Lie algebra of $\mathfrak{su}(N)$. Here we use the Gothic font to denote the Lie algebra. Consequently,</p> <ul> <li>we have N-1 different kinds of electric charges, field may have various linear combination of these charges.</li> <li>There will be monopole solutions with respect to $U(1)^{N-1}$</li> </ul> <p>There are already some pretty nice references on SU(3) monopole, for example this one: https://doi.org/10.1103/PhysRevD.14.2016.</p> <h1 id="abelian-projection-of-sun-qcd">Abelian Projection of SU(N) QCD</h1> <p>In studying nonperturbative effects like quark confinement in SU(N) QCD, it is often useful to extract an effective Abelian theory from QCD, which retains essential physics while simplifying computations. This approach is called the Abelian projection.</p> <p>Abelian projection is a method that extracts an Abelian $U(1)^{N-1}$ theory (for gauge group $SU(N)$) from a non-Abelian gauge theory like QCD by <em>partially fixing the gauge</em> so that only the Cartan subgroup (the maximal torus group) remains unbroken.</p> <p>There is strong evidence that confinement may be related to the concept of dual superconductivity, where the QCD vacuum behaves like a type-II superconductor with condensation of magnetic monopoles. In electrodynamics, magnetic charges and electric charges are treated symmetrically in Maxwell’s equations. In QCD, however:</p> <ul> <li>The electric sector corresponds to the color charges carried by quarks.</li> <li>The magnetic sector involves topological objects like monopoles that arise from the non-Abelian nature of the gauge group.</li> </ul> <p>The idea behind the Abelian projection is that even though QCD is a non-Abelian theory, its <strong>long-distance behavior</strong> might be dominated by Abelian degrees of freedom, similar to how superconductors exhibit Abelian gauge behavior.</p> <p>The Abelian projection involves fixing the gauge partially, such that only the maximal Abelian subgroup (Cartan subgroup) remains unbroken.</p> <p>Take $SU(3)$ QCD for example. The Lie algebra $\mathfrak{su}(3)$ consists of eight generators $T^a (a=1,…,8a = 1, \dots, 8)$, corresponding to eight gluon fields $A_ \mu^a$. Its Cartan subgroup consists of diagonal elements that form a maximal Abelian subgroup:</p> \[U(1)^{N-1} \subset SU(N).\] <p>For SU(3), the maximal Abelian subgroup is $U(1) \times U(1)$, generated by the two diagonal Gell-Mann matrices $T^3$ and $T^8$.</p> <hr/> <p>Gauge fixing means choosing a specific gauge transformation to simplify the theory while preserving essential physics. In Abelian projection, we choose a gauge-fixing condition that preserves only the Cartan subgroup.</p> <p>A common choice is to diagonalize a field like the Polyakov loop $P(x)$ or the adjoint Higgs field in the fundamental representation:</p> \[P(x) = \mathcal{P} \exp \left( i \int_ 0^\beta A_ 0(x, t) dt \right),\] <p>where $A_0$ is the time component of the gauge field and $\mathcal{P}$ denotes path ordering. By fixing a gauge such that $P(x)$ is diagonal, the remaining gauge freedom consists only of transformations in the Cartan subgroup (because $T^{3}$ and $T^{8}$ are also diagonal), effectively reducing the theory to an Abelian gauge theory.</p> <hr/> <p>After Abelian projection,</p> <ol> <li> <p>The gauge fields decompose into <strong>diagonal (Abelian) components</strong> and <strong>off-diagonal (charged) components</strong>:</p> \[A_ \mu = A_ \mu^{\text{diag}} + A_ \mu^{\text{off-diag}}.\] <p>The diagonal fields behave like photons in electromagnetism, while the off-diagonal fields carry color charge.</p> </li> <li> <p>Magnetic Monopoles Appear Naturally: The gauge fixing makes topological excitations like magnetic monopoles manifest.</p> </li> <li> <p>Supports the Dual Superconductor Picture: In a superconductor, electric charges form Cooper pairs that condense, leading to the <strong>Meissner effect</strong> (expulsion of magnetic fields). In QCD, the dual of this process suggests that <strong>monopoles condense</strong>, leading to the confinement of quarks via the dual Meissner effect.</p> </li> </ol> <h1 id="abelian-decomposition-in-su2">Abelian Decomposition in SU(2)</h1> <h2 id="decomposition-into-mathcalamathcalc-and-x">Decomposition into $\mathcal{A},\mathcal{C}$ and $X$</h2> <p>The Cho-Duan-Ge decomposition is a covariant separation of the gauge field into abelian and non-abelian components. Roughly speaking, the gauge potential is separated into</p> <ul> <li>abelian, restricted gauge potential. It is further separated into <ul> <li>topologically trivial part, or Maxwell part, or <code class="language-plaintext highlighter-rouge">neurons</code>. And</li> <li>topological part, or Dirac part. Monopole?</li> </ul> </li> <li>valance gauge field which describes colored gluons, or <code class="language-plaintext highlighter-rouge">chromons</code>.</li> </ul> <p>For the sake of simplicity we will consider $SU(2)$ Consider The $\mathfrak{g}$-valued gauge potential is decomposed to the abelian sub-algebra, so-called Cartan subalgebra, and the rest part that is orthogonal to it. Let $\hat{n}$ be a <strong>right-handed orthonormal frame</strong>, $\hat{n}=(\hat{n}^{1},\hat{n}^{2},\hat{n}^{3})$ and $\hat{n}^{i}\hat{n}^{i}=1$. Let’s put it in the Lie algebra $\mathfrak{g}$, by assigning each component $\hat{n}^{i}$ to a basis $T^{a}\in\mathfrak{g}$, $T^{a} = \frac{\sigma^{a}}{2}$ for $SU(2)$. To cling to the convention that we use fraktur letters to denote Lie algebra, let $\mathfrak{n}$ be the $\mathfrak{g}$-valued unit vector $\hat{n}$, i.e. $\mathfrak{n}:= \hat{n}^{i}T^{i}$. Now we can act the covariant derivative on $\mathfrak{n}$, just how we act covariant derivative on $\mathfrak{g}$-valued scalar fields. Similar to what we did at the presence of a monopole, we can decompose the gauge field into two parts, 1) the part that satisfies $D_ {\mu}\mathfrak{n}=0$ and 2) the part that does not. With the help of the following matrix identities:</p> \[\mathfrak{n}^{2}=\mathfrak{n}\cdot \mathfrak{n}=\hat{n}^{a}\hat{n}^{b}T^{a}T^{b}=\frac{1}{4},\quad T:=\frac{1}{2}\sigma\] <p>and consequently</p> \[\partial \mathfrak{n}^{2}=0=(\partial \mathfrak{n})\mathfrak{n}+\mathfrak{n}\partial \mathfrak{n}\implies \partial \mathfrak{n} \mathfrak{n} = -\mathfrak{n}\partial \mathfrak{n},\] <p>we can solve the $\mathfrak{g}$-valued equation $D_ {\mu}\mathfrak{n}=0$ for $A_ {\mu}$, like we did for the $SU(2)$ monopole in the previous chapter. Expand the covariant derivative,</p> \[D_ {\mu}\mathfrak{n} = (\partial_ {\mu}-i[A,-])\mathfrak{n} = \partial_ {\mu}\mathfrak{n}-i[A_ {\mu},\mathfrak{n}]=0,\] <p>we see that the solution can be written as the summation of the trivial part that is proportional to $\mathfrak{n}$, and the nontrivial part that is perpendicular to $\mathfrak{n}$. Some derivation shows that</p> \[[[\partial_ {\mu}\mathfrak{n},\mathfrak{n}],\mathfrak{n}] = \partial_ {\mu}\mathfrak{n},\] <p>thus the non-trivial part of the solution for $A_ {\mu}$, call it $\mathcal{C}_ {\mu}$, reads</p> \[\boxed{ \mathcal{C}_ {\mu} = i[\mathfrak{n},\partial_ {\mu} \mathfrak{n}] . }\] <p>This is a $\mathfrak{g}$-valued quantity, in terms of components in basis $T^{a}$ it can be written as</p> \[\boxed{ \mathcal{C}_ {\mu} = -\hat{n} \times (\partial_ {\mu}\hat{n}) \text{ in }\mathfrak{g} . }\] <p>The trivial part of the solution that is proportional to $\mathfrak{n}$, call it $\mathcal{A}$, is given by</p> \[\boxed{ \mathcal{A}_ {\mu} = a \mathfrak{n},\quad a\in \mathbb{R}. }\] <p>Put together, the total solution $\hat{A}$ is called the restricted gauge field or restricted gauge potential,</p> \[\hat{A}=\mathcal{A}+\mathcal{C}, \quad \mathcal{A}\propto \mathfrak{n}\text{ and }\mathcal{C}=i[\mathfrak{n},\partial \mathfrak{n}],\] <p>where we have omitted the spacetime index $\mu$, which can be put back whenever needed.</p> <p>Recall that if we want to convert to the other convention, we need to substitute $A\to gA$. <strong>This is the same for $\mathcal{C}$, we need to substitute $\mathcal{C}\to g\mathcal{C}$ while remembering that there is a factor of $\frac{1}{g^{2}}$ in our convention.</strong> Let $\mathcal{C}$ be that define in Cho’s paper, we have $\mathcal{C}=C/g$.</p> <p>If the symmetry is broken to the subgroup $N$ that is generated by $\mathfrak{n}$, then $\mathcal{A}$ is the massless $U(1)$ component that can propagate for long distance. $\mathcal{A}$ Geometrically, $\hat{A}$ is the component of the connection that leaves $\mathfrak{n}$ invariant during parallel transport.</p> <hr/> <p>Next we carry on to study the field strength $\hat{F}$ corresponding to the restricted field $\hat{A}$. Without any calculation we can see that $\hat{F}$ is proportional to $\mathfrak{n}$, since from $D_ {\mu}\mathfrak{n}=0$ we have</p> \[[\hat{D}_ {\mu},\hat{D}_ {\nu}]\mathfrak{n} =-i[\hat{F}_ {\mu \nu},\hat{n}]=0,\] <p>where $\hat{D}_ {\mu}$ is the covariant derivative w.r.t. $\hat{H}$, $\hat{D}=\partial-i[\hat{A},-]$.</p> <p>The following identities might be helpful:</p> \[\begin{align*} \mathfrak{n} \mathfrak{n} = \hat{n}^{a}\hat{n}^{b}\frac{1}{2}\sigma^{a} \frac{1}{2}\sigma^{b} &amp;= \frac{1}{4}, \\ [\mathfrak{n} ,[\mathfrak{n} ,\partial_ {\mu}\mathfrak{n} ]] &amp;= \partial_ {\mu}\mathfrak{n}, \\ [[\mathfrak{n},\partial_ {\nu}\mathfrak{n} ],[\mathfrak{n} ,\partial_ {\mu}\mathfrak{n} ]] &amp;= - \partial_ {\nu}\mathfrak{n} \partial_ {\mu}\mathfrak{n} . \end{align*}\] <p>We have</p> \[\begin{align*} \hat{F}:= &amp; \partial_ {\mu}\hat{A}_ {\nu}-\partial_ {\nu}\hat{A}_ {\mu}-i[\hat{A}_ {\mu},\hat{A}_ {\nu}]\\ =&amp; (\partial_ {\mu}a_ {\nu}-\partial_ {\nu}a_ {\mu})\mathfrak{n} +i[\partial_ {\mu}\mathfrak{n} ,\partial_ {\nu}\mathfrak{n} ]\\ =&amp;: f_ {\mu \nu}\mathfrak{n} +H_ {\mu \nu}\mathfrak{n} . \end{align*}\] <p>It is not obvious that $[\partial_ {\mu}\mathfrak{n},\partial_ {\nu}\mathfrak{n}]$ is proportional to $\hat{n}$. But after some derivation we find</p> \[[\partial_ {\mu}\mathfrak{n},\partial_ {\nu}\mathfrak{n}] = -(\partial_ {\mu}\hat{n}^{a})(\partial_ {\nu}\hat{n}^{b})\epsilon^{abc}T^{c},\] <p>which in $\mathfrak{g}$ is just $-(\partial_ {\mu}\hat{n})\times(\partial_ {\nu}\hat{n})$. Since $\partial \hat{n}$ is orthogonal to $\hat{n}$, their cross product is aligned with $\hat{n}$. So indeed $\hat{F}$ is aligned with $\mathfrak{n}$.</p> <p>In summary, we have</p> \[\begin{align*} \hat{F} &amp;= f_ {\mu \nu}\mathfrak{n} +H_ {\mu \nu}\mathfrak{n} ,\\ f_ {\mu \nu} &amp;= \partial_ {\mu}a_ {\nu}-\partial_ {\nu}a_ {\mu},\\ H_ {\mu \nu} \mathfrak{n} &amp;=i[\partial_ {\mu}\mathfrak{n} ,\partial_ {\nu}\mathfrak{n} ]. \end{align*}\] <p>If we further define $H_ {\mu \nu}=: \partial_ {\mu}C_ {\nu}-\partial_ {\nu}C_ {\mu}$, we have $C_ {\mu}=i\mathfrak{n}\partial_ {\mu}\mathfrak{n}$. You can verify that $C_ {\mu}$ indeed reproduces $H_ {\mu \nu}$.</p> <hr/> <p>We have talked about the monopole-like background $\mathcal{C}$ and the free, massless $U(1)$ field $\mathcal{A}$. The rest of the total $SU(N)$ gauge field can be symbolically denoted as $X$, thus we have</p> \[A = \mathcal{A}+\mathcal{C}+X\] <p>where</p> <ul> <li>$A$ is the entire $SU(N)$ field. This is denoted $\vec{A}$ in some papers.</li> <li>$\mathcal{A}$ is the free, massless $U(1)$ gauge field, called neuron by some.</li> <li>$\mathcal{C}$ is the monopole-like background field defined by $\hat{n}$, for example if $\hat{n}$ is chosen to be hedgehog-form, $\mathcal{C}$ is the monopole field configuration. It can not fluctuated, for it is bond to $\mathfrak{n}$.</li> <li>$X$ is the rest of $A$. I’d like to think of it as the non-abelian fluctuation about the background $\mathcal{C}$.</li> </ul> <p><strong>Question</strong>: Let $\hat{n}_ {\infty}$ be the asymptotic field configuration at the spatial boundary $\partial\mathbb{R}^{3}=\mathbb{S}^{2}$. Since $\hat{n}$ takes value in all unit-norm vectors in all directions, the collection of all $\hat{n}$ is homeomorphic to $\mathbb{S}^{2}$, think of it as the set of the end points of all possible $\hat{n}$. Thus,</p> \[\hat{n}_ {\infty}: \mathbb{S}^{2} \to \mathbb{S}^{2}.\] <p>If the winding number of $\hat{n}_ {\infty}$ is nonzero, then it can not be continuously deformed into a unit map. Then $\hat{n}$ can not be well-defined everywhere, there must be at least one singularity, where the direction of $\hat{n}$ is not defined. If $\hat{n}$ were a scalar field, we usually let it go to zero to avoid this problem, like in vortices. But here we can’t let $\hat{n}$ be zero, so how do you solve this problem?</p> <p>We have the following substitution rules for rescaling the fields:</p> <p>\(\boxed{ S=\frac{1}{g^{2}}\int d^{4}x \, \mathcal{L}(\mathcal{A},\mathcal{C},X) \to S=\frac{1}{g^{2}}\int d^{4}x \, \mathcal{L}\left( \mathcal{A}\to g\mathcal{A},\mathcal{C}\to g\mathcal{C},X\to gX \right) }\)</p> <h2 id="gauge-transform">Gauge transform</h2> <p>If we fix the $\mathfrak{n}(x)$ field, then gauge group is broken to $SU(2)\to U(1)$, where $U(1)$ is the little group of $\mathfrak{n}(x)$, which depends on the position $x$. The unbroken $U(1)$ symmetry corresponds to the massless “photon” that survives at long distance. What if we do not fix $\mathfrak{n}(x)$, and allow it to gauge-transform freely? The original $SU(2)$ gauge symmetry would still be intact, and under gauge transformation $\mathfrak{n}$ will rotate in a $x$-dependent way.</p> <p>To keep our notation in agree with others, let’s denote the gauge transformation as $\Omega=e^{ i\alpha }$, where $\alpha=\alpha^{a}T^{a}$ is a $\mathfrak{g}$-valued <strong>infinitesimal</strong> vector. The gauge field $A$ transforms as</p> \[A \to A+ D_ {\mu}\alpha,\quad D_ {\mu}=\partial_ {\mu}-i[A_ {\mu},-].\] <p>What about $\mathfrak{n}=\hat{n}^{a}T^{a}$? This $\mathfrak{g}$-valued field is not a part of the Lagrangian so it is not obvious how it should transform under gauge transform, or if it should transform at all. For example at the beginning I thought that since $\hat{n}$ is a vector in physical space, why should it be affected by gauge transformation? Then again, since $\mathfrak{n}$ plays a similar rule to the Higgs boson, maybe it should transform as a adjoint scalar. That is what was adopted by Cho and other people, so I’ll stick to it.</p> <p>The Higgs-like field $\mathfrak{n}$ transforms under an infinitesimal gauge transformation as</p> \[\mathfrak{n} \to \mathfrak{n} +i[\alpha,\mathfrak{n} ] = \mathfrak{n} -\vec{\alpha}\times \hat{n}{\Large\mid}_ {\mathfrak{g} }.\] <p>The notation $\vec{\alpha}\times \hat{n}{\Large\mid}_ {\mathfrak{g}}$ means that $\vec{\alpha}\times \hat{n}$ is a vector in Lie algebra.</p> <p>The background $\mathcal{C}$ together with $\mathcal{A}$ is the so-called restricted gauge field that is bonded to the “Higgs” field $\mathfrak{n}$, it is better to make it gauge transform like a gauge field, that is, under gauge transform</p> \[\hat{A}\to \hat{A}'=\Omega(\hat{A}+i\partial)\Omega ^{\dagger}.\] <p>The advantage is that now $\hat{A}$ is the solution to $\mathcal{D}_ {\mu}\mathfrak{n}=0$ in a <strong>gauge independent</strong> way: If $\hat{A}$ is a solution, after gauge transform, let $\mathfrak{n}\to\mathfrak{n}’$ be the transformed vector field, the transformed $\hat{A}’$ is still a solution to $D_ {\mu}\mathfrak{n}’=0$.</p> <p>What about the gauge transformation of $\mathcal{A}$ and $\mathcal{C}$ respectively? Expand the gauge transformation for $\hat{A}$, we have</p> \[\hat{A}=\mathcal{A}+\mathcal{C}\to \Omega(\mathcal{A}+\mathcal{C}+i\partial)\Omega ^{\dagger}\] <p>where the $i\partial$ term in the parenthesis is characteristic to gauge field. <strong>We require that the nontrivial solution $\mathcal{C}$ remains a solution independent of gauge transform,</strong> hence we put $\mathcal{C}$ and $i\partial$ together, so that $\mathcal{C}$ transforms as a gauge field:</p> \[\mathcal{C}\to \Omega(\mathcal{C}+i\partial)\Omega ^{\dagger}.\] <p>Now, there is only one $i\partial$ term, since it has already be assigned to $\mathcal{C}$, the rest of the $\hat{A}$, that is $\mathcal{A}$, will have to transform like a adjoint scalar field:</p> \[\mathcal{A}\to \Omega \mathcal{A}\Omega ^{\dagger}.\] <p>Similarly to $X$ in $A = \mathcal{A}+\mathcal{C}+X$. $X$ also transforms as a adjoint scalar,</p> \[X\to \Omega X \Omega ^{\dagger}.\] <p>How the field transforms tells us what gauge invariant terms we could construct to be included into the Lagrangian. We know that $\left\lvert D_ {\mu}\mathcal{A} \right\rvert^{2}$ and $\left\lvert D_ {\mu}X \right\rvert^{2}$ are gauge invariant, so they should appear in the Lagrangian. Next, let’s go to the details.</p> <h2 id="lagrangian-of-restrictedcd-and-extendedcd">Lagrangian of R(estricted)CD and E(xtended)CD</h2> <p>The <code class="language-plaintext highlighter-rouge">restricted QCD</code>, <code class="language-plaintext highlighter-rouge">RCD</code> for short, is the self-energy of restricted gauge field $\hat{A}$,</p> \[\mathcal{L}_ {\text{RCD}} = - \frac{1}{2g^{2}} \mathrm{Tr}\,\hat{F}_ {\mu \nu}^{2}.\] <p>Substitute in</p> \[\begin{align*} \hat{F} &amp;= f_ {\mu \nu}\mathfrak{n} +H_ {\mu \nu}\mathfrak{n} ,\\ f_ {\mu \nu} &amp;= \partial_ {\mu}a_ {\nu}-\partial_ {\nu}a_ {\mu},\\ H_ {\mu \nu} \mathfrak{n} &amp;=i[\partial_ {\mu}\mathfrak{n} ,\partial_ {\nu}\mathfrak{n} ]. \end{align*}\] <p>we have</p> \[\begin{align*} \mathcal{L}_ {\text{RCD}} =&amp; - \frac{1}{4g^{2}}f_ {\mu \nu}^{2} - \frac{1}{2g^{2}}\mathrm{Tr}\,H^{2}-\frac{1}{g^{2}}\mathrm{Tr}\,f_ {\mu \nu}H^{\mu \nu}\\ =&amp; - \frac{1}{4g^{2}} (\partial_ {\mu}a_ {\nu}-\partial_ {\nu}a_ {\mu})^{2}\\ &amp; - \frac{1}{4g^{2}}(\partial_ {\mu}\hat{n}^{a}\partial_ {\nu}\hat{n}^{b}-(a\longleftrightarrow b))^{2}\\ &amp;+ \frac{1}{2g^{2}} \epsilon^{abc}f_ {\mu \nu}\hat{n}^{a}\partial^{\mu}\hat{n}^{b}\partial^{\nu}\hat{n}^{c} \end{align*}.\] <p>OK, now let’s include final piece, the non-abelian fluctuation $X$. The total field strength $F_ {\mu \nu}$ is</p> \[F_ {\mu \nu} = \hat{F}_ {\mu \nu}+\hat{D}_ {\mu}X_ {\nu}-\hat{D}_ {\nu}X_ {\mu}-i[X_ {\mu},X_ {\nu}],\] <p>where we see the anticipated covariant derivative term $\hat{D}X$. $X$ behaves like an adjoint scalar, but has a Lorentz index.</p> <p>Take the above expression into the total gauge potential $-\frac{1}{2g^{2}}\mathrm{Tr}\,F^{2}$, we get what Cho calls Extended QCD, ECD for short. But it is just the gauge part, and it is just the same QCD Lagrangian, separated into different components. To simplify the extended QCD Lagrangian, recall that $f_ {\mu \nu}\mathfrak{n}$ is orthogonal to $X$ by construction, namely $\mathrm{Tr}\,\mathfrak{n}X_ {ny}=0$ for all $\mu$, then we have</p> \[\partial \mathrm{Tr}\,\mathfrak{n} X=0=\mathrm{Tr}\,(\partial \mathfrak{n} X+\mathfrak{n} \partial X)\implies \mathrm{Tr}\,\partial \mathfrak{n} X=-\mathrm{Tr}\,\mathfrak{n}\partial X.\] <p>Note that this “exchange the derivative and change the sign” operation is only valid under the trace.</p> <p>Also take into consideration the cyclic property of trace, we can simplify the following term:</p> \[\begin{align*} \mathrm{Tr}\,\left\lbrace \hat{F}_ {\mu \nu} \hat{D}_ {\mu}X_ {\nu} \right\rbrace =&amp; f_ {\mu \nu} \mathrm{Tr}\,\left\lbrace \mathfrak{n}(\partial_ {\mu}X_ {\nu}-i[\hat{A}_ {\mu},X_ {\nu}]) \right\rbrace \\ =&amp; f_ {\mu \nu} \mathrm{Tr}\,\left\lbrace \mathfrak{n} \partial_ {\mu}X_ {\nu}-\mathfrak{n} i(\hat{A}_ {\mu}X_ {\nu}-X_ {\nu}\hat{A}_ {\mu}) \right\rbrace \\ =&amp; f_ {\mu \nu}\mathrm{Tr}\,\left\lbrace -\partial_ {\mu}\mathfrak{n}X_ {\nu} -i\mathfrak{n} \hat{A}_ {\mu}X_ {\nu}+i\mathfrak{n} X_ {\nu}\hat{A}_ {\mu} \right\rbrace \\ =&amp; f_ {\mu \nu}\mathrm{Tr}\,\left\lbrace -(\partial_ {\mu}\mathfrak{n} X_ {\nu} -i\hat{A}_ {\mu}\mathfrak{n} X_ {\nu}+i\mathfrak{n} \hat{A}_ {\mu}X_ {\nu})\right\rbrace \\ =&amp; f_ {\mu \nu}\mathrm{Tr}\,\left\lbrace -(\partial_ {\mu}\mathfrak{n} -i(\hat{A}_ {\mu}\mathfrak{n} -\mathfrak{n} \hat{A}_ {\mu})X_ {\nu}) \right\rbrace \\ =&amp; f_ {\mu \nu}\mathrm{Tr}\,\left\lbrace -(\partial_ {\mu}\mathfrak{n} -i[\hat{A}_ {\mu},\mathfrak{n} ])X_ {\nu} \right\rbrace \\ =&amp;f_ {\mu \nu} \mathrm{Tr}\,\left\lbrace -(\hat{D}_ {\mu}\mathfrak{n} )X_ {\nu} \right\rbrace \\ =&amp;0. \end{align*}\] <p>With this great simplification, the extended Lagrangian reads</p> \[\begin{align*} \mathcal{L}_ {\text{ECD}} =&amp; - \frac{1}{2g^{2}}\mathrm{Tr}\,F^{2} \\ =&amp; - \frac{1}{2g^{2}}\mathrm{Tr}\,\left\lbrace \hat{F}^{2}-2iF_ {\mu \nu}[X_ {\mu}X_ {\nu}]+(\hat{D}_ {\mu}X_ {\nu}-\hat{D}_ {\nu}X_ {\mu})^{2} \right. \\ &amp;\left. - [X_ {\mu},X_ {\nu}]^{2}-2i(\hat{D}_ {\mu}X_ {\nu}-\hat{D}_ {\nu}X_ {\mu})[X_ {\mu},X_ {\nu}] \right\rbrace. \end{align*}\] <p>We are using the convention in Swartz’s QFT textbook that we do not distinct the upper indices and lower indices when summed.</p> <hr/> <p>Another possible gauge symmetry reads (in Cho’s notation)</p> \[\begin{align*} \delta \hat{n} &amp;=0, \quad \delta A_ {\mu} = \frac{1}{g} \hat{n}\cdot D_ {\mu}\vec{\alpha},\\ \delta \hat{A}_ {\mu} &amp;= \frac{1}{g}(\hat{n}\cdot D_ {\mu}\hat{\alpha})\hat{n},\\ \delta \vec{X}_ {\mu} &amp;= \frac{1}{g}[D_ {\mu}\vec{\alpha}-(\hat{n}\cdot D_ {\mu} \vec{\alpha})\hat{n}], \end{align*}\] <p>where $A_ {\mu}:=\hat{n}\cdot \vec{A}_ {\mu}$.</p> <p><strong>This seems to be the gauge symmetry that preserves the monopole-like background field solution $\mathfrak{n}$</strong>, as I will explain in the following. Since in the hedgehog ansatz, the monopole solution for the scalar $\phi_ {\text{m}}$ reads $\phi_ {\text{m}}\propto \mathfrak{n}$ at the boundary, and $\phi$ gauge transforms as $\phi\to\Omega \phi \Omega ^{\dagger}$, the gauge transformation that leaves $\phi_ {\text{m}}$ invariant is whatever $\Omega$ that commutes with $\phi_ {\text{m}}$, then it has to be proportional to $\mathfrak{n}$ too. Write it $\Omega_ {n}=e^{ i\omega \mathfrak{n} }$. The covariant derivative $D_ {\mu}(\omega \mathfrak{n})$ is not necessarily proportional to $\mathfrak{n}$, to see that, note</p> \[D_ {\mu}(\omega \mathfrak{n} ) = \partial(\omega \mathfrak{n} )-i[A_ {\mu},\omega \mathfrak{n} ]=(\partial_ {\mu} \omega)\mathfrak{n} +\omega \partial_ {\mu}\mathfrak{n} -i\omega[A_ {\mu},\mathfrak{n} ].\] <p>The first term is proportional to $\mathfrak{n}$, while the second and last is orthogonal to $\mathfrak{n}$ (just think of $\mathfrak{n}$ as a vector and commutator as a cross product).</p> <p>The gauge transformation for $U(1)$ component $\mathcal{A}$ is just like a photon (the part proportional to $\mathfrak{n}$)</p> \[\mathcal{A}\to \mathcal{A}+\partial_ {\mu}\omega \mathfrak{n} ,\] <p>while the $X$ transforms as a $SU(N)$ field (the part orthogonal to $\mathfrak{n}$):</p> \[X\to X+\omega D_ {\mu}\mathfrak{n}.\] <p>But is this a gauge transformation really?</p> <hr/>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[Convention]]></summary></entry><entry><title type="html">Mass Renormalization</title><link href="https://baiyangzhang.github.io/blog/2025/Mass-Renormalization/" rel="alternate" type="text/html" title="Mass Renormalization"/><published>2025-03-14T00:00:00+00:00</published><updated>2025-03-14T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2025/Mass-Renormalization</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2025/Mass-Renormalization/"><![CDATA[<h1 id="model-and-convention">Model and convention</h1> <p>As an example to illustrate the idea of mass renormalization, we consider the so-called Model 3 by Sydney Coleman, with one real scalar $\phi$ and one complex scalar $\psi$:</p> \[\mathcal{L} = \frac{1}{2}(\partial \phi)^{2}- \frac{1}{2}\mu^{2}\phi^{2} + \left\lvert \partial \psi \right\rvert ^{2}- m^{2}\psi ^\ast \psi - V,\] <p>where $V$ is a Yukawa-like potential</p> \[V = g \psi^{\ast} \psi \phi.\] <p>$\phi$ is called meason and $\psi$ is called nucleon.</p> <p>The four-momentum eigenstate is defined by</p> \[\left\lvert p \right\rangle := (2\pi)^{3/2} \sqrt{2\omega_ {p}} \left\lvert \vec{p} \right\rangle\] <p>where $\left\lvert \vec{p} \right\rangle$ is the 3-momentum state, $\left\lvert \vec{p} \right\rangle=a_ {\vec{p}}^{\dagger}\left\lvert 0 \right\rangle$.</p> <h1 id="effective-mass">Effective mass</h1> <p>Immanuel Kant argues that we can never directly access ding an sich, thing-in-itself, the objective reality independent of our perception. Instead, we can only know objects as they appear to us through our senses, structured by the cognitive framework imposed by our mind. By the way, Kant has his own “category theory”, it has nothing to do with the category theory by Mac Lane and others.</p> <p>Physics does not directly access fundamental parameters (such as mass, charge). It is not only true experimentally, but also true theoretically. Take mass $m$ for example, we don’t know what is its raw form; instead, we define it through equations, such as Newton’s second law $F=ma$, and we infer its value by measurements. These measurements can never prove Newton’s second law for there is always room for new theory hiding in the error or accuracy in experiments; instead the measurements can only say that the experiments didn’t falsify the theory up to what accuracy. I am a Popperist in this sense.</p> <p>Borrowing the words from Kantian epistemology, theories consist the <strong>categories of understanding</strong> that shape all human knowledge. What we study are the phenomena, the measurable, empirical consequences of the <code class="language-plaintext highlighter-rouge">underlying reality</code>. Physics theories provide a framework for describing these empirical observations. In a Kantian sense, these are part of the phenomenal world—models that help us systematize experience, rather than direct insights into the noumenon, which is the inherent essence, thing-in-themselves.</p> <p>For example, we can define the mass using inertia: $F = ma$. However, you can define the mass using another equally plausible equation: $F = mg$, which is the gravitational force. Different phenomena gives different noumena, we have two very different concepts of masses. The fact that they are equivalent to each other has profound meaning, this equivalence is not a analytical knowledge, but synthetic knowledge. It says that gravitation is actually the same thing as acceleration.</p> <hr/> <p>To have a feeling of what effective mass is, consider a rigid sphere of volume $V$, immersed in a perfect fluid (with zero viscosity) of density $\rho$. Let $m_ {0}$ be the “bare” mass of the ball, $\rho$ be the density of the water, and suppose</p> \[m_ {0}= \frac{1}{10} \rho V.\] <p>The buoyancy force is $\rho V$ and the gravitational force is $m_ {0}g$. The total force exerted on the ball is $9m_ {0} g$, so if we let go of the ball, it should accelerate upward with acceleration $9 g$. However, in reality this never happens, because the ball has to spread the water and that costs energy. There is also the frictional force, but that is only important when the ball has already gained some velocity. A more realistic case is that, when you let go the ball, the acceleration is around $a=\frac{3}{2}g$, instead $a=9 g$. Now if you insist on using $F=ma$ to define the mass of the moving ball, then we have</p> \[F = 9 m_ {0} g = m a = m \frac{3}{2} g\implies m = 6 m_ {0}.\] <p>We can say the when immersed in the liquid, the ball has effective mass $6m_ {0}$. But it is really the way that you decided to interpret the experiment.</p> <hr/> <p>I will not talk about the bump function $f(t)$ since it is really problematic. Actually, you can write down a smooth $C^{\infty}$ bump function explicitly. Define</p> \[h(t) = \begin{cases} e^{ -1 / t }, &amp; t&gt;0 , \\ 0, &amp; t\leq 0, \end{cases}\] <p>$h(t)$ is smooth. Given any real numbers $t_ {1}$ and $t_ {2}$ such that $t_ {1}&lt;t_ {2}$, define</p> \[f(t) = \frac{h(t-t_ {1})}{h(t-t_ {1})+h(t_ {2}-t)}\] <p>then</p> \[f(t) = \begin{cases} 0, &amp; t&lt;t_ {1}, \\ [0,1], &amp; t_ {1}&lt;t&lt;t_ {2}, \\ 1, &amp; t&gt;t_ {2}. \end{cases}\] <p>However, the free theory is not smoothly connected to the interacting theory. It is like expanding an algebraic equation</p> \[\epsilon x^{4}+ax^{2}+c=0,\] <p>when $\epsilon=0$ there are two roots, when $\epsilon \neq 0$ there are four roots, the extra two solutions goes to infinity as $\epsilon\to 0$, if we want to talk about these two extra roots, it doesn’t help to expand about $\epsilon=0$. So I’ll try to leave the bump function out of this talk.</p> <hr/> <p>In general, whenever a particle is interacting with a continuum system, may it be perfect fluid or electric-magnetic field, or any other field, the mass of the particle changes from when there is no interaction.</p> <p>Consider the meson only for now, the relevant terms are</p> \[\mathcal{L} = \frac{1}{2}(\partial \phi)^{2} - \frac{1}{2}\mu_ {0}^{2}\phi^{2}-g\psi ^\ast \psi \phi+\cdots\] <p>where in the spirit of previous discussion, we write $\mu_ {0}$ instead of $\mu$. $\mu_ {0}$ is the bare mass of the meson, whose value can not be measured directly. Compare to our previous example of a ball, this Lagrangian corresponds to $F=m_ {0} a$ when the ball is free, not in water.</p> <p>In real world there is interaction. The physical mass $\mu$ of the meson is <code class="language-plaintext highlighter-rouge">dressed up</code> by the interaction,</p> \[\mu \neq \mu_ {0} \quad \text{ in the presence of interaction.}\] <p>Even there is interaction, the $S$ matrix element for vacuum to vacuum should be $1$. To ensure that, we need to introduce the constant counter term $a$, which should be added to the Lagrangian density, $\mathcal{L}=\cdots+a,$</p> \[a \text{ is fiexed by } \left\langle 0 \right\rvert S \left\lvert 0 \right\rangle=1.\] <p>We can write $\mu_ {0}^{2}=\nu^{2}-b$ and $m_ {0}^{2}=m^{2}-c$. This introduces the mass renormalization terms $\frac{1}{2} b\phi^{2}$ and $c \psi ^\ast\psi$,</p> \[b,c \text{ are fixed by } \left\langle \vec{p} \right\rvert S \left\lvert \vec{p}' \right\rangle = \delta^{3}(\vec{p}-\vec{p}').\] <p>where $\vec{p},\vec{p}’$ are momentum of mesons and nucleons.</p> <h1 id="feynman-rules">Feynman Rules</h1> <p>Wick diagrams are used to calculate the time-ordered operators, Feynman diagrams are used to calculate the $S$-matrix. Wick diagrams don’t pay attention to loose external lines, but Feynman diagram does.</p> <p>Consider the $\psi \psi\to\psi \psi$ scatter,</p> \[N(\vec{p}_ {1})+N(\vec{p}_ {2})\to N(\vec{p}_ {1}')+N(\vec{p}_ {2}'),\] <p>The $\mathcal{O}(g^{2})$ $S$-matrix element we want to calculate is $\left\langle \vec{p}_ {1}’\vec{p}_ {2}’ \right\rvert(S-1)\left\lvert \vec{p}_ {1}\vec{p}_ {2} \right\rangle$. The term in $S-1$ of $\mathcal{O}(g^{2})$ is</p> \[\frac{1}{2}\int d^{4}x_ {1}d^{4}x_ {2} \, \mathcal{T}(-i\mathcal{H}_ {I}(x_ {1}))(-i\mathcal{H}_ {I}(x_ {2}))\] <p>which is</p> \[\frac{1}{2}(-ig)^{2}\int d^{4}x_ {1}d^{4}x_ {2} \, \mathcal{T} \psi ^\ast_ {1}\psi_ {1}\phi_ {1}\psi_ {2}^\ast \psi_ {2}\phi_ {2}\] <p>where $\psi_ {1}$ is short for $\psi(x_ {1})$, etc.</p> <p>This is just the time-evolution, it acts on the initial and final states, which we will take care of now. All the fields that are going to annihilate a particle in the initial state will be put to the right, where the initial state is. All the fields that are gonna contract particles in the final state will be put to the left, where the final state is. We will label the external legs with the momenta of corresponding particles.</p> <p>Let’s calculate the external lines first. With our convention, the contraction with an incoming particle with momentum $p$ gives a factor $e^{ -ip\cdot x }$. We have</p> \[\left\langle \vec{p}_ {1}' \vec{p}_ {2}' \right\rvert \mathcal{N}\left\lbrace \psi_ {1}^\ast \psi_ {1}\psi_ {2}^\ast \psi_ {2} \right\rbrace \left\lvert \vec{p}_ {1}\vec{p}_ {2} \right\rangle = \left\langle \vec{p}_ {1}' \vec{p}_ {2}' \right\rvert \psi_ {1}^\ast \psi_ {2}^\ast \psi_ {1} \psi_ {2} \left\lvert \vec{p}_ {1}\vec{p}_ {2} \right\rangle\] <p>which is a bunch of phases:</p> \[e^{ ix_ {1}\cdot(p_ {1}'-p_ {1})+ix_ {2}\cdot(p_ {2}'-p_ {2}) }+e^{ ix_ {1}\cdot(p_ {1}'-p_ {2})+ix_ {2}\cdot(p_ {2}'-p_ {1}) } + (x_ {1}\longleftrightarrow x_ {2}).\] <p>Note that the exchange of two vertices $x_ {1}\longleftrightarrow x_ {2}$ will cancel the factor $\frac{1}{2}$ in Dyson’s formula.</p> <p>The contraction between two meson field was calculated before,</p> \[\text{contraction} \left\lbrace \phi_ {1}\phi_ {2} \right\rbrace =\int \frac{d^{4}p}{(2\pi)^{4}} \, e^{ -iq(x_ {1}-x_ {2}) } \frac{i}{q^{2}-\mu^{2}+i\epsilon}.\] <p>Now we can insert all of them into the $\mathcal{O}(g^{2})$ term, the $x_ {1},x_ {2}$ integrals can be done explicitly since they just give a delta function,</p> \[\begin{align*} \left\langle \vec{p}_ {1}' \vec{p}_ {2}' \right\rvert S-1 \left\lvert \vec{p}_ {1}\vec{p}_ {2} \right\rangle =&amp; (-ig)^{2}\int \frac{d^{4}q}{(2\pi)^{4}} \, \frac{i}{q^{2}-\mu^{2}+i\epsilon} \\ &amp;\times \int d^{4}x_ {1} d^{4}x_ {2} \, e^{ ix_ {1}\cdot(p_ {1}'-p_ {1}-q)+ix_ {2}\cdot(p_ {2}'-p_ {2}+q) }+(p_ {1}\longleftrightarrow p_ {2}). \end{align*}\] <p>Since</p> \[\int d^{4}x \, e^{ ipx } = (2\pi)^{4}\delta (p)\] <p>and $\delta(x-a)\delta(x-b)=\delta(a-b)\delta(x-a)$, we have</p> \[\begin{align*} \left\langle \vec{p}_ {1}' \vec{p}_ {2}' \right\rvert S-1 \left\lvert \vec{p}_ {1}\vec{p}_ {2} \right\rangle =&amp; (-ig)^{2}(2\pi)^{4} \delta^{4}(p_ {1}'+p_ {2}'-p_ {1}-p_ {2})\\ &amp; \times \int \frac{d^{4q}}{(2\pi)^{4}} \,(2\pi)^{4} \frac{i}{q^{2}-\mu^{2}+\epsilon}[\delta^{4}(p_ {1}'-p_ {1}-q)+\delta^{4}(p_ {2}'-p_ {1}+q)]. \end{align*}\] <p>The RHS of the first line gives us the conservation of total momentum. We can define the invariant Feynman amplitude $\mathcal{A}$ as</p> \[\left\langle f \right\rvert S-1\left\lvert i \right\rangle =i (2\pi)^{4}\delta^{4}\left( \sum p \right) \mathcal{A}\] <p>Then in our example</p> \[\mathcal{A} = (-ig)^{2} \frac{1}{(p_ {1}'-p_ {1})^{2}-\mu^{2}+i\epsilon} + (-ig)^{2} \frac{1}{(p_ {2}'-p_ {1})^{2}-\mu^{2}+i\epsilon} ,\] <p>they corresponds to the following two diagrams:</p> <p><strong>Draw $t,u$-channel diagrams and calculate one of them.</strong></p> <p>See page 256 of Coleman’s lecture.</p> <p><strong>Talk about a 1-loop example</strong> $\mathcal{O}(g^{4})$ diagram.</p> <p>The contractions are called propagators, which is not really some particle propagating in spacetime. Factors like $i / (q^{2}-\mu^{2}+i\epsilon)$ give the probability amplitude, in this metaphorical language, for the virtual particle going between two vertices. They describe how a virtual particle propagates from one vertex to another. For this reason, they are called Feynman propagators. The language, I stress, is purely metaphorical. If you don’t want to use it, don’t use it. But then you’ll find 90% of the physicists in the world will be unintelligible to you when they give seminars. It’s very convenient, but it should not be taken too seriously.</p> <hr/> <p><strong>March 24th</strong></p> <hr/> <h1 id="feynman-diagrams-in-model-3-to-order-g2">Feynman diagrams in Model 3 to order $g^{2}$</h1> <p>The a counterterm is just a number. I don’t need a special diagrammatic rule for that. The a counterterm has no momentum associated with it, and its delta function has an argument of zero. If the system were in a box, the term $(2\pi)^{4}\delta^{4}(0)$ would turn into $VT$, the volume of spacetime in the box. This counterterm is designed to cancel all the vacuum bubble diagrams, those without external lines, which you will see also have a factor of $(2\pi)^{4}\delta^{4}(0)$. Like the counterterm a, the counterterms $b$ and $c$ will be expressed as infinite power series in the coupling constant $g$. I will explain to you shortly how we determine them order by order.</p> <p>The Feynman rules for model three are shown in the figure below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/FeynmanRulesModel3-480.webp 480w,/img/FeynmanRulesModel3-800.webp 800w,/img/FeynmanRulesModel3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/FeynmanRulesModel3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Feynman rules for model 3, in the momentum representation, not momentum representation. </div> <p>For Feynman diagrams, see Page 261 in Sydney Coleman’s book.</p> <p>Regarding the $\mathcal{O}(g^{2})$ correction to meson mass. Here is where we need counterterm $b$. The renormalization condition that fixes be is define as following: there is no phase mismatch between one-meson states $\left\lvert \vec{q} \right\rangle$ and $\left\lvert \vec{q}’ \right\rangle$,</p> \[\left\langle \vec{q} \right\rvert S \left\lvert \vec{q}' \right\rangle = \delta^{3}(\vec{q}-\vec{q}').\] <p>which in terms of invariant amplitude reads</p> \[\left\langle \vec{q} \right\rvert S-1 \left\lvert \vec{q}' \right\rangle =0.\] <p>It means that the sum of two diagrams is zero.</p> <p>It is nearly the same story for the nucleon. See page 262.</p>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[Model and convention]]></summary></entry><entry><title type="html">Introduction to Resurgence Note 2</title><link href="https://baiyangzhang.github.io/blog/2025/Introduction-to-Resurgence-2/" rel="alternate" type="text/html" title="Introduction to Resurgence Note 2"/><published>2025-03-09T00:00:00+00:00</published><updated>2025-03-09T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2025/Introduction-to-Resurgence-2</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2025/Introduction-to-Resurgence-2/"><![CDATA[<blockquote> <p>Series don’t just diverge for no reasons. The divergence of a series must reflect its cause….A divergent series is not meaningless, or a nuisance, but an essential and informative coded representation of the function. — Michael Berry</p> </blockquote> <p>Resurgence aim to seeing things <em>sub specie totius</em>, from the perspective of the whole. It does not only re-sums divergent series, but also regenerates non-perturbative information from perturbative, and connects topological sector with trivial sector.</p> <h1 id="one-more-example-eulers-equation">One more example: Euler’s equation</h1> <p>Let’s consider the Euler’s equation,</p> \[x^{2}f'-x+f=0\] <p>and look for the solution of $f(x)$ on $\mathbb{R}$. This equation can be solved using the method of <strong>integrating factors</strong>, but we try to solve it using the method of power series. Assume the solution is of form</p> \[f = \sum_ {k=0} a_ {k} x^{k},\] <p>substitute this to the equation and compare terms order-by-order, we get the following relation:</p> \[\begin{align*} a_ {0}&amp;= 0, \\ a_ {1} &amp;= 1, \\ (k-1) a_ {k-} &amp;= -a_ {k} \text{ for } k\geq 2. \end{align*}\] <p>Solving it we get</p> \[a_ {k} = (-1)^{k-1}(k-1)!,\] <p>nice and simple. The only problem is that, the power series defined by it diverges for any $x$:</p> \[f(x) = \sum_ {k=0}^{\infty} (-1)^{k} k!x^{k+1},\] <p>Also, we know that the solution of first order differential solution should have a constant of integral, which is a free parameter. But here we found no trace of it.</p> <p>It turns out that, the method of power series can only find solutions that has a sensible Taylor expansion. Note that this method is aimed at finding the recursive relation between the coefficients of the power expansion of the possible solution, what if the solution has no sensible power expansion in the first place? It is not impossible, for some function are themselves well-defined, but indeed has trivial Taylor expansion about certain points. The most famous example of such functions, sometimes called non-perturbative, is $e^{ \bullet/x }$ at $x=0$, where $\bullet$ is any constant. We will see that the (family of) solutions that we are missing is indeed of such form.</p> <p>The homogenous form of the Euler equation read</p> \[x^{2}g' +g=0,\] <p>its solution has form</p> \[g(x) = a e^{ 1/x }, \quad a = \text{const}.\] <p>Hence the general solution to the Euler equation is</p> \[f = \sum(\cdots) + g(x) = \sum(\cdots) + a e^{ 1/x },\] <p>now $a$ is the missing constant of integral we are looking for.</p> <p>Now the crisis of divergence still remains. This prolem can not be solved by shifting the point about which we expand the power series. In the previous case we are expanding about $x_ {0}=0$, have we changed it to $x_ {0}=c$ for some number $c$ would not render the power expansion to be convergent. To tackle the problem we must do something more radical. It is called the Borel resummation.</p> <hr/> <p>There are many approaches to resumming divergent series, such as Abel summation, Cesaro summation, $\zeta$-function regularization, etc. The method that is appropriate for our setting was introduced by Emile Borel in 1899, and is therefore called <code class="language-plaintext highlighter-rouge">Borel summation</code>.</p> <p>Roughly speaking, we make use of an variation of Euler’s expression of functorial formula, to change the factorial (which is the reason for the divergence) into an integral:</p> \[k! x^{k+1} = \int_ {0}^{\infty} dt \, t^{k} e^{ -t/x } .\] <p>Recall that the power series solution we got for Euler’s equation read</p> \[f(x) = \sum_ {k=0}^{\infty} (-1)^{k} k!x^{k+1},\] <p>replace the $k!x^{k+1}$ term with the integral we get</p> \[f(x) = \sum \int (\cdots) = \int \sum (\cdots) = \int_ {0}^{\infty} dt \, \frac{1}{1+t} e^{ -t/x } .\] <p>We have exchanged the order of integral and summation, which was actually not allow, but we do it anyway. That’s the secret of the method, later we will see that the reason why we got a divergent power series is that in obtaining the series, we have done a secrete exchange of integral and summation, which caused all the problems; the cure turns out to be the cause: we exchange it back!</p> <p>The integral actually defines a nice smooth function when $x&gt;0$. This function is called the <code class="language-plaintext highlighter-rouge">Borel sum</code> of our divergent series. Reader can verify directly that it is indeed a solution of the original function.</p> <p>There exists a simple relation between the divergent series and the Borel-resummed function: The power series is the Taylor expansion of the function at $x=0$. Furthermore, we state without proof that the approximation given by the first $n$ terms of the series differs with the smooth function with an upper bound:</p> \[\left\lvert f(x) - \sum_ {k=0}^{n-1}(-1)^{k}k!x^{k+1} \right\rvert &lt; n! \left\lvert x \right\rvert ^{n+1}.\] <h2 id="integral-contour">Integral Contour</h2> <p>The integral</p> \[\int_ {0}^{\infty} dt \, \frac{1}{1+t} e^{ -t/x }\] <p>works fine when $x&gt;0$, but what if $x&lt;0$? The integral blows up!</p> <p>At this point we need to expand our view and upgrade $x$ to a complex number. Now instead of living on the real number axis, $x$ lives on the 2D complex plane. Obviously the area on which the integral is well-defined is the half-plane where $\text{Re}(x)&gt;0$. Then we can analytically continuate the value to the other half plane where $\text{Re}(x)&lt;0$. This allows us to ask all kinds of interesting questions, for example what is the structure of the poles on the $\text{Re}(x)&gt;0$ half-plane, after continuation what is the topological struction of the whole Riemann surface, what does the monodromy look like, etc. The question is, how to realize the analytical continuation?</p> <p>To do so, it’s not enough to just complexify $x$, we need to do the same to the integral variable $t$, and modify the path of integral. Let $x=\left\lvert x \right\rvert e^{ i\theta }$. Define $\gamma_ {x}:= [0,\infty ] \cdot x = [0,\infty ] \cdot e^{ i\theta }$ to be the ray starting from the origin and passes through $x$, as long as the ray does not pass $-1$, which is a pole of the integrand, the integral is always well-defined:</p> \[\int_ {\gamma_ {x}} dt \, \frac{1}{1+t} e^{ -\left\lvert t \right\rvert e^{ i\theta }/\left\lvert x \right\rvert e^{ i\theta } } = \int_ {0}^{\infty} d \left\lvert t \right\rvert \, \frac{1}{1+\left\lvert t \right\rvert e^{ i\theta }} e^{ -\left\lvert t \right\rvert /\left\lvert x \right\rvert } &lt;\infty.\] <p>In this way we can define $f(x)$ for $x\in \mathbb{C} - \mathbb{R}^{-}$. The negative real line (include zero) is a branch cut of $f(x)$.</p> <p>Since we can’t go pass the pole, can we avoid it with an infinitesimal half-circle? This is a trick commonly used in physics. The modified contour is shown in the figure below. The problem is, there are two ways to modify the contour, with path $\Gamma_ {+}$ and $\Gamma_ {-}$ respectively. Denote the corresponding integrals as $f_ {\pm}$:</p> \[f_ {+} (x) = \int_ {\Gamma_ {+}} \, (\cdots),\quad f_ {-} (x) = \int_ {\Gamma_ {-}} \, (\cdots).\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/resurgence/negativeRealContour-480.webp 480w,/img/resurgence/negativeRealContour-800.webp 800w,/img/resurgence/negativeRealContour-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/resurgence/negativeRealContour.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> There are two ways to modify the contour to circumvent the tour, introuding ambiguity. </div> <p>Now, we can’t calculate $f_ {+}$ or $f_ {-}$ but we can calculate $f_ {+}-f_ {-}$. A simple integral by contour gives us</p> \[f_ {+}-f_ {-} = 2\pi i \text{Res}(t=-1) = 2\pi i e^{ 1/x }.\] <p>Here pops up a non-perturbative term! Recall that the difference between two solutions is itself another solution, thus $e^{ 1/x }$ is another solution to the Euler equation. This is exactly the missing solution we were talking about. It is hidden in the complex structure of the Borel resummed functions.</p> <p>This behavior, in which the function defined by a divergent series jumps along a ray, is an example of the <code class="language-plaintext highlighter-rouge">Stokes phenomenon</code>. It is the key to resurgence theory: the crucial missing term $e^{ 1/x }$ has “resurged”.</p> <h1 id="borel-resummation-along-mathbbr">Borel (re)summation along $\mathbb{R}^{+}$</h1> <h2 id="a-rough-idea">A rough idea</h2> <p>The Borel summation procedure is roughly as follows. Given a power series in $x$ (without a constant term),</p> \[f= \sum_ {k=0}^{\infty} a_ {k} x^{k+1},\] <ol> <li>Introduce a new variable $t$ to perform the <code class="language-plaintext highlighter-rouge">Borel transformation</code>:</li> </ol> \[\hat{f} := \sum_ {k=0}^{\infty} \frac{a_ {k} }{k!} t^{k},\] <p>note that the power has changed from $x^{k+1}$ to $t^{k}$. The extra factor of $1 / k!$ is the key: it can make the originally divergent power series convergent.</p> <ol> <li>Perform the Borel resummation,</li> </ol> \[f(x) := \int_ {0}^{\infty} dt \, \hat{f}(t) e^{ -t/x }.\] <p>If the whole procedure works, for example if the integral does not encounter any singularity and does not involve further infinity, then we say that $f$ is <code class="language-plaintext highlighter-rouge">Borel summable</code>.</p> <p>In general, there will be certain directions in which the singularities lie, such as $\mathbb{R}^{-}$ in the previous example. In most examples of physical interest, there will be infinitely many such singularities. These singularities carry important information about the functions that are defined by the divergent series. The theory of resurgence is about understanding the singularities, and how they affect the summation process.</p> <h2 id="formal-borel-transform">Formal Borel transform</h2> <p>Here we follow the notation by Sauzin. Use $\mathbb{C}\left\lbrace z^{-1} \right\rbrace$ to denote polynomial with finite radius of convergence, they give as a holomorphic germ. Let $z^{-1}\mathbb{C}[[z^{-1}]]$ be formal power series without constant term, and let $\widetilde{\phi}$ be an element with it. The reason to exclude the constant term is so that it is more compatible with inverse Borel transform, as we will see shortly.</p> <p>Borel transform takes $\widetilde{\phi}$ and tries to make it <em>more convergent</em>, by dividing the $n$-th coefficient by $n!$. To be exact, the Borel transform is the linear map</p> \[\mathcal{B}:\quad \widetilde{\phi}:=\sum_ {n\geq 0}a_ {n} z^{-n-1} \mapsto \hat{\phi} := \sum_ {n\geq 0} \frac{a_ {n}}{n!} \zeta^{n}.\] <p>The notation tilde and hat are used by Sauzin, and the change of indeterminate from $z$ to $\zeta$ is customary. Note that the power is shifted, $a_ {0} z^{-1}$ is mapped to $a_ {0} \zeta_ {0}=a_ {0}$, a constant term in $\hat{\phi}$.</p> <p>If $\widetilde{\phi}$ is already convergent with a finite radius of convergence, then $\hat{\phi}$ converges even better, its radius of convergence is $\infty$. To see this, note that if $\widetilde{\phi}$ is convergent, its terms must be bounded by</p> \[a_ {n} \leq A c^{n}, \quad A,c&gt;0.\] <p>The resulting $\hat{\phi}$ is a exponentially bounded function, it is $\leq Ae^{ c\left\lvert \zeta \right\rvert }$ on the entire complex plane.</p> <p>To prove the inverse, that if $\hat{\phi}$ is an entire function of bounded exponential type then $\widetilde{\phi}$ defines a germ, we need to use Cauchy’s inequality that gives an upper bound of the derivative of a holomorphic function. We will not go to details here.</p> <h2 id="gevrey-classes">Gevrey classes</h2> <p>The <code class="language-plaintext highlighter-rouge">Gevrey classes</code> of formal power series describe different levels of growth for the coefficients of a formal power series. A formal power series</p> \[\hat{f}(\zeta) = \sum_ {n=0}^{\infty} a_ n \zeta^n\] <p>is said to be of Gevrey order $s$, or Gevrey type $s$, where $s &gt; 0$, if there exists constant $A,B &gt; 0$ such that:</p> \[\left\lvert a_ {n} \right\rvert \leq A B^{n} (n!)^{s} \quad \;\forall\; n\geq 0.\] <p>If $s = 1$, the series is called 1-Gevrey (or simply Gevrey). If $s &lt; 1$, the series is closer to an <strong>analytic</strong> function (with a better convergence rate). Many divergent but resummable functions are of Gevrey type 1.</p> <p>1-Gevrey formal power series constitutes a vector space, which we will denote by $\mathbb{C}[[z^{-1}]]_ {1}$. These formal power series can be Borel-resummed to give a holomorphic germ. In more mathematical terms, we have the following:</p> <p>Let $\widetilde{\phi}\in z^{-1} \mathbb{C}[[z^{-1}]]_ {1}$, note that there is no constant term. Then the Borel transformed series $\hat{\phi}$ gives a holomorphic germ (having a positive radius of convergence), namely $\hat{\phi}\in \mathbb{C}\left\lbrace z^{-1} \right\rbrace$. Vise versa.</p> <p>Sometimes we will denote $z^{-1} \mathbb{C}[[z^{-1}]]_ {1}$ as $\mathcal{B}^{-1}(\mathbb{C}\left\lbrace z^{-1} \right\rbrace)$.</p> <p>Let $T_ {c}$ be the translation operator, defined by</p> \[T_ {c} f(z) := f(z+c),\] <p>it can be shown that</p> \[\mathcal{B} T_ {c}\widetilde{\phi} = e^{ -c\zeta } \mathcal{B}\widetilde{\phi}.\] <p>So far we haven’t talked about why it has to be without constant terms. Another question is, why doesn’t 1-Gevrey formal power series $\mathbb{C}[[z^{-1}]]_ {1}$ form an algebra? That is to ask, why doesn’t the multiplication of two 1-Gevrey series 1-Gevrey? To study that we need the concept of convolution.</p> <h2 id="convolution">Convolution</h2> <p>Let $\widetilde{\phi}, \widetilde{\psi}$ be Borel transformed to $\hat{\phi}$, $\hat{\psi}$ respectively. The convolution is defined between Borel-transformed objects, between $\hat{\phi}$ and $\hat{\psi}$. The convolution $\ast$ is defined as</p> \[\hat{\phi}\ast \hat{\psi} := \mathcal{B}\left\lbrace \widetilde{\phi}\widetilde{\psi} \right\rbrace .\] <p>In other words, convolution is defined as the image of Borel transform of multiplication.</p> <p>If $\hat{\phi}$ and $\hat{\psi}$ define two holomorphic germs, denoted as $\Phi$ and $\Psi$ respectively, then the convolution appears as the familiar form:</p> \[\Phi \ast \Psi(\zeta) = \int_ {0}^{\zeta} \, \Phi(t)\Psi(\zeta-t) dt.\] <p>A difference between convolution $\ast$ and multiplication is that, so far we haven’t defined a unit element with respect to convolution. The functional form of convolution inspires as to introduce the delta function $\delta$ as an abstract unit element,</p> \[\delta \ast \Psi (\zeta) = \Psi(\zeta).\] <p>Then $\delta \oplus \mathbb{C}[[\zeta]]$ form an algebra under convolution. We also define the preimage of $\delta$ under Borel transform to be $1$ in $z^{-1}\mathbb{C}[[z]]$.</p> <h2 id="the-fine-borel-laplace-summation">The fine Borel-Laplace summation</h2> <p>Let $S_ {\delta}$ be a half strip of width $2\delta$:</p> \[S_ {\delta} := \left\lbrace \zeta \in \mathbb{C} \,\middle\vert\, \text{dist}(\zeta,\mathbb{R}^{+}) &lt;\delta\right\rbrace ,\] <p>where $\text{dist(a,S)}$ is the distance between element $a$ and set $S$.</p> <p>Let $\mathcal{N}_ {c_ {0}}(\mathbb{R}^{+})$ be the sheaf of analytic functions $\hat{\phi}(\zeta)$ on $S_ {\delta}$ which is bounded by</p> \[\left\lvert \hat{\phi}(\zeta) \right\rvert \leq Ae^{ c_ {0}\left\lvert \zeta \right\rvert }.\] <p>We also define</p> \[\mathcal{N}(\mathbb{R}^{+}) := \bigcup_ {c_ {0}\in \mathbb{R}}\mathcal{N}_ {c_ {0}}(\mathbb{R}^{+}).\] <h1 id="appendix">Appendix</h1> <p>Here are some related definitions and concepts.</p> <p>A <code class="language-plaintext highlighter-rouge">resurgent function</code> is a an analytic function of $1$ complex variable obtained by Borel-Laplace summation method, from a resurgent series. This is not really a strict definition but rather an explanation.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="resurgence"/><summary type="html"><![CDATA[Series don’t just diverge for no reasons. The divergence of a series must reflect its cause….A divergent series is not meaningless, or a nuisance, but an essential and informative coded representation of the function. — Michael Berry]]></summary></entry><entry><title type="html">Lie Groups Revisited</title><link href="https://baiyangzhang.github.io/blog/2025/Lie-Groups/" rel="alternate" type="text/html" title="Lie Groups Revisited"/><published>2025-03-03T00:00:00+00:00</published><updated>2025-03-03T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2025/Lie-Groups</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2025/Lie-Groups/"><![CDATA[<p><em>Disclaimer: Nothing in this note is original.</em></p> <p>Let’s start with an example. Let $M(n,\mathbb{R})$ be the set of all $n\times n$ real matrices, it has $n\times n$ elements thus is topologically isomorphic to a $n\times n$ dimensional real space $\mathbb{R}^{n\times n}$. The <em>real general linear group</em> $\text{GL}(n,\mathbb{R})$ is the group of all <em>invertible</em> (with non-zero determinant) $n\times n$ real matrices. We can regard such matrices as a point $x$ in an abstract “space of matrices”, with the coordinates given by all its elements $x_ {ij}$. Obviously $\text{GL}(n,\mathbb{R})$ is a sub-space in $\mathbb{R}^{n\times n}$. The question is, what exactly is this subspace? Is it a codimension one open set or something else? To answer this question, notice that the determinant is a map from $M(n,\mathbb{R})$ to $\mathbb{R}$ which is <strong>continuous</strong>. The image of $GL(n,\mathbb{R})$ under the map is $\mathbb{R} - 0$ and is open in regular topology, thus the preimage of determinant function is also open, namely $GL(n,\mathbb{R})$ is a open subspace of $M(n,\mathbb{R})$. Product and inverse are also continuous functions defined on $GL(n,\mathbb{R})$, thus when we consider $GL(n,\mathbb{R})$ as a manifold and consider the matrix points, product and inverse as smooth functions on this manifold. This brings as to the definition of a <code class="language-plaintext highlighter-rouge">Lie group</code>:</p> <p><strong>Definition.</strong> A <code class="language-plaintext highlighter-rouge">Lie group</code>$G$ is a smooth manifold endowed with a product and an inverse. Product is a continuous map</p> \[G\times G \to G,\quad (g,h)\mapsto gh\] <p>making $G$ into a group. We demand that this map, as well as the “inversion map” be <strong>differentiable</strong>. Particularly Lie group is a <code class="language-plaintext highlighter-rouge">topological group</code>, meaning that it is a topological space with group structures such that the multiplication and inversion maps are continuous.</p> <p>For example, $\mathbb{R}$ is a Lie group under addition. This group is commutative, or <code class="language-plaintext highlighter-rouge">abelian</code>. The positive real numbers $\mathbb{R}^{+}$ also form a abelian group under multiplication. The real and complex general linear matrices $GL(n,\mathbb{R})$ and $GL(n,\mathbb{C})$ are also Lie groups under the usual matrix multiplication. The <code class="language-plaintext highlighter-rouge">special linear</code> group, $SL(n,\mathbb{R})$ is the subset of $GL(n,\mathbb{R})$ that have determinant one. For any matrix group, the adjective <strong>special</strong> means that the determinant is equal to one. Other examples include $\mathbb{T}^{n}$ the $n$-torus group, which is the abelian group of diagonal matrices of the form</p> \[z = \text{diag }\left\{ \exp i \theta_ {1},\dots,\exp i \theta _ {n} \right\} .\] <p>The $\mathbb{T}^{n}$ group is topologically homeomorphic to $\mathbb{S}^{1}\times\dots \times \mathbb{S}^{1}$, $n$ copies of them. Since the circles are connected, it follows that $\mathbb{T}^{n}$ is also connected.</p> <p>As a manifold, a Lie group is very special for the following reason. A Lie group always has two families of diffeomorphisms, the left and right translations. For $g\in G$, these translations are defined by</p> \[L_ {g}: G \to G, \quad h \mapsto gh, \text{multiply from the left}\] <p>and similarly</p> \[R_ {g}: G \to G, \quad h \mapsto hg, \text{multiply from the right.}\] <p><strong>Theorem.</strong> $U(N)$ is connected.</p> <p>To see it, just notice that any unitary matrix can be diagonalized to a $\mathbb{T}^{n}$ under some similar transformation, we casually write it as $\mathbb{T}^{n} = g U(N) g^{-1}$ for some $g\in U(N)$. Then $U(N)=g^{-1}\mathbb{T}^{n}g$ is connected to $\mathbb{1}$ smoothly by varying $\mathbb{T}^{n}$ smoothly from $\mathbb{1}$.</p> <p>The subgroup $\mathbb{T}^{n}$ of $U (n)$ given by (1) is called a maximal torus of $U(n)$. Any conjugate $h \mathbb{T}^{n} h^{-1}$ of this maximal torus is also called a maximal torus. By the same type of reasoning we may deal with the rotation group.</p> <p><strong>Theorem.</strong> $O(n)$ consists of two connected components, and $SO(n)$ is the part connected with the identity.</p> <p>As our last example, consider the set of linear transformation of points in 1-dimension affine space. The points in 1-dimensional affine space are denoted by $(t,1)^{T}$ where $t$ is the coordinate and $1$ denotes that it is a point. A vector in affine space would be denoted by $(v,0)^{T}$, thus the difference of two points yields a vector. The linear transformations, $G=\mathbb{A}^{1}$, the affine group of lines. They consist of real $2\times 2$ matrices of form</p> \[\begin{pmatrix} x &amp; y \\ 0 &amp; 1 \end{pmatrix}.\] <p>with $x&gt;0$.</p> <p>A matrix group is a subgroup of $Gl(n)$ that is also a submanifold of $Gl(n)$.</p> <h3 id="invariant-vector-fields-and-forms">Invariant Vector Fields and Forms</h3> <p>Lie groups are special as manifolds in the sense that, given a tangent vector $X_ {e}$ at the unity $e$, we may use the group elements to translate $X_ {e}$ to each point of $G$, by left or right group action. The left-translated tangent vector $X_ {g}$ is given by</p> \[X_ {g} := L_ {g\ast}\, X_ {e}\] <p>and the right-translated vector (denoted by the same notation)</p> \[X_ {g}:= R_ {g\ast }\, X_ {e}.\] <p>In this way we have two non-vanishing vector fields all over $G$.</p> <p>In fact, given a set of basis $X_ {i}$ of the tangent space $T_ {e}G$ at $e$, we can translate them to any point $g\in G$ thus yielding a basis $L_ {g\ast}\, X_ {i}$ of $T_ {g}G$. The importance of this claim has to do with the orientation of $G$ as a manifold. Recall that the orientation is defined by the order of basis, if we can translate the basis $X_ {i}$ to any point of the group then the group is orientable! We conclude that <em>every Lie group is a orientable manifold</em>.</p> <p>Consider for instance, a closed orientable surface $M^{2}$ of genus $g$. We shall see that of these surfaces only the torus (genus $1$) can support even a single nonvanishing tangent vector field. To be specific, $\mathbb{T}^{2}$ supports two sets of vector fields $\partial_ {\theta}$ and $\partial_ {\phi}$ where $\theta$ and $\phi$ parametrize the circles in $\mathbb{T}\cong \mathbb{S}^{1}\times\mathbb{S}^{1}$. The Torus can be regarded as the abelian Lie group $\mathbb{S}^{1}\times\mathbb{S}^{1}$ with multiplication</p> \[(\theta,\phi)\times (\theta',\phi'):=(\theta+\theta',\phi+\phi').\] <p>Topologically, the <strong>only</strong> compact Lie group of dimension 2 is the torus.</p> <p>A vector field $X$ on $G$ is said to be <code class="language-plaintext highlighter-rouge">left-invariant</code> or <code class="language-plaintext highlighter-rouge">right-invariant</code> if it is invariant under left or right translation,</p> \[L_ {h\ast }\,X_ {g} = X_ {hg}, \quad \text{left invariant}\] <p>and similarly for right invariant.</p> <p>We can define similar concept for differential forms. A differential form field $\omega$ on $G$ is said to be <code class="language-plaintext highlighter-rouge">left invariant</code> if</p> \[L_ {g}^{\ast }\, \omega_ {gh} = \omega_ {h}.\] <p>Note the direction of the pull-back.</p> <p>Consider the example of Lie group $G:=Gl(n)$ of invertible matrices. Let $t\to h(t)$ be a curve in $G$, namely a G-valued curve parametrized by one parameter $t$. Let $h(0)=h$ and $h’(0)=X_ {h}$, where $h’$ is the $t$-derivative of $h$. This curve is simply a matrix whose entries $h_ {mn}$ are functions of $t$, $h(t)$ describes a curve in $n^{2}$ dimensional space. Then $X_ {h}$ is simple the matrix whose entries are $t$-derivatives of $h$ at $t=0$, $h’_ {mn}(t=0)$. Of course just because $h(0)$ is in $G$ does not mean $h’(0)$ is also in $G$. Then for the constant matrix $g$, the curve $gh(t)$ given by matrix multiplication is the left translated curve of $h(t)$ by $L_ {g}$. The tangent vector of $gh(t)$ at $t=0$ is given by $gX_ {h}$,</p> \[L_ {g\ast }X_ {h} = gX_ {h}\] <p>which is again the simple matrix multiplication.</p> <p>Note that in any Lie group, if $X_ {i}$ is a basis for the left invariant vector fields and if $\sigma^{i}$ is the dual basis of 1-forms, then this dual basis is automatically left invariant. This can be shown using the relation $f^{\ast}\alpha(X)=\alpha(f_ {\ast}X)$. Also, note that if $\alpha$ and $\beta$ are invariant differential forms on $G$ then so are $d\alpha$ and $\alpha \wedge\beta$. A left invariant area form is also called a left <code class="language-plaintext highlighter-rouge">Haar measure</code>, which can be seen as a convenient way to use the left action $L_ {g\ast}$ to translate some well-defined infinitesimal areas about the entire group space to measure quantities defined on it.</p> <h3 id="one-parameter-subgroups">One-Parameter Subgroups</h3> <p>Recall that a group homomorphism is a map between groups</p> \[f: G\to H\] <p>that preserves the group multiplication. If it is further one-to-one (injective) and on-to (surjective) then it is an isomorphism. The exponential function is an example of group homomorphism, since $e^{ a }e^{ b }=e^{ a+b }$ for $a,b\in\mathbb{R}$ we have</p> \[\exp: (\mathbb{R},\times )\to (\mathbb{R}^{+},+).\] <p>A <code class="language-plaintext highlighter-rouge">1-parameter subgroup</code> of $G$ is by definition a differentiable homomorphism (a path)</p> \[g: \mathbb{R}\to G,\quad t\to g(t)\in G\] <p>of the additive group of the reals into the group $G$. Thus</p> \[g(s+t) = g(s)g(t) = g(t)g(s),\quad \text{(abelian)}.\] <p>Next let’s look at the most important example, the one parameter subgroup of matrix group $G$. As matrices</p> \[g(t+s)=g(t)g(s),\quad \text{matrix multiplication.}\] <p>To find $g$ that satisfy the condition, let’s take derivatives on both sides w.r.t. $s$ then put $s$ to zero, \(g'(t) = g(t)g'(0),\quad g'(0)=\text{const matrix.}\)</p> <p>The solution is of exponential form,</p> \[g(t) =g(0) \exp \left\{ t g'(0) \right\} .\] <p>Since $g(0)=e$ for any homomorphism, since homomorphism has to map identity to identity, we conclude that</p> \[g(t) = \exp \left\{ tg'(0) \right\} ,\quad g'(0)\in T_ {e}(G).\] <p>This is also the most general one-parameter subgroup of matrix group.</p> <p>The group multiplication property Eq. (2) tells us the how to proceed for arbitrary group. Regard $g’(0)$ as a tangent vector at $g(0)$ and regard the left multiplication by $g(t)$ as a map, Eq. (2) can be generalized to</p> \[g'(t) = L_ {g(t)\ast }\, g'(0), \quad L_ {g(t)\ast } \text{ is the induced map of }L_ {g(t)}.\] <p>It says that, the tangent vector $X$ of the one-parameter subgroup (a curve, if you like) is <em>left translated</em> along the subgroup (curve). Thus, given a tangent vector $X_ {\mathbb{1}}$ at the identity $\mathbb{1}$ in $G$,</p> <p><em>the 1-parameter subgroup of $G$ whose tangent at $\mathbb{1}$ is $X_ {\mathbb{1}}$ is the integral curve through $\mathbb{1}$ of the vector field $X$ on $G$ resulting from left translation of $X_ {\mathbb{1}}$ over all of $G$.</em></p> <p>The vector $X_ {\mathbb{1}}$ is called the <code class="language-plaintext highlighter-rouge">infinitesimal generator</code> of the 1-parameter subgroup.</p> <p>For any Lie group $G$ we shall denote the 1-parameter subgroup whose generator at $\mathbb{1}$ is $X$ by</p> \[g(t):= e^{ tX } \equiv \exp(tX).\] <p><em>Example.</em> The matrix</p> \[X= \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix}\] <p>can be considered a tangent vector at $\mathbb{1}$ of group $G=GL(2,\mathbb{R})$. The 1-parameter subgroup of $G$ is</p> \[\exp(tX) = \begin{bmatrix} \cos t &amp; -\sin t \\ \sin t &amp; \cos t \end{bmatrix}.\] <p>Note that $X$ is a vector and $\exp X$ is a group element, different things fundamentally.</p> <h3 id="the-lie-algebra-of-a-lie-group">The Lie Algebra of a Lie Group</h3> <p>Let $G$ be a Lie group regarded as a manifold (with group structure), the tangent vector space at the identity $\mathbb{1}$ of $G$ plays an important role and is denoted by Gothic letter ${\frak g}$. Sometimes it is denoted by script letters, which we will not do here.</p> <p><strong>Definition.</strong> The Lie group ${\frak g}$ of group $G$ is</p> \[{\frak g}:= T_ {\mathbb{1}}\,G.\] <p>The fancy notation might appears to be pretentious and uncomfortable at first, but once you get used to it, you’ll find them clear and precise. You’ll never mistake a point in the group space with a vector at the origin.</p> <p>Let $X_ {i}$ be a basis for ${\frak g}$, we will also use $X_ {i}$ to denote the left translation of this vector to all of $G$. Since any left invariant vector field is determined by its value at $\mathbb{1}$, the most general left invariant vector field is then of the form</p> \[X = \sum_ {i} v^{i}X_ {i},\quad v^{i} \text{ are constants.}\] <p>Let $\sigma^{i}$ be the dual basis of left invariant 1-forms on $G$. They are determined by their values on vectors from the Lie algebra. The most general left invariant r-form on $G$ is of the form</p> \[\alpha^{r} = \sum_ {I_ {&lt;}}\alpha_ {I} \, \sigma^{I}, \quad \sigma^{I} = \sigma^{I_ {1}}\wedge \sigma^{I_ {2}}\wedge \dots\] <p>It is again determined by its value on a $r$-tuple of vectors from ${\frak g}$. The components $\alpha_ {I}$ are again constants.</p> <p>It shouldn’t come as a surprise that,</p> <p><strong>Theorem.</strong> The Lie bracket $[X,Y]$ of two left invariant vector fields is again left invariant.</p> <p>To prove it, notice that $X$ is left invariant iff $\left\langle X,\sigma \right\rangle=\text{const}$ for some left invariant 1-form $\sigma$. For such a $\sigma$, since $X,Y$ are left invariant we have $\left\langle X,\sigma \right\rangle=\text{const}$ and $\left\langle Y,\sigma \right\rangle=\text{const}$. So the question is, is $\left\langle [X,Y],\sigma \right\rangle=\text{const}$? Then since for general $\omega \in\Omega^{1}$</p> <p>\(d\omega(X,Y)=-\omega([X,Y])+X(\omega(Y))-Y(\omega(X))\) we have</p> \[\left\langle [X,Y],\sigma \right\rangle =\sigma([X,Y]) =- d\sigma(X,Y)\] <p>where, if $\sigma$ is left invariant then so is $d\sigma$ since $L_ {g}^{\ast}d\sigma \Large\mid_ {g} =dL_ {g^{\ast}}\sigma \Large\mid_ {g} =d \sigma \Large\mid_ {\mathbb{1}}$. Thus</p> \[\sigma([X,Y])=\text{const}.\] <p>Q.E.D.</p> <p>We may then write</p> \[[X_ {i},X_ {j}]=if_ {ij}^{k}X_ {k},\quad f_ {ij}^{k}=-f_ {ji}^{k}.\] <p>$f_ {ij}^{k}$ is called the <code class="language-plaintext highlighter-rouge">structure constants</code>.</p> <p>The structure constant also can be applied to the left invariant 1-forms $\sigma$.</p> <p><strong>Theorem.</strong> The <code class="language-plaintext highlighter-rouge">Maurer-Cartan</code> equations</p> \[d\sigma^{i} = -\sum_ {j&lt;k}if^{i}_ {jk}\,\sigma^{j}\wedge \sigma^{k} =-\frac{1}{2}\sum_ {j,k}if^{i}_ {jk}\,\sigma^{j}\wedge \sigma^{k}\] <p>hold, and $d^{2}\sigma^{i}=0$ yields the <code class="language-plaintext highlighter-rouge">Jacobi identity </code> \(f^{m}_ {na}f^{n}_ {bc}+f^{m}_ {nb}f^{n}_ {ca}+f^{m}_ {nc}f^{n}_ {ab}=0.\)</p> <p>This Jacobi identity for left invariant I -forms is also a consequence of a general Jacobi identity for vector fields on any manifold $M^{n}$, which reads</p> \[[X,[Y,Z]]+[Y,[Z,X]]+[Z,[X,Y]]=0.\] <p>We now make the vector space $T_ {\mathbb{1}}G={\frak g}$ into a <code class="language-plaintext highlighter-rouge">Lie algebra</code> by defining a product</p> \[{\frak g}\times {\frak g}\to {\frak g}\] <p>as follows.</p> <p>Let $X,Y\in{\frak g}$, extend them to be vector fields $X’,Y’$ on all of $G$. Define the product of $X$ and $Y$ to be the Lie bracket</p> \[[X,Y]:= [X',Y']_ {\mathbb{1}}.\] <p>Note that <em>the Lie algebra product is not associative</em>! Take $SO(3)$ for example, there exists three basis vectors $X,Y,Z$ satisfying $[X,Y]=Z$, $[Y,Z]=X$ and $[X,Z]=-Y$, then $[X,[X,Y]]=-Y\neq[[X,X],Y]=0$.</p> <p><strong>We shall consistently identify the Lie algebra ${\frak g}$ with the $N(= \text{dim} G)$ dimensional vector space of left invariant fields on $G$.</strong></p> <p>Classically the Lie algebra was known as the “infinitesimal group” of $G$, for classically a vector was thought of roughly as going from a point to an “infinitesimally nearby” point. Then the Lie algebra consisted of group elements infinitesimally near the identity! We shall <strong>not</strong> use this picture.</p> <h3 id="the-exponential-map">The Exponential Map</h3> <p><strong>Theorem.</strong> For <em>any</em> matrix $A$,</p> \[\det \exp A = \exp \mathrm{Tr}\,A,\quad \text{or equivalently, } \det e^{ A }=e^{ \mathrm{Tr}\,A }.\] <p><strong>Theorem.</strong> The map</p> \[\exp: {\frak g}\to G,\quad A \mapsto e^{ A }\] <p>is a diffeomorphism of some neighborhood of $0\in{\frak g}$ onto a neighborhood of $\mathbb{1}\in G$.</p> <p>In a general Lie group, the 1-parameter subgroup $\text{exp}(t X)$ is the integral curve of a vector field on $G$, and thus it would seem that this need only be defined for $t$ small. In this case of a left invariant vector field on a group, it can be shown that the curve exists for all $t$, just as it does in the matrix case.</p> <h3 id="examples-of-lie-algebras">Examples of Lie Algebras</h3> <p>Consider our old friend, the matrix group $GL(n)$. Let $M(n\times n)$ be the vector space of <em>all</em> <em>real</em> $n\times n$ matrices, the dimension is clearly $n^{2}$. For $A\in M(n\times n)$, we have</p> \[\det e^{ A }=e^{ \mathrm{Tr}\,A }\geq 0.\] <p>Therefore the matrix $e^{ A }$ is invertible, the exponential map maps any $n\times n$ matrix to an invertible matrix,</p> \[\exp: M(n\times n)\to GL(n).\] <p>We usually use the same name of the group but all in lowercase Gothic font to denote the corresponding Lie algebra, for example the Lie algebra of $GL(n)$ group is denoted ${\frak gl}(n)$. Then</p> \[{\frak gl}(n)= M(n\times n).\] <p>If $G$ is a matrix group, that is, a <em>subgroup</em> of $GL(n)$, including $SO(N)$, $SU(N)$, etc., then its Lie algebra ${\frak g}$ is the largest subspace of $M(n\times n)$ such that $\exp {\frak g}\to G$.</p> <p>For special orthogonal matrices $SO(n)$, we simply state that its Lie algebra ${\frak so}(n)$ consists of anti-symmetric matrices.</p> <p>The group of unitary matrices $U(n)$, on the other hand, has Lie algebra ${\frak u}(n)$ consists of anti-hermitian matrices, $A^{\dagger}=-A$. As a special case, the special unitary groups $SU(n)$ has Lie algebra ${\frak su}(n)$ made of anti-hermitian matrices with zero trace, since $\exp \mathrm{Tr}\,=\det$.</p> <h3 id="do-the-1-parameter-subgroups-cover-g">Do the 1-Parameter Subgroups Cover $G$?</h3> <p>The question is, can every $g\in G$ be generated by an $A\in {\frak g}$? In other words,</p> \[\text{is the map } \exp: {\frak g}\to G \text{ onto?}\] <p>It can be shown that, $\exp$ is indeed onto if the group as a manifold is <em>connected</em> and <em>compact</em>. It is clear that a 1-parameter subgroup must lie in the connected piece of $G$ that contains the identity. The matrix group $SL(2,\mathbb{R})$ is not compact since the only constrain for an element</p> \[g=\begin{bmatrix} x &amp; y \\ z &amp; w \end{bmatrix}\] <p>is that</p> \[\det g = xw-yz=1\] <p>still $x$ can be arbitrary large even with this condition.</p> <p>However, it can be shown that $SL(2,\mathbb{R})$ is <em>connected</em>. To see it, it is helpful to adopt a geometric viewpoint. $SL(2,\mathbb{R})$ can be pictured as a pair of column vectors $(x z)^{T}$ and $(y w)^{T}$ in $\mathbb{R}^{2}$ spanning a parallelogram of area $1$. Deform he lengths of both so that the first becomes a unit vector, keeping the area $1$. This deforms $SL(2,\mathbb{R})$ into itself. Next, “Gram-Schmidt” the second so that the columns are orthonormal. This can be done continuously. The resulting matrix is then in the subgroup $SO(2,\mathbb{R})$ That is, it represents a rotation of the plane. The $SO(2)$ group is nothing but the rotation in two-plane, parametrized by a single angle $\theta$, topologically a circle, which is connected.</p> <p>In fact, Let $V^{r}$ be s submanifold of $M^{d}$, furthermore suppose $V$ is a <code class="language-plaintext highlighter-rouge">deformation retract</code> of $M$. A deformation retract means that there exists a 1-parameter, continuous map $r_ {t}: M\to M$ such that</p> <ol> <li>$r_ {0}$ is the identity map on $M$,</li> <li>$r_ {1}$ maps all of $M$ to $V$,</li> <li>$r_ {t}$ for all $t$ is the identity map on $V$.</li> </ol> <p>Then, consider homology group of any coefficient group. The deformation retract will change continuous by definition, meaning during the deformation from $r_ {t}$ to $r_ {t+\Delta t}$ it sweeps over a area on $M$ whose boundary is $r_ {t+\Delta t}-r_ {t}$. Then the deformation process will not change the homology group, thus</p> <p><strong>Theorem.</strong> If $V \subset M$ is a <em>deformation retract</em>, then $V$ and $M$ have isomorphic homology groups, regardless of the coefficient group $G$,</p> \[H_ {p}(M;G)\cong H_ {p}(V; G).\] <p>As an application of the above theorem, consider $SO(2)$ and $GL(2)$. Since $SO(2)$ is topologically a circle, we have</p> \[H_ {1}(SL(2,\mathbb{R})) \cong H_ {1}(\mathbb{S}^{1}) \cong \mathbb{Z}.\] <p>Since we are talking about $2\times 2$ matrices, there exists an interesting result for $A\in 2\times 2$ matrices called the <code class="language-plaintext highlighter-rouge">Cayley-Hamilton</code> theorem, stating that</p> <p>\(A^{2}-(\mathrm{Tr}\,A)A+\det A \,\mathbb{1}=0.\)</p> <h3 id="subgroups-and-subalgebras">Subgroups and Subalgebras</h3> <p>Since the group $G$ is generated by its Lie algebra ${\frak g}$, at least for the connected component, there should be a way to find the subgroup of $G$ by looking at ${\frak g}$.</p> <p>We will not distinguish between Lie algebra elements, namely the vectors at the identity of the group $X_ {\mathbb{1}}$, and the left-invariant vector field $X_ {g}$, since they are isomorphic to each other.</p> <p>The exp map $\exp X$ gives the integral curve of $X$. Since $X$ is left invariant, the integral curve starting at $g$ is connected to the integral curve starting at $\mathbb{1}$, to be specific $g(t) = L_ {g}g(0)$. To be specif, notice with an infinitesimal parameter $dt$, the tangent vector $X$ will “move” the point $\mathbb{1}$ to $\mathbb{1}+dt X = \mathbb{1}(1+dt X)$, since $X$ is left-invariantly translated to $g$, the vector $X_ {g}$ will move the point $g$ to $g(1+dX)$. On the other hand, the $X$ vector field, like any vector field, generates a flow $\phi_ {t}(g)$ where $g$ is the starting point.</p> <p>Convince yourself that the flow generated by the left invariant field $X$ is the 1-parameter group of right translations</p> \[\phi_ {t}(g) = g \exp(tX).\] <h4 id="commutator-of-matrices">Commutator of matrices</h4> <p>Recall that the Lie algebra ${\frak g}$ as a vector space, is simply the tangent space to $G$ at $\mathbb{1}$, but as an algebra it is identified with the left invariant vector fields on $G$. (Of course this is merely a convention; we could have used right invariant fields just as well.) If $X,Y\in{\frak g}$, then their Lie bracket is given by the Lie derivative since</p> \[[X,Y]=\mathcal{L}_ {X}Y.\] <p>If $G$ is a matrix group, then the elements of ${\frak g}$ are matrices as well, not in $G$ but the derivative of $G$ at origin, and $[X,Y]$ is merely the commutator of two matrices.</p> <h4 id="subgroup-and-subalgebra">Subgroup and subalgebra</h4> <p>The subset $H\subset G$ qualifies as a sub Lie group if H, if not embedded, is at least an <code class="language-plaintext highlighter-rouge">immersed</code>(locally embedded) submanifold of $G$. Furthermore, if the Lie algebra ${\frak h}$ is closed under Lie brackets, then ${\frak h}$ define a Lie algebra, called the <code class="language-plaintext highlighter-rouge">subalgebra</code> of ${\frak g}$.</p> <p>In general, if $H$ is a subgroup of $G$, then the Lie algebra ${\frak h}$ of $H$ is a subalgebra of ${\frak g}$. The converse of this is also true and of immense importance.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="Geometry"/><category term="Minkowski"/><category term="Frankel"/><summary type="html"><![CDATA[Disclaimer: Nothing in this note is original.]]></summary></entry><entry><title type="html">Introduction to Resurgence Note 1</title><link href="https://baiyangzhang.github.io/blog/2025/Introduction-to-Resurgence-1/" rel="alternate" type="text/html" title="Introduction to Resurgence Note 1"/><published>2025-02-27T00:00:00+00:00</published><updated>2025-02-27T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2025/Introduction-to-Resurgence-1</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2025/Introduction-to-Resurgence-1/"><![CDATA[<h1 id="table-of-content">Table of Content</h1> <ul> <li><a href="#1-motivation">1. Motivation</a></li> <li><a href="#2-analytic-continuation-and-monodromy">2. Analytic continuation and monodromy</a></li> <li><a href="#3-another-example-by-poincare">3. Another example by Poincare</a></li> <li><a href="#4-the-differential-algebra">4. The differential algebra</a></li> </ul> <h1 id="1-motivation">1. Motivation</h1> <p>The following are some examples of potential applications of resurgence theory.</p> <ul> <li>Normal forms for dynamical systems</li> <li>Gauge theory of singular connections</li> <li>Quantization of symplectic and Poisson manifolds</li> <li><code class="language-plaintext highlighter-rouge">Floer</code> homology and <code class="language-plaintext highlighter-rouge">Fukaya</code> categories</li> <li>Knot invariants</li> <li>Wall-crossing and stability conditions in algebraic geometry</li> <li>Spectral networks</li> <li>WKB approximation in quantum mechanics</li> <li><strong>Perturbative expansions in quantum field theory (QFT)</strong></li> </ul> <p>One of the most astonishing achievements of resurgence theory in QFT is that one can uncover the non-perturbative results from perturbative expansion alone! Typically, calculating non-perturbative results requires every possible resource you can have, such as the topology of the vacuum manifold, some real special cancellation, but perturbative calculation is much more straightforward, how can it contain the information that was so hardly revealed by all kinds of non-perturbative techniques? Nevertheless, resurgence theory enables us to derive non-perturbative results, such as instanton contribution, through the analytical continuation of perturbative data! On the one hand, it feels like black magic; on the other hand, perhaps I shouldn’t be so surprised, since if we could obtain the full perturbative expansions to all orders, we can essentially reproduce the equation of motion, the Lagrangian, which inherently includes all non-perturbative information. At least in theory then, perturbative expansions should be capable of yielding non-perturbative insights. However, this is only in theory. It’s still utterly astonishing to witness it happening in front of your eyes in practice. Rainbow doesn’t become less beautiful just because you’ve learnt the science behind it.</p> <p>Actually, I would argue that it is impossible to make sense of perturbation theory <strong>without</strong> knowing at least some facets of resurgence theory. We include more and more perturbative terms in any perturbative power expansion, the power series eventually diverges, so what sense does it make to just consider the first few terms? Claiming that the first few terms of a divergent series give the dominant results would be ridiculous. However the miraculous agreement between perturbative QED and experiments clearly suggests that perturbation expansion makes sense, it is by no chance an accident. The answer to this question lies in resurgence theory.</p> <hr/> <p>H. Poincare mentioned “a kind of misunderstanding between geometers and astronomers about the meaning of the word convergence”, he proposed a simple example: consider the following series</p> \[\sum \frac{1000^{n}}{n!} \text{ and }\sum \frac{n!}{1000^{n}},\] <p>Poincare said that for geometers, i.e. mathematicians in his time, the first one converges because at large $n$ the terms gets smaller and smaller. But for astronomers the first one is as good as divergent since the next terms doesn’t get smaller until $n$ is larger than $1000$. For astronomers the second series diverges because the first $1000$ terms decreases quickly.</p> <p>He then proposes to reconcile both points of view by clarifying the role that divergent series (in the sense of geometers) can play in the approximation of certain functions. This is the origin of the modern theory of asymptotic expansion.</p> <p>In physics, the divergence of power series can be shown via different approaches:</p> <ol> <li>Dyson Freeman show it in a heuristic way, see his short paper;</li> <li>By combinatorial argument. The number of Feynman diagrams grows factorially as $n!$, while the contribution of each diagram decreases as $g^{n}$, then if there is no cancellation between different diagrams, the perturbative series grows as $g^{n} n!$, which eventually diverges. Lipatov<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> first show that in scalar QFT the series indeed grows as $n!$. Similar growth are found in quantum mechanics<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> and matrix models<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>.</li> </ol> <h1 id="2-analytic-continuation-and-monodromy">2. Analytic continuation and monodromy</h1> <p>In this note we shall focus on <code class="language-plaintext highlighter-rouge">formal power series</code>, sometimes called the <code class="language-plaintext highlighter-rouge">polynomial forms</code>. When regarded as formal power series with real coefficients of indeterminant $t$, denoted $\mathbb{R}[t]$, the previous example should be written as (note the appearance of extra variable $t$)</p> \[\sum \frac{1000^{n}}{n!}t^{n} \quad \text{ and } \quad \sum \frac{n!}{1000^{n}}t^{n}\] <p>where the first one has <strong>infinite</strong> <strong>radius of convergence</strong> with respect to $t$, while the second one has <strong>zero radius of convergence</strong>. For us, <code class="language-plaintext highlighter-rouge">divergent series</code> will usually mean a formal power series with zero radius of convergence.</p> <p><code class="language-plaintext highlighter-rouge">Formal power series</code> is a generalization of normal power series, or polynomial, in the sense that we consider a power series of infinite order as a formal object and don’t worry about evaluation, or if it is convergent. Given a ring $R$, consider the set of formal power series in $X$ with coefficients from $R$, denoted by $R[[X]]$, which is itself another ring. To see it, note that multiply one polynomial to another and we have a new polynomial, similarly for other requirements for a ring. It is called the <strong>ring of formal power series in the variable $X$ over $R$</strong>.</p> <p>First we clarify some definitions and concepts that might be useful in the future.</p> <p>We say a complex-valued function $f:\Omega \to \mathbb{C}$ is <code class="language-plaintext highlighter-rouge">analytic</code> if $f$ is represented by a convergent power series expansion on a neighborhood around every point $a\in\Omega$.</p> <p>We say a complex-valued function $f:\Omega \to \mathbb{C}$ is <code class="language-plaintext highlighter-rouge">holomorphic</code> iff it satisfied Cauchy-Riemann relation, which is equivalent to $\partial f/\partial \overline{z}=0$ . If we regard $z$ and $\overline{z}$ as independent variables, a holomorphic function $f$ is only a function of $z$ not $\overline{z}$.</p> <p>A differential manifold is a topological space (given by closed sets and all that stuff) with differential structure, which is Hausdorff and second-separable. It practically means that you can find a way to do derivatives on the manifold. A $n$-Dimensional real (Complex) manifold is locally homeomorphic to $\mathbb{R}^N (\mathbb{C}^n)$, plus the condition that the transition from one chart (coordinate system) to another is <code class="language-plaintext highlighter-rouge">homeomorphic (holomorphic)</code>.</p> <p>An <code class="language-plaintext highlighter-rouge">open disk</code> of radius $r$ around $z_ 0$ is the set of points $z$ on $\mathbb{C}$ that satisfies</p> \[\left\lvert z-z_ 0 \right\rvert &lt; r.\] <p>A <strong>open deleted disk of radius $r$ around $z_ 0$</strong> is the set of points with</p> \[0 &lt; \left\lvert z-z_ 0 \right\rvert &lt; r.\] <p>a deleted disk is also called a punctured disk, since the origin $z_ {0}$ has been taken out.</p> <hr/> <p>A complex atlas on a 2-dimensional manifold is a collection of charts that cover the manifold, where each chart maps a portion of the manifold to an open subset of the complex plane $\mathbb{C}$, and the transition functions between overlapping charts are holomorphic (complex differentiable).</p> <ul> <li> <p>$\mathbb{C} P^n$ Model: The (n+1)-tuple $z = (z^0,\cdots,z^n)$ defines a vector in space $\mathbb{C}^{n+1}$. Define a equivalence relation $\sim$ between two vectors,</p> \[(u^0,\cdots,u^n) \sim (v^0,\cdots,v^n) \text{ if } \exists \lambda\neq 0 \in \mathbb{C} \text{ so that } \mathbf{u} = \lambda \mathbf{v}\] </li> </ul> <p>then</p> \[\mathbb{C} P^{n} \equiv (\mathbb{C}^{n+1}-\left\lbrace0\right\rbrace)/\sim,\] <p>and the $n+1$ numbers are call <code class="language-plaintext highlighter-rouge">homogeneous coordinates</code> and is denoted by $[z^0,\cdots,z^n]$. We can make one of them constantly equal to 1, then only the rest are really coordinates, it is called <code class="language-plaintext highlighter-rouge">inhomogeneous coordinates</code>.</p> <p>The Riemann sphere $\mathbb{C}P^1$ is a classic example of real-dimension two (denoted $\text{dim}_ {\mathbb{R}}=2$) complex manifold. It can be visualized as the complex plane plus a point at infinity. To see that all the infinities on a complex plain can be identified as a single point, notice that in $[z_ {0},z_ {1}]$ when $z_ {1}\to \infty e^{ i\theta }$ it is equivalent to $\left[ \frac{z_ {0}}{\infty e^{ i\theta }},1 \right]$, and no matter the angle $\theta$ it always goes to $[0,1]$. To construct a complex atlas for the Riemann sphere, we use two charts:</p> <ol> <li> <p><strong>Chart $U_ 1$</strong>: This chart covers the sphere minus the point at infinity. It maps each point $z$ in the complex plane $\mathbb{C}$ to itself: \(\phi_ 1: U_ 1 \rightarrow \mathbb{C}, \quad \phi_ 1(z) = z\) Here, $U_ 1 = \mathbb{C}$.</p> </li> <li> <p><strong>Chart $U_ 2$</strong>: This chart covers the sphere minus the origin. It maps each point $z$ in the complex plane, excluding the origin, to its reciprocal: \(\phi_ 2: U_ 2 \rightarrow \mathbb{C}, \quad \phi_ 2(z) = \frac{1}{z}\) Here, $U_ 2 = \mathbb{C} \setminus {0}$.</p> </li> </ol> <p>The two charts overlap on $U_ 1 \cap U_ 2 = \mathbb{C} \setminus {0}$. The transition functions between these charts are:</p> <ul> <li>From $U_ 1$ to $U_ 2$: \(\phi_ 2 \circ \phi_ 1^{-1}(z) = \frac{1}{z}\)</li> <li>From $U_ 2$ to $U_ 1$: \(\phi_ 1 \circ \phi_ 2^{-1}(z) = \frac{1}{z}\)</li> </ul> <p>Both transition functions are holomorphic, as the function $f(z) = \frac{1}{z}$ is complex differentiable wherever $z \neq 0$.</p> <p>To say that two complex atlases on a manifold are <code class="language-plaintext highlighter-rouge">analytically equivalent</code> means that the combined atlas they form (by taking all the charts from both atlases) is itself a complex atlas. This implies that the transition functions between the charts of one atlas and the charts of the other atlas are holomorphic wherever they overlap.</p> <hr/> <p>By a <code class="language-plaintext highlighter-rouge">complex structure</code> on a manifold, we mean an equivalent class of analytically equivalent complex atlases on the manifold. If we give a manifold a complex atlas, then we have given it a complex structure.</p> <p>A Riemann surface is a pair $(X,\Sigma)$ where $X$ is a connected 2-dimensional manifold and $\Sigma$ is a complex structure on $X$. The simplest Riemann surface is the complex plane itself.</p> <hr/> <p><code class="language-plaintext highlighter-rouge">Meromouphic functions</code> are functions holomorphic except at a discrete set of isolated poles.</p> <p>A <code class="language-plaintext highlighter-rouge">domain</code> in $\mathbb{C}$ is a connected non-empty open subset of $\mathbb{C}$. Note that it has nothing to do with integral domain in abstract algebra.</p> <hr/> <p><strong>Riemann Sphere</strong></p> <p>There are two ways to think of $\overline{C} \equiv \mathbb{C} \cup \left\lbrace\infty\right\rbrace$, namely the compactified complex plane, by adding a point called infinity $\infty$.</p> <ul> <li>complex projective plane $\mathbb{C}P^1$.</li> <li>a 2D sphere $\mathbb{S}^2$. The coordinates is given by the stereographic projection, except for the north pole $N$, $\pi : \mathbb{S}^2-\left\lbrace N \right\rbrace \to \mathbb{C}$.</li> </ul> <p>The space $\overline{\mathbb{C}}$ has the structure of a Riemann surface and it is called the <code class="language-plaintext highlighter-rouge">Riemann sphere</code>. We can introduce two charts on the Riemann sphere,</p> \[\mathfrak{U}_ 1 = \mathbb{C},\quad \mathfrak{U}_ 2 = \mathbb{C}^\ast \cup \left\lbrace\infty\right\rbrace\] <p>where $\mathbb{C}^\ast$ is the punctured complex plane, $\mathbb{C}^\ast \equiv \mathbb{C} - \left\lbrace0\right\rbrace$.</p> <p><strong>Theorem.</strong> A function is meromorphic on $\overline{\mathbb{C}}$ iff its restriction on $\mathbb{C}$ is a rational function.</p> <p>By the restriction of a function, we mean that to restrict the domain so that the it is well defined, no singularities. For example, the function $f:x\to 1/x$ has a singularity at $x=0$, then if we want something which is just like $f$ but has no singularity, we can just restrict the domain to $\mathbb{R} - \left\lbrace0\right\rbrace$, which is said to be a restriction of $f$.</p> <p>A <code class="language-plaintext highlighter-rouge">germ</code> of functions at a point $a\in\mathbb{C}$ is a set of function defined in the neighborhood of $a$ which all have the same Taylor expansion. A more mathematical definition is as following.</p> <p>Use $\mathcal{O}(a)$ to denote the set of functions which are defined in a neighborhood of $a$ and is holomorphic at $a$. Define an equivalence relation $\sim$ so that if $f\sim g$ for $f,g \in \mathcal{O}(a)$, then $f,g$ are identical on some neighborhood of a. The equivalence class is called a germ of holomorphic functions at $a$.</p> <p>Intuitively, the germ of a function tells us how a function behaves locally at point $a$.</p> <hr/> <p>The <code class="language-plaintext highlighter-rouge">Fundamental Uniqueness Theorem (FUT)</code> for holomorphic functions:</p> <p><strong>Theorem.</strong> If $f,g$ are holomorphic functions on a domain $D\in\mathbb{C}$ and $f \sim g$ for some point $a\in D$, namely f and g are in the same germ at a, then f is identical to g on $D$.</p> <p>The germ of $f$ will be denoted by $\overline{f}$, if there is no ambiguity about around which point it is defined.</p> <hr/> <p><strong>Analytic Continuation, Monodromy</strong></p> <p>First we introduce the analytic continuation in a different way, more formal and more mathematical. We begin by defining <code class="language-plaintext highlighter-rouge">pairs</code>. The idea is that, a function is not only defined by the values but also the domain on which it is defined.</p> <p>A <code class="language-plaintext highlighter-rouge">pair</code> $(U,f)$ is a non-zero open disk $U\subset \mathbb{C}$ (why does it has to be open? I don’t know.) and a function, holomorphic on $U$, such that the radius of $U$ is the maximum radius of convergence of the series expansion of $f$. The center of the pair is the center of $U$. Usually $U$ will stop at some singular point.</p> <p>Another concept is adjacency, two pairs $(U,f)$ and $(V,g)$ are said to be <code class="language-plaintext highlighter-rouge">adjacent</code> if $U\cap V \neq 0$ and $f \equiv g$ on $U\cap V$.</p> <p>If there is a <code class="language-plaintext highlighter-rouge">finite</code> sequence of pairs $(U_ i,f_ i),\quad i = 0,1,\cdots, n$ so that for all $i$, $(U_ i,f_ i)$ is adjacent to $(U_ {i\pm 1},f_ {i\pm 1})$, then $(U_ 0,f_ 0)$ is said to be the analytical continuation of $(U_ n,f_ n)$.</p> <p>We can define a analytical continuation along a curve $\gamma: [0,1] \to \mathbb{C}$. The definition is kind of intuitive so I will skip it here. Now the question is, is the analytical continued function $(U_ n,f_ n)$ dependent on the path? The answer is the monodromy theorem:</p> <p><strong>Theorem.</strong> If the two path are homotopic, then the analytical continuation results to the same functions.</p> <p>If in any doubt, just think of the $\ln{z}$ function.</p> <hr/> <p><strong>Linear Differential System</strong></p> <p>A <code class="language-plaintext highlighter-rouge">linear system</code> is short for a complex linear ordinary differential system. A linear system of order p is a system with p first order ordinary differential equations.</p> \[\frac{dy(x)}{dx} = A(x) y(x),\quad y(x) = \begin{pmatrix} y_ 1(x)\\ \vdots \\ y_ p(x) \end{pmatrix}\] <p>where $y(x)$ is a column vector of functions, and $A$ is the $p\times p$ coefficient matrix. The system is referred to as $(S)$.</p> <p>We used $\mathbb{C}(x)$ to denote the field of complex rational functions, and $\mathscr{M}(D)$ the field of meromorphic functions on domain $D$, and $\mathscr{O}(D)$ the ring of holomorphic functions on domain $D$.</p> <p>A <code class="language-plaintext highlighter-rouge">fundamental solution</code> of $(S)$ is a $p\times p$ matrix whose columns are $\mathbb{C}$-linear independent solutions to $(S)$.</p> <p>The <code class="language-plaintext highlighter-rouge">Wronskian</code> for $n$ functions are defined to be</p> \[W(f_ 1,\cdots, f_ n)(x) \equiv \begin{vmatrix} f_ 1(x) &amp; \cdots &amp; f_ n(x)\\ f'_ 1(x)&amp; \cdots &amp; f'_ n(x)\\ \cdots \\ f^{(n-1)}(x) &amp; \cdots &amp; f^{(n-1)}(x) \end{vmatrix}.\] <p>Let $\Sigma= \left\lbrace a_ 1,\cdots,a_ n \right\rbrace\subset \overline{\mathbb{C}}$ be the set of singular points of $(S)$. Let $U_ \Sigma \equiv \overline{\mathbb{C}}\backslash \Sigma$.</p> <p>Recall a nice property of Wronskian: Let $\mathscr{D}$ a domain in $U_ \Sigma$, $W$ a $p\times p$ solution of $(S)$, the following are equivalent:</p> <ul> <li>W is a fundamental solution of $(S)$</li> <li>$\det{W(x)}\neq 0$ for some $x \in \mathscr{D}$</li> <li>$\det{W(x)}\neq 0$ for all $x \in \mathscr{D}$</li> </ul> <p>In other words, the Wronskian is either nonzero on the entire domain, or identically zero.</p> <hr/> <p><strong>Differential Galois Theory</strong></p> <p>A <code class="language-plaintext highlighter-rouge">differential field</code> $(k,\partial)$ is a field $k$ with derivation. A <code class="language-plaintext highlighter-rouge">differential homomorphism</code> $\phi:(k_ 1,\partial)\to(k_ 2,\partial)$ from $k_ 1$ to $k_ 2$ is a field homomorphism that commutes with $\partial$. A triple $(k_ 1, \phi, k_ 2)$ is called a differential extension. $k_ 2$ is also called a differential extension of $k_ 1$.</p> <p>A differential system</p> \[\partial y = A y\] <p>where $A$ is a $p \times p$ matrix.</p> <h1 id="3-another-example-by-poincare">3. Another example by Poincare</h1> <p>To get some feeling about resurgence, let’s start with an example first given by Poincare. This example shows <strong>how an divergent series emerges from a function.</strong></p> <p>Fix $w\in \mathbb{C}$ and $\left\lvert w \right\rvert&lt;1$. Consider the series of functions of the complex variable $t$,</p> \[\phi_ {k}(t) := \frac{w^{k}}{1+kt},\quad \phi(t):= \sum_ {k\geq 0} \phi_ {k}(t).\] <p>This series is uniformly convergent on</p> \[U:=C^{\ast } - \left\lbrace -1,-\frac{1}{2},-\frac{1}{3},\dots \right\rbrace .\] <p>Hence the sum $\phi$ is <code class="language-plaintext highlighter-rouge">holomorphic</code> in $U$. Actually $\phi$ is meromorphic on $\mathbb{C}^{\ast}$ (not on $\mathbb{C}$ since the origin would be a limiting point of the poles) with simple poles at $1 / \mathbb{N}$.</p> <p>We now show how this function $\phi$ gives rise to a divergent formal series when $t$ approaches $0$. The idea is to expand each $\phi_ {k}$ first in terms of $t$. For each $k \in \mathbb{N}$, we have a <em>convergent</em> Taylor expansion at $t=0$,</p> \[\phi_ {k}(t) = w^{k}\sum_ {n\geq 0}(-1)^{n}(kt)^{n}, \quad \left\lvert t \right\rvert&lt; \frac{1}{k} .\] <p>One might be tempted to recombine the (convergent) Taylor expansion of $\phi_ {k}$ to give $\phi(t)$. It amounts to considering the well-defined <strong>formal series</strong></p> \[\tilde{\phi}(t) := \sum_ {n\geq 0}(-1)^{n} b_ {n} t^{n}, \quad b_ {n}:= \sum_ {k\geq 0}k^{n} w^{k}.\] <p>We see that $b_ {n}$ is convergent since</p> \[\lim_ { k \to \infty } \frac{(k+1)^{n}w^{k+1}}{k^{n}w^{k}} = w &lt;1 \text{ by construction}.\] <p>However, it turns out that <strong>this formal series is divergent!</strong></p> <p>To see this, make the substitution $w = e^{ s }$. Then, since $\text{Re }w &lt;1$, we have</p> \[b_ {0}=(1-w)^{-1} = (1-e^{ s })^{-1}\] <p>and</p> \[b_ {n} = \left( w \frac{d}{dw} \right)^{n}b_ {0} = \left( \frac{d}{ds} \right)^{n} b_ {0}.\] <p>There is an easy way to tell if a series has non-zero radius of convergence, by some kind of a “dominance criterion”. If the series of study $a_ {n}$ is dominated by $AB^{n}$ where $A,B$ are real and $A,B&gt;0$, namely for all but finite $n$ we have $\left\lvert a_ {n} \right\rvert \leq AB^{n}$. Then the formal series</p> \[F(\xi) := \sum(-1)^{n} a_ {n} \frac{\xi^{n}}{n!}\] <p>would have infinite radius of convergence. <strong>$F$ can be seen as a map of the formal series to a function of $\xi$, note that we have inserted a factor of $1 / n!$ into the sum to make is more convergent</strong>. In our case of $b_ {n}$, we see that</p> \[F(\xi) = \sum(-1)^{n} b_ {n} \frac{\xi^{n}}{n!} = \sum(-1)^{n} \frac{\xi^{n}}{n!}\left( \frac{d}{d s} \right)^{n} b_ {0}(s) = b_ {0}(s-\xi) = \frac{1}{1-e^{ s-\xi }} .\] <p>This functions has finite radius of convergence, since the last expression in the equation above diverges at $\xi=s+2\pi i \mathbb{Z}$. The radius of convergence is not infinite! Thus $\tilde{\phi}$ must have zero radius of convergence.</p> <hr/> <p>Now the question is to understand the relation between $\tilde{\phi}$ and $\phi$. We shall see in this note that the <code class="language-plaintext highlighter-rouge">Borel-Laplace</code> summation is a way of going from the divergent formal series $\tilde{\phi}$ to the finite function $\phi$. $\tilde{\phi}$ is actually the <code class="language-plaintext highlighter-rouge">asymptotic expansion</code> of $\phi(t)$ at $t=0$. We shall explain what it means next.</p> <p>We can already observe that the moduli of the coefficients $b_ {n}$ satisfy</p> \[\left\lvert b_ {n} \right\rvert &lt;AB^{n} n! ,\quad n \in \mathbb{N}\] <p>for some $A,B&gt;0$. Such inequalities are called <code class="language-plaintext highlighter-rouge">1-Gevrey estimates</code> for the formal series $\tilde{\phi}(t)=\sum b_ {n}t^{n}$.</p> <p>We remark that, since the original function $\phi(t)$ is not holomorphic (nor meromorphic) in any neighborhood of 0, because of the accumulation at the origin of the sequence of simple poles $- \frac{1}{k}$. Thus it would be very surprising to find a positive radius of convergence for $\tilde{\phi}$.</p> <hr/> <p>Resurgence theory can also be used to study power series of the form</p> \[\sum_ {i} \left( \sum_ {j} a_ {ij}t^{j} \right) e^{ -c_ {i} / t }\] <p>note that the variable $t$ appears at two places, once in the series and once in the exponent. The exponent term is the small correction that is invisible to Taylor expansion at $t=0$, and the formal series in the parenthesis diverges.</p> <p>In the next note we will dive into the details of resurgence theory, beginning with the differential algebra $(\mathbb{C}[[1 / z]],\partial)$.</p> <h1 id="4-the-differential-algebra">4. The differential algebra</h1> <p>It will be convenient for us to set $z = 1/t$ in order to “work at $\infty$” rather than at the origin, since we will often talk about compactified spaces. This means that we shall deal with expansions involving <em>non-positive</em> integer powers of the indeterminate. We denote the set of all the <code class="language-plaintext highlighter-rouge">formal power series</code>, i.e., polynomials in $1 / z$ by</p> \[\mathbb{C}[\![z^{-1}]\!] = \left\{ \phi=\sum_ {n\geq 0}a_ {n}z^{-n} \,\middle\vert\, a_ {i} \in \mathbb{C} \right\}.\] <p>This is a vector space with basis $1, z^{-1},z^{-2}$, etc. It is also an algebra when we take into account the Cauchy product</p> \[\left( \sum a_ {n}z^{-n} \right) \left( \sum b_ {n} z^{-n} \right) = \sum c_ {n} z^{-n}, \quad c_ {n} = \sum_ {p+q=n} a_ {p} b_ {q}.\] <p>The derivation</p> \[\partial = \frac{d}{d z}\] <p>further makes it a <em>differential algebra</em>, which simply means $\partial$ is a linear map which satisfied the Leibniz rules.</p> <p>This is the derivative in terms of $z$, it is natural to ask what is the derivative in terms of $t$. The answer is straightforward,</p> \[\partial = \frac{d}{d z} = \frac{d}{d t^{-1}} = \frac{d}{-t^{-2}dt} =-t^{2} \frac{d}{d t} .\] <p>Then, in mathematical terminology, there is an isomorphism of differential algebra between $(\mathbb{C}[[z^{-1}]],\partial)$ and $(\mathbb{C}[[t]],\partial)$.</p> <hr/> <p>The <code class="language-plaintext highlighter-rouge">standard valuation</code>, or sometimes called the <code class="language-plaintext highlighter-rouge">order</code>, on $\mathbb{C}[[z^{-1}]]$ is the map</p> \[\text{val}: \mathbb{C}[\![z^{-1}]\!] \to \mathbb{N} \cup \infty\] <p>defined by $\text{val }(0)=\infty$ and</p> \[\boxed{ \text{val }(\phi) := \text{min } \left\{ n\in \mathbb{N} \,\middle\vert\, a_ {n}\neq 0 \right\} ,\quad \phi =\sum a_ {n} z^{-n} \neq 0. }\] <p>For $\nu \in\mathbb{N}$, we will use the notation</p> \[z^{-\nu}\mathbb{C}[\![z^{-1}]\!] = \left\{ \sum_ {n\geq \nu} a_ {n}z^{-n} \,\middle\vert\, a_ {\nu},a_ {n+1},\dots \in \mathbb{C} \right\}.\] <p>This is the set of all the complex polynomials in $z^{-1}$ such that the standard valuation is no less than $\nu$.</p> <p>From the viewpoint of the ring structure, ${\frak I} = z^{-1}\mathbb{C}[[z^{-1}]]$ is the maximal ideal of the ring $\mathbb{C}[[z^{-1}]]$. It is often referred to as the <em>formal series without constant term</em>.</p> <p>It is obvious that</p> \[\text{val }(\partial \phi) \geq \text{val }(\phi)+1\] <p>with equality iff there is no constant term.</p> <hr/> <p>With the help of the standard valuation, we can introduce the concept of <code class="language-plaintext highlighter-rouge">distance</code> into the ring of the formal series. Define</p> \[d(\phi,\psi) := 2^{-\text{val }(\phi-\psi)},\quad \phi,\psi \in \mathbb{C}[\![z^{-1}]\!]\] <p>as the distance between $\phi$ and $\psi$. It can only take discrete values, such as $1, 1 / 2, 1 / 4,$ etc.</p> <p>With the definition of distance, $\mathbb{C}[[z^{-1}]]$ becomes a <code class="language-plaintext highlighter-rouge">complete metric space</code>. The topology induced by this distance is called the <code class="language-plaintext highlighter-rouge">Krull topology</code> or the <code class="language-plaintext highlighter-rouge">topology of the formal convergence</code> (or the ${\frak I}$-adic topology). It provides a simple way of using the language of topology to describe certain algebraic properties.</p> <p>We mention that a sequence $\phi_ {n}$ of formal series is a Cauchy sequence iff for each $i\in\mathbb{N}$, the $i$-the coefficient is stationary, namely the $i$-th coefficient of $\phi_ {n}$ becomes a constant when $n$ is larger than certain natural number.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Divergence of the Perturbation Theory Series and the Quasiclassical Theory, Published in: Sov.Phys.JETP 45 (1977), 216-223, Zh.Eksp.Teor.Fiz. 72 (1977), 411-427. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>J. Zinn-Justin, “Perturbation Series at Large Orders in Quantum Mechanics and Field Theories: Application to the Problem of Resummation”, Phys. Rept., vol. 70, p. 109, 1981. doi: 10.1016/0370-1573(81)90016-8;C. M. Bender and T. T. Wu, “Anharmonic oscillator. 2: A Study of perturbation theory in large order”, Phys. Rev., vol. D7, pp. 1620–1636, 1973. doi: 10.1103/PhysRevD.7.1620. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>M. Marino, R. Schiappa, and M. Weiss, “Nonperturbative E↵ects and the Large-Order Behavior of Matrix Models and Topological Strings”, Commun. Num. Theor. Phys., vol. 2, pp. 349–419, 2008. doi: 10.4310/CNTP.2008.v2.n2.a3. arXiv: 0711.1954 [hep-th]. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Baiyang Zhang</name></author><category term="resurgence"/><summary type="html"><![CDATA[Table of Content]]></summary></entry><entry><title type="html">Perturbative SSB and Effective Action</title><link href="https://baiyangzhang.github.io/blog/2025/Perturbative-SSB-and-Effective-Action/" rel="alternate" type="text/html" title="Perturbative SSB and Effective Action"/><published>2025-01-06T00:00:00+00:00</published><updated>2025-01-06T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2025/Perturbative-SSB-and-Effective-Action</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2025/Perturbative-SSB-and-Effective-Action/"><![CDATA[<ul> <li><a href="#1-spontaneous-symmetry-breaking">1. <em>Spontaneous</em> symmetry breaking</a></li> <li><a href="#2-degenerate-vacua-good-and-bad">2. Degenerate Vacua, Good and Bad</a></li> <li><a href="#3-effective-action">3. Effective Action</a> <ul> <li><a href="#31-calculating-the-1-loop-effective-potential">3.1. Calculating the 1-Loop Effective Potential</a></li> </ul> </li> <li><a href="#4-the-physical-meaning-of-the-effective-potential">4. The Physical Meaning of the Effective Potential</a></li> </ul> <p>This note is based on Chapter 44 of <em>Lectures of Sidney Coleman on Quantum Field Theory</em>.</p> <h1 id="1-spontaneous-symmetry-breaking">1. <em>Spontaneous</em> symmetry breaking</h1> <p>This section can be neglected.</p> <p>The word <em>spontaneous</em> used to baffle me. It doesn’t seem so “spontaneous” in the context of quantum field theory, such as $\phi^{4}$ model. At least not spontaneous enough in comparison with that in the Ising model. In Ising model, starting from high temperature, above Curie temperature to be specific, the ferromagnetic system is in a rotational symmetric phase, all the little spins point to random directions. As the temperature drops, at the Curie temperature the system undergoes a second order phase transition, the system picks a direction and all of a sudden, most of the spins are aligned in that directions,as a result the rotational symmetry is broken, without any external manipulation, hence the word spontaneous. To be more specific, take 2D (spatial) Ising model for example, which is famously solved by Lars Onsager. We don’t choose 1D Ising model because in 1D, spontaneous symmetry breaking doesn’t occur due to the lack of a phase transition at finite temperatures, there is a critical dimension under which the free energy is dominated by entropy, so the order phase is not preferred, according to the so-called energy-entropy argument.</p> <hr/> <p>For a 2D lattice with spins (which can take values +1 or -1, representing up or down magnetic moments) located on each site, the Hamiltonian is given by:</p> \[H = -J \sum_ {\langle i, j \rangle} s_ i s_ j - h \sum_ i s_ i\] <p>where $J$ is the exchange interaction energy between neighboring spins. It determines the strength of the interaction and can be positive (favoring alignment) or negative (favoring anti-alignment). The sum $\sum_ {\langle i, j \rangle}$ runs over all nearest-neighbor pairs of lattice sites, ensuring that each pair is counted once. $s_ i$ and $s_ j$ are the spin values at sites $i$ and $j$, respectively. $h$ is the external magnetic field, but for the spontaneous phase transition discussion, we consider $h = 0$.</p> <p>The free energy (Helmholtz free energy, to be specific) $F$ of the system combines the <em>internal energy</em> and the <em>entropy</em>. Classically speaking, the free energy describes the work that can be extracted from a system at a constant temperature, but here the physical meaning is that the equilibrium state is such that minimizes the free energy. The free energy in the context of the Ising model can be expressed as:</p> \[F = -k_ B T \ln(Z)\] <p>where $k_ B$ is the Boltzmann constant, $T$ is the temperature, and $Z$ is the partition function of the system, given by the sum over all possible configurations:</p> \[Z = \sum_ {\left\lbrace s \right\rbrace } e^{-\beta H(\left\lbrace s \right\rbrace )}\] <p>where $\beta = \frac{1}{k_ B T}$ and $H(\left\lbrace s \right\rbrace )$ is the Hamiltonian for a particular configuration $\left\lbrace s \right\rbrace$ of spins.</p> <p>The true ground state must minimize the free energy, the free energy is energy minus temperature times entropy, hence the competition between energy and entropy underlies the phase transition:</p> <ul> <li>At low temperatures, the system tends to minimize its energy, leading to an ordered phase where spins align (ferromagnetism).</li> <li>At high temperatures, the entropy dominates, favoring a disordered state where spins are randomly oriented.</li> </ul> <p>The balance between these two tendencies determines the critical temperature $T_ c$ at which the phase transition occurs.</p> <p>The critical dimension $d_ c$ of a system is the minimum dimension in which a phase transition can occur at a nonzero temperature. For the Ising model, it is known that $d_ c = 1$ for a lower critical dimension, meaning that there is no phase transition at any nonzero temperature in 1D. In 2D, however, the model does exhibit a phase transition, as shown by Onsager’s solution.</p> <p>Onsager’s exact solution for the 2D Ising model on a square lattice without an external field showed that the critical temperature $T_ c$ is given by:</p> \[\sinh\left(\frac{2J}{k_ B T_ c}\right) = 1\] <p>This solution demonstrated that a spontaneous magnetization (order) emerges below $T_ c$ and disappears above $T_ c$, marking a second-order phase transition characterized by a continuous change in the order parameter (magnetization).</p> <p>The calculation of the critical properties, such as the critical exponents that describe how physical quantities diverge near $T_ c$, is more intricate and relies on sophisticated mathematical techniques, including renormalization group analyses, which go beyond the scope of this note.</p> <hr/> <p>In the contest of quantum field theory, spontaneous symmetry breaking occurs when the most stable state (or states) of a system, namely its ground state or vacuum state, does not share the symmetry of the system’s Lagrangian (or Hamiltonian). This can happen even though the governing equations or Lagrangian of the system remain symmetric. The system “chooses” a state that breaks some of the symmetry of the Lagrangian.</p> <p>The Higgs mechanism is a well-known example of spontaneous symmetry breaking in QFT. The potential of the Higgs field is symmetric, resembling a “Mexican hat” or “sombrero,” but the lowest energy state (the ground state) is not at the symmetric center (where the field value is zero) but rather at some nonzero value along the “brim” of the hat. This asymmetric ground state breaks the symmetry of the potential.</p> <p>The “spontaneous” aspect is that there’s no external force or parameter explicitly breaking the symmetry; it’s the intrinsic dynamics of the field settling into a minimum energy state that breaks the symmetry. Spontaneous in the context of QFT is not in the dynamical sense, as it is in Ising model.</p> <h1 id="2-degenerate-vacua-good-and-bad">2. Degenerate Vacua, Good and Bad</h1> <p>In a simple quantum system, you might expect a unique ground state. However, in systems where spontaneous symmetry breaking occurs, there can be multiple degenerate ground states, all having the same energy but different physical configurations.</p> <p>All the vacua together form the manifold of vacuum of the Lagrangian, where the basis are just different vacua states, continuous or not. The thing is, there is more than one set of basis for the vacuum states. Let $\left\lvert{0,\alpha}\right\rangle$ be degenerate vacuum states label by some $\alpha$, then any linear combination, properly normalized, is another vacuum state, call it $\left\lvert{0,\beta}\right\rangle$, then we can use the <em>Gram-Schmidt procedure</em> to construct another set of orthonormal basis. Among the infinite sets of basis, there do exists good sets and bad ones. But before defining what is a good vacuum, we must first define what is a quasi-local operator.</p> <p>Recall that operators correspond to observable quantities or actions that can be performed on the field, and “quasilocal” operators are those whose effects are confined to a limited region of space. Take the scalar field theory for example, let $\phi(x)$ be field operator, they are local since it is defined on a point. A <em>quasilocal operator</em> $A$ is something can be constructed from $\phi$ via</p> \[A = \int d^{d}x \, f(x_ {1},\cdots,x_ {n}) \phi(x_ {1})\cdots\phi(x_ {n})\] <p>where $f(x_ {1},\cdots,x_ {n})$ is a function with finite support (support is the closure of the points where $f$ is nonzero).</p> <p>Now coming back to vacua. There is a basis where all the vacua are <em>globally independent</em>, where one vacua can not be transformed into another using some local (or quasi-local, as Coleman called it) operator. Such a basis is called <strong>good</strong> vacuum states. For a set of good vacua, different vacuum states are not just distinct but are fundamentally separate in the sense that you cannot use a simple, localized operation to move from one to another. By construction, the vev of any quasilocal operators between two distinct good vacua is zero,</p> <p><strong>Theroem 1</strong> There exists a basis for the vacuum states $\left\lvert{0,\alpha}\right\rangle$, so called good vacuum states, such that for any quasilocal operator $A$, we have</p> \[\left\langle{0,\alpha}\right\rvert A\left\lvert{0,\alpha'}\right\rangle =0.\] <p>We’ll neglect the proof of the theorem here, just mention that it involves translational invariance of vacuum states, causality, and some cluster decomposition-ish argument.</p> <p>The significance of the theorem is this: it doesn’t matter if you say there’s one vacuum or many; there are always good vacua. It shows that, even if we don’t know anything about spontaneous symmetry breaking, and we’ve chosen a bad set of vacua, by a systematic constructive procedure we can always find a good choice of bases for the vacuum subspace, such that no local operator can connect one vacuum to another.</p> <hr/> <p>In the following we will need to make use of quantum effective action $\Gamma[\varphi]$, for details on this topic see my other note <a href="https://www.mathlimbo.net/blog/2024/Coleman-Weinberg-Potential/">here</a>.</p> <p>As explained by Coleman,</p> <blockquote> <p>…(using perturbation theory), but with the effective action $\Gamma[\overline{\phi}]$ substituted for the classical action $S[\phi]$. Instead of trying to find minima by finding the stationary points of the classical action, I <strong>find ground states by looking at the stationary points of the effective action</strong>; instead of finding effective coupling constants and masses by expanding about the minima of the classical action, I <strong>find 1PI Green’s functions by expanding about the minima of the effective action</strong>. It’s exactly the same game in the quantum and classical theories.</p> </blockquote> <p>Sometimes Coleman talks about classical vev of $\phi$ and quantum vev of $\phi$. By his definition, the classical vev of $\phi$, i.e. $\left\langle \phi \right\rangle$ classical, usually denoted as just $\left\langle \phi \right\rangle$, is the vev of $\phi$ without loop corrections, the quantum one, usually denoted as $\overline{\phi}$, is that with loop corrections. In any cases, $\left\langle \phi \right\rangle$ is supposedly a constant in spacetime. This means that both classical and quantum vacuum preserves the translational symmetry.</p> <p>In solitonic quantum field theory we constantly deal with ground states that are not translations symmetric. Regarding the possibility of SSB of spontaneous translational symmetry breaking, Coleman comment</p> <blockquote> <p>There’s no reason why translation invariance should not be spontaneously broken in a theory that describes the real world. It occurs in statistical mechanics, for example, where the phenomenon is called <code class="language-plaintext highlighter-rouge">crystallization</code>. There, <em>instead of changing the square of the mass to cause the manifest symmetry to break spontaneously, one changes the temperature</em>. Let’s take a typical material such as iron, and imagine an iron universe, spatially infinite. If the temperature is above a certain point, the ground state (in the sense of statistical mechanics) is spatially homogeneous; it’s iron vapor. We lower the temperature below the freezing point of iron, and the ground state becomes an infinite iron crystal, which does not have spatial homogeneity. If we now consider the rotation of a crystal somewhere in the frozen iron, how it rotates depends on its position relative to a central lattice point. That’s an example of spontaneous symmetry breakdown of translational invariance.</p> </blockquote> <p>Note that <strong>spontaneous symmetry breaking does not affect the renormalization</strong>, SSB is essentially a shift of field, has nothing to do with regularization and renormalization. A SSB model has exactly the same counter terms as the original one. Thus we could renormalize first and shift the field later. This is useful in $\phi^{4}$ model with spontaneous symmetry breaking, since the original theory has no $\phi^{3}$ terms but the symmetry broken theory has, so naturally one could ask, do we need a counter term for $\phi^{3}$ interaction, like Mark Srednicki did in his textbook? The answer is no since such a counter term is not needed in the original theory. Then what about the divergence introduced by $\phi^{3}$ theory? Well, if we had done everything correctly, it should be taken care of by itself, by some destined cancellation. Like Coleman said,</p> <blockquote> <p>After we do the shift, of course, a $\phi^{3}$ interaction will appear in the effective action, but we still don’t need a $\phi^{3}$ counterterm, because we’ve already gotten rid of all infinities in computing $\Gamma$ before we’ve made the shift. The shift is a purely algebraic operation without a single integration over internal momenta, and therefore cannot possibly introduce new ultraviolet infinities.</p> </blockquote> <p>Say we are doing renormalization in the original theory. What are the renormalization conditions? We have MS, $\overline{\text{MS}}$, Pauli-Villas, mass-shell, lattice, Momentum Subtraction, etc. It is not a good idea to adopt the mass shell renormalization since in the original theory $m^{2}&lt;0$. Then we could choose a tentative renormalization condition, then adjust it in the kink sector so that the renormalized parameters are physical. The vev of field $\overline{\phi}$ will depend on the renormalization condition, but different condition will describe the same physics.</p> <p>Recall that the effective action $\Gamma[\overline{\phi}]$ is made of 1PI diagrams, where $\overline{\phi}(x)$ itself serves as the external legs, just like the source term $J(x)$ with classical action $S$. We have assumed the $\overline{\phi}$ is a const in spacetime, which translates to zero momentum after we Fourier transform it to the momentum space. So we only need to consider the case where the external legs has zero momenta. Next let’s dive into calculation.</p> <hr/> <h1 id="3-effective-action">3. Effective Action</h1> <p>Let $V(\overline{\phi})$ be the <code class="language-plaintext highlighter-rouge">effective action</code> of $\overline{\phi}$, which if you recall is the quantum vev of $\phi$. For the details of effective action see my other note mentioned at the beginning of this note, here we only present the definition,</p> \[\Gamma[\overline{\phi}] =: -V(\overline{\phi}) \cdot \text{Vol}^{d},\] <p>where $\text{Vol}^{d}$ is the total space of the $d$-dimensional spacetime manifold.</p> <p>The connection between $\Gamma[\overline{\phi}]$ and $S[\overline{\phi}]$ is that, at tree level, that is if you forget about quantum corrections, $\Gamma$ is equal go $S$. When the quantum corrections are included, since $\Gamma$ is exact (incorporates all the quantum effects in path integral) at tree level, we have</p> \[Z[J] = \lim_ { \hbar \to 0 } \int \mathcal{D}\phi \, \exp \left\lbrace \frac{i}{\hbar}\left( \Gamma[\phi]+\int J\phi \right) \right\rbrace = \exp \left\lbrace i\Gamma[\overline{\phi}]+\int dJ\overline{\phi} \, \right\rbrace ,\] <p>it is understood that the last equality is up to multiplicative constants. Also note that in the last expression it is not just any $\phi$, but $\overline{\phi}$ that satisfies certain functional equation, which is exactly the $\overline{\phi}$ we have been using.</p> <p>However we also have</p> \[Z[J] = \int \mathcal{D}\phi \, \exp \left\lbrace \frac{i}{\hbar}\left( S[\phi] +\int J\phi \right) \right\rbrace = \exp \left\lbrace iS[\left\langle \phi \right\rangle ] + \text{loops} \right\rbrace\] <p>where loops are a result of the fluctuations about the classical field configuration $\left\langle \phi \right\rangle$. Thus we have</p> \[\Gamma[\overline{\phi}] = S[\left\langle \phi \right\rangle] + \text{loops}= S[\overline{\phi}] + \text{loops},\] <p>since a swap between $\overline{\phi}$ and $\left\langle \phi \right\rangle$ introduces loop corrections only. But so far let’s stick with $S[\left\langle \phi \right\rangle]$ instead of $S[\overline{\phi}]$, since $\left\langle \phi \right\rangle$ is the quantum vev thus unknown, while $\left\langle \phi \right\rangle$ is the classical vev thus easily known, one just need to solve for the equation of motion.</p> <p>Write the action in terms of the Lagrangian we have</p> \[\Gamma[\overline{\phi}] = \int d^{d}x \, \mathcal{L}(\left\langle \phi \right\rangle ) + \text{loops}.\] <p>We have</p> \[\mathcal{L} = \frac{1}{2} (\partial \phi)^{2} - U(\phi) + \mathcal{L}_ {\text{ct}},\] <p>where $U$ is the classical potential and $\mathcal{L}_ {\text{ct}}$ are the counter terms. Then, since $\overline{\phi}$ is constant we have</p> \[V(\overline{\phi}) = U(\left\langle \phi \right\rangle ) + \text{loops},\] <p>where loop contribution is of form $\hbar(\text{1-loops})+h^{2}(\text{2-loops})+\cdots$. Note that $V(\overline{\phi})$ is not a functional but rather a function of $\overline{\phi}$, it is the negative of the constant density of $\Gamma[\overline{\phi}]$. Since $\Gamma$ generates all the 1PI diagrams, $V(\overline{\phi})$ represents the collection of all the 1PI diagrams with all the external momenta equal to zero and with the $(2\pi)^{d}\delta^{d}(0)$ from overall energy-momentum conservation divided out. That’s just the Fourier space equivalent of the integral $\int d^dx$.</p> <p>As Coleman explained in his lecture notes,</p> <blockquote> <p>The rule for computing $V[\overline{\phi}]$ is very simple. You don’t have to worry about any external momentum. You just have external lines each carrying zero momentum. Sum up all those graphs to one loop or two loops or however many loops you’re going to do.</p> </blockquote> <h2 id="31-calculating-the-1-loop-effective-potential">3.1. Calculating the 1-Loop Effective Potential</h2> <p>With a general renormalizable potential, we start with the Lagrangian that reads</p> \[\mathcal{L} = \frac{1}{2} (\partial \phi)^{2} -U(\phi)+\mathcal{L}_ {\text{ct}}.\] <p>If there is a mass term it goes to $U$. As a result, the propagator is</p> \[\frac{i}{k^{2}+i\epsilon}.\] <p>Recall that calculating $\Gamma[\overline{\phi}]$ means calculating the 1PI diagrams with external legs amputated and replaced by a factor of $\overline{\phi}$. Why are they amputated? We know that when calculating the S-matrix the external legs are also truncated due to the LSZ theorem, but here there is no LSZ theorem so why does it happen? The reason is mostly that 1PI diagrams are used to represent the interaction vertices or the “effective vertices” in the theory, rather than full scattering processes. In the context of effective theory you usually don’t hear things like asymptotic states, scattering matrix, things that you must discuss when talking about S-matrix. The 1PI diagrams contribute to the n-point functions, which are essentially the building blocks of the full scattering amplitudes. These vertex functions describe how particles interact at a point, disregarding the propagation of particles to and from this point. In the renormalization process, 1PI diagrams are essential because they contain the divergences that need to be renormalized. The external propagators do not need to be renormalized in the same way, so they are not included in the 1PI diagrams. The renormalization of the theory focuses on the interactions themselves, which are represented by the 1PI diagrams without the external propagators. For details please refer to note <a href="https://www.mathlimbo.net/blog/2024/Coleman-Weinberg-Potential/">here</a>, at the paragraph before Eq. (15).</p> <p>From the functional Taylor expansion of $i\Gamma[\varphi]$ in momentum space,</p> \[\begin{align*} i\Gamma[\varphi] &amp;= \sum_ {n} \frac{1}{n!} \int \frac{dk_ {1}^{d}}{(2\pi)^{d}} \cdots \frac{dk_ {n}^{d}}{(2\pi)^{d}}\, \tilde{\varphi}(-k_ {1})\cdots \tilde{\varphi}(-k_ {n} )\tilde{\Gamma}^{(n)}(k_ {1},\cdots, k_ {n} )\\ &amp;\;\;\;\; \times (2\pi)^{d}\delta^{d}(k_ {1}+\cdots +k_ {n} ), \end{align*}\] <p>we can expand $\tilde{\Gamma}^{(n)}(k_ {1},\cdots,k_ {n})$ in terms of $k$’s, while keeping the $\tilde{\varphi}$’s untouched, the reason why we don’t expand $\tilde{\varphi}$ is purely technical, because an expansion in $\Gamma^{(n)}$ suffices. If we do that, at the leading order where $k=0$ for all $k$, then we get</p> \[i\Gamma[\varphi] = \sum_ {n} \frac{1}{n!} \int d^{d}x \, \tilde{\Gamma}^{(n)}(0,\cdots ,0) \varphi^{n}(x) + \mathcal{O}(k).\] <p>Note that $\varphi(x)$ are functions of $x$, not its Fourier transformed $\tilde{\varphi}$, which is a result from <em>not</em> expanding $\tilde{\varphi}(k)$ in $k$.</p> <p>By the definition of effective action</p> \[\Gamma[\varphi] = -\int d^{d}x \, V_ {\text{eff}}(\overline{\phi}) + \mathcal{O}(\text{derivatives}),\] <p>we have</p> \[\boxed { V_ {\text{eff}}(\overline{\phi}) = i\sum_ {n} \frac{1}{n!} \int d^{d}x \, \tilde{\Gamma}^{(n)}(0,\cdots ,0) \overline{\phi}^{n}(x) . }\] <p>note we have replaced $\varphi$, a generic classical field to $\overline{\phi}$, the vacuum solution we want to expand about. It doesn’t change the logic or the derivation, just replacing the general case with a special case.</p> <p>In $\phi^{4}$ model, at tree level, we have</p> \[\Gamma^{(2)} (0,0) = -i \mu^{2},\quad \Gamma^{(4)}(0,0,0,0) = -i \lambda,\] <p>where $\mu^{2}$ is the mass of the particle and $\lambda$ the coupling. Of course it depends on how you write the Lagrangian, could differ a factor of $3!$ or something like that.</p> <p>To include the 1-loop corrections, we need to consider 1PI diagrams shown in Fig. (1).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/img/effectiveAction_1Loop-480.webp 480w,/img/effectiveAction_1Loop-800.webp 800w,/img/effectiveAction_1Loop-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/img/effectiveAction_1Loop.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig.1. One loop contributions to the effective action $\Gamma[\varphi]$ in the $\phi^4$ model. The first diagram contributes to $\Gamma^{2}$, the second $\Gamma^{4}$, etc. Diagrams with odd number of external legs vanish due to the $\mathbb{Z}_2$ symmetry. Credit: Professor Gustavo Burdman. </div> <p>The $2n$-point amplitude shown in Fig. 1 read</p> \[\Gamma^{(2n)} (0,\cdots,0) = \frac{(2n)!}{2^{n}(2n)} \int \frac{d^{d}k}{(2\pi)^{d}} \, \left( \frac{i}{k^{2}+i\epsilon}(-i\lambda) \right)^{n}.\] <p>Some explanation is in order: for $2n$ external legs, there will be $n$ propagators with the same momentum $k$ (since the external legs all carry zero momentum by construction), and there are $n$ vertices. $2^{n}$ is the symmetry factor for each vertex (for the two external legs), $2n$ in the denominator is the symmetry factor for the entire loop, for $n$ rotations and $n$ reflections. There are $(2n)!$ different ways to assign $x_ {1},\cdots,x_ {2n}$ (in coordinate representation of course) to the $2n$ “bulbs”, hence the factor $(2n)!$. If you are confused about this, just think of what we did with $iW[J]$ when using it to generate connected diagrams, where for a diagram with $n$ external legs, there are $n!$ ways to assign it. For more details refer to Chapter 13 in <em>lectures of Sidney Coleman on quantum field theory</em>, here I just quote a short passage from Coleman that is relevant to our discussion:</p> <blockquote> <p>If we imagine restricting ourselves to the case where the first $\rho$ gives up momentum $k_ {1}$, the second gives a momentum $k_ {2}$, etc., then all of our lines are well-defined, and we have no factor of $1/n!$. On the other hand, when we integrate over all $k$’s in this expression, we overcount each those terms $n!$ times, corresponding to the $n!$ permutations of a given set of $k$’s, and therefore we need a $1/n!$ to cancel it out. I know combinatoric arguments are often not clear the first time you hear them, but after a little thought, they become clear.</p> </blockquote> <p>Coleman also emphasized that here we are <strong>not</strong> normal ordering anything. Normal ordering tend to cause some problems, including 1) it is not compatible with gauge transform, 2) it is not compatible with field shift and 3) it sometimes messes up the symmetry.</p> <p>Also note that even we are talking about 1-loop only, there is arbitrary high power of coupling $\lambda$. This is different from what I am used to in calculating loops, where higher power of coupling usually implies higher number of loops. This is not a problem though.</p> <p>We can go beyond $\phi^{4}$ model to a general polynomial potential $U(\phi)$ (recall that $V$ is preserved for quantum potential), then each vertex on the circle would contribute $-iU’’(\phi)$, where prime means the derivative w.r.t. $\phi$ field. Then for loops with $n$ vertices as shown in Fig.1 we have</p> \[\text{circle with }n \text{ vertices} = \frac{1}{(2n)} \int \frac{d^{d}k}{(2\pi)^{d}} \, \left( \frac{U''(\overline{\phi})}{k^{2}+i\epsilon} \right)^{n}.\] <p>To obtain $\Gamma[\overline{\phi}]$ we just need to sum them up together, we have</p> \[\text{loop correction} = \sum_ {n} \frac{i}{2n} \int \frac{d^{d}k}{(2\pi)^{d}} \, \left( \frac{U''(\overline{\phi})}{k^{2}+i\epsilon} \right)^{n}\] <p>which is the Taylor expansion of $-\ln(1-\bullet)$, if we forget about convergence for now,</p> \[\sum_ {n} \frac{i}{2n} \int \frac{d^{d}k}{(2\pi)^{d}} \, \left( \frac{U''(\overline{\phi})}{k^{2}+i\epsilon} \right)^{n} = - \frac{i}{2} \int \frac{d^{d}k}{(2\pi)^{d}} \, \ln\left( 1-\frac{U''(\overline{\phi})}{k^{2}+i\epsilon} \right),\] <p>to proceed we need to go to Euclidean space by performing Wick rotation,</p> \[\text{loops} = \frac{1}{2} \int \frac{d^{d}k_ {E}}{(2\pi)^{d}} \, \ln\left( 1-\frac{U''(\overline{\phi})}{-k_ {E}^{2}+i\epsilon} \right),\] <p>where $d^{d}k_ {E} = dk_ {E} k^{d-1}_ {E } \Omega_ {d-1}$ for spherically symmetric functions, $\Omega_ {d-1}$ is the area of unit $(d-1)$-sphere. We are only interested in what depends on $\overline{\phi}$, not the constant part (w.r.t $\overline{\phi}$), infinite or not.</p> <p>Cut off $k_ {E}$ at $\Lambda$ and discard some additive infinite stuff which we can absorb into the normalization, we get</p> \[\begin{align*} \text{loops} &amp;= \frac{1}{2} \int^{\Lambda}_ {0} \frac{d^{d}k_ {E}}{(2\pi)^{d}} \, \ln\left( 1 +\frac{U''(\overline{\phi})}{k_ {E}^{2}-i\epsilon} \right) \\ &amp;= \frac{1}{2} \int_ {0}^{\Lambda} \frac{d^{d}k_ {E}}{(2\pi)^{d}} \, \ln\left( \frac{k_ {E}^{2}+U''(\overline{\phi})-i\epsilon}{k_ {E}^{2}-i\epsilon} \right) \\ &amp;= \frac{1}{2} \int_ {0}^{\Lambda} \frac{d^{d}k_ {E}}{(2\pi)^{d}} \, \ln(k_ {E}^{2}+U''(\overline{\phi})-i\epsilon) + \text{Const} \\ &amp;= \frac{1}{2(2\pi)^{d}} \int_ {0}^{\Lambda} d^{d}k_ {E} \, k_ {E}^{d-1} \Omega_ {d-1} \ln(k_ {E}^{2}+U''(\overline{\phi})-i\epsilon) \end{align*}\] <p>where in the last step we have carelessly thrown away the constant term. Since</p> \[\Omega_ {d-1} = \frac{2\pi^{d/2}}{\Gamma\left( \frac{d}{2} \right)}, \quad \Gamma\left( \frac{1}{2} \right) = \sqrt{ \pi } ,\] <p>we have</p> \[\text{loops} = \frac{1}{2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)} \int_ {0}^{\Lambda} d k_ {E} \, k_ {E}^{d-1} \ln(k_ {E}^{2}+U''(\overline{\phi})-i\epsilon).\] <p>For now let’s assume $U’’(\overline{\phi})$ is a positive constant, and neglect $-i\epsilon$, for whenever we need it we can always put it after $U’’$. Define two dimensionless new variables to replace $k_ {E}$ and $U’’$:</p> \[\boxed{ u := \frac{k_ {E}}{\Lambda},\quad t := \frac{U''(\overline{\phi})}{\Lambda^{2}}, }\] <p>note that since $U’’(\overline{\phi})$ is supposed to be a constant, <strong>$t$ goes to zero at the $\Lambda\to \infty$ limit, making it possible for as to expand in powers of it</strong>. We have</p> \[\begin{align*} \text{loops} &amp;= \frac{\Lambda^{d}}{2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)} \int_ {0}^{1} d u \, u^{d-1} [2\ln \Lambda+\ln(u^{2}+t)] \\ &amp;= \frac{\Lambda^{d}}{2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)} \left[ \frac{2}{d} \ln \Lambda+\int_ {0}^{1} du \, u^{d-1} \ln(u^{2}+t) \right]. \end{align*}\] <p>Regarding the integral in the line line, if we perform an integral by part first then use Mathematica code</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">Integrate</span><span class="p">[</span><span class="nv">u</span><span class="o">^</span><span class="p">(</span><span class="nv">d</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nv">u</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">t</span><span class="p">)</span><span class="w"> </span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">u</span><span class="o">,</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="m">1</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nb">Assumptions</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">{</span><span class="nv">t</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="o">,</span><span class="w"> </span><span class="nv">t</span><span class="w"> </span><span class="nv">\[Element]</span><span class="w"> </span><span class="nb">Reals</span><span class="o">,</span><span class="w"> </span><span class="nv">d</span><span class="w"> </span><span class="nv">\[Element]</span><span class="w"> </span><span class="nb">Integers</span><span class="o">,</span><span class="w"> </span><span class="nv">d</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="p">}]</span><span class="w">
</span></code></pre></div></div> <p>then we get an expression with Gaussian hypergeometric function\brace</p> \[\text{loops} = \frac{\Lambda^{d}}{d\,2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)} \left\lbrace 2 \ln \Lambda + \left[ \ln(1+t)-2 \times \left( _ {2}F_ {1}\left( 1,\frac{2+d}{2}, \frac{4+d}{2},- \frac{1}{t} \right) \right) \right] \right\rbrace.\] <p>So ugly. So I decided to just put the whole integral into Mathematica, I got instead</p> \[\boxed{ \text{loops} = \frac{\Lambda^{d}}{d\,2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)}\left[ 2\ln \Lambda+\ln(1+t)- \frac{1}{t} \Phi\left( -\frac{1}{t},1,1+d/2\right) \right], }\] <p>where $\Phi(-1,1,1+d / 2)$ is the Lerch transcendent function, defined as:</p> \[\Phi(z, s, a) = \sum_ {n=0}^{\infty} \frac{z^n}{(n+a)^s}\] <p>where $z$ and $a$ are complex numbers, and $s$ is a complex parameter. The function is defined for $\left\lvert z \right\rvert &lt; 1$ or $\left\lvert z \right\rvert = 1$ with $\Re(a) &gt; 0$. Roughly speaking,</p> <ul> <li>$z$ is the value at which the series is evaluated.</li> <li>$s$ is a parameter that controls the power in the denominator.</li> <li>$a$ is a shift parameter in the denominator, which affects the starting point of the summation.</li> </ul> <p>After we fix a dimension $d$, we would have fixed all two parameters of Lerch function, then we can expand $\Phi\left( -\frac{1}{t},1,1+\frac{d}{2} \right)$ at $-\infty$. It certainly feels weird but if you have experience with resurgence theory, it wouldn’t be your first time to expand something at infinity. One way to make you more comfortable with expanding at infinity is to consider the complex plane, compactify all the points at infinity to get a Riemann surface, then infinity becomes just another point on the sphere, and expanding about it is no less natural than expanding about any point.</p> <hr/> <p><strong>Special case at $d=4$:</strong></p> <p>In this case we have</p> \[\begin{align*} u &amp;:= \frac{k_ {E}}{\Lambda},\quad t := \frac{U''(\overline{\phi})}{\Lambda^{2}}, \\ \text{loops} &amp;= \frac{\Lambda^{d}}{d\,2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)}\left[ 2\ln \Lambda+\ln(1+t)- \frac{1}{t} \Phi\left( -\frac{1}{t},1,3\right) \right]. \end{align*}\] <p>Let $x:= 1 / t$ and the last term concerning Lerch function becomes</p> \[- x\, \Phi\left( - x,1,3 \right),\quad x \to \infty.\] <p>I like to drag the minus sign wherever the term goes, harder to make mistakes with a minus sign this way. Using Mathematica command to expand it at infinity</p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">Series</span><span class="p">[</span><span class="nb">LerchPhi</span><span class="p">[</span><span class="o">-</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="m">3</span><span class="p">]</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="nv">x</span><span class="o">,</span><span class="w"> </span><span class="nv">\[Infinity]</span><span class="o">,</span><span class="w"> </span><span class="m">8</span><span class="p">}]</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nb">Normal</span><span class="w">
</span></code></pre></div></div> <p>we get</p> \[- x\, \Phi\left( - x,1,3 \right)= - \frac{1}{2} + \frac{1}{x} - \frac{\ln(x)}{x^{2}} - \frac{1}{x^{3}}+\mathcal{O}(x^{-4}),\] <p>insert this into the expression of loops we have</p> \[\text{loops} = \frac{\Lambda^{d}}{d\,2^{d} \pi^{d/2}\Gamma\left( \frac{d}{2} \right)}\left[ \ln(1+t) + t +t^{2} \ln t \right],\] <p>where $d=4$ and $d 2^{d}\pi^{d/2} \Gamma(d / 2)=64\pi^{2}$. We have discarded surely irrelevant terms such as $\frac{1}{2}$, $2\ln \Lambda$ (they will be absorbed into the renormalization factor), and wrote $x$ in $t$. Next expand</p> \[\ln(1+t) = t - \frac{t^{2}}{2} + \frac{t^{3}}{3} + \mathcal{O}(t^{4})\] <p>we get</p> \[\begin{align*} \text{loops} &amp;= \frac{\Lambda^{4}}{64\pi^{2}}\left[ \ln(1+t) + t +t^{2} \ln t \right] \\ &amp;= \frac{\Lambda^{4}}{64\pi^{2}}\left[ t-\frac{t^{2}}{2} + t +t^{2} \ln t \right] \\ &amp;= \frac{\Lambda^{4}}{64\pi^{2}}\left[ 2t + t^{2}\left( \ln t-\frac{1}{2} \right)\right] \\ &amp;=\frac{1}{64\pi^{2}}\left[ 2\Lambda^{2} U''(\overline{\phi}) + (U''(\overline{\phi}))^{2} \left( \ln \frac{U''}{\Lambda^{2}} -\frac{1}{2} \right)\right]. \end{align*}\] <p>The last result agrees with Coleman’s 1-loop correction, which is shown in Eq. (44.51) in <em>Lectures of Sidney Coleman on Quantum Field Theory</em>, page 976.</p> <p>For a 4D scalar theory, renormalizability strongly constraints the powers you could have on the interactions, it can be no more than $4$, otherwise some terms, such as $(U’’)^{2} \ln \Lambda^{2}$ will generate divergent terms proportional to $(\phi’’^{5})^{2}\sim \phi^{6}$, with no counter terms in the Lagrangian to absorb it, hence non-renormalizable. Coleman remarked in his lecture that</p> <blockquote> <p>Non-renormalizable theories are sick no matter how you look at them; they’re no healthier from this vantage point.</p> </blockquote> <p>However this is the old-fashioned point of view, Wilson will have something more to say on that. But that’s not for this note, here we proceed in the old-fashioned way, to continue to renormalized $\phi^{4}$ theory in 4D.</p> <p>Including the counter terms, the effective potential reads</p> \[V(\overline{\phi}) = U(\overline{\phi})+U_ {ct}(\overline{\phi}) + \frac{1}{64\pi^{2}}(U''(\overline{\phi}))^{2}\ln[U''(\overline{\phi})]+(\Lambda\text{-dependent}),\] <p>where $U_ {ct}$ include all the counter terms and we have written all the divergent terms as $\Lambda$-dependent. After using $U_ {ct}$ to counter the divergence, there could still be some finite part of $U_ {ct}$ left, depending on the renormalization condition. Also, to be mathematically strict, the parameter of $\ln$ function should be dimensionless, while $U’’$ is not. Anyway, this can be easily fixed by introducing another parameter $\mu$ with dimension of mass, and write $\ln(U’’ / \mu^{2})$, this $\mu$ will be fixed once the renormalization condition is fixed.</p> <p>This formula (also with the generalization to $n$ different scalars) was first derived by Coleman and Weinberg: this Coleman and the other Weinberg, Erick Weinberg. Steve Weinberg refers to this work as “that paper with pseudo-Goldstone bosons and a pseudo-Weinberg.”</p> <h1 id="4-the-physical-meaning-of-the-effective-potential">4. The Physical Meaning of the Effective Potential</h1> <p>Roughly speaking, the quantum effective potential $V(\overline{\phi})$ is the quantum generalization of classical potential $U(\phi)$, and $\overline{\phi}$ is the quantum generalization of the $\phi_ {c}$ (or $\left\langle \phi \right\rangle$ as Coleman used), the classical solution to the equation of motion. They agree at tree level and quantum correction kicks in at higher loops, as well the divergences, hence counter terms. Mathematically, $\left\langle \phi \right\rangle$ is the stationary point of the classical action, $\delta S / \delta \phi=0$ at $\left\langle \phi \right\rangle$; $\overline{\phi}$ is the same thing with quantum corrections, meaning it is the expectation value of $\phi$ with quantum corrections, it is the stationary point of the quantum effective action $\Gamma$, $\delta \Gamma / \delta \phi=0$ at $\overline{\phi}$. Physically, $V(\overline{\phi})$ gives the energy density of the vacuum in which expectation value of $\phi$ is $\overline{\phi}$.</p> <p>When $V(\overline{\phi})$ has two local minima, things become interesting. Take the tilted double well for example, there are two local minima but the one with higher energy density is a false vacuum. If we start in the false vacuum, we expect to quantum tunnel (barrier penetration) into the true vacuum eventually. This tunneling phenomenon is non-perturbative.</p> <p>What we are interested in is finding the vacuum state of a QFT model, then it is helpful to look into a similar situation in quantum mechanics. Let’s say we have some Hamiltonian $H$ with interaction, and we want to find the vacuum state $\left\lvert{\psi}\right\rangle$ that minimizes $\left\langle{\psi}\right\rvert H\left\lvert{\psi}\right\rangle$. We further want the the state to be corrected normalized, namely $\left\langle \psi \middle\vert \psi \right\rangle=1$. But that’s not enough, since in the case of QFT we require that the vev of $\phi$ is some certain value, denoted $\overline{\phi}$, here in the quantum mechanical example we also require the vev of some operator $A$ to be $\overline{A}$. To account for these two constraints, we use the Lagrangian multiplier method, defining the Lagrange function to be</p> \[L = \left\langle{\psi}\right\rvert H\left\lvert \psi\right\rangle-E\left\langle \psi \middle\vert \psi \right\rangle -J\left\langle{\psi}\right\rvert A\left\lvert{\psi}\right\rangle ,\] <p>where $E$ and $J$ are Lagrange multipliers, and minimize it. We solve this variational problem with arbitrary $J$, and then eliminate $J$ from the problem to satisfy the constraint condition. We could define a function</p> \[\mathcal{W}(J) = \left\langle{\psi}\right\rvert -H+JA\left\lvert{\psi}\right\rangle ,\] <p>now we assume $\left\lvert{\psi}\right\rangle$ is the ground state with all the constraints. Or, equivalently, $\left\lvert{\psi}\right\rangle$ is the ground state of a modified Hamiltonian $H-JA$. This is similar to the field theoretic $S+\int J \phi$. Now we can replace $J$ by another variable using Legendre transform, define a new variable</p> \[\frac{d W(J)}{dJ} = \left\langle{\psi}\right\rvert A \left\lvert{\psi}\right\rangle = \overline{A}\] <p>and the Legendre transformed function</p> \[\Gamma := \mathcal{W}- J\overline{A},\] <p>what would this $\Gamma$ be? Turns out, it is just the negative energy,</p> \[\Gamma = \left\langle{\psi}\right\rvert -H \left\lvert{\psi}\right\rangle = -E.\] <p>Just like $\Gamma[\varphi]$ in QFT. The above quantum mechanical example is very similar to the field theoretical effective method of effective theory.</p> <p>As remarked by Coleman, the method of effective action may be unreliable under certain circumstances, as he explained in his lecture,</p> <blockquote> <p>…we may run into trouble if level crossing takes place. When the coupling constants are weak, another state that is not the ground state may come up and cross that energy level, and we may find ourselves following the wrong state as we sum up our Feynman graphs. If perturbation theory cannot tell us the true ground state energy, then we won’t get the true ground state energy for the constrained problem, either. On the other hand if perturbation theory serves to give the true ground state energy without constraint, it will also give us the true ground state energy with constraints.</p> </blockquote>]]></content><author><name>Baiyang Zhang</name></author><summary type="html"><![CDATA[1. Spontaneous symmetry breaking 2. Degenerate Vacua, Good and Bad 3. Effective Action 3.1. Calculating the 1-Loop Effective Potential 4. The Physical Meaning of the Effective Potential]]></summary></entry><entry><title type="html">Note on The Moral Foundations of Politics by Ian Shapiro</title><link href="https://baiyangzhang.github.io/blog/2025/Note-on-The-Moral-Foundations-of-Politics/" rel="alternate" type="text/html" title="Note on The Moral Foundations of Politics by Ian Shapiro"/><published>2025-01-02T00:00:00+00:00</published><updated>2025-01-02T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2025/Note-on-The-Moral-Foundations-of-Politics</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2025/Note-on-The-Moral-Foundations-of-Politics/"><![CDATA[<h1 id="enlightenment-politics">Enlightenment Politics</h1> <blockquote> <p>If there is a single overarching idea shared in common by adherents to different strands of Enlightenment thinking, it is faith in the power of human reason to understand the true nature of our circumstances and ourselves. Human improvement is measured by the yardstick of <strong>individual rights</strong> that embody, and protect, <strong>human freedom</strong>.</p> </blockquote> <blockquote> <p>Descartes announced that he was in search of propositions that are impossible to doubt. His famous example, known as the <code class="language-plaintext highlighter-rouge">cogito</code>, was ‘‘I think, therefore I am.’’</p> </blockquote> <blockquote> <p>Immanuel Kant defined in <code class="language-plaintext highlighter-rouge">The Critique of Pure Reason</code> (1781), of placing knowledge ‘‘on the secure path of a science.’’</p> </blockquote> <blockquote> <p>These developments in philosophy reflected and reinforced the emergence of modern scientific consciousness.</p> </blockquote> <p>Such ideas, as necessary conditions for the development of natural science (not merely technology), seems to never had appeared in China. Year 1781 is the year 乾隆四十六年 in China, one of the most closed, ignorant, and autocratic era in history.</p> <blockquote> <p>During the seventeenth and eighteenth centuries, when the hallmark of scientific knowledge was indubitable certainty, ethics, political philosophy, and the human sciences were regarded as superior to the natural sciences. This view seems strange from the vantage point of the twenty-first century, when fields like physics, chemistry, astronomy, geology, and biology have all advanced with astonishing speed to discoveries that would have been unimaginable in the eighteenth century.</p> </blockquote> <h2 id="the-workmanship-ideal-of-knowledge">The Workmanship Ideal of Knowledge</h2> <blockquote> <p>The first distinctive feature of the early Enlightenment concerns the range of <code class="language-plaintext highlighter-rouge">a priori knowledge</code>, the kind of knowledge that either follows from definitions or is otherwise deduced from covering principals. This is the kind of knowledge Descartes had in mind when he formulated his cogito and that Kant located in the realm of ‘‘analytic judgments.’</p> </blockquote> <p><strong>Epistemology</strong> is a branch of philosophy that studies the nature, origin, and limits of human knowledge. The term comes from the Greek words “episteme,” meaning knowledge or understanding, and “logos,” meaning study or discourse. Epistemology addresses questions such as:</p> <ul> <li>What is knowledge?</li> <li>How is knowledge acquired?</li> <li>What do people know?</li> <li>How do we know what we know?</li> <li>What are the limits of human knowledge?</li> <li>What makes beliefs justified or rational?</li> </ul> <p>In exploring these questions, epistemology deals with the definition of knowledge and its scope and limits. It often involves debating between different theories of knowledge, such as empiricism (the idea that knowledge comes primarily from sensory experience), rationalism (the idea that reason is the main source of knowledge), and constructivism (the idea that knowledge is constructed by individuals through their interactions with the world).</p> <p>Immanuel Kant distinguished between two types of judgments: <code class="language-plaintext highlighter-rouge">analytic</code> and <code class="language-plaintext highlighter-rouge">synthetic</code>. These distinctions are central to his philosophy, especially in his work “Critique of Pure Reason.”</p> <ol> <li> <p><strong>Analytic Judgments</strong>: An analytic judgment is one where the predicate (the part of the sentence that says something about the subject) is contained within the subject itself. The truth of an analytic judgment is derived from the meanings of the words involved and logical reasoning. They are tautological in nature and do not add any new information about the world. For example, the statement “All bachelors are unmarried” is analytic because the predicate “unmarried” is part of the definition of the subject “bachelor.”</p> </li> <li> <p><strong>Synthetic Judgments</strong>: A synthetic judgment, on the other hand, is one where the predicate adds something to the subject that is not contained within it. The truth of a synthetic judgment is determined through how our concepts relate to the world and cannot be known just by understanding the meanings of the words. They require empirical investigation or intuition. For instance, “The cat is on the mat” is a synthetic judgment because the concept of “the cat” does not inherently include the concept of “being on the mat.”</p> </li> </ol> <p>Kant’s distinction between analytic and synthetic judgments is fundamental to his epistemology, particularly in addressing the question of how human beings can have knowledge about the world. He further introduced the concept of “synthetic a priori” judgments, which are synthetic judgments that are known independently of experience (a priori), like mathematical truths.</p> <hr/> <p>The <code class="language-plaintext highlighter-rouge">creationist</code> or <code class="language-plaintext highlighter-rouge">workmanship</code> theory in political science, often associated with the work of John Locke, is a theory of political obligation. It suggests that political authority and legitimacy derive from the consent of the governed, likening the role of the government or ruler to that of a craftsman or creator who constructs a system with the consent and for the benefit of the people.</p> <p>This theory is rooted in the idea that political and social structures are artificial constructs, made by human beings, unlike natural phenomena. The “creationist” aspect implies that political structures are deliberately created or constructed, rather than organically evolved. The “workmanship” aspect emphasizes the idea that the creators or rulers of these structures have a responsibility to the people they govern, similar to how a craftsman is responsible for the quality and function of their creation.</p> <p>Locke’s theory was revolutionary at its time because it challenged the prevailing notion of the divine right of kings, suggesting instead that political authority is justified only when it serves the interests of the governed and respects their rights. This theory laid the groundwork for modern concepts of democracy, individual rights, and the social contract.</p> <hr/> <p>Thomas Hobbes and John Locke, two prominent philosophers, had distinct views on natural law, reflecting their differing perspectives on human nature and the ideal structure of society.</p> <p>Hobbes, in his work “Leviathan,” presented a rather pessimistic view of human nature. He believed that in the state of nature (a hypothetical condition without government or laws), humans are driven by self-interest and a desire for self-preservation, leading to a “war of all against all” (bellum omnium contra omnes). In this state, life would be “solitary, poor, nasty, brutish, and short.”</p> <p>For Hobbes, natural law is a set of precepts or general rules, <em>discovered by reason</em>, which prohibit anything destructive to one’s own life. <em>It’s based on the right of every individual to preserve their own life</em>, leading to the conclusion that humans should seek peace. This is where his famous concept of the social contract comes into play: individuals surrender some of their freedoms and submit to the authority of a ruler (or a ruling assembly) to ensure their own safety and peace. Thus, Hobbes’s natural law is fundamentally about self-preservation and the avoidance of harm to others as a means of securing one’s own safety.</p> <p>Locke’s view, as articulated in “Two Treatises of Government,” is more optimistic about human nature. He believed that in the state of nature, humans live in a state of equality and freedom, not inherently prone to violence or war. For Locke, the <em>law of nature is a moral guide based on the belief that God has given the world to all people in common</em>. It teaches that, since all are equal and independent, no one ought to harm another in their life, health, liberty, or possessions.</p> <p>Locke’s natural law is grounded in the rights to life, liberty, and property. It includes the idea that people have the obligation to respect the rights of others. His social contract theory suggests that people form governments to protect these natural rights. If a government fails to do so, citizens have the right to overthrow it. This view laid the groundwork for modern democracy and significantly influenced the development of political philosophy in the Western world.</p> <p>So, Hobbes saw natural law as a means of avoiding the brutal state of nature through self-preservation and peace, whereas Locke viewed natural law as a moral guide ensuring equality and the inherent rights of life, liberty, and property.</p> <hr/> <blockquote> <p>A basic issue for Locke and many of his contemporaries was the ontological status of natural law and in particular its relation to God’s will.</p> </blockquote> <p>In this sentence, “ontological status” refers to the fundamental nature or essence of natural law, especially in relation to its existence and its relationship to God’s will. Ontology, in philosophy, is the study of being or existence, and it deals with questions concerning what entities exist or can be said to exist, and how such entities can be grouped, related within a hierarchy, and subdivided according to similarities and differences.</p> <p>So, when discussing the “ontological status of natural law” in the context of John Locke and his contemporaries, the focus is on understanding the very essence of natural law: whether it exists as an objective reality independent of human beings, how it relates to or derives from God’s will, and what its fundamental characteristics are. This was a central topic in the philosophical and theological debates of that era, particularly in the context of determining the basis and legitimacy of moral and legal principles. Locke and many others were engaged in trying to understand whether natural laws were inherent aspects of the universe, ordained by God, or whether they were constructs of human reason and society.</p> <p>“Will-centered” refers to the philosophical position known as voluntarism. This is a theory that emphasizes the role of the will, either divine or human, in various philosophical contexts. In the context of Locke’s moral and political writings, being “will-centered” or a voluntarist means that Locke ultimately leaned towards the view that natural law and moral principles are determined by the will, particularly the will of God, rather than being inherent or objective truths that exist independently of any will.</p> <p>In Locke’s time, the debate about the nature of natural law often centered around whether natural laws were intrinsic to the universe (a position known as intellectualism or rationalism) or whether they were decrees of God’s will (voluntarism). A will-centered or voluntarist approach suggests that moral and legal norms derive their authority from an act of will, particularly the divine will, rather than from reason alone or from the inherent nature of reality. In this view, what is right or wrong, just or unjust, is so because God wills it to be that way, and human beings understand and follow these laws through revelation, religious teachings, or other means of discerning God’s will.</p> <blockquote> <p>Locke distinguished “ectype”’ from “archetype” ideas: ectypes are general ideas of substances, and archetypes are ideas constructed by man.</p> </blockquote> <p>John Locke’s distinction between “ectype” and “archetype” ideas is a crucial aspect of his epistemological theory, which he discusses in his work “An Essay Concerning Human Understanding.” This distinction is part of his broader inquiry into the nature of human knowledge and understanding.</p> <p>In Locke’s philosophy, archetypes are the original models or patterns from which copies are made. They are the fundamental, primary ideas that exist in the mind of God or, in a more secular interpretation, the perfect, abstract forms of things. When Locke refers to archetypes as ideas constructed by man, he means that these are the ideal standards or criteria we hold in our minds for categorizing and understanding the world. They represent our understanding of what the essential characteristics of a particular thing are.</p> <p>For instance, the archetype of a tree would be the idealized concept or mental representation of what a tree is supposed to be. This archetype is not derived from any particular tree but is a kind of composite or abstracted idea of “treeness” that we use to recognize and categorize individual trees.</p> <p>Ectypes, on the other hand, are derivative or secondary ideas. They are the imperfect copies or generalizations that we derive from our experience with individual instances in the world. Ectype ideas are more about the general ideas of substances we form based on our sensory experiences and observations. When we see many individual trees, for example, we form a general idea of what a tree is - this is an ectype. It’s a more practical, experiential idea based on the aggregation of real-world instances.</p> <p>In summary, Locke’s distinction between archetype and ectype ideas can be understood as a differentiation between the idealized, abstract concepts we hold in our minds as standards (archetypes) and the more practical, general ideas we form based on our sensory experience of the world (ectypes). Archetypes are about the essence or ideal form of things, while ectypes are about the general, often imperfect, concepts we derive from actual experiences.</p> <h2 id="the-preoccupation-with-certainty">The Preoccupation with Certainty</h2> <blockquote> <p>The post-Humean Enlightenment tradition has been marked by a fallibilist view of knowledge. All knowledge claims are fallible on this account, and science advances not by making knowledge more certain but by producing more knowledge. Recognizing the corrigibility of all knowledge claims and the possibility that one might always be wrong exemplifies the modern scientific attitude. As Karl Popper (1902-1994) noted, the most that we can say, when hypotheses survive empirical tests, is that they have not been falsified so that we can accept them provisionally.</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">Value judgments</code> are statements or opinions that express an evaluation, typically of something’s worth, beauty, goodness, or morality. Examples include statements like “Lying is wrong,” or “This painting is beautiful.” A.J. Ayer was a key figure in the logical positivist movement, which held that for a statement to be meaningful, it must be either empirically verifiable (i.e., testable by observation or experiment) or analytically true (true by definition, like mathematical or logical statements). In logical positivism, a <code class="language-plaintext highlighter-rouge">proposition</code> is a statement that can be either true or false. It’s a claim about the world that can, <em>at least in principle</em>, be tested and verified or falsified.</p> <p>The Logical Positivist movement, also known as Logical Empiricism, was a philosophical movement that emerged in the early 20th century. It primarily revolved around a group of philosophers associated with the Vienna Circle (<code class="language-plaintext highlighter-rouge">Moritz Schlick</code>, <code class="language-plaintext highlighter-rouge">Hans Hahn</code>, ), along with others like A.J. Ayer in Britain. This movement sought to apply the rigor of scientific methodology to philosophy, with a significant focus on the analysis of language and the verification of statements.</p> <p>Key Features of Logical Positivism include</p> <ol> <li> <p><strong>Verification Principle</strong>: The central tenet of Logical Positivism is the verification principle. This principle asserts that a statement is only meaningful if it can be empirically verified or is analytically true (true by virtue of its meaning, like “All bachelors are unmarried”). The idea was to eliminate metaphysical and abstract discussions that couldn’t be supported by empirical evidence or logical reasoning.</p> </li> <li> <p><strong>Empiricism and Science</strong>: Logical Positivists emphasized the importance of empirical evidence and scientific methods in acquiring knowledge. They viewed science as the model for all true knowledge.</p> </li> <li> <p><strong>Rejection of Metaphysics</strong>: They were critical of metaphysics and other traditional philosophical endeavors, which they saw as meaningless since such statements couldn’t be empirically verified. They believed that many philosophical problems arose from misunderstandings of language and could be resolved by clarifying the language used.</p> </li> <li> <p><strong>Language and Meaning</strong>: A significant focus was placed on the analysis of language, particularly the language of science. They aimed to clarify how language is used in scientific theories and to distinguish between meaningful and meaningless statements.</p> </li> <li> <p><strong>Influence of Wittgenstein</strong>: Although not officially part of the Vienna Circle, Ludwig Wittgenstein’s early work, especially his “Tractatus Logico-Philosophicus,” significantly influenced Logical Positivism. Wittgenstein argued that <em>much of philosophy consists of nonsensical propositions and that the role of philosophy should be to clarify thought and language</em>.</p> </li> <li> <p><strong>Ethical and Aesthetic Statements</strong>: Logical Positivists generally considered ethical and aesthetic statements to be expressions of emotions or subjective preferences, rather than statements that could be true or false.</p> </li> </ol> <p>The “positivism” component is linked to the movement’s commitment to a scientific and empirical approach to knowledge. Positivism, as a philosophical stance, argues that knowledge should be based on positive, observable facts and their logical and mathematical treatment. It rejects introspection and intuition as sources of knowledge and instead emphasizes empirical evidence obtained through observation and experimentation. Logical Positivists extended this approach by asserting that statements must be empirically verifiable (or analytically true) to be meaningful.</p> <hr/> <p>Somewhat to my surprise, Karl Popper is not a member of the Vienna circle even though they shared many intellectual engagements. Furthermore, Karl Popper is even critically oppositional. The Vienna Circle advocated for the verification principle, which held that a statement is meaningful only if it can be empirically <em>verified</em>. Popper challenged this view, proposing <em>falsificationism</em> instead. According to Popper, scientific theories cannot be conclusively verified but can be falsified. He argued that a theory is scientific if it is testable and can potentially be refuted by evidence. This approach places a greater emphasis on the role of empirical refutation rather than verification.</p> <p>Also, Popper was critical of what he called <code class="language-plaintext highlighter-rouge">historicism</code> – the belief that <em>history unfolds according to deterministic laws or principles</em>. He argued that such theories, which <em>were often used to justify authoritarian regimes</em>, are fundamentally flawed. He believed that historicism led to totalitarianism because it promoted the idea that certain individuals or groups had access to inevitable truths about societal development, thus justifying their absolute rule. Popper advocated for what he termed an <code class="language-plaintext highlighter-rouge">open society</code>. An open society, in his view, is characterized by a democratic government, individual freedoms, and a critical attitude towards tradition and authority. It allows for change and improvement through rational and critical discourse, as opposed to the unquestioning acceptance of dogmatic principles.</p> <p>Just as Popper applied the <em>principle of falsifiability</em> to scientific theories, he suggested that political policies should also be subjected to critical scrutiny and should be alterable in the face of new evidence or arguments. He was wary of any political theory or system that claimed to have absolute or final answers.</p> <hr/> <p>According to Ayer, the expression of a value judgment is not a proposition since it can not be judged by right and wrong, the question of truth or falsehood does not here arise.</p> <p>Regarding ethics, Ayer points out that many theorists in ethics tend to treat statements about the causes and characteristics of our ethical feelings as if these statements were definitions of ethical concepts. For example, a theory might claim that an action is good if it promotes happiness. Here, the cause of the ethical feeling (happiness) is used to define the ethical concept (good). Ayer argues that ethical concepts are <em>pseudo-concepts</em>, since ethical concepts, in his view, is neither empirically verifiable or analytically correct.</p> <p>Ayer’s stance is closely associated with <code class="language-plaintext highlighter-rouge">emotivism</code>, a meta-ethical view that suggests <em>ethical statements do not assert propositions but express emotional attitudes</em>. According to emotivism, saying “Stealing is wrong” is akin to expressing one’s disapproval of stealing, rather than making an objective claim about the nature of stealing.</p> <h2 id="the-centrality-of-individual-rights">The Centrality of Individual Rights</h2> <blockquote> <p>In addition to faith in science, the Enlightenment’s central focus on individual rights differentiates its political philosophy from the ancient and medieval commitments to order and hierarchy. This focus brings the freedom of the individual to the center of arguments about politics. This move was signaled in the natural law tradition by a shift in emphasis from the logic of law to the idea of natural right.</p> </blockquote> <p>Hobbes contended that it was customary to conflate “Jus and Lex, law and right”. Yet he made the distinction that right, consisted in liberty to do, or to forbeare, whereas law, determines and binds to one of them. Similarly by Locke.</p> <p>John Locke’s oppinion on natural law is as the following. In his work <em>Essays on the Law of Nature</em>, Locke argues a moral law inherent in the world and discoverable through reason.</p> <p>Key points of Locke’s argument include:</p> <ol> <li> <p><strong>Natural Law and Reason:</strong> Locke posits that natural law is an aspect of the natural world, similar to physical laws. According to him, this <em>moral law can be discovered through the use of reason, without the need for divine revelation</em>.</p> </li> <li> <p><strong>Moral Obligations:</strong> He argues that <em>natural law imposes moral obligations on individuals</em>. These moral principles are universal and apply to all people, regardless of their culture or society.</p> </li> <li> <p><strong>Rights and Duties:</strong> Locke’s view of natural law is closely tied to his ideas about individual rights and duties. He believes that natural law forms the basis for understanding human rights, especially the right to life, liberty, and property.</p> </li> <li> <p><strong>Foundation for Political Theory:</strong> These essays lay the groundwork for Locke’s later political theories, particularly those presented in his famous works, “Two Treatises of Government.” He uses the concept of natural law to argue for the rights of individuals and the limitations of governmental power.</p> </li> <li> <p><strong>Human Equality:</strong> Locke emphasizes the inherent equality of all human beings, derived from their natural state. This idea is a critical aspect of his argument against absolute monarchy and for the formation of governments based on the consent of the governed.</p> </li> <li> <p><strong>Religious Tolerance:</strong> Although not as explicitly developed in these essays as in his later works, Locke’s concept of natural law also leads to his advocacy for religious tolerance, seeing religious belief as a matter of individual conscience.</p> </li> </ol> <p>In summary, Locke’s “Essays on the Law of Nature” propose that there is a moral law inherent in the natural world, understandable through human reason, and that this law underpins human rights and forms the basis for just and ethical governance.</p> <hr/> <p>John Locke’s <code class="language-plaintext highlighter-rouge">voluntarist theology</code> reflects his views on the nature of God and the relationship between divine will and moral law. The emphasis is on <em>the will will (voluntas in Latin, hence the name) of God of God as the primary or sole source of moral law</em>. Locke’s voluntarism posits that <em>moral laws are decrees of God’s will</em>. In this view, what is morally right or wrong is so because God wills it, and not necessarily because it aligns with any intrinsic moral truths or rational principles independent of God’s will. Locke emphasizes the <em>absolute freedom</em> and <em>omnipotence</em> of God. He argues that God’s will is not bound by any external standards or principles. Therefore, moral laws are a product of God’s free choice.</p> <p>While Locke is a proponent of reason and believes that human beings can discover moral truths through rational inquiry, he also upholds the importance of divine revelation. In his voluntarist theology, revelation plays a crucial role in imparting knowledge of God’s will, which might not be entirely accessible through reason alone. Locke’s voluntarism is tied to his rejection of innate ideas, a concept he famously critiques in his “Essay Concerning Human Understanding.” He argues against the notion that <em>moral principles are innately known</em>, instead positing that our understanding of moral laws comes from experience, reason, and revelation. Locke’s voluntarist approach suggests that moral obligations are ultimately grounded in obedience to God’s will. This perspective can lead to a form of ethical subjectivism, where moral truths depend on the decrees of a divine authority.</p> <hr/> <blockquote> <p>In Locke’s formulation, natural law dictates that man is subject to divine imperatives to live in certain ways, but, within the limits set by the law of nature, men can act in a godlike fashion. Man as maker has a maker’s knowledge of his intentional actions, and a natural right to dominion over man’s products. … Provided we do not violate natural law, we stand in the same relation to the objects we create as God stands to us; we own them just as he owns us.</p> </blockquote> <h2 id="tensions-between-science-and-individual-rights">Tensions Between Science and Individual Rights</h2> <p>The two enlightenment values, the preoccupation of science and the commitment to individual rights, seem to be in contradiction with each other. Science is deterministic, concerned with discovering the laws that govern the universe, with human being included. This has potential for conflict with an ethic that emphasizes individual freedom, for now the freedom has to be subjugated to the laws (of nature, of God).</p> <p>In Locke’s theory, the freedom to comprehend natural law by one’s own lights supplied the basis of Locke’s right to resist, which could be invoked against the sovereign. No one is in a higher position to monopolize the right to interpret the scripture.</p> <blockquote> <p>We will see this tension surface repeatedly in the utilitarian, Marxist, and social contract traditions, without ever being fully resolved.</p> </blockquote> <h1 id="classical-utilitarianism">Classical Utilitarianism</h1> <p>Jeremy Bentham famously wrote that</p> <blockquote> <p>Nature has placed mankind under the governance of two sovereign masters, <em>pain</em> and <em>pleasure</em>. It is for them alone to point out what we ought to do, as well as to determine what we shall do. On the one hand the standard of right and wrong, on the other the chain of causes and effects, are fastened to their throne. They govern us in all we do, in all we say, in all we think: every effort we can make to throw off our subjection, will serve but to demonstrate and confirm it. In words a man may pretend to abjure their empire: but in reality he will remain subject to it all the while. The principle of utility recognizes this subjection, and assumes it for the foundation of that system, the object of which is to rear the fabric of felicity by the hands of reason and law. Systems which attempt to question it, deal in sounds instead of senses, in caprice instead of reason, in darkness instead of light.</p> </blockquote> <p>Some regimes indeed deals “in sounds instead of senses, in caprice instead of reason,” yet as long as they get only one thing right, as long as they supress the alternative, their reign will continue.</p> <p>The <code class="language-plaintext highlighter-rouge">principle of utility</code>, as Bentham explains, “approves or disapproves of every action whatsoever, according to the tendency which it appears to have to augment or diminish the happiness of the party whose interest is in question: or, what is the same thing in other words, to promote or to oppose that happiness.”</p> <blockquote> <p>A century later Marx and Engels would write of a utopian order in which politics could be replaced by administration. Bentham believed that it could be done in eighteenth-century England.</p> </blockquote> <p>Funny enough, Marx thought very little of Jeremy Benthem. Marx wrote, in <em>Das Capita</em>, that Bentham was a “panegyric of bourgeois society,” and “With the driest naiveté he (Bentham) takes the modern shopkeeper, especially the English shopkeeper, as the normal man. Whatever is useful to this queer normal man, and to his world, is absolutely useful. This yard-measure, then, he applies to past, present, and future. The Christian religion, for example, is ‘useful,’ ‘because it forbids in the name of religion the same faults which the penal code condemns in the name of the law.’ Artistic criticism is ‘harmful,’ because it disturbs worthy people in their enjoyment of Martin Tupper, etc.”</p> <p>Bentham’s happiness principal, when applied to governments, requires us to maximize the greatest happiness of the greatest number in the community.</p> <p>Bentham defended an extensive system of political rights, but he saw rights as human artifacts, created by the legal system and enforced by the sovereign. He insisted that there are no rights without enforcement and no enforcement without government, a blunt statement of the view that would subsequently become known as legal positivism.</p> <h2 id="individual-versus-collective-utility-and-the-need-for-government">Individual Versus Collective Utility and the Need for Government</h2> <p>In Bentham’s <em>Principles of the Civil Code</em>, he wrote</p> <blockquote> <p>Law does not say to man, Work and I will reward you but it says: Labour, and by stopping the hand that would take them from you, I will ensure to you the fruits of your labour – its natural and sufficient reward, which without me you cannot preserve. If industry creates, it is law which preserves; if at the first moment we owe everything to labour, at the second, and every succeeding moment, we owe everything to law.</p> </blockquote> <p>Law should limit itself to ensuring that people can pursue utility for themselves.</p> <p>The necessity for having a government comes down to two things.</p> <ol> <li>Selfish behavior can be self-defeating. The problem of funding the provision of public goods is one of a class of market failures, where the market’s invisible hand leads to sub-optimal outcomes for all concerned.</li> </ol> <h1 id="synthesizing-rights-and-utility">Synthesizing rights and Utility</h1> <p>Contemptuous though he generally was of <code class="language-plaintext highlighter-rouge">normative inquiry</code>, for the most part the Pareto of the Manual saw it as superfluous.</p> <p>Pareto’s denial of the possibility of interpersonal comparisons had the effect of importing a powerful doctrine of individual autonomy into the core logic of utilitarianism.</p> <hr/> <p>Although Pareto’s is not a normative theory, it nonetheless has normative implications. These derive from the pivotal role it ascribes to free individual choice as embodied in and expressed through market transactions.</p> <p>The core notion here is that of an indifference curve. Indifference means exactly what it says: someone is indifferent between two goods if exchanging one for the other would neither increase nor decrease his or her utility.</p> <hr/> <p>John Stuart Mill.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="politics"/><summary type="html"><![CDATA[Enlightenment Politics]]></summary></entry><entry><title type="html">Constructing a Finite Tension Domain Wall in 4D Part III</title><link href="https://baiyangzhang.github.io/blog/2024/Constructing-a-Finite-Tension-Domain-Wall-in-4D-Part-III/" rel="alternate" type="text/html" title="Constructing a Finite Tension Domain Wall in 4D Part III"/><published>2024-12-25T00:00:00+00:00</published><updated>2024-12-25T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Constructing-a-Finite-Tension-Domain-Wall-in-4D-Part-III</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Constructing-a-Finite-Tension-Domain-Wall-in-4D-Part-III/"><![CDATA[<h1 id="divergence-cancellation-in-kink-sector">Divergence cancellation in kink sector</h1> <p>We want to calculate the continuum contribution to the tension, which is Eq. (4.42) in the long version of the note:</p> \[\begin{align*} \rho_ {1}^{A} =&amp; -\int \frac{dk_ {1}}{2\pi} \frac{dp_ {1}}{2\pi} \, \frac{9\pi^{2}p_ {1}^{2}}{(m^{2}+k_ {1}^{2})(m^{2}+4k_ {1}^{2})}\text{csch}^{2}\left( \frac{\pi(p_ {1}-k_ {1})}{m} \right) \\ &amp;\times \int \frac{dk_ {2}dk_ {3}}{(2\pi)^{2}} \, \frac{(\omega _ {k} -\omega_ {p_ {1}k_ {2}k_ {3}})^{2}}{\omega_ {p_ {1}k_ {2}k_ {3}}}, \end{align*}\] <p>where in the last line $\omega _ {k} = \omega_ {k_ {1}k_ {2}k_ {3}}=\sqrt{k_ {1}^{2}+k_ {2}^{2}+k_ {3}^{2}+m^{2}}$, and $\omega_ {p_ {1}k_ {2}k_ {3}}=\sqrt{p_ {1}^{2}+k_ {2}^{2}+k_ {3}^{2}+m^{2}}$.</p> <p>We want to integrate the second line first, call it $I$:</p> \[I = \int \frac{dk_ {2}dk_ {3}}{(2\pi)^{2}} \, \frac{(\omega _ {k} -\omega_ {p_ {1}k_ {2}k_ {3}})^{2}}{\omega_ {p_ {1}k_ {2}k_ {3}}},\] <p>Using the formula</p> \[\int_ {-\infty}^{\infty} dk \, \frac{(\sqrt{a+k^{2}}-\sqrt{b+k^{2}})^{2}}{\sqrt{b+k^{2}}} = -a+b+a\ln\left( \frac{a}{b} \right),\] <p>we can integrate $k_ {3}$ out first,</p> \[I = \int_ {-\infty}^{\infty} \frac{dk_ {2}}{(2\pi)^{2}} \, [(-a+a\ln a)-(-b+a\ln b)].\] <p>where</p> \[a = k_ {1}^{2}+k_ {2}^{2}+m^{2}, \quad b = p_ {1}^{2}+k_ {2}^{2}+m^{2}.\] <p>Define</p> \[a := \alpha+k_ {2}^{2}, \quad b = \beta+k_ {2}^{2},\] <p>and in order to study the behavior at infinite momenta we insert a cutoff $\Lambda$, define:</p> \[\begin{align*} f_ {\Lambda}(k_ {1}):=&amp; \frac{1}{2\pi^{2}}\int_ {0}^{\infty} dk_ {2} \, [-\alpha-k_ {2}^{2}+(\alpha+k_ {2}^{2}\ln(\alpha+k_ {2}^{2}))], \\ f_ {\Lambda}(p_ {1}):=&amp; \frac{1}{2\pi^{2}}\int_ {0}^{\infty} dk_ {2} \, [-\beta-k_ {2}^{2}+(\alpha+k_ {2}^{2}\ln(\beta+k_ {2}^{2}))]. \end{align*}\] <p>so that</p> \[I = f_ {\Lambda}(k_ {1}) - f_ {\Lambda}(p_ {1}).\] <p>We can do the integrals analytically now. Note that when simplifying the integrated result, we need to expand $\ln(\alpha+\Lambda^{2})$ to $\ln \Lambda^{2}+\alpha / \Lambda^{2}-\alpha^{2} / 2\Lambda^{4}$. In the end we have</p> \[\begin{align*} 2\pi^{2}f_ {\Lambda}(k_ {1}) =&amp; \frac{2}{3} \pi \alpha ^{3/2}+\alpha \Lambda \log \left(\Lambda ^2\right)-2 \alpha \Lambda -\frac{5 \Lambda ^3}{9}+\frac{1}{3} \Lambda ^3 \log \left(\Lambda ^2\right), \\ 2\pi^{2}f_ {\Lambda}(p_ {1}) =&amp; \pi \alpha \sqrt{\beta }+\alpha \Lambda \log \left(\Lambda ^2\right)-2 \alpha \Lambda -\frac{5 \Lambda ^3}{9} -\frac{1}{3} \pi \beta ^{3/2}+\frac{1}{3} \Lambda ^3 \log \left(\Lambda ^2\right). \end{align*}\] <p>miraculously the divergence, namely the terms containing $\Lambda$, all cancels out. Thus</p> \[I = f_ {\Lambda}(k_ {1}) - f_ {\Lambda}(p_ {1}) = \frac{\alpha ^{3/2}}{3 \pi }-\frac{\alpha \sqrt{\beta }}{2 \pi }+\frac{\beta ^{3/2}}{6 \pi }.\] <p>The original integral now reads</p> \[\begin{align*} \rho_ {1}^{A} =&amp; -\int \frac{dk_ {1}}{2\pi} \frac{dp_ {1}}{2\pi} \, \frac{9\pi^{2}p_ {1}^{2}}{(m^{2}+k_ {1}^{2})(m^{2}+4k_ {1}^{2})}\text{csch}^{2}\left( \frac{\pi(p_ {1}-k_ {1})}{m} \right) \\ &amp;\times \left( \frac{(k_ {1}^{2}+m^{2})^{3/2}}{3\pi}- \frac{(k_ {1}^{2}+m^{2})\sqrt{p_ {1}^{2}+m^{2}}}{2\pi}+ \frac{(p_ {1}^{2}+m^{2})^{3/2}}{6\pi} \right) \\ =&amp; - \frac{3\pi}{2} \int \frac{dk_ {1}}{(2\pi)} \frac{dp_ {1}}{(2\pi)} \, \frac{p_ {1}^{2}\sqrt{k_ {1}^{2}+m^{2}}}{(m^{2}+4k_ {1}^{2})}\text{csch}^{2}\left( \frac{\pi}{m}(p_ {1}-k_ {1}) \right)\\ &amp;\times \left( 2-3\sqrt{\frac{p_ {1}^{2}+m^{2}}{k_ {1}+m^{2}}}+\left( \frac{p_ {1}^{2}+m^{2}}{k_ {1}^{2}+m^{2}} \right)^{3/2} \right) \end{align*}\] <h1 id="renormalization-with-quantum-action">Renormalization with Quantum Action</h1> <p>The three point function is given by</p> \[\begin{align*} \left\langle \Omega \right\rvert \mathcal{T} \phi(\vec{x}_ {1},E_ {1})\phi(\vec{x}_ {2},E_ {2})\phi(0) \left\lvert \Omega \right\rangle =&amp; -i \int \frac{dE_ {1}dE_ {2}}{(2\pi)^{2}} \frac{d^{d}p_ {1}d^{d}p_ {2}}{(2\pi)^{2d}}\, \frac{e^{ -i(E_ {1}t_ {1}+E_ {2}t_ {2}+\vec{x}_ {1}\cdot \vec{p}_ {1}+\vec{x}_ {2}\cdot \vec{p}_ {2}) }}{(E_ {1}^{2}-\omega_ {1}^{2}+i\epsilon)(E_ {2}^{2}-\omega_ {2}^{2}+i\epsilon)} \\ &amp;\times \frac{\Gamma(\vec{p}_ {1},E_ {1},\vec{p}_ {2},E_ {2})}{(E_ {1}+E_ {2})^{2}-\omega_ {1+2}^{2}+i\epsilon} \end{align*}\] <p>where $\omega_ {1}:=\omega_ {\vec{p}_ {1}}$, $\mathcal{T}$ stands for the time-ordered product. Does integrating over $E_ {1}$ gives us the time ordered product, similar to how we obtained the Feynman propagator? Or is it just the basis of Fourier transform? To see how it works, let’s assume that $t_ {1}&gt;t_ {2}&gt;0$. We can perform the integral over $E_ {1}$ using the contour method, and get</p> \[\left\langle \Omega \right\rvert \mathcal{T} \phi(\vec{x}_ {1},E_ {1})\phi(\vec{x}_ {2},E_ {2})\phi(0) \left\lvert \Omega \right\rangle = \text{I}+\text{II},\] <p>where</p> \[\begin{align*} \text{I} =&amp; - \int \frac{d^{d}p_ {1} d^{d}p_ {2}}{(2\pi)^{2d}} \, e^{ -i(\vec{p}_ {1}\cdot \vec{x}_ {1}+\vec{p}_ {2}\cdot \vec{x}_ {2}+\omega_ {1}t_ {1}) } \int \frac{dE_ {2}}{(2\pi)} \frac{e^{ -iE_ {2}t_ {2} }}{2\omega_ {1}} \\ &amp;\times \frac{\Gamma(E_ {1}=\omega_ {1})}{(E_ {2}+\omega_ {2}-i\epsilon)(E_ {2}-\omega_ {2}+i\epsilon)(E_ {2}+\omega_ {1}+\omega_ {1+2}-i\epsilon)(E_ {2}+\omega_ {1}-\omega_ {1+2})+i\epsilon}, \\ \text{II} =&amp; - \int \frac{d^{d}p_ {1} d^{d}p_ {2}}{(2\pi)^{2d}} \, e^{ -i(\vec{p}_ {1}\cdot \vec{x}_ {1}+\vec{p}_ {2}\cdot \vec{x}_ {2}+\omega_ {1+2}t_ {1}) } \int \frac{dE_ {2}}{2\pi} \frac{e^{ -i E_ {2}(t_ {2}-t_ {1}) }}{(E_ {2}+\omega_ {2})(E_ {2}-\omega_ {2})} \\ &amp;\times \frac{\Gamma(E_ {1}=\omega_ {1+2}-E_ {2})}{2(\omega_ {1+2}+E_ {2}-i\epsilon)(E_ {2}-\omega_ {1+2}+\omega_ {1}-i\epsilon)(E_ {2}-\omega_ {1+2}-\omega_ {1}+i\epsilon)}, \end{align*}\] <p>whenever needed, we can replace $\omega_ {1,2}$ with $\omega_ {1,2}-i\epsilon$. This is consistent with Mark Sredinickie’s convention that $m^{2}$ are to be replaced by $m^{2}-i\epsilon$ when necessary.</p> <p>Integrate $\text{I}$ with respect to $E_ {2}$ follow the lower semi-circle, we have</p> \[\begin{align*} \text{I} =&amp; i \int \frac{d^{d}p_ {1}d^{d}p_ {2}}{(2\pi)^{2d}} \, \frac{e^{ -i(\vec{p}_ {1}\cdot \vec{x}_ {1}+\vec{p}_ {2}\cdot \vec{x}_ {2}+\omega_ {1}t_ {1}) }}{2\omega_ {1}} \\ &amp; \times \left\lbrace \frac{e^{ -it_ {2}\omega_ {2} }\Gamma(E_ {1}=\omega_ {1},E_ {2}=\omega_ {2})}{2\omega_ {2}(\omega_ {1}+\omega_ {2}+\omega_ {1+2})(\omega_ {1}+\omega_ {2}-\omega_ {1+2})} \right. \\ &amp;+\left. \frac{e^{ -it_ {2}(\omega_ {1+2}-\omega_ {1})}\Gamma(E_ {1}=\omega_ {1},E_ {2}=\omega_ {1+2}-\omega_ {1})}{2\omega_ {1+2}(\omega_ {1+2}-\omega_ {1}+\omega_ {2})(\omega_ {1+2}-\omega_ {1}-\omega_ {2})} \right\rbrace \end{align*}\] <p>and</p> \[\begin{align*} \text{II} =&amp; -i \int \frac{d^{d}p_ {1}d^{d}p_ {2}}{(2\pi)^{2d}} \, e^{ -i(\vec{p}_ {1}\cdot \vec{x}1+\vec{p}_ {2}\cdot \vec{x}_ {2}+\omega_ {1+2}t_ {1}) } \\ &amp; \times \left\lbrace \frac{e^{ -i(t_ {1}-t_ {2})\omega_ {2} } \Gamma(E_ {1}=\omega_ {1+2}-\omega_ {2},E_ {2}=-\omega_ {2})}{4\omega_ {2}(\omega_ {1+2}-\omega_ {2})(-\omega_ {1+2}-\omega_ {2}+\omega_ {1})(\omega_ {1}+\omega_ {2}+\omega_ {1+2})} \right. \\ &amp;+ \frac{e^{ -i(t_ {1}-t_ {2})\omega_ {1+2} } \Gamma(E_ {1}=2\omega_ {1+2},E_ {2}=-\omega_ {1+2})}{2(\omega_ {1+2}-\omega_ {2})(\omega_ {1+2}+\omega_ {2})(2\omega_ {1+2}-\omega_ {1})(2\omega_ {1+2}+\omega_ {1})}\\ &amp;- \left. \frac{e^{ -i(t_ {1}-t_ {2})(\omega_ {1}-\omega_ {1+2}) }\Gamma(E_ {1}=\omega_ {1},E_ {2}=\omega_ {1+2}-\omega_ {1})}{4\omega_ {1}(\omega_ {1+2}-\omega_ {1}+\omega_ {2})(\omega_ {1+2}-\omega_ {1}-\omega_ {2})(2\omega_ {1+2}-\omega_ {1})} \right\rbrace . \end{align*}\] <p>the difference is that between $E’^{2}-\omega’^{2}-m^{2}+i\epsilon$ and $\omega_ {\vec{p}}^{2}$, I think</p> <p>\(\int d^{d}x \, \frac{\Delta+g}{(2\pi)^{d}}:\phi^{3}(\vec{x}):\)</p> <hr/> <p>Hi Jarah! In your note,</p> \[\text{(2.4)} = -6g \int \frac{d^{d}p_ {1}d^{d}p_ {2}}{(2\pi)^{2d}} \, e^{ i(\vec{x}_ {1}\cdot \vec{p}_ {1}+ \vec{x}_ {2}\cdot \vec{p}_ {2}) } J,\] <p>where</p> \[J = \int \frac{dE_ {1}dE_ {2}}{(2\pi)^{2}} \, e^{ -i(E_ {1}t_ {1}+E_ {2}t_ {2}) }[(E_ {1}^{2}-\omega_ {1}^{2}+i\epsilon)(E_ {2}^{2}-\omega_ {2}^{2}+i\epsilon)((E_ {1}+E_ {2})^{2}-\omega_ {1+2}^{2}+i\epsilon)] .\] <p>I use $\omega_ {1}$ as a short hand notation for $\omega_ {\vec{p}_ {1}}$.</p> <p><code class="language-plaintext highlighter-rouge">Question 1:</code> the factor of $g$ in $(-6g)$ was not in the paper?</p> \[J = \frac{1}{8\omega_ {1}\omega_ {2}\omega_ {1+2}}K,\] \[K = \int \frac{dE_ {1}dE_ {2}}{(2\pi)^{2}} \, e^{ -i(E_ {1}t_ {1}+E_ {2}t_ {2}) }[\cdots]\] <p>For integrate contour, assume $t_ {1}&gt;0$ and $t_ {2}&gt;0$, we can close the lower half plane. At last we get</p> \[J =\] \[\frac{e^{ -i\omega_ {p_ {1}}t_ {1} }}{\omega_ {p_ {1}}-\frac{i\epsilon}{2\omega_ {p_ {1}}}+E_ {2}-\omega_ {p_ {1}+p_ {2}}+\frac{i\epsilon}{2\omega_ {p_ {1}+p_ {2}}}}\] <p>My extra terms to Eq.(2.6):</p> \[\begin{align*} &amp; - \frac{3g}{4} \int \frac{d^{d}p_ {1}}{(2\pi)^{d}}\frac{d^{d}p_ {2}}{(2\pi)^{d}} \, \frac{e^{ i(\vec{p}_ {1}\cdot \vec{x}_ {1}+\vec{p}_ {2}\cdot \vec{x}_ {2}) }}{\omega_ {1}\omega_ {2}\omega_ {1+2}}\\ &amp;\times \left\lbrace \frac{e^{ -i(\omega_ {1}t_ {1}-\omega_ {1}t_ {2}+\omega_ {1+2}t_ {2}) }}{\omega_ {1}+\omega_ {2}-\omega_ {1+2}} + \frac{e^{ -i(\omega_ {1}t_ {1}-\omega_ {1}t_ {2}+\omega_ {1+2}t_ {2}) }}{\omega_ {1+2}+\omega_ {2}-\omega_ {1}} \right\rbrace , \end{align*}\] <p>it can be simplified to</p> \[\frac{3g}{2}\int \frac{d^{d}p_ {1}}{(2\pi)^{d}}\frac{d^{d}p_ {2}}{(2\pi)^{d}} \, \frac{e^{ i(\vec{p}_ {1}\cdot \vec{x}_ {1}+\vec{p}_ {2}\cdot \vec{x}_ {2}) } e^{ -i(\omega_ {1}(t_ {1}-t_ {2})+\omega_ {1+2}t_ {2}) }}{\omega_ {1}\omega_ {1+2}[(\omega_ {1+2}-\omega_ {1})^{2}-\omega_ {2}^{2}]},\] <p>Earlier today, by mistake I canceled them.</p> <h1 id="jarahs-note">Jarah’s note</h1> <p>Derivation of Eq.(2.6) from the LHS of Eq. (2.4).</p> <p>The cubic Hamiltonian in terms of ladder operators:</p> \[\begin{align*} H_ {3}^{(3)} =&amp; g\int \frac{d^{3}p}{(2\pi)^{3}} \, A^{\ddagger}_ {p_ {1}}A^{\ddagger}_ {p_ {2}}A^{\ddagger}_ {-p_ {1}-p_ {2}}, \\ H_ {3}^{(1)} =&amp; \frac{3g}{2} \int \frac{d^{3}p}{(2\pi)^{3}} \, A^{\ddagger}_ {p_ {1}}A^{\ddagger}_ {p_ {2}} \frac{A_ {p_ {1}+p_ {2}}}{\omega_ {p_ {1}+p_ {2}}}, \\ H_ {3}^{(-1)} =&amp; \frac{3g}{4} \int \frac{d^{3}p}{(2\pi)^{3}} \, A^{\ddagger}_ {p_ {1}+p_ {2}} \frac{A_ {p_ {1}}}{\omega_ {p_ {1}}} \frac{A_ {p_ {2}}}{\omega_ {p_ {2}}}, \\ H_ {3}^{(-3)} =&amp; \frac{g}{8} \int \frac{d^{3}p}{(2\pi)^{3}} \, \frac{A_ {p_ {1}}}{\omega_ {p_ {1}}} \frac{A_ {p_ {2}}}{\omega_ {p_ {2}}} \frac{A_ {-p_ {1}-p_ {2}}}{\omega_ {p_ {1}+p_ {2}}} \end{align*}\] <p>At $\mathcal{O}(g)$, we have</p> \[H_ {3}\left\lvert \Omega \right\rangle = g\int \frac{d^{3}p}{(2\pi)^{3}} \left\lvert \vec{p}_ {1},\vec{p}_ {2},-\vec{p}_ {1}-\vec{p}_ {2} \right\rangle\]]]></content><author><name>Baiyang Zhang</name></author><category term="kink"/><summary type="html"><![CDATA[Divergence cancellation in kink sector]]></summary></entry><entry><title type="html">Note on Coleman-Weinberg Potential</title><link href="https://baiyangzhang.github.io/blog/2024/Coleman-Weinberg-Potential/" rel="alternate" type="text/html" title="Note on Coleman-Weinberg Potential"/><published>2024-12-23T00:00:00+00:00</published><updated>2024-12-23T00:00:00+00:00</updated><id>https://baiyangzhang.github.io/blog/2024/Coleman-Weinberg-Potential</id><content type="html" xml:base="https://baiyangzhang.github.io/blog/2024/Coleman-Weinberg-Potential/"><![CDATA[<ul> <li><a href="#1-quantum-effective-action">1. Quantum Effective Action</a> <ul> <li><a href="#11-quantum-action">1.1. Quantum Action</a></li> <li><a href="#12-effective-potential">1.2. Effective Potential</a></li> </ul> </li> <li><a href="#2-coleman-weinberg-potential">2. Coleman-Weinberg Potential</a> <ul> <li><a href="#21-background-fields">2.1. Background Fields</a></li> <li><a href="#22-coleman-weinberg-potential">2.2. Coleman-Weinberg Potential</a></li> </ul> </li> </ul> <h1 id="1-quantum-effective-action">1. Quantum Effective Action</h1> <p>The goal of this chapter is to introduce the quantum action and quantum potential. There are (at least) two kinds of quantum actions, one is the <code class="language-plaintext highlighter-rouge">Wilsonian quantum action</code>, where the higher momentum modes are integrated out to focus on the low-energy, long-distance behavior of a system. It embodies the effective interactions of a quantum system <em>at a certain scale</em>, accounting for the influence of fluctuations at smaller scales. This potential is central to understanding how physical phenomena emerge at different length scales from the underlying quantum fields, particularly in the study of critical phenomena and phase transitions. Take a scalar field theory $\phi$ for example. The “integrating out” procedure is done using the path integral approach, it involves <em>separating the field into high and low energy parts</em>. To be more specific, the scalar field is split into $\phi = \phi_ {\text{low}} + \phi_ {\text{high}}$, where $\phi_ {\text{low}}$ contains modes below a certain energy scale $\Lambda$ and $\phi_ {\text{high}}$ contains modes above $\Lambda$. $\phi_ {\text{high}}$ can be thought of as a thin shell in the momentum space. The path integral over the full field is then re-written as a path integral over these two components of fields. The crucial step is to integrate out the high-energy modes $\phi_ {\text{high}}$. This can be done perturbatively by treating $\phi_ {\text{high}}$ as a different field from $\phi_ {\text{low}}$, using Feynman diagram techniques. The resulting effective action should only depends on the low-energy modes. The Wilsonian effective action captures the dynamics of the field <em>at energies below</em> $\Lambda$. It will typically have a form different from the original action, often with new interactions generated as a result of integrating out the high-energy modes. New scale could even emerge, as in the case of dimensional transmutation, which we will not talk too much about in this note.</p> <p>The other kind of effective action is the so-called <code class="language-plaintext highlighter-rouge">quantum effective actions</code> which is the Legendre transformation of the generating functional of connected diagrams. It is quite a mouthful and we will talk more about it in the next section. When the original action is replaced by the quantum effective action, the tree-level diagrams generated by it will contain <em>all the quantum corrections</em> of the full theory (i.e. the original theory). The quantum effective action has a useful property that it can give us the vacuum expectation value (VEV) of the field operator with quantum correction. As we will see, the <code class="language-plaintext highlighter-rouge">Coleman-Weinberg potential</code> is a special case of the effective action.</p> <p>As M. D. Schwartz put it:</p> <blockquote> <p>Generally speaking, the term effective action, denoted by $\Gamma$, generally refers to a functional of fields (like any action) defined to give the same Green’s functions and S-matrix elements as a given action $S$, which is often called the action for the full theory. We write $\Gamma=\int d^4x \, \mathcal{L}_ {\text{eff}}(x)$, where $\mathcal{L}_ {\text{eff}}$ is called the effective Lagrangian.</p> </blockquote> <hr/> <p>There are in general three ways to calculate the effective action, as listed in the following.</p> <ul> <li><strong>Matching</strong>. We require</li> </ul> \[\int \mathcal{D}\{\text{original dof}\} e^{iS} = \int \mathcal{D}\{\text{effective dof}\} e^{i\Gamma}\] <p>where dof stands for the <code class="language-plaintext highlighter-rouge">degrees of freedom</code>, the LHS is the original theory while the RHS is the effective theory, with different degree of freedom. For example, QCD (at high energy) has quark and gluon as the degrees of freedom, when the energy is lowered the degrees of freedom becomes color singlet particles. $\Gamma$ is the quantum action.</p> <ul> <li> <p><strong>Legendre transformation</strong>, as we will show shortly.</p> </li> <li> <p><strong>Background field method</strong>, that is to separate the field into a static non-propagating background field $\phi_ b$ and a dynamic propagating field $\tilde{\phi}$, the dynamic fields are the fluctuations around the background field. Integrating out the fluctuations (usually done perturbatively) leaves us the effective potential $\Gamma[\phi_ b]$,</p> </li> </ul> \[\int \mathcal{D} \phi e^{i S[\phi_ b + \tilde{\phi}]} = e^{i\Gamma[\phi_ b]}.\] <p>The background field method is also closely related to how we calculate quantum corrections to classical solitonic solutions, such as the quantum correction to kink mass.</p> <p>The first try, before all the three mentioned above, is usually an educated guess. Given that <strong>the effective action must possess the same symmetries as the original action</strong>, one can propose various <em>local terms</em> that fulfill this requirement. The coefficients of these terms are then adjusted based on experimental data. This method relies on symmetry considerations to guide the formulation of acceptable terms in the effective action.</p> <p>For the rest of the note, we will confine our discussion to $\phi^4$ model with real scalar fields.</p> <h2 id="11-quantum-action">1.1. Quantum Action</h2> <p>To keep the notation simple, consider a single real scalar field $\phi$, with possibly mass term and self interaction. The partition function (with source) reads</p> \[Z[J] = \int \mathcal{D} \phi e^{iS[\phi] + i\int \phi J},\] <p>where $S$ is the action, $J$ is the source, $\int \phi J$ is short for $\int_ {M} \phi(x) J(x)$. $\phi$ is integrated out hence $Z$ is a functional of $J$ only.</p> <p>From it we define the generating function $W[J]$ by</p> \[Z[J] = e^{iW[J]} \implies W[J] = -i \ln Z[J],\] <p>$W[J]$ is the summation of <em>connected diagrams</em> with source, connected roughly because if you consider all the diagrams, the disconnected but replica-forming (namely the disconnected diagrams formed by putting two or more replicas of the same diagrams together) diagrams can be arranged into forms of $\bullet^{n}/n!$, where $\bullet$ is (the expression of) some connected diagram. Then we can organized them into an exponential function $e^{ \bullet }$, now $\bullet$ contains information only about connected diagrams. For more details, please refer to Mark Srednicki’s text book on quantum field theory. To repeat, $W[J]$ generates <em>connected diagrams</em> only.</p> <hr/> <p>The expectation value of $\phi$ in the presence of a source $J$ is given by</p> \[\left\langle {\phi} \right\rangle_ J=\frac{1}{Z} \int \mathcal{D} \phi e^{iS[\phi] + i\int \phi J} \phi = \frac{\delta W[J]}{\delta J(x)}\equiv \varphi_ {J},\] <p>Note the difference between $\phi$ and the so-called <code class="language-plaintext highlighter-rouge">varphi</code> $\varphi$, the former is an operator while the latter is a classical field. The subscript $J$ in $\varphi_ {J}$ is to emphasize that the vev of $\phi$ depends on the source $J$. In comparison to classical mechanics of point particles, $W$ is like Lagrangian $L$, $J$ is like $\dot{q}$, and $\delta W / \delta J$ is like $\partial L / \partial \dot{q}$, which introduces a new variable.</p> <p>Since the generating functional $W[J]$ is a functional of $J$, we can perform Legendre transform to define a new functional in terms of $\delta W / \delta J =: \varphi_ {J}$. The result is the quantum action:</p> \[\boxed{ \Gamma[\varphi_ {J}] = W[J] - \int J\varphi_ {J} , }\] <p>which is indeed a functional of $\varphi_ {J}$ and not $J$, since it is independent of variation $\delta J$ of $J$, as the readers can verify. Again we have omitted the measure under the integral sign. Some direct calculation shows that</p> \[\frac{\delta\Gamma[\varphi_ {J}]}{\delta\varphi_ {J}(x)} = -J(x).\] <p>It is not supposed to be obvious, but the effective action $\Gamma$ is the generating functional for 1-particle irreducible (1PI) diagrams! The significance of 1PI diagrams is best explained by Coleman in his lecture note on QFT, which I quote:</p> <blockquote> <p>If we treat the 1PI graphs as giving us effective interaction vertices, then to find the full Green’s functions we only have to sum up tree graphs, never any loops, because all the loops have been stuffed inside the definition of the propagators and the 1PI graphs. This marvelous property of the 1PI graphs is important. Taking the 1PI graph generating functional for a quantum action enables us to turn the combinatorics of building up full Green’s functions from 1PI Green’s functions into an analytic statement, and we end up with the correct expressions for the full Green’s functions. We’re turning a topological statement of one-particle irreducibility into an analytic statement that we will find easy to handle.</p> </blockquote> <p>To see that $\Gamma[\varphi]$ indeed generates the 1PI diagrams, it is easiest (for myself) to inverse the chain of reasoning, first we define an effective action such that its tree level diagrams reproduces the quantum result (which is easier than it looks), then show that such constructed action satisfies the same equation as $\Gamma[\phi_ {J}]$, so they are the same (up to some insignificant constant, such as the normalization constant). We will proceed in this direction.</p> <p>For now, forget about $\varphi_ {J}$. Let’s starting from defining the an effective action $\Gamma[\phi_ {c}]$, which is a functional of of some classical field $\phi_ {c}$. The role of $\phi_ {c}$ in $\Gamma[\phi_ {c}]$ is the same as the role of $\phi$ in the classical action $S[\phi]$, where we usually don’t bother to emphasize that $\phi$ is classical rather than a quantum field, but here we do. $\Gamma[\phi_ {c}]$ is defined such that, if we treat $\Gamma[\phi_ {c}]$ as the classical action $S[\phi]$, substitute $\Gamma$ with $S$ in the path integral, and calculate $Z[J]$ (or equivalently $W[J]$), using <strong>only the tree diagrams</strong>, then we get exact $Z[J]$ with every bit of the quantum correction! At first glance, this might seem almost too convenient, making our calculations significantly simpler, too good to be true. However, there’s no shortcut to the truth; ultimately, we still need to buckle down and work through the loop diagrams. Essentially, the effective action is a clever reorganization of the contributions from these loop diagrams. Even though the effective action doesn’t simplify the calculations per se, it is still quite valuable to us since it provides a new perspective, serving as a powerful tool in quantum field theory, enabling the study of quantum phenomena with a formalism that extends the classical action to include quantum effects.</p> <p>So how should $\Gamma[\phi_ {c}]$ be constructed? For any function $\phi_ {c}$, the effective action $\Gamma[\phi_ {c}]$ has a functional Taylor expansion:</p> \[i\Gamma[\phi_ {c}] = \sum_ {n} \frac{1}{n!} \int d^{d}x_ {1}\cdots d^{d}x_ {n} \, \Gamma^{(n)} (x_ {1},\cdots ,x_ {n}) \phi_ {c}(x_ {1})\cdots \phi_ {c}(x_ {n}).\] <p>For example, if $\Gamma[\phi_ {c}] = \int \, \phi_ {c}^{2}$, then the only non-zero component in the functional Taylor expansion is $\Gamma^{(2)}(x_ {1},x_ {2}) = 2\delta^{d}(x_ {1}-x_ {2})$.</p> <p>When talking about Feynman diagrams, it is more convenient to go to momentum representation, hence we define a modified version of the Fourier transform of $\Gamma^{(n)}$, such that</p> \[\Gamma^{(n)}(x_ {1},\cdots x_ {n}) := \int \frac{dp_ {1}}{(2\pi)^{d}}\cdots \frac{dp_ {n}}{(2\pi)^{d}}\, \tilde{\Gamma}^{(n)} (p_ {1},\cdots ,p_ {n} ) (2\pi)^{d}\delta^{d}(p_ {1}+\cdots +p_ {n} ) .\] <p>This definition contains extra $\delta$-function for future convenience.</p> <p>Recall that the filed $\phi_ {c}(x_ {i})$ themselves in the action eventually becomes <strong>amputated</strong> external legs, amputated in the sense that no propagator is associated to it. All the information is contained in $\tilde{\Gamma}$! We can brutally stuff all the 1PI diagrams, including loop corrections from $S[\phi]$, into $\tilde{\Gamma}$ so that we only need to take into consider the tree diagrams of $\Gamma$. For example, we can draw all the 1PI diagrams with three external legs, calculate them, and define it to be $\tilde{\Gamma}^{(3)}$. If we regard $\Gamma[\phi_ {c}]$ as a machine that takes a function $\phi_ {c}$ as input and spits out a number, then $\tilde{\Gamma}$’s are like its components.</p> <p>To summarize, for $n\geq 2$, the $\tilde{\Gamma}^{(n)}$ are <strong>defined</strong> by the sum of 1PI diagrams with $n$ external legs. As usual the external legs are amputated. The external momenta need not be conserved, that point is taken care of by the $\delta$-function in the definition of $\tilde{\Gamma}$. For $n=2$ the case is slightly more complicated, we need to include a propagator into the definition, but the philosophy is the same.</p> <p>Next we combine the tree-level exactness of $\Gamma[\phi_ {c}]$ with another concept: loop expansion. Loop expansion is equivalent to both semi-classical expansion (expansion in $\hbar$) and perturbative expansion (expansion in coupling $g$), should the right $\hbar$-dependence be made. Sidney Coleman thinks that $\hbar$ expansion is rubbish for two reasons (that I know of), 1) $\hbar$ is dimensional, with dimension of energy multiplies time, therefore is not a good expanding parameter and 2) if we make $\hbar$ dimensionless like we did with natural units, we could always change the units such that $\hbar=1$. In loop expansion, the tree level diagrams dominates the partition function $Z[J]$ when $\hbar$ is small, and becomes exact at $\hbar\to 0$. I am tempted to write</p> \[\text{tree diagrams} = \lim_ { \hbar \to 0 } \int \mathcal{D}\phi_ {c} \, \exp \left\lbrace \frac{i}{\hbar}\Gamma[\phi_ {c}] + \int J\phi_ {c} \right\rbrace\] <p>And this turns out to be correct. I used to think of $\phi_ {c}$ as some pre-determined classical function, which has caused me a lot of confusion. From now on let’s get rid of the subscript $c$ in $\phi_ {c}$, since fields appear under the path integral are always classical field. We will put the subscript back when possible confusion could rise.</p> <p>Thanks to the $\hbar\to 0$ limit, the path-integral can be worked out using the method of stationary phase, up to some normalization constant we have</p> \[\lim_ { \hbar \to 0 } \int \mathcal{D}\phi \, \exp \left\lbrace \frac{i}{\hbar}\Gamma[\phi] +\frac{i}{\hbar} \int J\phi \right\rbrace = \exp \left\lbrace \frac{i}{\hbar}\Gamma[\overline{\phi}]+ \frac{i}{\hbar} \int \, J\overline{\phi} \right\rbrace ,\] <p>where $\overline{\phi}$ is the solution that <strong>extremizes</strong> the exponent on the LHS,</p> \[\frac{\delta \Gamma[\phi]}{\delta \phi}{\Large\mid}_ {\phi=\overline{\phi}} \equiv\frac{\delta \Gamma[\overline{\phi}]}{\delta \overline{\phi}} = -J(x).\] <p>This is exactly the same functional equation we got for $\varphi_ {J}$ before! They might differ by a constant, but that can be absorbed into the normalization factors and cancels out eventually. Now we can comfortably write $\overline{\phi} =\phi_ {J}$ in the quantum action. This equation connects the quantum action we obtained before via a Legendre transform from $iW[J]$ with the generating functional for 1PI diagrams, identifying these two seemingly different quantities. To show the connection ever more clearly, recall that the partition function in terms of $\Gamma$ is</p> \[Z[J] = \exp \left\lbrace \frac{i}{\hbar} \left( \Gamma [\varphi_ {J}]+\int \, J\varphi_ {J} \right) \right\rbrace =\exp \left\lbrace \frac{i}{\hbar}W[J] \right\rbrace\] <p>we have</p> \[W[J] = \Gamma[\phi_ {J}] + \int \, J\phi_ {J}, \quad J \text{ given a priori.}\] <p>Note that we could equally write $W$ as $W+2\pi \mathbb{N}$ but the additive constant can be absorbed into the partition functions as well. This is the Legendre transform we wrote down before!</p> <p>For the sake of completeness we put the pair of Legendre transforms below,</p> \[\begin{align*} W[J] &amp;= \Gamma[\varphi_ {J}] +\int \, J\varphi_ {J} ,\quad -J = \frac{\delta \Gamma[\varphi_ {J}]}{\delta \varphi_ {J}}, \\ \Gamma[\varphi] &amp;= W[J_ {\varphi}] - \int \, J_ {\varphi} \varphi, \quad \varphi=\frac{\delta W[J]}{\delta J}, \end{align*}\] <p>where $\varphi_ {J}$ means that $\varphi$ is determined by $J$, namely $\varphi$ is a (non-local) function of $J$, while $J_ {\varphi}$ means the opposite. Also keep in mind that $\varphi_ {J}$ is the vev of quantum operator $\phi$ in the presence of $J$.</p> <p><strong>Summary.</strong></p> <ul> <li>$\Gamma[\varphi]$ generates 1PI diagrams;</li> <li>$W[J]$ generates connected diagrams;</li> <li>$Z[J]$ generates all kinds of diagrams.</li> </ul> <p><strong>Remark.</strong> The generating functionals such as $W[J]$ and $\Gamma[\varphi]$ are classical functional, dealing with c-numbered functions, no operators and commutation relations involved. In fact, the language of path integral has a close connection with classical, statistical field theory, and many concepts exists in both disciplines, for example, people dealing with statistical field theory also talk about renormalization flow (Wilsonian), and QFT-ists also talk about critical exponents. A great textbook on statistical field theory is that by <a href="https://guava.physics.ucsd.edu/~nigel/"><code class="language-plaintext highlighter-rouge">Nigel Goldenfeld</code></a>.</p> <hr/> <p>Recently I found another approach to effective action, which I will copy here. This new approach gives a different perspective to stuff we talked about before, and it made it manifest that the external legs of $\Gamma[\phi]$ should be amputated, thus I consider it worthy to write it down.</p> <p>Let $G^{(n)}(x_ {1},\cdots,x_ {n})$ be the most general kind of $n$-point function, including disconnected ones. It is generated by the partition function $Z[J]$, which can be written as</p> \[Z[J] = \sum_ {n=0}^{\infty} \frac{i^{n}}{n!} \int d^{d}x_ {1} \cdots d^{d}x_ {n} \, G^{(n)}(x_ {1},\cdots,x_ {n}) J(x_ {1})\cdots J(x_ {n}).\] <p>As you can see, acting $n$-times the functional derivative $\delta / i\delta J$ gets us the $n$-point function.</p> <p>Similarly, the generating functional of connected diagrams $iW[J]$ adopts the functional Taylor expansion</p> \[iW[J] = \sum_ {n=0}^{\infty} \frac{i^{n}}{n!} \int d^{d}x_ {1} \cdots d^{d}x_ {n} \, G_ {c}^{(n)}(x_ {1},\cdots,x_ {n}) J(x_ {1})\cdots J(x_ {n}).\] <p>where $G_ {c}^{(n)}(x_ {1},\cdots)$ is the connected n-point function. Likewise for $\Gamma$ but we have already wrote it down. In the next we will neglect $J$ in $\varphi_ {J}$, it is understood that $\varphi$ is the canonical transformed variable of $J$ and vise versa.</p> <p>Using the functional relation</p> \[\frac{\delta W}{\delta J} = \varphi\] <p>we can do something interesting with the connected 2-point function. Neglect the normalization factor for now, we have</p> \[\begin{align*} iD(x-y) &amp;= \int D\phi \, e^{ i\left( S+\int J\phi \right) } \phi(x)\phi(y)\\ &amp;= \frac{\delta^{2}W}{\delta J(x)\delta J(y)} = \frac{\delta}{\delta J(y)} \frac{\delta W}{\delta J(x)}\\ &amp;= \frac{\delta \varphi(x)}{\delta J(y)} , \end{align*}\] <p>amazingly the functional derivative between $\varphi$ and $J$ is nothing but the quantum, full propagator! On the other hand,</p> \[\frac{\delta J(x)}{\delta J(y)} = \delta^{d}(x-y) = - \frac{\delta}{\delta J(y)} \frac{\delta \Gamma[\varphi]}{\delta \varphi(x)},\] <p>write</p> \[\boxed{ \frac{\delta}{\delta J(y)} = \int d^{d}z \, \frac{\delta \varphi(z)}{\delta J(y)} \frac{\delta}{\delta\varphi(z)} = \int d^{d}z \, iD(z-y) \frac{\delta}{\delta \varphi(z)} }\] <p>where the first equal sign is nothing but the chain rule of functionals, we have</p> \[\begin{align*} \delta^{d}(x-y) &amp;= - \int d^{d}z \, \frac{\delta \varphi(z)}{\delta J(y)} \frac{\delta^{2} \Gamma[\varphi]}{\delta\varphi(z)\delta\varphi(x)} \\ &amp;=-i \int d^{d}z \, D(y-z) \frac{\delta^{2} \Gamma[\varphi]}{\delta\varphi(z)\delta\varphi(x)} . \end{align*}\] <p>Recall that $\delta$-function is the equivalence of identity with functionals, we see that $\delta^{2} \Gamma[\varphi] / \delta\varphi(z)\delta\varphi(x)$ is the inverse of 2-point functions! To be specific</p> \[\boxed{ \left( \frac{\delta^{2}\Gamma[\varphi]}{\delta \varphi(x)\delta \varphi(y)} \right)^{-1} = -i D (y-z)= -\frac{\delta^{2} W[J]}{\delta J(y)\delta J(z)}. }\] <p>Is helps to think of $\delta^{2} / \delta_ {x} \delta_ {y}$ as a matrix $M_ {xy}$, then this inverse relation is for matrices. This is both intuitive and not… intuitive because, recall that with regular Lagrangian $\mathcal{L}$, $\partial^{2} \mathcal{L} / (\partial \phi)^{2}$ is roughly speaking the inverse of the propagator, here the effective action kind of takes the position of $\mathcal{L}$; Counter intuitive since, well, it took me a lot effort to find it intuitive.</p> <p>Now let’s carry on with other n-point functions where $n&gt;2$. But before that we need to solve a math problem: how to take the functional derivative of an inverse matrix.</p> <p>Let $M$ be a matrix function and $M^{-1}$ its inverse. Start with the identity, $I = MM^{-1}$, differentiating this identity yields $0 = dM^{-1}M + M^{-1}dM$, leading to the expression for the differential of the inverse $dM^{-1} = -M^{-1}(dM)M^{-1}$. From this, it follows that the derivative of $M^{-1}$ with respect to some variable $a$ is</p> \[\frac{\partial M^{-1}}{\partial a} = -M^{-1} \frac{\partial M}{\partial a} M^{-1}.\] <p>Now lets consider the 3-point function</p> \[\begin{align*} G_ {c}^{(3)}(x,y,z) &amp;= \frac{\delta^{3}W}{\delta J(x)\delta J(y)\delta J(z)} \\ &amp;= i \int d^{d}w \, D(z,w) \frac{\delta}{\delta\varphi(w)} \frac{\delta^{2}W[J]}{\delta J(y) \delta J(x)} \\ &amp;= - i \int d^{d}w \, D(z,w) \frac{\delta}{\delta\varphi(w)}\left( \frac{\delta^{2}\Gamma[\varphi]}{\delta \varphi(x)\delta \varphi(y)} \right)^{-1}\\ &amp;= -i \int d^{d}w d^{d}w' d^{d}w'' \, D(z-w) \left( \frac{\delta^{2}\Gamma}{\delta\varphi(x)\delta\varphi(w')} \right)^{-1} \\ &amp;\;\;\;\;\;\times \frac{\delta^{3}\Gamma}{\delta\varphi(w)\delta\varphi(w')\delta\varphi(w'')} \left( \frac{\delta \Gamma}{\delta\varphi(w'')\delta\varphi(y)} \right)^{-1} \\ &amp;= - i \int d^{d}w d^{d}w' d^{d}w'' \, D(z-w) D(x-w') D(y-w'')\\ &amp;\;\;\;\;\;\times \frac{\delta^{3}\Gamma[\varphi]}{\delta\varphi(w)\delta\varphi(w')\delta\varphi(w'')} \\ &amp;= -i \int d^{d}w d^{d}w' d^{d}w'' \, D(z-w) D(x-w') D(y-w'') \Gamma^{(3)}(w,w',w''). \end{align*}\] <p>Now, $G_ {c}^{3}(x,y,z)$ is the connected 3-point function defined at $x,y$ and $z$, with its external legs <strong>not amputated</strong>! On the LHS, since all the three external legs are accounted for by the three propagators $D(z-2)$ etc., $\Gamma^{(3)}(w,w’,w’’)$ has its external legs <strong>amputated</strong>! As we dig deeper, you’ll find that this is a general conclusion: the external legs of $\Gamma[\varphi]$ are amputated.</p> <p>We have been sloppy with factor of $i$’s. Taking care of it, the n-point correlation function reads</p> \[\left( i\frac{\delta}{\delta J} \right)^{n} (iW[J]) = \left\langle T \,\phi_ {1}\cdots \phi _ {n} \right\rangle _ {J} =: G^{(n)}_ {\text{c}}(x_ {1},\cdots ,x_ {n}),\] <p>where $c$ is for connected. Since $\Gamma[\varphi]$ generates 1PI diagrams,</p> \[G^{(3)}_ {\text{1PI,am}}(x,y,z) = \frac{\delta^{3} i\Gamma[\varphi]}{\delta\varphi(x) \delta\varphi(y) \delta\varphi(z)}\] <p>where $\text{am}$ for amputated. We have found the relation between connected, not-amputated 3-point functions between 3-point 1PI connections:</p> \[G^{(n)}_ {c}(x,y,z) = \int d^{d}w \, d^{d}w' \, d^{d}w'' \, D(x-w)D(y-w')D(z-w'') G^{(3)}_ {\text{1PI,am}}(w,w',w'').\] <p>Note that $G^{(n)}_ {\text{1PI,am}}$ is nothing but the same $\Gamma^{(n)}$ in the functional Taylor expansion of $\Gamma$ (by construction). The generalization to $n&gt;3$ is straightforward.</p> <p><strong>A Tree-level Example</strong></p> <p>In the classical limit, that is in the limit $\hbar \to 0$, the partition function</p> \[Z = \int \mathcal{D}\phi e^{ \frac{i}{\hbar} \left( S + \int \phi J \right)}\] <p>receives dominant contribution from the stationary configuration, given by</p> \[\frac{\delta}{\delta\phi}\left( S + \int \phi J \right) = 0 \implies \frac{\delta S}{\delta\phi} = - J\] <p>which has a solution $\varphi_ {J}$. This is exactly the euqation satisfied by the quantum action $\Gamma[\phi]$! Anyway, we can carry on to talk about the partition function which is now</p> \[Z = e^{iS[\varphi_ {J}]+i\int \varphi_ {J} J}\] <p>up to a normalization factor. We have</p> \[W[J] = -i \ln Z = S[\varphi_ {J}] + \int \varphi_ {J} J,\] <p>thus</p> \[\Gamma[\varphi] = W - \int \varphi_ {J} J = S[\varphi].\] <p>As expected, at the tree-level, the quantum effective action and the original action are the same.</p> <h2 id="12-effective-potential">1.2. Effective Potential</h2> <p>In the classical dynamics, the vacuum (lowest energy state) configuration of the system is given by the minimum of the potential, which fixes the value of the field in spacetime (usually a constant in spacetime). In the quantum theory, everything receives quantum correction, including the vacuum expectation value (VEV) $\left\langle \phi \right\rangle$ of the field operator $\phi$. In a QFT, the potential term in the Lagrangian or Hamiltonian has minima given by the classical vacuum field configuration, however that’s not the full story, since the field always fluctuates around the vacuum, giving rise to a correction to $\left\langle \phi \right\rangle$. That’s when the effective potential comes to rescue.</p> <p>The effective potential in QFT is a crucial concept, especially when studying systems with spontaneous symmetry breaking, phase transitions, and nonperturbative dynamics. It represents a modification of the classical potential to include quantum corrections, providing a more accurate description of the dynamics of quantum fields. Its application include:</p> <ol> <li> <p><strong>Spontaneous Symmetry Breaking</strong>: The effective potential is instrumental in understanding spontaneous symmetry breaking, a phenomenon where the ground state (vacuum) of a system does not inherit the symmetry of the action. In the context of the Higgs mechanism in the Standard Model of particle physics, the effective potential reveals how the Higgs field acquires a nonzero vacuum expectation value, leading to the generation of masses for the $W$ and $Z$ bosons.</p> </li> <li> <p><strong>Phase Transitions</strong>: In the study of early universe cosmology or condensed matter physics, the effective potential reveals how a system transitions between different phases. For example, it can describe the transition from a symmetric phase to a broken-symmetry phase as the universe cools. The shape of the effective potential changes with temperature, and these changes can indicate phase transitions, such as from a high-temperature symmetric phase to a low-temperature phase where symmetry is broken.</p> </li> <li> <p><strong>Quantum Corrections and Renormalization</strong>: The effective potential incorporates quantum corrections to the classical potential, which are crucial for making precise predictions in QFT. These corrections can significantly alter the behavior of the system, especially at high energies or short distances. The process of renormalization is deeply connected to the effective potential, ensuring that physical quantities remain finite and meaningful.</p> </li> <li> <p><strong>Nonperturbative Effects</strong>: The effective potential can capture nonperturbative effects, which are not accessible through standard perturbative techniques. For instance, in theories with strong coupling or in situations where the perturbative series does not converge (actually it doesn’t converge in any cases), the effective potential can provide insights into the structure and dynamics of the vacuum, solitonic solutions, and other nonperturbative phenomena like instantons and tunneling effects.</p> </li> <li> <p><strong>Dynamical Mass Generation</strong>: In theories where particles are massless at the classical level, the effective potential can show how interactions lead to dynamical mass generation. This is particularly significant in quantum chromodynamics (QCD) and models of dynamical symmetry breaking, where the vacuum structure induced by strong interactions gives rise to constituent masses for particles.</p> </li> <li> <p><strong>Vacuum Stability and Tunneling</strong>: The effective potential allows for the analysis of vacuum stability in various field theories. It can be used to study the probability of tunneling between different vacua, which has implications for the stability of our universe and the decay of false vacuum states.</p> </li> </ol> <p>Overall, the effective potential is a powerful tool in quantum field theory, providing deep insights into the quantum dynamics of fields, the structure of the vacuum, and the various nonperturbative phenomena that arise in complex quantum systems. Next let’s dig into it.</p> <hr/> <p>Recall that the quantum effective action</p> <ul> <li>is a functional of $\varphi_ {J}$ where $\varphi_ {J} = \left\langle {\phi} \right\rangle_ J$, namely $\varphi$ is the expectation value of $\phi$ in the presence of a source term $J$, and</li> <li>satisfies ${\delta \Gamma}/{\delta \varphi} = J$.</li> </ul> <p>Thus <strong>when $J=0$, the solution to ${\delta \Gamma}/{\delta \varphi} = J$ is the vev of $\phi$.</strong></p> <p>Additionally, let’s assume the vacuum exhibits translational symmetry, meaning that the vacuum expectation value (vev) of $\phi$ remains constant across space and time. Under this condition, we only require a single number to describe the field configuration. Consequently, we can introduce a function, $\mathcal{V}_{\text{eff}}$, the minimum of which represents the vev of $\phi$,</p> \[\Gamma[\varphi]|_ {\varphi = \text{const}} = -VT \mathcal{V}_ {\text{eff}}(\varphi)\] <p>where $V$ is the volume of the space and $T$ the extension of time, $\mathcal{V} _ {\text{eff}}(\varphi)$ is the quantum effective potential. Apparently $\mathcal{V} _ {\text{eff}}$ is an intensive, and ${\partial\mathcal{V}}/{\partial\phi} = 0$ reproduces ${\delta \Gamma}/{\delta \varphi} = 0$.</p> <h1 id="2-coleman-weinberg-potential">2. Coleman-Weinberg Potential</h1> <h2 id="21-background-fields">2.1. Background Fields</h2> <p>The method of background field is very useful for calculating beta functions and effective action. For a real scalar field $\phi$, the general idea is as following</p> <ul> <li>separate the field into the static background field $\phi_ b$ and dynamic field $\phi$, $\phi \to \phi_ b + \phi$. By static we mean it is not path-integrated, thus don’t participate in quantum or static activities, such as propagate or fluctuate. This will simplify the calculation a lot (not supposed to be obvious).</li> <li>Define the corresponding action with background field $S_ b[\phi_ b;\phi]$, the resulting generating functional $W_ b[\phi_ b;\phi]$, and the 1PI effective action $\Gamma_ b[\phi_ b;\varphi]$, where $\varphi$ is defined to be the vev of $\phi$ in the presence of the background field $\phi_ b$.</li> <li>$\Gamma_ b[\phi_ b;\varphi]$ has a useful property:</li> </ul> \[\Gamma_ b[\phi_ b=\phi;0] = \Gamma_ b[\phi_ b=0;\varphi] = \Gamma[\varphi],\] <p>due to the fact that $\Gamma[\phi_ {b}+\varphi]$ is a functional of $\phi_ {b}+\varphi$ as a whole, it shows that how to choose the background field is kind of arbitrary, we should choose whatever makes our calculation the simplest. The first term means that the effective action with background field $\phi_ b = \phi$ and zero dynamic field $\varphi = 0$. Since the dynamic field will be set to zero at last, if a diagram has dynamic field external legs, it is also zero. It is similar to the Feynman diagrams with sources, when we set $J=0$ in the end then all the diagrams where all the source bulbs vanish (people who have read Srednicki will know what I am talking about).</p> <p>p.s. In Sidney Coleman’s lectures on QFT, in Eq. (44.31), his $\left\langle \phi \right\rangle$ is our $\phi_ {b}$ and his $\overline{\phi}’$ is our $\varphi$. In our notation, Coleman chose $\phi_ {b}$ to be the vev of $\phi$ with $J=0$ (since he is interested in studying spontaneous symmetry breaking), then he goes on and expand $\varphi$ about $\varphi=0$.</p> <p>The action with a background field is defined as</p> \[S_ b[\phi_ b;\phi] = \int d^4x\mathcal{L}(\phi_ b+\phi),\] <p>the partition function with source is</p> \[\mathcal{Z}[\phi_ b;J] = \int \mathcal{D}\phi \exp\{ i S_ b[\phi_ b;\phi]+i\int J\phi \} = \mathcal{Z}[J] e^{-i\int J\phi_ b},\] <p>note that <strong>the source only couples to the dynamic field</strong>. Since $\phi$ is integrated over, $\mathcal{Z}$ can only be a functional of $\phi_ b$ and $J$. Define the generating functional as</p> \[W_ b[\phi_ b;J] \equiv -i \ln \mathcal{Z}_ b[\phi_ b;J] \implies W_b[\phi_ b;J] = W[J] - \int J\phi.\] <p>Define</p> \[\varphi_ b(x) = \frac{\delta W_ b[J]}{\delta J(x)}\] <p>we have</p> \[\varphi_ b = \varphi - \phi_ b\] <p>which means that in the presence of a background field, $\left\langle \phi \right\rangle$ will be shifted by $\phi_ b$, as expected.</p> <p>Now we need to define the effective action in the presence of a background field, by the means of Legendre transformation again.</p> \[\Gamma_ b [\phi_ b;\varphi_ b] = W_ b - \int \frac{\delta W_ b[J]}{\delta J}J = W_ b - \int \varphi_ b J\] <p>you can check that</p> \[\frac{\delta \Gamma_ b[\phi_ b;\varphi_ b]}{\delta\varphi_ b}= J .\] <p>Replace $W_ b[J]$ with its expression in terms of $W[J]$, we can check that</p> \[\Gamma_ b[\phi_ b;\varphi_ b] = W[J] - \int J (\varphi_ b+\phi_ b) = \Gamma[\phi_ b + \varphi_ b]\] <p>so for example if we want to calculate $\Gamma[\eta(x)]$, we can set $\phi_ b = \eta,\,\varphi = 0$ and use that to simplify calculations.</p> <h2 id="22-coleman-weinberg-potential">2.2. Coleman-Weinberg Potential</h2> <p>Consider the real scalar Lagrangian</p> \[\mathcal{L} = -\frac{1}{2} \phi \partial^2\phi - \frac{1}{2} m^2\phi^2-\frac{1}{4!}\phi^4\] <p>the question is, when $m = 0$, will the quantum effects modify the shape of the potential?</p> <p>Introduce a background field $\phi \to \phi_ b +\phi$, take it into the action and expand, keep in mind that the path integral is over field $\phi$ only, we have</p> \[\begin{align*} e^{i\Gamma[\phi_ b]} &amp;= e^{i\int d^4 x (-\frac{1}{2} \phi_ b \partial^2 \phi_ b-V(\phi_ b))} \\ \\ &amp;\times\int \mathcal{D}\phi e^{i \int d^4 x (-\frac{1}{2} \phi \partial^2 \phi -V(\phi_ b) - \phi V'(\phi_ b)-\frac{1}{2}V''[\phi_ b]\phi^2-\frac{1}{3!}V'''[\phi_ b]\phi^3-\cdots)}. \end{align*}\] <p>In the path integral, one of the terms in the Lagrangian, i.e. $\int \mathcal{D} \phi e^{i\phi V’}$ can be thrown away because we only need to consider 1PI diagrams in the calculation of $\Gamma$, while the Feynman diagram given by $\phi V’(\phi_ b)$ will never contribute to the 1PI diagrams. For the same reason we can also discard $\phi^3$ term in the Lagrangian. At one loop, there will be no contributions from $\phi^4$ and higher terms either. Hence we are left with</p> \[e^{i\Gamma[\phi_ b]} = e^{i\int d^4 x (-\frac{1}{2} \phi_ b \partial^2 \phi_ b-V(\phi_ b))} \int \mathcal{D} e^{i \int d^4 x (-\frac{1}{2} \phi \partial^2 \phi -\frac{1}{2}V''[\phi_ b]\phi^2)}.\] <p>use the master formulae in QFT</p> \[\int\mathcal{D}\phi\exp\left\{ i \int d^4x (\phi M \phi) \right\} = \mathcal{N}\frac{1}{\text{det}^{ {1/2} }{M}}\] <p>we have</p> \[e^{i\Gamma[\phi_ b]} = \mathcal{N}e^{i\int d^4 x (-\frac{1}{2} \phi_ b \partial^2 \phi_ b-V(\phi_ b))} \text{det}^{-1/2}(\partial^2 + V''(\phi_ b))\] <p>In order to calculate the functional determinant, we need to put it in a specific representation, such as the position representation or the momentum representation, to turn $\partial^2 + V’’(\phi_ b)$ into a matrix, then calculate the determinant of that infinite dimensional matrix. Writing</p> \[\Gamma[\phi_ b] = \int d^4 x (-\frac{1}{2} \phi_ b \partial^2 \phi_ b-V(\phi_ b)) + \Delta \Gamma[\phi_ b],\] <p>where</p> \[i\Delta \Gamma[\phi_ b] = -\frac{1}{2}\ln \det (\partial^2 +V''(\phi_ b)) + \text{const}.\] <p>With the help of identity</p> \[\ln \det M = \ln \prod_ i \lambda_ i = \sum_ i \ln \lambda_ i = \text{tr } {\ln M}\] <p>where $\lambda_ i$ are the eigenvalues of $M$, we have</p> \[i\Delta \Gamma[\phi_ b] = -\frac{1}{2} \text{tr } {\ln (\partial^2 +V''(\phi_ b))} + \text{const}.\] <p>Next we assume that $\phi_ b$ is a const in space-time, and define</p> \[m_ {\text{eff}}^2(\phi_ b) \equiv V''(\phi_ b),\] <p>calculate the functional determinant in the representation of $x$, with the help of</p> \[\mathbb{1}= \int \frac{dp^4}{(2\pi)^4} \left\lvert k \right\rangle \left\langle{k}\right\rvert\] <p>we have</p> \[\begin{align*} i\Delta \Gamma[\phi_ b] &amp;= -\frac{1}{2}\int d^4 x \left\langle{x}\right\rvert \ln\left( 1+\frac{V''}{\partial^2} \right) \left\lvert{x}\right\rangle +\text{const}\\ &amp;= -\frac{1}{2}\int d^4 x \int\frac{d^4 k}{(2\pi)^4}\ln\left( 1-\frac{m_ {\text{eff}}^2}{k^2} \right)+\text{const} \end{align*}\] <p>where $\int d^4 x = VT$ is the space-time volume of the system.</p> <p>The integral over momentum is divergent, we will render it finite by a hard cut-off, that is to Wick rotate the system into Euclidean space and insert the momentum cut-off $\Lambda$, yielding</p> \[\begin{align*} \Delta\Gamma[\phi_ b] &amp;= -VT \frac{2\pi^2}{2(2\pi)^4} \int_ 0^\Lambda dk_ E k_ E^3 \ln(1+\frac{m_ {\text{eff}}^2}{k_ E3^2}) +\text{const}\\ &amp;= - \frac{VT}{128\pi^2} \left( 2 m_ {\text{eff}}^2 \Lambda^2 + 2m_ {\text{eff}}^4 \ln\frac{m_ {\text{eff}}^2}{\Lambda^2}+\text{const}\right) \end{align*}\] <p>where $k_ E$ is the Wick-rotated momentum and we have used the relation $\Lambda \gg m_ {\text{eff}}$. The effective potential accordingly is</p> \[V_ {\text{eff}} = V(\phi_ b) + c_ 1 + c_ 2 m_ {\text{eff}}^2 + \frac{1}{64\pi^2} m_ {\text{eff}}^4 \ln\frac{m_ {\text{eff}}^2}{c_ 3}\] <p>where $c_ 1,c_ 2,c_ 3$ are some $\Lambda$-dependent constants. They are independent of $\phi_ b$ thus in general are not of interests to us. For example, $c_ 2 = \Lambda^2 / 64\pi^2$.</p> <p>Next we need to add the counter terms to the potential</p> \[V(\phi) = \frac{1}{2}m_ R^2(1+\delta_ m)\phi^2 +\frac{\lambda_ R}{4!}(1+\delta_ \lambda)\phi^4 +\Lambda_ R (1+\delta_ \Lambda),\] <p>with all the counter terms starting at 1-loop level, that is of $\mathcal{O}(\lambda_ R)$. $m_ {\text{eff}}^2$ is still defined as $V’’$.</p> <p>What about the renormalization conditions?</p> <ul> <li>The question we want to ask is how the quantum corrections change the shape of the potential, when the mass term is zero, thus we require $\lambda_ R^2 = 0$</li> <li>$V(0) = 0 \implies \Lambda_ R = 0$</li> <li>$\lambda_ R = V’’’’(\phi_ R)$ for some arbitrary fixed scale $\phi_ R$</li> </ul> <p>They will fix the counter terms, plugging them in gives</p> \[\boxed{ V_ {\text{eff}}(\phi) = \frac{1}{4!}\phi^4\left\{ \lambda_ R + \frac{3 \lambda_ R^2}{32\pi^2}\left[ \ln \left( \frac{\phi^2}{\phi_ R^2}-\frac{25}{6} \right) \right] \right\} }\] <p>which is known as Coleman-Weinberg potential.</p> <p>Now having the effective potential at hand, we can answer the question: does the quantum correction change the vacuum expectation value of $\phi$? Originally, without the quantum correction, the minimum of the potential is at $\phi = 0$ since there is no quadratic term and only a quartic term. With quantum correction, the vev of $\phi$ is given by the minimum of the effective potential, which is the Coleman-Weinberg potential, which is minimized when</p> \[\lambda_ R \ln \frac{\left\langle \phi^{2} \right\rangle}{\phi_ R^2} = \frac{11}{3} \lambda_ R - \frac{32}{3}\pi^2.\] <p>which gives a nonzero $\left\langle \phi \right\rangle$. It means now we have a double-well potential, instead of the original single-well potential, due to the quantum correction.</p>]]></content><author><name>Baiyang Zhang</name></author><category term="colemanWeinbergPotential"/><category term="effectiveTheory"/><category term="coleman"/><category term="effectivePotential"/><summary type="html"><![CDATA[1. Quantum Effective Action 1.1. Quantum Action 1.2. Effective Potential 2. Coleman-Weinberg Potential 2.1. Background Fields 2.2. Coleman-Weinberg Potential]]></summary></entry></feed>