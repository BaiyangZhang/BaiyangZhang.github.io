<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="talk-1">Talk 1</h1> <p>Say, we want to solve the following equation perturbatively:</p> \[x^{5}+x=1.\] <p>Perturbation theory is usually done in three steps:</p> <ol> <li>Introduce a small variable $\epsilon$, it could be taht when $\epsilon=1$ we recover the original problem;</li> <li>At small $\epsilon$ the original problem is simplified enough to be solvable, the answer is a power expansion of $\epsilon$;</li> <li>Setting $\epsilon=1$ and recover the approximate answer to the original problem.</li> </ol> <p>In our example, we can make $x$ to be the perturbation, introducing an $\epsilon$ to make the equation look like</p> \[x^{5} +\epsilon x=1\] <p>so that when $\epsilon=1$ we rocover the original problem. The solution at $\epsilon=0$ is $x=1$, well at least one of the solution. We next expand $x$ in a powe series of $\epsilon$, writine $x=1+\sum_ {i\geq1}c_ {i}\epsilon_ {i}$, substitute it into the equation and work out the coefficients. This works pretty neatly.</p> <p>However there is another way to introduce the $\epsilon$-parameter, that is to put it in the $x^{5}$ term:</p> \[\epsilon x^{5} + x=1,\] <p>at $\epsilon=0$ the solution is also $x=1$, but if you next expand $x$ in a power series of $\epsilon$, you’ll find that this perturbaton theory does not work at all at $\epsilon=1$. Why?</p> <p>It is because if you put $\epsilon$ with the $x$, the original problem and $\epsilon=0$ problem has similar behavior: they both have five roots! In a sense the perturbation theory is “continuous” at the origin, $\epsilon=0$. But if you put $\epsilon$ with $x^{5}$, then when $\epsilon=0$ there are only one root, but for nonzero $\epsilon$ there will be five, there is a disrupt jump in the number of solutions, this hints the poor behavior of the perturbation theory.</p> <hr> <p>Carl Bender said “nothing is ever asymptotic to zero”, but isn’t a lot of things asymptotic to zero, like $e^{ -1/x }$?</p> <p>$x\ll y$ in our context means $x$ is negligible compare to $y$.</p> <p>As $x\to x_ {0}$, if $\frac{f(x)}{g(x)}$ goes to $1$, we say $f(x)$ is asymptotic to $g(x)$ at $x_ {0}$; if $\frac{f(x)}{g(x)}$ goes to zero, we say $f(x)$ is negligible compare to $g(x)$.</p> <hr> <p><strong>Method of dominance balance:</strong></p> <p>The method of dominance balance is a useful technique in the analysis of asymptotic series, particularly when dealing with differential equations or perturbation problems. This method helps identify the dominant balance between different terms in an equation, which leads to understanding the behavior of the solution in certain limits (e.g., as a parameter tends to zero or infinity).</p> <p>The method of dominance balance involves the following steps:</p> <ol> <li> <p>Identify Relevant Terms: Start by identifying the leading terms in the equation, which usually involve different powers of $\epsilon$.</p> </li> <li> <p>Balance Dominant Terms: Assume that a subset of terms will dominate in a certain limit of $\epsilon$ (e.g., $\epsilon \rightarrow 0$). Equate these terms to determine how the variables scale with $\epsilon$.</p> </li> <li> <p>Check Consistency: Substitute the scaling obtained from the dominant balance back into the original equation to verify that the terms you’ve neglected are indeed smaller compared to the dominant ones.</p> </li> <li> <p>Determine the Solution: Use the balance to simplify the equation, making it easier to solve, and obtain an asymptotic expression for the solution.</p> </li> </ol> <p>Consider a simple differential equation that arises in boundary layer theory:</p> <p>\(\epsilon y'' + y' - y = 0,\) where $y = y(x)$ and $\epsilon$ is a small positive parameter.</p> <p>The equation has three terms: $\epsilon y’’$, $y’$, and $-y$. For small $\epsilon$, let’s assume $y(x)$ behaves as $y \sim e^{\lambda x}$. Substituting this into the equation:</p> \[\epsilon \lambda^2 e^{\lambda x} + \lambda e^{\lambda x} - e^{\lambda x} = 0.\] <p>Canceling out the exponential term (since it’s non-zero), we get:</p> \[\epsilon \lambda^2 + \lambda - 1 = 0.\] <p>Now, let’s balance the terms based on the assumption that one term might dominate. There are two possible balances:</p> <ul> <li> <p>Case 1: Balance between $\epsilon \lambda^2$ and $\lambda$.</p> <p>Assume $\epsilon \lambda^2 \sim \lambda$, giving $\lambda \sim \frac{1}{\epsilon}$. Substituting back, we find that the neglected $-1$ term is not consistent with this balance as $\lambda$ becomes very large when $\epsilon$ is small.</p> </li> <li> <p>Case 2: Balance between $\lambda$ and $-1$.</p> <p>Assume $\lambda \sim 1$. Then the equation becomes $\lambda - 1 = 0$, so $\lambda = 1$, which implies that $\epsilon \lambda^2 \sim \epsilon$ is small compared to the dominant balance.</p> </li> </ul> <p>The second balance ($\lambda = 1$) is consistent since the neglected term $\epsilon \lambda^2$ is small when $\epsilon$ is small.</p> <p>Thus, the leading order solution is $y(x) \sim e^x$. To refine this, we could look at higher-order corrections by considering the small terms we neglected.</p> <p>As another example, consider the integral:</p> \[I(\epsilon) = \int_ 0^\infty e^{-\frac{x^2}{\epsilon}} \, dx.\] <p>For small $\epsilon$, we expect the main contribution to the integral to come from the region near $x = 0$.</p> <p>The exponential term $-\frac{x^2}{\epsilon}$ suggests that for large $x$, the integrand decays rapidly. We balance the term inside the exponential by scaling $x$ as $x = \sqrt{\epsilon} u$, which leads to:</p> \[I(\epsilon) = \sqrt{\epsilon} \int_ 0^\infty e^{-u^2} \, du.\] <p>This change of variables is consistent as it simplifies the integral to a Gaussian integral that does not depend on $\epsilon$ (aside from the overall factor).</p> <p>The integral $\int_ 0^\infty e^{-u^2} \, du$ is known and equals $\frac{\sqrt{\pi}}{2}$. Thus,</p> \[I(\epsilon) \sim \sqrt{\epsilon} \cdot \frac{\sqrt{\pi}}{2}.\] <p>This gives us the asymptotic expansion $I(\epsilon) \sim \frac{\sqrt{\pi \epsilon}}{2}$ as $\epsilon \rightarrow 0$.</p> <h1 id="talk-2">Talk 2</h1> <p>The below ODE</p> <p>\(y'' + a(x)y' + b(x) y =0\) is a linear equation, but it is fantastically difficult to solve. It can always be reduced to</p> \[\xi''+d\xi=0.\] <p>This facilitates the use of method of power series.</p> <p>Actually there is anothe way to work it out. Let $D$ be the differential operator, the original ODE can be written as</p> \[(D^{2}+aD+b)y(x)=0.\] <p>A quadratic form can be factorized, so we in principal can write</p> \[(D^{2}+aD+b)y(x) = (D+\alpha)(D+\beta)y,\] <p>where $\alpha,\beta$ are functions of $x$. Expand it we get some equations, one of which is the so-called Riccati equation, and there already exists a procedure of how to solve it.</p> <p>The problem is that this method works only for finite $x$, not the entire plane.</p> <p>Perturbation method fails when you try to use it to solve the Schrodinger equation for anharmonic osscilator. The reason is that, in perturbation theory we put $\epsilon$ to $\phi^{4}$, the potential reads</p> \[V(x)=\frac{m\omega^{2}}{2}x^{2}+\epsilon x^{4},\] <p>but something abrupt happens at $\epsilon$, when $\epsilon&gt;0$ the potential is bounded from below, but when $\epsilon\leq$ it is not.</p> <blockquote> <p>Quantum mechanics is not just about a bunch of isolated eigenvalues, there is really just one energy eigenvalue, with different sheets of Riemann surface!</p> </blockquote> <h1 id="talk-3">Talk 3</h1> <p>The different eigenvalues of a Hamiltonian can be seen as coming from different sheets of the same function! Take</p> \[H=\begin{pmatrix} E &amp; \epsilon \\ \epsilon &amp; E \end{pmatrix}, \quad \epsilon \in \mathbb{C}\] <p>for example. When we smoothly move $\epsilon$ in the complex plane, we can move from one eigenvalue to another!</p> <p>The <code class="language-plaintext highlighter-rouge">Shanks transform</code> is a sequence transformation technique used to accelerate the convergence of a series or to sum divergent series. It’s particularly useful when dealing with slowly converging sequences.</p> <p>Consider a sequence $S_ n$ that converges to a limit $S$. If the convergence is slow, applying the Shanks transform can often yield a new sequence that converges faster to $S$. The transform is defined as:</p> \[S'_ n = \frac{S_ {n+1} S_ {n-1} - S_ n^2}{S_ {n+1} + S_ {n-1} - 2S_ n}\] <p>Where $S’_ n$ is the transformed sequence.</p> <p>The series you mentioned is the alternating harmonic series:</p> \[S = 1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \frac{1}{5} - \frac{1}{6} + \cdots\] <p>This series converges to $\ln(2)$. We can apply the Shanks transform to accelerate its convergence.</p> <p>First, we calculate the partial sums of the series:</p> <ul> <li>$S_ 1 = 1$</li> <li>$S_ 2 = 1 - \frac{1}{2} = \frac{1}{2}$</li> <li>$S_ 3 = 1 - \frac{1}{2} + \frac{1}{3} = \frac{5}{6}$</li> <li>$S_ 4 = 1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} = \frac{7}{12}$</li> <li>$S_ 5 = 1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \frac{1}{5} = \frac{47}{60}$</li> </ul> <p>Use the Shanks formula, let’s apply it to our problem and we get:</p> \[S'_ 3 = \frac{-29}{72} \times \frac{12}{-7} = \frac{348}{504} = \frac{29}{42}\] <p>Finally,</p> \[S'_ 3 = \frac{29}{42} \approx 0.69048\] <p>The actual value of $\ln(2) \approx 0.69315$.</p> <p>After applying the Shanks transform, the new sequence $S’_ 3 \approx 0.69048$ is closer to the actual value $\ln(2)$ compared to the original partial sum $S_ 3 = 0.83333$. The Shanks transform effectively accelerates the convergence of the alternating harmonic series to its limit.</p> <h1 id="talk-4">Talk 4</h1> <p>First Bender introduced the Richardson extrapolation to accelarate the speed of convergence when summing a convergent series.</p> <p>Richardson extrapolation can be combined with the trapezoid rule when doing numerical integral. Maybe it can be applied to our project?</p> <p>The rules in arithmetics that we are so familiar with, the law of associativity, commutivity, are only true <strong>finitely</strong>. They are not true infinitely. For exampole, Bender explained that if they were true, then the following series:</p> \[1- \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \cdots\] <p>can add up to <strong>any</strong> numbe, such as $\pi$, $52$, etc. The law of distribution is more robust then the law of associativety and commutivity; the latter two have to do with grouping, re-arranging, while the former has to do with multiplication.</p> <h1 id="talk-5">Talk 5</h1> <p><code class="language-plaintext highlighter-rouge">Euler summation</code> is a technique used to accelerate the convergence of a series, particularly when dealing with slowly converging or divergent series. The method works by transforming the original series into a new series that converges more quickly, making it useful in both theoretical and applied contexts. It works as</p> \[\sum_ {i\in \mathbb{N}} a_ {n} \to \sum_ {i\in \mathbb{N}} a_ {n} x^{n}\to f(x)\] <p>then assigning the original summation (supposed divergent) to be $f(1)$. It can be used to sum, for example, $1-1+1-1+1\cdots$.</p> <p>Then Bender introduced Borel summation method, which we will not repeat here for it is already cover in author’s other notes.</p> <hr> <p>Given any summation machine $\mathcal{S}$, there should be two properties that all of them should satisfy:</p> <ol> <li>They should behave like a sum. For example, $\mathcal{S}(a_ {0}+a_ {1}+a_ {2}+\cdots)$ should be equal to $a_ {0}+\mathcal{S}(a_ {1}+a_ {2}+\cdots)$.</li> <li>Linearity. $\mathcal{S}\left( \sum \lambda a_ {i} + \mu b_ {i} \right)$ should be equal to $\lambda \mathcal{S}\left( \sum a_ {i} \right)+\mu \mathcal{S}\left( \sum b_ {i} \right)$.</li> </ol> <p>These two properties, combined together, prove that both Euler’s method and Borel’s method, when applied to $1-1+1-1\cdots$, give the same result.</p> <hr> <p>Why is it that when we sum infinite positive numbers, throught some summation machine it is possible to obtain a negative number? One way to think of it is to compactify the real number line into a circle using sterographic projection. When we add infinite positive numbers, the sum goes to positive infinite on the real axis, but on the compactified circle it runs to the north pole. The thing is, on a circle it is possible that as the summation continues, the sum could pass the north pole and goes through to reach the negative part of the real number circle.</p> <p>Another thing is that, the summation machine really works on a Riemann surface, with complex coordinates. Between complex numbers there is no greater or smaller than relation.</p> <p>On the complex plane, the infinity really should be regarded as a single number, unlike on the real line. one way to see that is to recall the inverse is a bijection, $z_ {1}^{-1} = z_ {2}^{-1}$ implies taht $z_ {1}=z_ {2}$, vise versa. The complex infinite is the inverse of zero, we can suggestively write</p> \[\lim_ { \epsilon \to 0 }( \epsilon e^{ i\theta })^{-1} = \infty.\] <p>For different $\theta$ we get infinity at different direction. But since different $\theta$ all goes to the same zero, and inverse map is bijective, hence infinity in different direction should all be identified as one point. So the compolex sterotype projection is not just a convenient tool but reflects the essential topology of complex plane.</p> <p><code class="language-plaintext highlighter-rouge">Continued functions</code> are like continuted fractals. Turns out, continued exponentials $\exp(x\exp(x\exp(x\cdots)))$ has very interesting convergence behaviour, with fractal structure.</p> <h1 id="talk-6">Talk 6</h1> <p>An example of the connection between continued functin (exponential functions in this case) and Taylor series:</p> \[e^{ ze^{ ze^{ \cdots } } } = \sum_ {n=0}^{\infty} \frac{(n+1)^{n-1}}{n!}z^{n}.\] <p>The infinite series has a radius of convergence $\frac{1}{e}$, while the continued exponential function has a much bigger area of convergence in the complex plane.</p> <hr> <p>divergent series can sometimes be turned into continued fractions which is convergent. Given a divergent asymptotic series which we can’t directly sum up, we can sometimes turn it into a sequence, which makes up certain continued functions, it may be continued fraction.</p> <p>An example of it is the <code class="language-plaintext highlighter-rouge">Pade sequence</code>. Pade summation seems to be quite magical. Pade doesn’t have to converge on a circle too, it is smarter than power series. Pade summation can converge on a area except for a line.</p> <h1 id="talk-9">Talk 9</h1> <p>The Airy’s equation</p> \[y'' = xy\] <p>has two solutions, the airy function $Ai(x)$ and $Bi(x)$. Using the method of dominance balance, we can get a really good asymptotic behavior of them.</p> <p>Introduce the Stokes phenomenon, the Stokes wedge.Take the example of a simple function: $\sinh z$.</p> <h1 id="talk-11">Talk 11</h1> <p>There is a class of functions called Stieltjes functions,</p> <p>\(f(x) = \int_ {0}^{\infty} dt \, \frac{\rho(t)}{1+xt}, \quad \rho(t) \geq 0.\) Furthermore, the $n$-th moment of $\rho(t)$ must exist. It has the following properties:</p> <ol> <li>$f(x)$ is analytic on the cut plane without negative real axis;</li> <li>$f(x)\to 0$ as $x\to \infty$.</li> <li>At $x\to 0$, the Stieltjes functions has asymptotic series $\sum_ {i}(-1)^{n}a_ {n}x^{n}$, called Stieltjes series,</li> <li>the negative $-f(x)$ is Herglotz, meaning that in the upper half plane the imaginary part of $f$ is positive, in the lower half plane the imaginary part of $f(x)$ is negative. The sign of the imaginary part of the function is the same as the sign of the imaginary part of the argument $x$.</li> </ol> <p>Herglotz property is a very powerful property. If a function is both entire (analytic everywhere) and Herglotz, it is trivially $c x$, where $c$ is a real constant.</p> <hr> <p>The point is, if $f(x)$ have the above four properties, then $f$ is Stieltjes.</p> </body></html>